{
  "2303.11436": {
    "id": "http://arxiv.org/abs/2303.11436v2",
    "title": "Mind meets machine: Unravelling GPT-4's cognitive psychology",
    "authors": [
      "Sifatkaur Dhingra",
      "Manmeet Singh",
      "Vaisakh SB",
      "Neetiraj Malviya",
      "Sukhpal Singh Gill"
    ],
    "abstract": "  Cognitive psychology delves on understanding perception, attention, memory,\nlanguage, problem-solving, decision-making, and reasoning. Large language\nmodels (LLMs) are emerging as potent tools increasingly capable of performing\nhuman-level tasks. The recent development in the form of GPT-4 and its\ndemonstrated success in tasks complex to humans exam and complex problems has\nled to an increased confidence in the LLMs to become perfect instruments of\nintelligence. Although GPT-4 report has shown performance on some cognitive\npsychology tasks, a comprehensive assessment of GPT-4, via the existing\nwell-established datasets is required. In this study, we focus on the\nevaluation of GPT-4's performance on a set of cognitive psychology datasets\nsuch as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how\nGPT-4 processes and integrates cognitive psychology with contextual\ninformation, providing insight into the underlying cognitive processes that\nenable its ability to generate the responses. We show that GPT-4 exhibits a\nhigh level of accuracy in cognitive psychology tasks relative to the prior\nstate-of-the-art models. Our results strengthen the already available\nassessments and confidence on GPT-4's cognitive psychology abilities. It has\nsignificant potential to revolutionize the field of AI, by enabling machines to\nbridge the gap between human and machine reasoning.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-20T20:28:26Z",
    "updated": "2023-04-12T15:46:20Z",
    "doi": null
  },
  "2310.00874": {
    "id": "http://arxiv.org/abs/2310.00874v1",
    "title": "PC-NeRF: Parent-Child Neural Radiance Fields under Partial Sensor Data\n  Loss in Autonomous Driving Environments",
    "authors": [
      "Xiuzhong Hu",
      "Guangming Xiong",
      "Zheng Zang",
      "Peng Jia",
      "Yuxuan Han",
      "Junyi Ma"
    ],
    "abstract": "  Reconstructing large-scale 3D scenes is essential for autonomous vehicles,\nespecially when partial sensor data is lost. Although the recently developed\nneural radiance fields (NeRF) have shown compelling results in implicit\nrepresentations, the large-scale 3D scene reconstruction using partially lost\nLiDAR point cloud data still needs to be explored. To bridge this gap, we\npropose a novel 3D scene reconstruction framework called parent-child neural\nradiance field (PC-NeRF). The framework comprises two modules, the parent NeRF\nand the child NeRF, to simultaneously optimize scene-level, segment-level, and\npoint-level scene representations. Sensor data can be utilized more efficiently\nby leveraging the segment-level representation capabilities of child NeRFs, and\nan approximate volumetric representation of the scene can be quickly obtained\neven with limited observations. With extensive experiments, our proposed\nPC-NeRF is proven to achieve high-precision 3D reconstruction in large-scale\nscenes. Moreover, PC-NeRF can effectively tackle situations where partial\nsensor data is lost and has high deployment efficiency with limited training\ntime. Our approach implementation and the pre-trained models will be available\nat https://github.com/biter0088/pc-nerf.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-02T03:32:35Z",
    "updated": "2023-10-02T03:32:35Z",
    "doi": null
  },
  "2308.11606": {
    "id": "http://arxiv.org/abs/2308.11606v2",
    "title": "StoryBench: A Multifaceted Benchmark for Continuous Story Visualization",
    "authors": [
      "Emanuele Bugliarello",
      "Hernan Moraldo",
      "Ruben Villegas",
      "Mohammad Babaeizadeh",
      "Mohammad Taghi Saffar",
      "Han Zhang",
      "Dumitru Erhan",
      "Vittorio Ferrari",
      "Pieter-Jan Kindermans",
      "Paul Voigtlaender"
    ],
    "abstract": "  Generating video stories from text prompts is a complex task. In addition to\nhaving high visual quality, videos need to realistically adhere to a sequence\nof text prompts whilst being consistent throughout the frames. Creating a\nbenchmark for video generation requires data annotated over time, which\ncontrasts with the single caption used often in video datasets. To fill this\ngap, we collect comprehensive human annotations on three existing datasets, and\nintroduce StoryBench: a new, challenging multi-task benchmark to reliably\nevaluate forthcoming text-to-video models. Our benchmark includes three video\ngeneration tasks of increasing difficulty: action execution, where the next\naction must be generated starting from a conditioning video; story\ncontinuation, where a sequence of actions must be executed starting from a\nconditioning video; and story generation, where a video must be generated from\nonly text prompts. We evaluate small yet strong text-to-video baselines, and\nshow the benefits of training on story-like data algorithmically generated from\nexisting video captions. Finally, we establish guidelines for human evaluation\nof video stories, and reaffirm the need of better automatic metrics for video\ngeneration. StoryBench aims at encouraging future research efforts in this\nexciting new area.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-22T17:53:55Z",
    "updated": "2023-10-12T17:50:38Z",
    "doi": null
  },
  "2304.02008": {
    "id": "http://arxiv.org/abs/2304.02008v3",
    "title": "GlueStick: Robust Image Matching by Sticking Points and Lines Together",
    "authors": [
      "R\u00e9mi Pautrat",
      "Iago Su\u00e1rez",
      "Yifan Yu",
      "Marc Pollefeys",
      "Viktor Larsson"
    ],
    "abstract": "  Line segments are powerful features complementary to points. They offer\nstructural cues, robust to drastic viewpoint and illumination changes, and can\nbe present even in texture-less areas. However, describing and matching them is\nmore challenging compared to points due to partial occlusions, lack of texture,\nor repetitiveness. This paper introduces a new matching paradigm, where points,\nlines, and their descriptors are unified into a single wireframe structure. We\npropose GlueStick, a deep matching Graph Neural Network (GNN) that takes two\nwireframes from different images and leverages the connectivity information\nbetween nodes to better glue them together. In addition to the increased\nefficiency brought by the joint matching, we also demonstrate a large boost of\nperformance when leveraging the complementary nature of these two features in a\nsingle architecture. We show that our matching strategy outperforms the\nstate-of-the-art approaches independently matching line segments and points for\na wide variety of datasets and tasks. The code is available at\nhttps://github.com/cvg/GlueStick.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-04T17:58:14Z",
    "updated": "2023-10-20T14:27:45Z",
    "doi": null
  },
  "2303.12326": {
    "id": "http://arxiv.org/abs/2303.12326v1",
    "title": "Make Encoder Great Again in 3D GAN Inversion through Geometry and\n  Occlusion-Aware Encoding",
    "authors": [
      "Ziyang Yuan",
      "Yiming Zhu",
      "Yu Li",
      "Hongyu Liu",
      "Chun Yuan"
    ],
    "abstract": "  3D GAN inversion aims to achieve high reconstruction fidelity and reasonable\n3D geometry simultaneously from a single image input. However, existing 3D GAN\ninversion methods rely on time-consuming optimization for each individual case.\nIn this work, we introduce a novel encoder-based inversion framework based on\nEG3D, one of the most widely-used 3D GAN models. We leverage the inherent\nproperties of EG3D's latent space to design a discriminator and a background\ndepth regularization. This enables us to train a geometry-aware encoder capable\nof converting the input image into corresponding latent code. Additionally, we\nexplore the feature space of EG3D and develop an adaptive refinement stage that\nimproves the representation ability of features in EG3D to enhance the recovery\nof fine-grained textural details. Finally, we propose an occlusion-aware fusion\noperation to prevent distortion in unobserved regions. Our method achieves\nimpressive results comparable to optimization-based methods while operating up\nto 500 times faster. Our framework is well-suited for applications such as\nsemantic editing.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "14J60 (Primary) 14F05, 14J26 (Secondary)",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-22T05:51:53Z",
    "updated": "2023-03-22T05:51:53Z",
    "doi": null
  },
  "2104.13562": {
    "id": "http://arxiv.org/abs/2104.13562v2",
    "title": "Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and\n  View Synthesis",
    "authors": [
      "Julian Knodt",
      "Joe Bartusek",
      "Seung-Hwan Baek",
      "Felix Heide"
    ],
    "abstract": "  Recent neural rendering methods have demonstrated accurate view interpolation\nby predicting volumetric density and color with a neural network. Although such\nvolumetric representations can be supervised on static and dynamic scenes,\nexisting methods implicitly bake the complete scene light transport into a\nsingle neural network for a given scene, including surface modeling,\nbidirectional scattering distribution functions, and indirect lighting effects.\nIn contrast to traditional rendering pipelines, this prohibits changing surface\nreflectance, illumination, or composing other objects in the scene.\n  In this work, we explicitly model the light transport between scene surfaces\nand we rely on traditional integration schemes and the rendering equation to\nreconstruct a scene. The proposed method allows BSDF recovery with unknown\nlight conditions and classic light transports such as pathtracing. By learning\ndecomposed transport with surface representations established in conventional\nrendering methods, the method naturally facilitates editing shape, reflectance,\nlighting and scene composition. The method outperforms NeRV for relighting\nunder known lighting conditions, and produces realistic reconstructions for\nrelit and edited scenes. We validate the proposed approach for scene editing,\nrelighting and reflectance estimation learned from synthetic and captured views\non a subset of NeRV's datasets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-28T03:47:48Z",
    "updated": "2021-12-04T20:40:20Z",
    "doi": null
  },
  "2305.16223": {
    "id": "http://arxiv.org/abs/2305.16223v2",
    "title": "Prompt-Free Diffusion: Taking \"Text\" out of Text-to-Image Diffusion\n  Models",
    "authors": [
      "Xingqian Xu",
      "Jiayi Guo",
      "Zhangyang Wang",
      "Gao Huang",
      "Irfan Essa",
      "Humphrey Shi"
    ],
    "abstract": "  Text-to-image (T2I) research has grown explosively in the past year, owing to\nthe large-scale pre-trained diffusion models and many emerging personalization\nand editing approaches. Yet, one pain point persists: the text prompt\nengineering, and searching high-quality text prompts for customized results is\nmore art than science. Moreover, as commonly argued: \"an image is worth a\nthousand words\" - the attempt to describe a desired image with texts often ends\nup being ambiguous and cannot comprehensively cover delicate visual details,\nhence necessitating more additional controls from the visual domain. In this\npaper, we take a bold step forward: taking \"Text\" out of a pre-trained T2I\ndiffusion model, to reduce the burdensome prompt engineering efforts for users.\nOur proposed framework, Prompt-Free Diffusion, relies on only visual inputs to\ngenerate new images: it takes a reference image as \"context\", an optional image\nstructural conditioning, and an initial noise, with absolutely no text prompt.\nThe core architecture behind the scene is Semantic Context Encoder (SeeCoder),\nsubstituting the commonly used CLIP-based or LLM-based text encoder. The\nreusability of SeeCoder also makes it a convenient drop-in component: one can\nalso pre-train a SeeCoder in one T2I model and reuse it for another. Through\nextensive experiments, Prompt-Free Diffusion is experimentally found to (i)\noutperform prior exemplar-based image synthesis approaches; (ii) perform on par\nwith state-of-the-art T2I models using prompts following the best practice; and\n(iii) be naturally extensible to other downstream applications such as anime\nfigure generation and virtual try-on, with promising quality. Our code and\nmodels are open-sourced at https://github.com/SHI-Labs/Prompt-Free-Diffusion.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-25T16:30:07Z",
    "updated": "2023-06-01T02:27:42Z",
    "doi": null
  },
  "2305.13516": {
    "id": "http://arxiv.org/abs/2305.13516v1",
    "title": "Scaling Speech Technology to 1,000+ Languages",
    "authors": [
      "Vineel Pratap",
      "Andros Tjandra",
      "Bowen Shi",
      "Paden Tomasello",
      "Arun Babu",
      "Sayani Kundu",
      "Ali Elkahky",
      "Zhaoheng Ni",
      "Apoorv Vyas",
      "Maryam Fazel-Zarandi",
      "Alexei Baevski",
      "Yossi Adi",
      "Xiaohui Zhang",
      "Wei-Ning Hsu",
      "Alexis Conneau",
      "Michael Auli"
    ],
    "abstract": "  Expanding the language coverage of speech technology has the potential to\nimprove access to information for many more people. However, current speech\ntechnology is restricted to about one hundred languages which is a small\nfraction of the over 7,000 languages spoken around the world. The Massively\nMultilingual Speech (MMS) project increases the number of supported languages\nby 10-40x, depending on the task. The main ingredients are a new dataset based\non readings of publicly available religious texts and effectively leveraging\nself-supervised learning. We built pre-trained wav2vec 2.0 models covering\n1,406 languages, a single multilingual automatic speech recognition model for\n1,107 languages, speech synthesis models for the same number of languages, as\nwell as a language identification model for 4,017 languages. Experiments show\nthat our multilingual speech recognition model more than halves the word error\nrate of Whisper on 54 languages of the FLEURS benchmark while being trained on\na small fraction of the labeled data.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-22T22:09:41Z",
    "updated": "2023-05-22T22:09:41Z",
    "doi": null
  },
  "2308.04995": {
    "id": "http://arxiv.org/abs/2308.04995v2",
    "title": "IDiff-Face: Synthetic-based Face Recognition through Fizzy\n  Identity-Conditioned Diffusion Models",
    "authors": [
      "Fadi Boutros",
      "Jonas Henry Grebe",
      "Arjan Kuijper",
      "Naser Damer"
    ],
    "abstract": "  The availability of large-scale authentic face databases has been crucial to\nthe significant advances made in face recognition research over the past\ndecade. However, legal and ethical concerns led to the recent retraction of\nmany of these databases by their creators, raising questions about the\ncontinuity of future face recognition research without one of its key\nresources. Synthetic datasets have emerged as a promising alternative to\nprivacy-sensitive authentic data for face recognition development. However,\nrecent synthetic datasets that are used to train face recognition models suffer\neither from limitations in intra-class diversity or cross-class (identity)\ndiscrimination, leading to less optimal accuracies, far away from the\naccuracies achieved by models trained on authentic data. This paper targets\nthis issue by proposing IDiff-Face, a novel approach based on conditional\nlatent diffusion models for synthetic identity generation with realistic\nidentity variations for face recognition training. Through extensive\nevaluations, our proposed synthetic-based face recognition approach pushed the\nlimits of state-of-the-art performances, achieving, for example, 98.00%\naccuracy on the Labeled Faces in the Wild (LFW) benchmark, far ahead from the\nrecent synthetic-based face recognition solutions with 95.40% and bridging the\ngap to authentic-based face recognition with 99.82% accuracy.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-09T14:48:31Z",
    "updated": "2023-08-10T10:43:53Z",
    "doi": null
  },
  "2310.00106": {
    "id": "http://arxiv.org/abs/2310.00106v2",
    "title": "FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video\n  Synthesis from Static Imagery",
    "authors": [
      "Tasin Islam",
      "Alina Miron",
      "XiaoHui Liu",
      "Yongmin Li"
    ],
    "abstract": "  Our study introduces a new image-to-video generator called FashionFlow to\ngenerate fashion videos. By utilising a diffusion model, we are able to create\nshort videos from still fashion images. Our approach involves developing and\nconnecting relevant components with the diffusion model, which results in the\ncreation of high-fidelity videos that are aligned with the conditional image.\nThe components include the use of pseudo-3D convolutional layers to generate\nvideos efficiently. VAE and CLIP encoders capture vital characteristics from\nstill images to condition the diffusion model at a global level. Our research\ndemonstrates a successful synthesis of fashion videos featuring models posing\nfrom various angles, showcasing the fit and appearance of the garment. Our\nfindings hold great promise for improving and enhancing the shopping experience\nfor the online fashion industry.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-29T19:34:32Z",
    "updated": "2024-01-20T09:57:47Z",
    "doi": null
  },
  "2312.04687": {
    "id": "http://arxiv.org/abs/2312.04687v1",
    "title": "LLM4TDD: Best Practices for Test Driven Development Using Large Language\n  Models",
    "authors": [
      "Sanyogita Piya",
      "Allison Sullivan"
    ],
    "abstract": "  In today's society, we are becoming increasingly dependent on software\nsystems. However, we also constantly witness the negative impacts of buggy\nsoftware. Program synthesis aims to improve software correctness by\nautomatically generating the program given an outline of the expected behavior.\nFor decades, program synthesis has been an active research field, with recent\napproaches looking to incorporate Large Language Models to help generate code.\nThis paper explores the concept of LLM4TDD, where we guide Large Language\nModels to generate code iteratively using a test-driven development\nmethodology. We conduct an empirical evaluation using ChatGPT and coding\nproblems from LeetCode to investigate the impact of different test, prompt and\nproblem attributes on the efficacy of LLM4TDD.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-07T20:37:54Z",
    "updated": "2023-12-07T20:37:54Z",
    "doi": null
  },
  "2403.11415": {
    "id": "http://arxiv.org/abs/2403.11415v2",
    "title": "DreamSampler: Unifying Diffusion Sampling and Score Distillation for\n  Image Manipulation",
    "authors": [
      "Jeongsol Kim",
      "Geon Yeong Park",
      "Jong Chul Ye"
    ],
    "abstract": "  Reverse sampling and score-distillation have emerged as main workhorses in\nrecent years for image manipulation using latent diffusion models (LDMs). While\nreverse diffusion sampling often requires adjustments of LDM architecture or\nfeature engineering, score distillation offers a simple yet powerful\nmodel-agnostic approach, but it is often prone to mode-collapsing. To address\nthese limitations and leverage the strengths of both approaches, here we\nintroduce a novel framework called {\\em DreamSampler}, which seamlessly\nintegrates these two distinct approaches through the lens of regularized latent\noptimization. Similar to score-distillation, DreamSampler is a model-agnostic\napproach applicable to any LDM architecture, but it allows both distillation\nand reverse sampling with additional guidance for image editing and\nreconstruction. Through experiments involving image editing, SVG reconstruction\nand etc, we demonstrate the competitive performance of DreamSampler compared to\nexisting approaches, while providing new applications. Code:\nhttps://github.com/DreamSampler/dream-sampler\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-18T02:08:58Z",
    "updated": "2024-09-23T06:47:08Z",
    "doi": null
  },
  "2210.15947": {
    "id": "http://arxiv.org/abs/2210.15947v2",
    "title": "NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed\n  Neural Radiance Fields",
    "authors": [
      "Liangchen Song",
      "Anpei Chen",
      "Zhong Li",
      "Zhang Chen",
      "Lele Chen",
      "Junsong Yuan",
      "Yi Xu",
      "Andreas Geiger"
    ],
    "abstract": "  Visually exploring in a real-world 4D spatiotemporal space freely in VR has\nbeen a long-term quest. The task is especially appealing when only a few or\neven single RGB cameras are used for capturing the dynamic scene. To this end,\nwe present an efficient framework capable of fast reconstruction, compact\nmodeling, and streamable rendering. First, we propose to decompose the 4D\nspatiotemporal space according to temporal characteristics. Points in the 4D\nspace are associated with probabilities of belonging to three categories:\nstatic, deforming, and new areas. Each area is represented and regularized by a\nseparate neural field. Second, we propose a hybrid representations based\nfeature streaming scheme for efficiently modeling the neural fields. Our\napproach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single\nhand-held cameras and multi-camera arrays, achieving comparable or superior\nrendering performance in terms of quality and speed comparable to recent\nstate-of-the-art methods, achieving reconstruction in 10 seconds per frame and\ninteractive rendering.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-28T07:11:05Z",
    "updated": "2023-02-18T07:00:29Z",
    "doi": null
  },
  "2205.04421": {
    "id": "http://arxiv.org/abs/2205.04421v2",
    "title": "NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level\n  Quality",
    "authors": [
      "Xu Tan",
      "Jiawei Chen",
      "Haohe Liu",
      "Jian Cong",
      "Chen Zhang",
      "Yanqing Liu",
      "Xi Wang",
      "Yichong Leng",
      "Yuanhao Yi",
      "Lei He",
      "Frank Soong",
      "Tao Qin",
      "Sheng Zhao",
      "Tie-Yan Liu"
    ],
    "abstract": "  Text to speech (TTS) has made rapid progress in both academia and industry in\nrecent years. Some questions naturally arise that whether a TTS system can\nachieve human-level quality, how to define/judge that quality and how to\nachieve it. In this paper, we answer these questions by first defining the\nhuman-level quality based on the statistical significance of subjective measure\nand introducing appropriate guidelines to judge it, and then developing a TTS\nsystem called NaturalSpeech that achieves human-level quality on a benchmark\ndataset. Specifically, we leverage a variational autoencoder (VAE) for\nend-to-end text to waveform generation, with several key modules to enhance the\ncapacity of the prior from text and reduce the complexity of the posterior from\nspeech, including phoneme pre-training, differentiable duration modeling,\nbidirectional prior/posterior modeling, and a memory mechanism in VAE.\nExperiment evaluations on popular LJSpeech dataset show that our proposed\nNaturalSpeech achieves -0.01 CMOS (comparative mean opinion score) to human\nrecordings at the sentence level, with Wilcoxon signed rank test at p-level p\n>> 0.05, which demonstrates no statistically significant difference from human\nrecordings for the first time on this dataset.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-09T16:57:35Z",
    "updated": "2022-05-10T15:25:20Z",
    "doi": null
  },
  "2201.13195": {
    "id": "http://arxiv.org/abs/2201.13195v3",
    "title": "Memory-Efficient Backpropagation through Large Linear Layers",
    "authors": [
      "Daniel Bershatsky",
      "Aleksandr Mikhalev",
      "Alexandr Katrutsa",
      "Julia Gusak",
      "Daniil Merkulov",
      "Ivan Oseledets"
    ],
    "abstract": "  In modern neural networks like Transformers, linear layers require\nsignificant memory to store activations during backward pass. This study\nproposes a memory reduction approach to perform backpropagation through linear\nlayers. Since the gradients of linear layers are computed by matrix\nmultiplications, we consider methods for randomized matrix multiplications and\ndemonstrate that they require less memory with a moderate decrease of the test\naccuracy. Also, we investigate the variance of the gradient estimate induced by\nthe randomized matrix multiplication. We compare this variance with the\nvariance coming from gradient estimation based on the batch of samples. We\ndemonstrate the benefits of the proposed method on the fine-tuning of the\npre-trained RoBERTa model on GLUE tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-01-31T13:02:41Z",
    "updated": "2022-02-02T21:24:49Z",
    "doi": null
  },
  "2303.05125": {
    "id": "http://arxiv.org/abs/2303.05125v1",
    "title": "Cones: Concept Neurons in Diffusion Models for Customized Generation",
    "authors": [
      "Zhiheng Liu",
      "Ruili Feng",
      "Kai Zhu",
      "Yifei Zhang",
      "Kecheng Zheng",
      "Yu Liu",
      "Deli Zhao",
      "Jingren Zhou",
      "Yang Cao"
    ],
    "abstract": "  Human brains respond to semantic features of presented stimuli with different\nneurons. It is then curious whether modern deep neural networks admit a similar\nbehavior pattern. Specifically, this paper finds a small cluster of neurons in\na diffusion model corresponding to a particular subject. We call those neurons\nthe concept neurons. They can be identified by statistics of network gradients\nto a stimulation connected with the given subject. The concept neurons\ndemonstrate magnetic properties in interpreting and manipulating generation\nresults. Shutting them can directly yield the related subject contextualized in\ndifferent scenes. Concatenating multiple clusters of concept neurons can\nvividly generate all related concepts in a single image. A few steps of further\nfine-tuning can enhance the multi-concept capability, which may be the first to\nmanage to generate up to four different subjects in a single image. For\nlarge-scale applications, the concept neurons are environmentally friendly as\nwe only need to store a sparse cluster of int index instead of dense float32\nvalues of the parameters, which reduces storage consumption by 90\\% compared\nwith previous subject-driven generation methods. Extensive qualitative and\nquantitative studies on diverse scenarios show the superiority of our method in\ninterpreting and manipulating diffusion models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-09T09:16:04Z",
    "updated": "2023-03-09T09:16:04Z",
    "doi": null
  },
  "2104.07689": {
    "id": "http://arxiv.org/abs/2104.07689v1",
    "title": "Dual Contrastive Learning for Unsupervised Image-to-Image Translation",
    "authors": [
      "Junlin Han",
      "Mehrdad Shoeiby",
      "Lars Petersson",
      "Mohammad Ali Armin"
    ],
    "abstract": "  Unsupervised image-to-image translation tasks aim to find a mapping between a\nsource domain X and a target domain Y from unpaired training data. Contrastive\nlearning for Unpaired image-to-image Translation (CUT) yields state-of-the-art\nresults in modeling unsupervised image-to-image translation by maximizing\nmutual information between input and output patches using only one encoder for\nboth domains. In this paper, we propose a novel method based on contrastive\nlearning and a dual learning setting (exploiting two encoders) to infer an\nefficient mapping between unpaired data. Additionally, while CUT suffers from\nmode collapse, a variant of our method efficiently addresses this issue. We\nfurther demonstrate the advantage of our approach through extensive ablation\nstudies demonstrating superior performance comparing to recent approaches in\nmultiple challenging image translation tasks. Lastly, we demonstrate that the\ngap between unsupervised methods and supervised methods can be efficiently\nclosed.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-15T18:00:22Z",
    "updated": "2021-04-15T18:00:22Z",
    "doi": null
  },
  "1812.01187": {
    "id": "http://arxiv.org/abs/1812.01187v2",
    "title": "Bag of Tricks for Image Classification with Convolutional Neural\n  Networks",
    "authors": [
      "Tong He",
      "Zhi Zhang",
      "Hang Zhang",
      "Zhongyue Zhang",
      "Junyuan Xie",
      "Mu Li"
    ],
    "abstract": "  Much of the recent progress made in image classification research can be\ncredited to training procedure refinements, such as changes in data\naugmentations and optimization methods. In the literature, however, most\nrefinements are either briefly mentioned as implementation details or only\nvisible in source code. In this paper, we will examine a collection of such\nrefinements and empirically evaluate their impact on the final model accuracy\nthrough ablation study. We will show that, by combining these refinements\ntogether, we are able to improve various CNN models significantly. For example,\nwe raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on\nImageNet. We will also demonstrate that improvement on image classification\naccuracy leads to better transfer learning performance in other application\ndomains such as object detection and semantic segmentation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-12-04T03:07:35Z",
    "updated": "2018-12-05T22:17:01Z",
    "doi": null
  },
  "2110.10596": {
    "id": "http://arxiv.org/abs/2110.10596v2",
    "title": "Look at What I'm Doing: Self-Supervised Spatial Grounding of Narrations\n  in Instructional Videos",
    "authors": [
      "Reuben Tan",
      "Bryan A. Plummer",
      "Kate Saenko",
      "Hailin Jin",
      "Bryan Russell"
    ],
    "abstract": "  We introduce the task of spatially localizing narrated interactions in\nvideos. Key to our approach is the ability to learn to spatially localize\ninteractions with self-supervision on a large corpus of videos with\naccompanying transcribed narrations. To achieve this goal, we propose a\nmultilayer cross-modal attention network that enables effective optimization of\na contrastive loss during training. We introduce a divided strategy that\nalternates between computing inter- and intra-modal attention across the visual\nand natural language modalities, which allows effective training via directly\ncontrasting the two modalities' representations. We demonstrate the\neffectiveness of our approach by self-training on the HowTo100M instructional\nvideo dataset and evaluating on a newly collected dataset of localized\ndescribed interactions in the YouCook2 dataset. We show that our approach\noutperforms alternative baselines, including shallow co-attention and full\ncross-modal attention. We also apply our approach to grounding phrases in\nimages with weak supervision on Flickr30K and show that stacking multiple\nattention layers is effective and, when combined with a word-to-region loss,\nachieves state of the art on recall-at-one and pointing hand accuracies.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-20T14:45:13Z",
    "updated": "2021-12-02T16:55:56Z",
    "doi": null
  },
  "2307.05950": {
    "id": "http://arxiv.org/abs/2307.05950v2",
    "title": "Exploring the Effectiveness of LLMs in Automated Logging Generation: An\n  Empirical Study",
    "authors": [
      "Yichen Li",
      "Yintong Huo",
      "Zhihan Jiang",
      "Renyi Zhong",
      "Pinjia He",
      "Yuxin Su",
      "Lionel Briand",
      "Michael R. Lyu"
    ],
    "abstract": "  Automated logging statement generation supports developers in documenting\ncritical software runtime behavior. Given the great success in natural language\ngeneration and programming language comprehension, large language models (LLMs)\nmight help developers generate logging statements, but this has not yet been\ninvestigated. To fill the gap, this paper performs the first study on exploring\nLLMs for logging statement generation.We first build a logging statement\ngeneration dataset, LogBench, with two parts: (1) LogBench-O: logging\nstatements collected from GitHub repositories, and (2) LogBench-T: the\ntransformed unseen code from LogBench-O. Then, we leverage LogBench to evaluate\nthe effectiveness and generalization capabilities (using LogBench-T) of eleven\ntop-performing LLMs. In addition, we examine the performance of these LLMs\nagainst classical retrieval-based and machine learning-based logging methods\nfrom the era preceding LLMs. We further evaluate LLM's logging generalization\ncapabilities using unseen data (LogBench-T) derived from code transformation\ntechniques. While existing LLMs deliver decent predictions on logging levels\nand logging variables, our study indicates that they only achieve a maximum\nBLEU score of 0.249, thus calling for improvements. The paper also highlights\nthe importance of prompt constructions and external factors (e.g., programming\ncontexts and code comments) for LLMs' logging performance. Based on these\nfindings, we identify five implications and provide practical advice for future\nlogging research. Our empirical analysis discloses the limitations of current\nlogging approaches while showcasing the potential of LLM-based logging tools,\nand provides actionable guidance for building more practical models.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-12T06:32:51Z",
    "updated": "2024-04-01T12:19:55Z",
    "doi": null
  },
  "2311.16918": {
    "id": "http://arxiv.org/abs/2311.16918v2",
    "title": "RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail\n  Richness in Text-to-3D",
    "authors": [
      "Lingteng Qiu",
      "Guanying Chen",
      "Xiaodong Gu",
      "Qi Zuo",
      "Mutian Xu",
      "Yushuang Wu",
      "Weihao Yuan",
      "Zilong Dong",
      "Liefeng Bo",
      "Xiaoguang Han"
    ],
    "abstract": "  Lifting 2D diffusion for 3D generation is a challenging problem due to the\nlack of geometric prior and the complex entanglement of materials and lighting\nin natural images. Existing methods have shown promise by first creating the\ngeometry through score-distillation sampling (SDS) applied to rendered surface\nnormals, followed by appearance modeling. However, relying on a 2D RGB\ndiffusion model to optimize surface normals is suboptimal due to the\ndistribution discrepancy between natural images and normals maps, leading to\ninstability in optimization. In this paper, recognizing that the normal and\ndepth information effectively describe scene geometry and be automatically\nestimated from images, we propose to learn a generalizable Normal-Depth\ndiffusion model for 3D generation. We achieve this by training on the\nlarge-scale LAION dataset together with the generalizable image-to-depth and\nnormal prior models. In an attempt to alleviate the mixed illumination effects\nin the generated materials, we introduce an albedo diffusion model to impose\ndata-driven constraints on the albedo component. Our experiments show that when\nintegrated into existing text-to-3D pipelines, our models significantly enhance\nthe detail richness, achieving state-of-the-art results. Our project page is\nhttps://aigc3d.github.io/richdreamer/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-28T16:22:33Z",
    "updated": "2023-12-24T16:36:09Z",
    "doi": null
  },
  "1912.00552": {
    "id": "http://arxiv.org/abs/1912.00552v2",
    "title": "Sparse Graph Attention Networks",
    "authors": [
      "Yang Ye",
      "Shihao Ji"
    ],
    "abstract": "  Graph Neural Networks (GNNs) have proved to be an effective representation\nlearning framework for graph-structured data, and have achieved\nstate-of-the-art performance on many practical predictive tasks, such as node\nclassification, link prediction and graph classification. Among the variants of\nGNNs, Graph Attention Networks (GATs) learn to assign dense attention\ncoefficients over all neighbors of a node for feature aggregation, and improve\nthe performance of many graph learning tasks. However, real-world graphs are\noften very large and noisy, and GATs are prone to overfitting if not\nregularized properly. Even worse, the local aggregation mechanism of GATs may\nfail on disassortative graphs, where nodes within local neighborhood provide\nmore noise than useful information for feature aggregation. In this paper, we\npropose Sparse Graph Attention Networks (SGATs) that learn sparse attention\ncoefficients under an $L_0$-norm regularization, and the learned sparse\nattentions are then used for all GNN layers, resulting in an edge-sparsified\ngraph. By doing so, we can identify noisy/task-irrelevant edges, and thus\nperform feature aggregation on most informative neighbors. Extensive\nexperiments on synthetic and real-world graph learning benchmarks demonstrate\nthe superior performance of SGATs. In particular, SGATs can remove about\n50\\%-80\\% edges from large assortative graphs, while retaining similar\nclassification accuracies. On disassortative graphs, SGATs prune majority of\nnoisy edges and outperform GATs in classification accuracies by significant\nmargins. Furthermore, the removed edges can be interpreted intuitively and\nquantitatively. To the best of our knowledge, this is the first graph learning\nalgorithm that shows significant redundancies in graphs and edge-sparsified\ngraphs can achieve similar or sometimes higher predictive performances than\noriginal graphs.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-12-02T02:25:01Z",
    "updated": "2021-04-11T02:54:26Z",
    "doi": null
  },
  "1704.03976": {
    "id": "http://arxiv.org/abs/1704.03976v2",
    "title": "Virtual Adversarial Training: A Regularization Method for Supervised and\n  Semi-Supervised Learning",
    "authors": [
      "Takeru Miyato",
      "Shin-ichi Maeda",
      "Masanori Koyama",
      "Shin Ishii"
    ],
    "abstract": "  We propose a new regularization method based on virtual adversarial loss: a\nnew measure of local smoothness of the conditional label distribution given\ninput. Virtual adversarial loss is defined as the robustness of the conditional\nlabel distribution around each input data point against local perturbation.\nUnlike adversarial training, our method defines the adversarial direction\nwithout label information and is hence applicable to semi-supervised learning.\nBecause the directions in which we smooth the model are only \"virtually\"\nadversarial, we call our method virtual adversarial training (VAT). The\ncomputational cost of VAT is relatively low. For neural networks, the\napproximated gradient of virtual adversarial loss can be computed with no more\nthan two pairs of forward- and back-propagations. In our experiments, we\napplied VAT to supervised and semi-supervised learning tasks on multiple\nbenchmark datasets. With a simple enhancement of the algorithm based on the\nentropy minimization principle, our VAT achieves state-of-the-art performance\nfor semi-supervised learning tasks on SVHN and CIFAR-10.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-04-13T02:45:27Z",
    "updated": "2018-06-27T04:52:47Z",
    "doi": null
  },
  "2012.15370": {
    "id": "http://arxiv.org/abs/2012.15370v1",
    "title": "OSTeC: One-Shot Texture Completion",
    "authors": [
      "Baris Gecer",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ],
    "abstract": "  The last few years have witnessed the great success of non-linear generative\nmodels in synthesizing high-quality photorealistic face images. Many recent 3D\nfacial texture reconstruction and pose manipulation from a single image\napproaches still rely on large and clean face datasets to train image-to-image\nGenerative Adversarial Networks (GANs). Yet the collection of such a large\nscale high-resolution 3D texture dataset is still very costly and difficult to\nmaintain age/ethnicity balance. Moreover, regression-based approaches suffer\nfrom generalization to the in-the-wild conditions and are unable to fine-tune\nto a target-image. In this work, we propose an unsupervised approach for\none-shot 3D facial texture completion that does not require large-scale texture\ndatasets, but rather harnesses the knowledge stored in 2D face generators. The\nproposed approach rotates an input image in 3D and fill-in the unseen regions\nby reconstructing the rotated image in a 2D face generator, based on the\nvisible parts. Finally, we stitch the most visible textures at different angles\nin the UV image-plane. Further, we frontalize the target image by projecting\nthe completed texture into the generator. The qualitative and quantitative\nexperiments demonstrate that the completed UV textures and frontalized images\nare of high quality, resembles the original identity, can be used to train a\ntexture GAN model for 3DMM fitting and improve pose-invariant face recognition.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-12-30T23:53:26Z",
    "updated": "2020-12-30T23:53:26Z",
    "doi": "10.1109/CVPR46437.2021.00754"
  },
  "2104.04670": {
    "id": "http://arxiv.org/abs/2104.04670v5",
    "title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on\n  Dataset and Prompt Collections",
    "authors": [
      "Ruiqi Zhong",
      "Kristy Lee",
      "Zheng Zhang",
      "Dan Klein"
    ],
    "abstract": "  Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-10T02:57:22Z",
    "updated": "2021-09-08T16:29:59Z",
    "doi": null
  },
  "2109.00783": {
    "id": "http://arxiv.org/abs/2109.00783v4",
    "title": "Computer Vision Self-supervised Learning Methods on Time Series",
    "authors": [
      "Daesoo Lee",
      "Erlend Aune"
    ],
    "abstract": "  Self-supervised learning (SSL) has had great success in both computer vision.\nMost of the current mainstream computer vision SSL frameworks are based on\nSiamese network architecture. These approaches often rely on cleverly crafted\nloss functions and training setups to avoid feature collapse. In this study, we\nevaluate if those computer-vision SSL frameworks are also effective on a\ndifferent modality (\\textit{i.e.,} time series). The effectiveness is\nexperimented and evaluated on the UCR and UEA archives, and we show that the\ncomputer vision SSL frameworks can be effective even for time series. In\naddition, we propose a new method that improves on the recently proposed VICReg\nmethod. Our method improves on a \\textit{covariance} term proposed in VICReg,\nand in addition we augment the head of the architecture by an iterative\nnormalization layer that accelerates the convergence of the model.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-02T08:45:53Z",
    "updated": "2024-01-26T22:16:07Z",
    "doi": null
  },
  "2303.06573": {
    "id": "http://arxiv.org/abs/2303.06573v2",
    "title": "Large Language Models Know Your Contextual Search Intent: A Prompting\n  Framework for Conversational Search",
    "authors": [
      "Kelong Mao",
      "Zhicheng Dou",
      "Fengran Mo",
      "Jiewen Hou",
      "Haonan Chen",
      "Hongjin Qian"
    ],
    "abstract": "  Precisely understanding users' contextual search intent has been an important\nchallenge for conversational search. As conversational search sessions are much\nmore diverse and long-tailed, existing methods trained on limited data still\nshow unsatisfactory effectiveness and robustness to handle real conversational\nsearch scenarios. Recently, large language models (LLMs) have demonstrated\namazing capabilities for text generation and conversation understanding. In\nthis work, we present a simple yet effective prompting framework, called\nLLM4CS, to leverage LLMs as a text-based search intent interpreter to help\nconversational search. Under this framework, we explore three prompting methods\nto generate multiple query rewrites and hypothetical responses, and propose to\naggregate them into an integrated representation that can robustly represent\nthe user's real contextual search intent. Extensive automatic evaluations and\nhuman evaluations on three widely used conversational search benchmarks,\nincluding CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance\nof our simple LLM4CS framework compared with existing methods and even using\nhuman rewrites. Our findings provide important evidence to better understand\nand leverage LLMs for conversational search.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-12T05:08:16Z",
    "updated": "2023-10-19T23:45:30Z",
    "doi": null
  },
  "2303.16765": {
    "id": "http://arxiv.org/abs/2303.16765v2",
    "title": "MDP: A Generalized Framework for Text-Guided Image Editing by\n  Manipulating the Diffusion Path",
    "authors": [
      "Qian Wang",
      "Biao Zhang",
      "Michael Birsak",
      "Peter Wonka"
    ],
    "abstract": "  Image generation using diffusion can be controlled in multiple ways. In this\npaper, we systematically analyze the equations of modern generative diffusion\nnetworks to propose a framework, called MDP, that explains the design space of\nsuitable manipulations. We identify 5 different manipulations, including\nintermediate latent, conditional embedding, cross attention maps, guidance, and\npredicted noise. We analyze the corresponding parameters of these manipulations\nand the manipulation schedule. We show that some previous editing methods fit\nnicely into our framework. Particularly, we identified one specific\nconfiguration as a new type of control by manipulating the predicted noise,\nwhich can perform higher-quality edits than previous work for a variety of\nlocal and global edits.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-29T14:57:54Z",
    "updated": "2023-03-30T11:42:41Z",
    "doi": null
  },
  "2005.01703": {
    "id": "http://arxiv.org/abs/2005.01703v2",
    "title": "Transforming and Projecting Images into Class-conditional Generative\n  Networks",
    "authors": [
      "Minyoung Huh",
      "Richard Zhang",
      "Jun-Yan Zhu",
      "Sylvain Paris",
      "Aaron Hertzmann"
    ],
    "abstract": "  We present a method for projecting an input image into the space of a\nclass-conditional generative neural network. We propose a method that optimizes\nfor transformation to counteract the model biases in generative neural\nnetworks. Specifically, we demonstrate that one can solve for image\ntranslation, scale, and global color transformation, during the projection\noptimization to address the object-center bias and color bias of a Generative\nAdversarial Network. This projection process poses a difficult optimization\nproblem, and purely gradient-based optimizations fail to find good solutions.\nWe describe a hybrid optimization strategy that finds good projections by\nestimating transformations and class parameters. We show the effectiveness of\nour method on real images and further demonstrate how the corresponding\nprojections lead to better editability of these images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-05-04T17:57:47Z",
    "updated": "2020-08-27T18:10:52Z",
    "doi": null
  },
  "1710.02971": {
    "id": "http://arxiv.org/abs/1710.02971v4",
    "title": "Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,\n  and node2vec",
    "authors": [
      "Jiezhong Qiu",
      "Yuxiao Dong",
      "Hao Ma",
      "Jian Li",
      "Kuansan Wang",
      "Jie Tang"
    ],
    "abstract": "  Since the invention of word2vec, the skip-gram model has significantly\nadvanced the research of network embedding, such as the recent emergence of the\nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of\nthe aforementioned models with negative sampling can be unified into the matrix\nfactorization framework with closed forms. Our analysis and proofs reveal that:\n(1) DeepWalk empirically produces a low-rank transformation of a network's\nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk\nwhen the size of vertices' context is set to one; (3) As an extension of LINE,\nPTE can be viewed as the joint factorization of multiple networks' Laplacians;\n(4) node2vec is factorizing a matrix related to the stationary distribution and\ntransition probability tensor of a 2nd-order random walk. We further provide\nthe theoretical connections between skip-gram based network embedding\nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF\nmethod as well as its approximation algorithm for computing network embedding.\nOur method offers significant improvements over DeepWalk and LINE for\nconventional network mining tasks. This work lays the theoretical foundation\nfor skip-gram based network embedding methods, leading to a better\nunderstanding of latent network representation learning.\n",
    "categories": [
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-10-09T07:28:46Z",
    "updated": "2018-02-08T09:51:03Z",
    "doi": "10.1145/3159652.3159706"
  },
  "2310.19248": {
    "id": "http://arxiv.org/abs/2310.19248v1",
    "title": "IMPRESS: Evaluating the Resilience of Imperceptible Perturbations\n  Against Unauthorized Data Usage in Diffusion-Based Generative AI",
    "authors": [
      "Bochuan Cao",
      "Changjiang Li",
      "Ting Wang",
      "Jinyuan Jia",
      "Bo Li",
      "Jinghui Chen"
    ],
    "abstract": "  Diffusion-based image generation models, such as Stable Diffusion or DALL-E\n2, are able to learn from given images and generate high-quality samples\nfollowing the guidance from prompts. For instance, they can be used to create\nartistic images that mimic the style of an artist based on his/her original\nartworks or to maliciously edit the original images for fake content. However,\nsuch ability also brings serious ethical issues without proper authorization\nfrom the owner of the original images. In response, several attempts have been\nmade to protect the original images from such unauthorized data usage by adding\nimperceptible perturbations, which are designed to mislead the diffusion model\nand make it unable to properly generate new samples. In this work, we introduce\na perturbation purification platform, named IMPRESS, to evaluate the\neffectiveness of imperceptible perturbations as a protective measure. IMPRESS\nis based on the key observation that imperceptible perturbations could lead to\na perceptible inconsistency between the original image and the\ndiffusion-reconstructed image, which can be used to devise a new optimization\nstrategy for purifying the image, which may weaken the protection of the\noriginal image from unauthorized data usage (e.g., style mimicking, malicious\nediting). The proposed IMPRESS platform offers a comprehensive evaluation of\nseveral contemporary protection methods, and can be used as an evaluation\nplatform for future protection methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-30T03:33:41Z",
    "updated": "2023-10-30T03:33:41Z",
    "doi": null
  },
  "2302.00438": {
    "id": "http://arxiv.org/abs/2302.00438v1",
    "title": "On the Robustness of Code Generation Techniques: An Empirical Study on\n  GitHub Copilot",
    "authors": [
      "Antonio Mastropaolo",
      "Luca Pascarella",
      "Emanuela Guglielmi",
      "Matteo Ciniselli",
      "Simone Scalabrino",
      "Rocco Oliveto",
      "Gabriele Bavota"
    ],
    "abstract": "  Software engineering research has always being concerned with the improvement\nof code completion approaches, which suggest the next tokens a developer will\nlikely type while coding. The release of GitHub Copilot constitutes a big step\nforward, also because of its unprecedented ability to automatically generate\neven entire functions from their natural language description. While the\nusefulness of Copilot is evident, it is still unclear to what extent it is\nrobust. Specifically, we do not know the extent to which semantic-preserving\nchanges in the natural language description provided to the model have an\neffect on the generated code function. In this paper we present an empirical\nstudy in which we aim at understanding whether different but semantically\nequivalent natural language descriptions result in the same recommended\nfunction. A negative answer would pose questions on the robustness of deep\nlearning (DL)-based code generators since it would imply that developers using\ndifferent wordings to describe the same code would obtain different\nrecommendations. We asked Copilot to automatically generate 892 Java methods\nstarting from their original Javadoc description. Then, we generated different\nsemantically equivalent descriptions for each method both manually and\nautomatically, and we analyzed the extent to which predictions generated by\nCopilot changed. Our results show that modifying the description results in\ndifferent code recommendations in ~46% of cases. Also, differences in the\nsemantically equivalent descriptions might impact the correctness of the\ngenerated code ~28%.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-01T13:36:53Z",
    "updated": "2023-02-01T13:36:53Z",
    "doi": null
  },
  "2206.05442": {
    "id": "http://arxiv.org/abs/2206.05442v7",
    "title": "From Human Days to Machine Seconds: Automatically Answering and\n  Generating Machine Learning Final Exams",
    "authors": [
      "Iddo Drori",
      "Sarah J. Zhang",
      "Reece Shuttleworth",
      "Sarah Zhang",
      "Keith Tyser",
      "Zad Chin",
      "Pedro Lantigua",
      "Saisamrit Surbehera",
      "Gregory Hunter",
      "Derek Austin",
      "Leonard Tang",
      "Yann Hicke",
      "Sage Simhon",
      "Sathwik Karnik",
      "Darnell Granberry",
      "Madeleine Udell"
    ],
    "abstract": "  A final exam in machine learning at a top institution such as MIT, Harvard,\nor Cornell typically takes faculty days to write, and students hours to solve.\nWe demonstrate that large language models pass machine learning finals at a\nhuman level, on finals available online after the models were trained, and\nautomatically generate new human-quality final exam questions in seconds.\nPrevious work has developed program synthesis and few-shot learning methods to\nsolve university-level problem set questions in mathematics and STEM courses.\nIn this work, we develop and compare methods that solve final exams, which\ndiffer from problem sets in several ways: the questions are longer, have\nmultiple parts, are more complicated, and span a broader set of topics. We\ncurate a dataset and benchmark of questions from machine learning final exams\navailable online and code for answering these questions and generating new\nquestions. We show how to generate new questions from other questions and\ncourse notes. For reproducibility and future research on this final exam\nbenchmark, we use automatic checkers for multiple-choice, numeric, and\nquestions with expression answers. We perform ablation studies comparing\nzero-shot learning with few-shot learning and chain-of-thought prompting using\nGPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that\nfew-shot learning methods perform best. We highlight the transformative\npotential of language models to streamline the writing and solution of\nlarge-scale assessments, significantly reducing the workload from human days to\nmere machine seconds. Our results suggest that rather than banning large\nlanguage models such as ChatGPT in class, instructors should teach students to\nharness them by asking students meta-questions about correctness, completeness,\nand originality of the responses generated, encouraging critical thinking in\nacademic studies.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-11T06:38:06Z",
    "updated": "2023-06-28T04:42:05Z",
    "doi": null
  },
  "2207.10642": {
    "id": "http://arxiv.org/abs/2207.10642v1",
    "title": "Generative Multiplane Images: Making a 2D GAN 3D-Aware",
    "authors": [
      "Xiaoming Zhao",
      "Fangchang Ma",
      "David G\u00fcera",
      "Zhile Ren",
      "Alexander G. Schwing",
      "Alex Colburn"
    ],
    "abstract": "  What is really needed to make an existing 2D GAN 3D-aware? To answer this\nquestion, we modify a classical GAN, i.e., StyleGANv2, as little as possible.\nWe find that only two modifications are absolutely necessary: 1) a multiplane\nimage style generator branch which produces a set of alpha maps conditioned on\ntheir depth; 2) a pose-conditioned discriminator. We refer to the generated\noutput as a 'generative multiplane image' (GMPI) and emphasize that its\nrenderings are not only high-quality but also guaranteed to be view-consistent,\nwhich makes GMPIs different from many prior works. Importantly, the number of\nalpha maps can be dynamically adjusted and can differ between training and\ninference, alleviating memory concerns and enabling fast training of GMPIs in\nless than half a day at a resolution of $1024^2$. Our findings are consistent\nacross three challenging and common high-resolution datasets, including FFHQ,\nAFHQv2, and MetFaces.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-21T17:50:16Z",
    "updated": "2022-07-21T17:50:16Z",
    "doi": null
  },
  "2401.08815": {
    "id": "http://arxiv.org/abs/2401.08815v1",
    "title": "Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive",
    "authors": [
      "Yumeng Li",
      "Margret Keuper",
      "Dan Zhang",
      "Anna Khoreva"
    ],
    "abstract": "  Despite the recent advances in large-scale diffusion models, little progress\nhas been made on the layout-to-image (L2I) synthesis task. Current L2I models\neither suffer from poor editability via text or weak alignment between the\ngenerated image and the input layout. This limits their usability in practice.\nTo mitigate this, we propose to integrate adversarial supervision into the\nconventional training pipeline of L2I diffusion models (ALDM). Specifically, we\nemploy a segmentation-based discriminator which provides explicit feedback to\nthe diffusion generator on the pixel-level alignment between the denoised image\nand the input layout. To encourage consistent adherence to the input layout\nover the sampling steps, we further introduce the multistep unrolling strategy.\nInstead of looking at a single timestep, we unroll a few steps recursively to\nimitate the inference process, and ask the discriminator to assess the\nalignment of denoised images with the layout over a certain time window. Our\nexperiments show that ALDM enables layout faithfulness of the generated images,\nwhile allowing broad editability via text prompts. Moreover, we showcase its\nusefulness for practical applications: by synthesizing target distribution\nsamples via text control, we improve domain generalization of semantic\nsegmentation models by a large margin (~12 mIoU points).\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-16T20:31:46Z",
    "updated": "2024-01-16T20:31:46Z",
    "doi": null
  },
  "2312.05664": {
    "id": "http://arxiv.org/abs/2312.05664v2",
    "title": "CoGS: Controllable Gaussian Splatting",
    "authors": [
      "Heng Yu",
      "Joel Julin",
      "Zolt\u00e1n \u00c1. Milacski",
      "Koichiro Niinuma",
      "L\u00e1szl\u00f3 A. Jeni"
    ],
    "abstract": "  Capturing and re-animating the 3D structure of articulated objects present\nsignificant barriers. On one hand, methods requiring extensively calibrated\nmulti-view setups are prohibitively complex and resource-intensive, limiting\ntheir practical applicability. On the other hand, while single-camera Neural\nRadiance Fields (NeRFs) offer a more streamlined approach, they have excessive\ntraining and rendering costs. 3D Gaussian Splatting would be a suitable\nalternative but for two reasons. Firstly, existing methods for 3D dynamic\nGaussians require synchronized multi-view cameras, and secondly, the lack of\ncontrollability in dynamic scenarios. We present CoGS, a method for\nControllable Gaussian Splatting, that enables the direct manipulation of scene\nelements, offering real-time control of dynamic scenes without the prerequisite\nof pre-computing control signals. We evaluated CoGS using both synthetic and\nreal-world datasets that include dynamic objects that differ in degree of\ndifficulty. In our evaluations, CoGS consistently outperformed existing dynamic\nand controllable neural representations in terms of visual fidelity.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-09T20:06:29Z",
    "updated": "2024-04-22T17:28:30Z",
    "doi": null
  },
  "2009.03300": {
    "id": "http://arxiv.org/abs/2009.03300v3",
    "title": "Measuring Massive Multitask Language Understanding",
    "authors": [
      "Dan Hendrycks",
      "Collin Burns",
      "Steven Basart",
      "Andy Zou",
      "Mantas Mazeika",
      "Dawn Song",
      "Jacob Steinhardt"
    ],
    "abstract": "  We propose a new test to measure a text model's multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess\nextensive world knowledge and problem solving ability. We find that while most\nrecent models have near random-chance accuracy, the very largest GPT-3 model\nimproves over random chance by almost 20 percentage points on average. However,\non every one of the 57 tasks, the best models still need substantial\nimprovements before they can reach expert-level accuracy. Models also have\nlopsided performance and frequently do not know when they are wrong. Worse,\nthey still have near-random accuracy on some socially important subjects such\nas morality and law. By comprehensively evaluating the breadth and depth of a\nmodel's academic and professional understanding, our test can be used to\nanalyze models across many tasks and to identify important shortcomings.\n",
    "categories": [
      {
        "@term": "cs.CY",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-09-07T17:59:25Z",
    "updated": "2021-01-12T18:57:11Z",
    "doi": null
  },
  "2307.15058": {
    "id": "http://arxiv.org/abs/2307.15058v1",
    "title": "MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous\n  Driving",
    "authors": [
      "Zirui Wu",
      "Tianyu Liu",
      "Liyi Luo",
      "Zhide Zhong",
      "Jianteng Chen",
      "Hongmin Xiao",
      "Chao Hou",
      "Haozhe Lou",
      "Yuantao Chen",
      "Runyi Yang",
      "Yuxin Huang",
      "Xiaoyu Ye",
      "Zike Yan",
      "Yongliang Shi",
      "Yiyi Liao",
      "Hao Zhao"
    ],
    "abstract": "  Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is\nwidely recognized that realistic sensor simulation will play a critical role in\nsolving remaining corner cases by simulating them. To this end, we propose an\nautonomous driving simulator based upon neural radiance fields (NeRFs).\nCompared with existing works, ours has three notable features: (1)\nInstance-aware. Our simulator models the foreground instances and background\nenvironments separately with independent networks so that the static (e.g.,\nsize and appearance) and dynamic (e.g., trajectory) properties of instances can\nbe controlled separately. (2) Modular. Our simulator allows flexible switching\nbetween different modern NeRF-related backbones, sampling strategies, input\nmodalities, etc. We expect this modular design to boost academic progress and\nindustrial deployment of NeRF-based autonomous driving simulation. (3)\nRealistic. Our simulator set new state-of-the-art photo-realism results given\nthe best module selection. Our simulator will be open-sourced while most of our\ncounterparts are not. Project page: https://open-air-sun.github.io/mars/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-27T17:59:52Z",
    "updated": "2023-07-27T17:59:52Z",
    "doi": "10.1007/978-981-99-8850-1_1"
  },
  "2409.16165": {
    "id": "http://arxiv.org/abs/2409.16165v1",
    "title": "EnIGMA: Enhanced Interactive Generative Model Agent for CTF Challenges",
    "authors": [
      "Talor Abramovich",
      "Meet Udeshi",
      "Minghao Shao",
      "Kilian Lieret",
      "Haoran Xi",
      "Kimberly Milner",
      "Sofija Jancheska",
      "John Yang",
      "Carlos E. Jimenez",
      "Farshad Khorrami",
      "Prashanth Krishnamurthy",
      "Brendan Dolan-Gavitt",
      "Muhammad Shafique",
      "Karthik Narasimhan",
      "Ramesh Karri",
      "Ofir Press"
    ],
    "abstract": "  Although language model (LM) agents are demonstrating growing potential in\nmany domains, their success in cybersecurity has been limited due to simplistic\ndesign and the lack of fundamental features for this domain. We present EnIGMA,\nan LM agent for autonomously solving Capture The Flag (CTF) challenges. EnIGMA\nintroduces new Agent-Computer Interfaces (ACIs) to improve the success rate on\nCTF challenges. We establish the novel Interactive Agent Tool concept, which\nenables LM agents to run interactive command-line utilities essential for these\nchallenges. Empirical analysis of EnIGMA on over 350 CTF challenges from three\ndifferent benchmarks indicates that providing a robust set of new tools with\ndemonstration of their usage helps the LM solve complex problems and achieves\nstate-of-the-art results on the NYU CTF and Intercode-CTF benchmarks. Finally,\nwe discuss insights on ACI design and agent behavior on cybersecurity tasks\nthat highlight the need to adapt real-world tools for LM agents.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-09-24T15:06:01Z",
    "updated": "2024-09-24T15:06:01Z",
    "doi": null
  },
  "2208.12415": {
    "id": "http://arxiv.org/abs/2208.12415v1",
    "title": "MuLan: A Joint Embedding of Music Audio and Natural Language",
    "authors": [
      "Qingqing Huang",
      "Aren Jansen",
      "Joonseok Lee",
      "Ravi Ganti",
      "Judith Yue Li",
      "Daniel P. W. Ellis"
    ],
    "abstract": "  Music tagging and content-based retrieval systems have traditionally been\nconstructed using pre-defined ontologies covering a rigid set of music\nattributes or text queries. This paper presents MuLan: a first attempt at a new\ngeneration of acoustic models that link music audio directly to unconstrained\nnatural language music descriptions. MuLan takes the form of a two-tower, joint\naudio-text embedding model trained using 44 million music recordings (370K\nhours) and weakly-associated, free-form text annotations. Through its\ncompatibility with a wide range of music genres and text styles (including\nconventional music tags), the resulting audio-text representation subsumes\nexisting ontologies while graduating to true zero-shot functionalities. We\ndemonstrate the versatility of the MuLan embeddings with a range of experiments\nincluding transfer learning, zero-shot music tagging, language understanding in\nthe music domain, and cross-modal retrieval applications.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-26T03:13:21Z",
    "updated": "2022-08-26T03:13:21Z",
    "doi": null
  },
  "2302.03453": {
    "id": "http://arxiv.org/abs/2302.03453v2",
    "title": "OSRT: Omnidirectional Image Super-Resolution with Distortion-aware\n  Transformer",
    "authors": [
      "Fanghua Yu",
      "Xintao Wang",
      "Mingdeng Cao",
      "Gen Li",
      "Ying Shan",
      "Chao Dong"
    ],
    "abstract": "  Omnidirectional images (ODIs) have obtained lots of research interest for\nimmersive experiences. Although ODIs require extremely high resolution to\ncapture details of the entire scene, the resolutions of most ODIs are\ninsufficient. Previous methods attempt to solve this issue by image\nsuper-resolution (SR) on equirectangular projection (ERP) images. However, they\nomit geometric properties of ERP in the degradation process, and their models\ncan hardly generalize to real ERP images. In this paper, we propose Fisheye\ndownsampling, which mimics the real-world imaging process and synthesizes more\nrealistic low-resolution samples. Then we design a distortion-aware Transformer\n(OSRT) to modulate ERP distortions continuously and self-adaptively. Without a\ncumbersome process, OSRT outperforms previous methods by about 0.2dB on PSNR.\nMoreover, we propose a convenient data augmentation strategy, which synthesizes\npseudo ERP images from plain images. This simple strategy can alleviate the\nover-fitting problem of large networks and significantly boost the performance\nof ODISR. Extensive experiments have demonstrated the state-of-the-art\nperformance of our OSRT. Codes and models will be available at\nhttps://github.com/Fanghua-Yu/OSRT.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-07T13:19:59Z",
    "updated": "2023-02-09T10:45:41Z",
    "doi": null
  },
  "2203.13301": {
    "id": "http://arxiv.org/abs/2203.13301v2",
    "title": "Multi-modal Multi-label Facial Action Unit Detection with Transformer",
    "authors": [
      "Lingfeng Wang",
      "Shisen Wang",
      "Jin Qi"
    ],
    "abstract": "  Facial Action Coding System is an important approach of facial expression\nanalysis.This paper describes our submission to the third Affective Behavior\nAnalysis (ABAW) 2022 competition. We proposed a transfomer based model to\ndetect facial action unit (FAU) in video. To be specific, we firstly trained a\nmulti-modal model to extract both audio and visual feature. After that, we\nproposed a action units correlation module to learn relationships between each\naction unit labels and refine action unit detection result. Experimental\nresults on validation dataset shows that our method achieves better performance\nthan baseline model, which verifies that the effectiveness of proposed network.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-24T18:59:31Z",
    "updated": "2022-03-28T05:17:31Z",
    "doi": null
  },
  "1810.00553": {
    "id": "http://arxiv.org/abs/1810.00553v1",
    "title": "Optimal Adaptive and Accelerated Stochastic Gradient Descent",
    "authors": [
      "Qi Deng",
      "Yi Cheng",
      "Guanghui Lan"
    ],
    "abstract": "  Stochastic gradient descent (\\textsc{Sgd}) methods are the most powerful\noptimization tools in training machine learning and deep learning models.\nMoreover, acceleration (a.k.a. momentum) methods and diagonal scaling (a.k.a.\nadaptive gradient) methods are the two main techniques to improve the slow\nconvergence of \\textsc{Sgd}. While empirical studies have demonstrated\npotential advantages of combining these two techniques, it remains unknown\nwhether these methods can achieve the optimal rate of convergence for\nstochastic optimization. In this paper, we present a new class of adaptive and\naccelerated stochastic gradient descent methods and show that they exhibit the\noptimal sampling and iteration complexity for stochastic optimization. More\nspecifically, we show that diagonal scaling, initially designed to improve\nvanilla stochastic gradient, can be incorporated into accelerated stochastic\ngradient descent to achieve the optimal rate of convergence for smooth\nstochastic optimization. We also show that momentum, apart from being known to\nspeed up the convergence rate of deterministic optimization, also provides us\nnew ways of designing non-uniform and aggressive moving average schemes in\nstochastic optimization. Finally, we present some heuristics that help to\nimplement adaptive accelerated stochastic gradient descent methods and to\nfurther improve their practical performance for machine learning and deep\nlearning.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-10-01T07:07:47Z",
    "updated": "2018-10-01T07:07:47Z",
    "doi": null
  },
  "2102.02808": {
    "id": "http://arxiv.org/abs/2102.02808v2",
    "title": "Multi-Stage Progressive Image Restoration",
    "authors": [
      "Syed Waqas Zamir",
      "Aditya Arora",
      "Salman Khan",
      "Munawar Hayat",
      "Fahad Shahbaz Khan",
      "Ming-Hsuan Yang",
      "Ling Shao"
    ],
    "abstract": "  Image restoration tasks demand a complex balance between spatial details and\nhigh-level contextualized information while recovering images. In this paper,\nwe propose a novel synergistic design that can optimally balance these\ncompeting goals. Our main proposal is a multi-stage architecture, that\nprogressively learns restoration functions for the degraded inputs, thereby\nbreaking down the overall recovery process into more manageable steps.\nSpecifically, our model first learns the contextualized features using\nencoder-decoder architectures and later combines them with a high-resolution\nbranch that retains local information. At each stage, we introduce a novel\nper-pixel adaptive design that leverages in-situ supervised attention to\nreweight the local features. A key ingredient in such a multi-stage\narchitecture is the information exchange between different stages. To this end,\nwe propose a two-faceted approach where the information is not only exchanged\nsequentially from early to late stages, but lateral connections between feature\nprocessing blocks also exist to avoid any loss of information. The resulting\ntightly interlinked multi-stage architecture, named as MPRNet, delivers strong\nperformance gains on ten datasets across a range of tasks including image\nderaining, deblurring, and denoising. The source code and pre-trained models\nare available at https://github.com/swz30/MPRNet.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-02-04T18:57:07Z",
    "updated": "2021-03-16T11:02:52Z",
    "doi": null
  },
  "2309.06180": {
    "id": "http://arxiv.org/abs/2309.06180v1",
    "title": "Efficient Memory Management for Large Language Model Serving with\n  PagedAttention",
    "authors": [
      "Woosuk Kwon",
      "Zhuohan Li",
      "Siyuan Zhuang",
      "Ying Sheng",
      "Lianmin Zheng",
      "Cody Hao Yu",
      "Joseph E. Gonzalez",
      "Hao Zhang",
      "Ion Stoica"
    ],
    "abstract": "  High throughput serving of large language models (LLMs) requires batching\nsufficiently many requests at a time. However, existing systems struggle\nbecause the key-value cache (KV cache) memory for each request is huge and\ngrows and shrinks dynamically. When managed inefficiently, this memory can be\nsignificantly wasted by fragmentation and redundant duplication, limiting the\nbatch size. To address this problem, we propose PagedAttention, an attention\nalgorithm inspired by the classical virtual memory and paging techniques in\noperating systems. On top of it, we build vLLM, an LLM serving system that\nachieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV\ncache within and across requests to further reduce memory usage. Our\nevaluations show that vLLM improves the throughput of popular LLMs by\n2-4$\\times$ with the same level of latency compared to the state-of-the-art\nsystems, such as FasterTransformer and Orca. The improvement is more pronounced\nwith longer sequences, larger models, and more complex decoding algorithms.\nvLLM's source code is publicly available at\nhttps://github.com/vllm-project/vllm\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DC",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-12T12:50:04Z",
    "updated": "2023-09-12T12:50:04Z",
    "doi": null
  },
  "2302.06235": {
    "id": "http://arxiv.org/abs/2302.06235v2",
    "title": "A Simple Zero-shot Prompt Weighting Technique to Improve Prompt\n  Ensembling in Text-Image Models",
    "authors": [
      "James Urquhart Allingham",
      "Jie Ren",
      "Michael W Dusenberry",
      "Xiuye Gu",
      "Yin Cui",
      "Dustin Tran",
      "Jeremiah Zhe Liu",
      "Balaji Lakshminarayanan"
    ],
    "abstract": "  Contrastively trained text-image models have the remarkable ability to\nperform zero-shot classification, that is, classifying previously unseen images\ninto categories that the model has never been explicitly trained to identify.\nHowever, these zero-shot classifiers need prompt engineering to achieve high\naccuracy. Prompt engineering typically requires hand-crafting a set of prompts\nfor individual downstream tasks. In this work, we aim to automate this prompt\nengineering and improve zero-shot accuracy through prompt ensembling. In\nparticular, we ask \"Given a large pool of prompts, can we automatically score\nthe prompts and ensemble those that are most suitable for a particular\ndownstream dataset, without needing access to labeled validation data?\". We\ndemonstrate that this is possible. In doing so, we identify several pathologies\nin a naive prompt scoring method where the score can be easily overconfident\ndue to biases in pre-training and test data, and we propose a novel prompt\nscoring method that corrects for the biases. Using our proposed scoring method\nto create a weighted average prompt ensemble, our method outperforms equal\naverage ensemble, as well as hand-crafted prompts, on ImageNet, 4 of its\nvariants, and 11 fine-grained classification benchmarks, all while being fully\nautomatic, optimization-free, and not requiring access to labeled validation\ndata.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-13T10:19:58Z",
    "updated": "2023-07-15T11:12:59Z",
    "doi": null
  },
  "2209.01578": {
    "id": "http://arxiv.org/abs/2209.01578v2",
    "title": "Spatial-Temporal Transformer for Video Snapshot Compressive Imaging",
    "authors": [
      "Lishun Wang",
      "Miao Cao",
      "Yong Zhong",
      "Xin Yuan"
    ],
    "abstract": "  Video snapshot compressive imaging (SCI) captures multiple sequential video\nframes by a single measurement using the idea of computational imaging. The\nunderlying principle is to modulate high-speed frames through different masks\nand these modulated frames are summed to a single measurement captured by a\nlow-speed 2D sensor (dubbed optical encoder); following this, algorithms are\nemployed to reconstruct the desired high-speed frames (dubbed software decoder)\nif needed. In this paper, we consider the reconstruction algorithm in video\nSCI, i.e., recovering a series of video frames from a compressed measurement.\nSpecifically, we propose a Spatial-Temporal transFormer (STFormer) to exploit\nthe correlation in both spatial and temporal domains. STFormer network is\ncomposed of a token generation block, a video reconstruction block, and these\ntwo blocks are connected by a series of STFormer blocks. Each STFormer block\nconsists of a spatial self-attention branch, a temporal self-attention branch\nand the outputs of these two branches are integrated by a fusion network.\nExtensive results on both simulated and real data demonstrate the\nstate-of-the-art performance of STFormer. The code and models are publicly\navailable at https://github.com/ucaswangls/STFormer.git\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-04T09:24:17Z",
    "updated": "2022-09-08T04:56:25Z",
    "doi": null
  },
  "2401.05998": {
    "id": "http://arxiv.org/abs/2401.05998v1",
    "title": "Combating Adversarial Attacks with Multi-Agent Debate",
    "authors": [
      "Steffi Chern",
      "Zhen Fan",
      "Andy Liu"
    ],
    "abstract": "  While state-of-the-art language models have achieved impressive results, they\nremain susceptible to inference-time adversarial attacks, such as adversarial\nprompts generated by red teams arXiv:2209.07858. One approach proposed to\nimprove the general quality of language model generations is multi-agent\ndebate, where language models self-evaluate through discussion and feedback\narXiv:2305.14325. We implement multi-agent debate between current\nstate-of-the-art language models and evaluate models' susceptibility to red\nteam attacks in both single- and multi-agent settings. We find that multi-agent\ndebate can reduce model toxicity when jailbroken or less capable models are\nforced to debate with non-jailbroken or more capable models. We also find\nmarginal improvements through the general usage of multi-agent interactions. We\nfurther perform adversarial prompt content classification via embedding\nclustering, and analyze the susceptibility of different models to different\ntypes of attack topics.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-11T15:57:38Z",
    "updated": "2024-01-11T15:57:38Z",
    "doi": null
  },
  "1907.01879": {
    "id": "http://arxiv.org/abs/1907.01879v1",
    "title": "Learning to Predict Robot Keypoints Using Artificially Generated Images",
    "authors": [
      "Christoph Heindl",
      "Sebastian Zambal",
      "Josef Scharinger"
    ],
    "abstract": "  This work considers robot keypoint estimation on color images as a supervised\nmachine learning task. We propose the use of probabilistically created\nrenderings to overcome the lack of labeled real images. Rather than sampling\nfrom stationary distributions, our approach introduces a feedback mechanism\nthat constantly adapts probability distributions according to current training\nprogress. Initial results show, our approach achieves near-human-level accuracy\non real images. Additionally, we demonstrate that feedback leads to fewer\nrequired training steps, while maintaining the same model quality on synthetic\ndata sets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-07-03T12:20:16Z",
    "updated": "2019-07-03T12:20:16Z",
    "doi": null
  },
  "2309.14751": {
    "id": "http://arxiv.org/abs/2309.14751v1",
    "title": "Text-image guided Diffusion Model for generating Deepfake celebrity\n  interactions",
    "authors": [
      "Yunzhuo Chen",
      "Nur Al Hasan Haldar",
      "Naveed Akhtar",
      "Ajmal Mian"
    ],
    "abstract": "  Deepfake images are fast becoming a serious concern due to their realism.\nDiffusion models have recently demonstrated highly realistic visual content\ngeneration, which makes them an excellent potential tool for Deepfake\ngeneration. To curb their exploitation for Deepfakes, it is imperative to first\nexplore the extent to which diffusion models can be used to generate realistic\ncontent that is controllable with convenient prompts. This paper devises and\nexplores a novel method in that regard. Our technique alters the popular stable\ndiffusion model to generate a controllable high-quality Deepfake image with\ntext and image prompts. In addition, the original stable model lacks severely\nin generating quality images that contain multiple persons. The modified\ndiffusion model is able to address this problem, it add input anchor image's\nlatent at the beginning of inferencing rather than Gaussian random latent as\ninput. Hence, we focus on generating forged content for celebrity interactions,\nwhich may be used to spread rumors. We also apply Dreambooth to enhance the\nrealism of our fake images. Dreambooth trains the pairing of center words and\nspecific features to produce more refined and personalized output images. Our\nresults show that with the devised scheme, it is possible to create fake visual\ncontent with alarming realism, such that the content can serve as believable\nevidence of meetings between powerful political figures.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-26T08:24:37Z",
    "updated": "2023-09-26T08:24:37Z",
    "doi": null
  },
  "2304.08477": {
    "id": "http://arxiv.org/abs/2304.08477v2",
    "title": "Latent-Shift: Latent Diffusion with Temporal Shift for Efficient\n  Text-to-Video Generation",
    "authors": [
      "Jie An",
      "Songyang Zhang",
      "Harry Yang",
      "Sonal Gupta",
      "Jia-Bin Huang",
      "Jiebo Luo",
      "Xi Yin"
    ],
    "abstract": "  We propose Latent-Shift -- an efficient text-to-video generation method based\non a pretrained text-to-image generation model that consists of an autoencoder\nand a U-Net diffusion model. Learning a video diffusion model in the latent\nspace is much more efficient than in the pixel space. The latter is often\nlimited to first generating a low-resolution video followed by a sequence of\nframe interpolation and super-resolution models, which makes the entire\npipeline very complex and computationally expensive. To extend a U-Net from\nimage generation to video generation, prior work proposes to add additional\nmodules like 1D temporal convolution and/or temporal attention layers. In\ncontrast, we propose a parameter-free temporal shift module that can leverage\nthe spatial U-Net as is for video generation. We achieve this by shifting two\nportions of the feature map channels forward and backward along the temporal\ndimension. The shifted features of the current frame thus receive the features\nfrom the previous and the subsequent frames, enabling motion learning without\nadditional parameters. We show that Latent-Shift achieves comparable or better\nresults while being significantly more efficient. Moreover, Latent-Shift can\ngenerate images despite being finetuned for T2V generation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-17T17:57:06Z",
    "updated": "2023-04-18T03:27:52Z",
    "doi": null
  },
  "1906.01277": {
    "id": "http://arxiv.org/abs/1906.01277v2",
    "title": "Wasserstein Weisfeiler-Lehman Graph Kernels",
    "authors": [
      "Matteo Togninalli",
      "Elisabetta Ghisu",
      "Felipe Llinares-L\u00f3pez",
      "Bastian Rieck",
      "Karsten Borgwardt"
    ],
    "abstract": "  Most graph kernels are an instance of the class of $\\mathcal{R}$-Convolution\nkernels, which measure the similarity of objects by comparing their\nsubstructures. Despite their empirical success, most graph kernels use a naive\naggregation of the final set of substructures, usually a sum or average,\nthereby potentially discarding valuable information about the distribution of\nindividual components. Furthermore, only a limited instance of these approaches\ncan be extended to continuously attributed graphs. We propose a novel method\nthat relies on the Wasserstein distance between the node feature vector\ndistributions of two graphs, which allows to find subtler differences in data\nsets by considering graphs as high-dimensional objects, rather than simple\nmeans. We further propose a Weisfeiler-Lehman inspired embedding scheme for\ngraphs with continuous node attributes and weighted edges, enhance it with the\ncomputed Wasserstein distance, and thus improve the state-of-the-art prediction\nperformance on several graph classification tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "q-bio.MN",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-06-04T08:52:47Z",
    "updated": "2019-10-30T14:25:05Z",
    "doi": null
  },
  "2203.08713": {
    "id": "http://arxiv.org/abs/2203.08713v2",
    "title": "DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation",
    "authors": [
      "Ailing Zeng",
      "Xuan Ju",
      "Lei Yang",
      "Ruiyuan Gao",
      "Xizhou Zhu",
      "Bo Dai",
      "Qiang Xu"
    ],
    "abstract": "  This paper proposes a simple baseline framework for video-based 2D/3D human\npose estimation that can achieve 10 times efficiency improvement over existing\nworks without any performance degradation, named DeciWatch. Unlike current\nsolutions that estimate each frame in a video, DeciWatch introduces a simple\nyet effective sample-denoise-recover framework that only watches sparsely\nsampled frames, taking advantage of the continuity of human motions and the\nlightweight pose representation. Specifically, DeciWatch uniformly samples less\nthan 10% video frames for detailed estimation, denoises the estimated 2D/3D\nposes with an efficient Transformer architecture, and then accurately recovers\nthe rest of the frames using another Transformer-based network. Comprehensive\nexperimental results on three video-based human pose estimation and body mesh\nrecovery tasks with four datasets validate the efficiency and effectiveness of\nDeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-16T16:03:37Z",
    "updated": "2022-07-20T18:02:53Z",
    "doi": null
  },
  "2203.04067": {
    "id": "http://arxiv.org/abs/2203.04067v1",
    "title": "Lane Detection with Versatile AtrousFormer and Local Semantic Guidance",
    "authors": [
      "Jiaxing Yang",
      "Lihe Zhang",
      "Huchuan Lu"
    ],
    "abstract": "  Lane detection is one of the core functions in autonomous driving and has\naroused widespread attention recently. The networks to segment lane instances,\nespecially with bad appearance, must be able to explore lane distribution\nproperties. Most existing methods tend to resort to CNN-based techniques. A few\nhave a try on incorporating the recent adorable, the seq2seq Transformer\n\\cite{transformer}. However, their innate drawbacks of weak global information\ncollection ability and exorbitant computation overhead prohibit a wide range of\nthe further applications. In this work, we propose Atrous Transformer\n(AtrousFormer) to solve the problem. Its variant local AtrousFormer is\ninterleaved into feature extractor to enhance extraction. Their collecting\ninformation first by rows and then by columns in a dedicated manner finally\nequips our network with stronger information gleaning ability and better\ncomputation efficiency. To further improve the performance, we also propose a\nlocal semantic guided decoder to delineate the identities and shapes of lanes\nmore accurately, in which the predicted Gaussian map of the starting point of\neach lane serves to guide the process. Extensive results on three challenging\nbenchmarks (CULane, TuSimple, and BDD100K) show that our network performs\nfavorably against the state of the arts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-08T13:25:35Z",
    "updated": "2022-03-08T13:25:35Z",
    "doi": null
  },
  "2305.18453": {
    "id": "http://arxiv.org/abs/2305.18453v5",
    "title": "Conditional Diffusion Models for Semantic 3D Brain MRI Synthesis",
    "authors": [
      "Zolnamar Dorjsembe",
      "Hsing-Kuo Pao",
      "Sodtavilan Odonchimed",
      "Furen Xiao"
    ],
    "abstract": "  Artificial intelligence (AI) in healthcare, especially in medical imaging,\nfaces challenges due to data scarcity and privacy concerns. Addressing these,\nwe introduce Med-DDPM, a diffusion model designed for 3D semantic brain MRI\nsynthesis. This model effectively tackles data scarcity and privacy issues by\nintegrating semantic conditioning. This involves the channel-wise concatenation\nof a conditioning image to the model input, enabling control in image\ngeneration. Med-DDPM demonstrates superior stability and performance compared\nto existing 3D brain imaging synthesis methods. It generates diverse,\nanatomically coherent images with high visual fidelity. In terms of dice score\naccuracy in the tumor segmentation task, Med-DDPM achieves 0.6207, close to the\n0.6531 accuracy of real images, and outperforms baseline models. Combined with\nreal images, it further increases segmentation accuracy to 0.6675, showing the\npotential of our proposed method for data augmentation. This model represents\nthe first use of a diffusion model in 3D semantic brain MRI synthesis,\nproducing high-quality images. Its semantic conditioning feature also shows\npotential for image anonymization in biomedical imaging, addressing data and\nprivacy issues. We provide the code and model weights for Med-DDPM on our\nGitHub repository (https://github.com/mobaidoctor/med-ddpm/) to support\nreproducibility.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-29T04:14:38Z",
    "updated": "2024-04-19T13:37:18Z",
    "doi": "10.1109/JBHI.2024.3385504"
  },
  "2203.13253": {
    "id": "http://arxiv.org/abs/2203.13253v1",
    "title": "Video Instance Segmentation via Multi-scale Spatio-temporal Split\n  Attention Transformer",
    "authors": [
      "Omkar Thawakar",
      "Sanath Narayan",
      "Jiale Cao",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Muhammad Haris Khan",
      "Salman Khan",
      "Michael Felsberg",
      "Fahad Shahbaz Khan"
    ],
    "abstract": "  State-of-the-art transformer-based video instance segmentation (VIS)\napproaches typically utilize either single-scale spatio-temporal features or\nper-frame multi-scale features during the attention computations. We argue that\nsuch an attention computation ignores the multi-scale spatio-temporal feature\nrelationships that are crucial to tackle target appearance deformations in\nvideos. To address this issue, we propose a transformer-based VIS framework,\nnamed MS-STS VIS, that comprises a novel multi-scale spatio-temporal split\n(MS-STS) attention module in the encoder. The proposed MS-STS module\neffectively captures spatio-temporal feature relationships at multiple scales\nacross frames in a video. We further introduce an attention block in the\ndecoder to enhance the temporal consistency of the detected instances in\ndifferent frames of a video. Moreover, an auxiliary discriminator is introduced\nduring training to ensure better foreground-background separability within the\nmulti-scale spatio-temporal feature space. We conduct extensive experiments on\ntwo benchmarks: Youtube-VIS (2019 and 2021). Our MS-STS VIS achieves\nstate-of-the-art performance on both benchmarks. When using the ResNet50\nbackbone, our MS-STS achieves a mask AP of 50.1 %, outperforming the best\nreported results in literature by 2.7 % and by 4.8 % at higher overlap\nthreshold of AP_75, while being comparable in model size and speed on\nYoutube-VIS 2019 val. set. When using the Swin Transformer backbone, MS-STS VIS\nachieves mask AP of 61.0 % on Youtube-VIS 2019 val. set. Our code and models\nare available at https://github.com/OmkarThawakar/MSSTS-VIS.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-24T17:59:20Z",
    "updated": "2022-03-24T17:59:20Z",
    "doi": null
  },
  "2312.04410": {
    "id": "http://arxiv.org/abs/2312.04410v1",
    "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
    "authors": [
      "Jiayi Guo",
      "Xingqian Xu",
      "Yifan Pu",
      "Zanlin Ni",
      "Chaofei Wang",
      "Manushree Vasu",
      "Shiji Song",
      "Gao Huang",
      "Humphrey Shi"
    ],
    "abstract": "  Recently, diffusion models have made remarkable progress in text-to-image\n(T2I) generation, synthesizing images with high fidelity and diverse contents.\nDespite this advancement, latent space smoothness within diffusion models\nremains largely unexplored. Smooth latent spaces ensure that a perturbation on\nan input latent corresponds to a steady change in the output image. This\nproperty proves beneficial in downstream tasks, including image interpolation,\ninversion, and editing. In this work, we expose the non-smoothness of diffusion\nlatent spaces by observing noticeable visual fluctuations resulting from minor\nlatent variations. To tackle this issue, we propose Smooth Diffusion, a new\ncategory of diffusion models that can be simultaneously high-performing and\nsmooth. Specifically, we introduce Step-wise Variation Regularization to\nenforce the proportion between the variations of an arbitrary input latent and\nthat of the output image is a constant at any diffusion training step. In\naddition, we devise an interpolation standard deviation (ISTD) metric to\neffectively assess the latent space smoothness of a diffusion model. Extensive\nquantitative and qualitative experiments demonstrate that Smooth Diffusion\nstands out as a more desirable solution not only in T2I generation but also\nacross various downstream tasks. Smooth Diffusion is implemented as a\nplug-and-play Smooth-LoRA to work with various community models. Code is\navailable at https://github.com/SHI-Labs/Smooth-Diffusion.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-07T16:26:23Z",
    "updated": "2023-12-07T16:26:23Z",
    "doi": null
  },
  "2311.16465": {
    "id": "http://arxiv.org/abs/2311.16465v1",
    "title": "TextDiffuser-2: Unleashing the Power of Language Models for Text\n  Rendering",
    "authors": [
      "Jingye Chen",
      "Yupan Huang",
      "Tengchao Lv",
      "Lei Cui",
      "Qifeng Chen",
      "Furu Wei"
    ],
    "abstract": "  The diffusion model has been proven a powerful generative model in recent\nyears, yet remains a challenge in generating visual text. Several methods\nalleviated this issue by incorporating explicit text position and content as\nguidance on where and what text to render. However, these methods still suffer\nfrom several drawbacks, such as limited flexibility and automation, constrained\ncapability of layout prediction, and restricted style diversity. In this paper,\nwe present TextDiffuser-2, aiming to unleash the power of language models for\ntext rendering. Firstly, we fine-tune a large language model for layout\nplanning. The large language model is capable of automatically generating\nkeywords for text rendering and also supports layout modification through\nchatting. Secondly, we utilize the language model within the diffusion model to\nencode the position and texts at the line level. Unlike previous methods that\nemployed tight character-level guidance, this approach generates more diverse\ntext images. We conduct extensive experiments and incorporate user studies\ninvolving human participants as well as GPT-4V, validating TextDiffuser-2's\ncapacity to achieve a more rational text layout and generation with enhanced\ndiversity. The code and model will be available at\n\\url{https://aka.ms/textdiffuser-2}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-28T04:02:40Z",
    "updated": "2023-11-28T04:02:40Z",
    "doi": null
  },
  "2209.07521": {
    "id": "http://arxiv.org/abs/2209.07521v2",
    "title": "On-Device Domain Generalization",
    "authors": [
      "Kaiyang Zhou",
      "Yuanhan Zhang",
      "Yuhang Zang",
      "Jingkang Yang",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "abstract": "  We present a systematic study of domain generalization (DG) for tiny neural\nnetworks. This problem is critical to on-device machine learning applications\nbut has been overlooked in the literature where research has been merely\nfocused on large models. Tiny neural networks have much fewer parameters and\nlower complexity and therefore should not be trained the same way as their\nlarge counterparts for DG applications. By conducting extensive experiments, we\nfind that knowledge distillation (KD), a well-known technique for model\ncompression, is much better for tackling the on-device DG problem than\nconventional DG methods. Another interesting observation is that the\nteacher-student gap on out-of-distribution data is bigger than that on\nin-distribution data, which highlights the capacity mismatch issue as well as\nthe shortcoming of KD. We further propose a method called out-of-distribution\nknowledge distillation (OKD) where the idea is to teach the student how the\nteacher handles out-of-distribution data synthesized via disruptive data\naugmentation. Without adding any extra parameter to the model -- hence keeping\nthe deployment cost unchanged -- OKD significantly improves DG performance for\ntiny neural networks in a variety of on-device DG scenarios for image and\nspeech applications. We also contribute a scalable approach for synthesizing\nvisual domain shifts, along with a new suite of DG datasets to complement\nexisting testbeds.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-15T17:59:31Z",
    "updated": "2022-11-08T04:32:30Z",
    "doi": null
  },
  "2406.11617": {
    "id": "http://arxiv.org/abs/2406.11617v1",
    "title": "DELLA-Merging: Reducing Interference in Model Merging through\n  Magnitude-Based Sampling",
    "authors": [
      "Pala Tej Deep",
      "Rishabh Bhardwaj",
      "Soujanya Poria"
    ],
    "abstract": "  With the proliferation of domain-specific models, model merging has emerged\nas a set of techniques that combine the capabilities of multiple models into\none that can multitask without the cost of additional training. In this paper,\nwe propose a new model merging technique, Drop and rEscaLe via sampLing with\nmAgnitude (DELLA-Merging), that employs a novel pruning technique, MAGPRUNE,\nwhich shows significant advantages over DARE and TIES. MAGPRUNE first ranks the\nparameters in order of their magnitude and assigns higher dropout probabilities\n(p) to parameters with lower ranks corresponding to lower magnitudes. To\napproximate the original embeddings, MAGPRUNE employs a rescaling operation on\nthe parameters that survive the random dropping by 1/(1 - p). On three\ndifferent expert models considered for merging (LM, Math, Code) and\ncorresponding benchmark datasets (AlpacaEval, GSM8K, MBPP), DELLA shows an\naverage improvement of 2.4 points over baseline methods employing delta\nparameter pruning (an improvement of 3.6 points over TIES, 1.2 points over\nDARE), and 11.1 points over the no-pruning baseline (TA). We release the source\ncode at: https://github.com/declare-lab/della.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-17T15:02:45Z",
    "updated": "2024-06-17T15:02:45Z",
    "doi": null
  },
  "2304.08483": {
    "id": "http://arxiv.org/abs/2304.08483v1",
    "title": "Text2Performer: Text-Driven Human Video Generation",
    "authors": [
      "Yuming Jiang",
      "Shuai Yang",
      "Tong Liang Koh",
      "Wayne Wu",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "abstract": "  Text-driven content creation has evolved to be a transformative technique\nthat revolutionizes creativity. Here we study the task of text-driven human\nvideo generation, where a video sequence is synthesized from texts describing\nthe appearance and motions of a target performer. Compared to general\ntext-driven video generation, human-centric video generation requires\nmaintaining the appearance of synthesized human while performing complex\nmotions. In this work, we present Text2Performer to generate vivid human videos\nwith articulated motions from texts. Text2Performer has two novel designs: 1)\ndecomposed human representation and 2) diffusion-based motion sampler. First,\nwe decompose the VQVAE latent space into human appearance and pose\nrepresentation in an unsupervised manner by utilizing the nature of human\nvideos. In this way, the appearance is well maintained along the generated\nframes. Then, we propose continuous VQ-diffuser to sample a sequence of pose\nembeddings. Unlike existing VQ-based methods that operate in the discrete\nspace, continuous VQ-diffuser directly outputs the continuous pose embeddings\nfor better motion modeling. Finally, motion-aware masking strategy is designed\nto mask the pose embeddings spatial-temporally to enhance the temporal\ncoherence. Moreover, to facilitate the task of text-driven human video\ngeneration, we contribute a Fashion-Text2Video dataset with manually annotated\naction labels and text descriptions. Extensive experiments demonstrate that\nText2Performer generates high-quality human videos (up to 512x256 resolution)\nwith diverse appearances and flexible motions.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-17T17:59:02Z",
    "updated": "2023-04-17T17:59:02Z",
    "doi": null
  },
  "2204.05991": {
    "id": "http://arxiv.org/abs/2204.05991v2",
    "title": "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression\n  Comprehension",
    "authors": [
      "Sanjay Subramanian",
      "William Merrill",
      "Trevor Darrell",
      "Matt Gardner",
      "Sameer Singh",
      "Anna Rohrbach"
    ],
    "abstract": "  Training a referring expression comprehension (ReC) model for a new visual\ndomain requires collecting referring expressions, and potentially corresponding\nbounding boxes, for images in the domain. While large-scale pre-trained models\nare useful for image classification across domains, it remains unclear if they\ncan be applied in a zero-shot manner to more complex tasks like ReC. We present\nReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a\nstate-of-the-art large-scale model, for ReC. Motivated by the close connection\nbetween ReC and CLIP's contrastive pre-training objective, the first component\nof ReCLIP is a region-scoring method that isolates object proposals via\ncropping and blurring, and passes them to CLIP. However, through controlled\nexperiments on a synthetic dataset, we find that CLIP is largely incapable of\nperforming spatial reasoning off-the-shelf. Thus, the second component of\nReCLIP is a spatial relation resolver that handles several types of spatial\nrelations. We reduce the gap between zero-shot baselines from prior work and\nsupervised models by as much as 29% on RefCOCOg, and on RefGTA (video game\nimagery), ReCLIP's relative improvement over supervised ReC models trained on\nreal images is 8%.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-12T17:55:38Z",
    "updated": "2022-05-02T20:08:17Z",
    "doi": null
  },
  "1906.00377": {
    "id": "http://arxiv.org/abs/1906.00377v1",
    "title": "Hierarchical Video Frame Sequence Representation with Deep Convolutional\n  Graph Network",
    "authors": [
      "Feng Mao",
      "Xiang Wu",
      "Hui Xue",
      "Rong Zhang"
    ],
    "abstract": "  High accuracy video label prediction (classification) models are attributed\nto large scale data. These data could be frame feature sequences extracted by a\npre-trained convolutional-neural-network, which promote the efficiency for\ncreating models. Unsupervised solutions such as feature average pooling, as a\nsimple label-independent parameter-free based method, has limited ability to\nrepresent the video. While the supervised methods, like RNN, can greatly\nimprove the recognition accuracy. However, the video length is usually long,\nand there are hierarchical relationships between frames across events in the\nvideo, the performance of RNN based models are decreased. In this paper, we\nproposes a novel video classification method based on a deep convolutional\ngraph neural network(DCGN). The proposed method utilize the characteristics of\nthe hierarchical structure of the video, and performed multi-level feature\nextraction on the video frame sequence through the graph network, obtained a\nvideo representation re ecting the event semantics hierarchically. We test our\nmodel on YouTube-8M Large-Scale Video Understanding dataset, and the result\noutperforms RNN based benchmarks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-06-02T10:02:39Z",
    "updated": "2019-06-02T10:02:39Z",
    "doi": "10.1007/978-3-030-11018-5_24"
  },
  "2402.06088": {
    "id": "http://arxiv.org/abs/2402.06088v1",
    "title": "Animated Stickers: Bringing Stickers to Life with Video Diffusion",
    "authors": [
      "David Yan",
      "Winnie Zhang",
      "Luxin Zhang",
      "Anmol Kalia",
      "Dingkang Wang",
      "Ankit Ramchandani",
      "Miao Liu",
      "Albert Pumarola",
      "Edgar Schoenfeld",
      "Elliot Blanchard",
      "Krishna Narni",
      "Yaqiao Luo",
      "Lawrence Chen",
      "Guan Pang",
      "Ali Thabet",
      "Peter Vajda",
      "Amy Bearman",
      "Licheng Yu"
    ],
    "abstract": "  We introduce animated stickers, a video diffusion model which generates an\nanimation conditioned on a text prompt and static sticker image. Our model is\nbuilt on top of the state-of-the-art Emu text-to-image model, with the addition\nof temporal layers to model motion. Due to the domain gap, i.e. differences in\nvisual and motion style, a model which performed well on generating natural\nvideos can no longer generate vivid videos when applied to stickers. To bridge\nthis gap, we employ a two-stage finetuning pipeline: first with weakly\nin-domain data, followed by human-in-the-loop (HITL) strategy which we term\nensemble-of-teachers. It distills the best qualities of multiple teachers into\na smaller student model. We show that this strategy allows us to specifically\ntarget improvements to motion quality while maintaining the style from the\nstatic image. With inference optimizations, our model is able to generate an\neight-frame video with high-quality, interesting, and relevant motion in under\none second.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-08T22:49:32Z",
    "updated": "2024-02-08T22:49:32Z",
    "doi": null
  },
  "2107.01358": {
    "id": "http://arxiv.org/abs/2107.01358v1",
    "title": "CInC Flow: Characterizable Invertible 3x3 Convolution",
    "authors": [
      "Sandeep Nagar",
      "Marius Dufraisse",
      "Girish Varma"
    ],
    "abstract": "  Normalizing flows are an essential alternative to GANs for generative\nmodelling, which can be optimized directly on the maximum likelihood of the\ndataset. They also allow computation of the exact latent vector corresponding\nto an image since they are composed of invertible transformations. However, the\nrequirement of invertibility of the transformation prevents standard and\nexpressive neural network models such as CNNs from being directly used.\nEmergent convolutions were proposed to construct an invertible 3$\\times$3 CNN\nlayer using a pair of masked CNN layers, making them inefficient. We study\nconditions such that 3$\\times$3 CNNs are invertible, allowing them to construct\nexpressive normalizing flows. We derive necessary and sufficient conditions on\na padded CNN for it to be invertible. Our conditions for invertibility are\nsimple, can easily be maintained during the training process. Since we require\nonly a single CNN layer for every effective invertible CNN layer, our approach\nis more efficient than emerging convolutions. We also proposed a coupling\nmethod, Quad-coupling. We benchmark our approach and show similar performance\nresults to emergent convolutions while improving the model's efficiency.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-07-03T06:55:24Z",
    "updated": "2021-07-03T06:55:24Z",
    "doi": null
  },
  "2403.07379": {
    "id": "http://arxiv.org/abs/2403.07379v2",
    "title": "Hallmarks of Optimization Trajectories in Neural Networks: Directional\n  Exploration and Redundancy",
    "authors": [
      "Sidak Pal Singh",
      "Bobby He",
      "Thomas Hofmann",
      "Bernhard Sch\u00f6lkopf"
    ],
    "abstract": "  We propose a fresh take on understanding the mechanisms of neural networks by\nanalyzing the rich directional structure of optimization trajectories,\nrepresented by their pointwise parameters. Towards this end, we introduce some\nnatural notions of the complexity of optimization trajectories, both\nqualitative and quantitative, which hallmark the directional nature of\noptimization in neural networks: when is there redundancy, and when\nexploration. We use them to reveal the inherent nuance and interplay involved\nbetween various optimization choices, such as momentum and weight decay.\nFurther, the trajectory perspective helps us see the effect of scale on\nregularizing the directional nature of trajectories, and as a by-product, we\nalso observe an intriguing heterogeneity of Q,K,V dynamics in the middle\nattention layers in LLMs and which is homogenized by scale. Importantly, we put\nthe significant directional redundancy observed to the test by demonstrating\nthat training only scalar batchnorm parameters some while into training matches\nthe performance of training the entire network, which thus exhibits the\npotential of hybrid optimization schemes that are geared towards efficiency.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-12T07:32:47Z",
    "updated": "2024-06-24T04:53:34Z",
    "doi": null
  },
  "1803.05268": {
    "id": "http://arxiv.org/abs/1803.05268v2",
    "title": "Transparency by Design: Closing the Gap Between Performance and\n  Interpretability in Visual Reasoning",
    "authors": [
      "David Mascharka",
      "Philip Tran",
      "Ryan Soklaski",
      "Arjun Majumdar"
    ],
    "abstract": "  Visual question answering requires high-order reasoning about an image, which\nis a fundamental capability needed by machine systems to follow complex\ndirectives. Recently, modular networks have been shown to be an effective\nframework for performing visual reasoning tasks. While modular networks were\ninitially designed with a degree of model transparency, their performance on\ncomplex visual reasoning benchmarks was lacking. Current state-of-the-art\napproaches do not provide an effective mechanism for understanding the\nreasoning process. In this paper, we close the performance gap between\ninterpretable models and state-of-the-art visual reasoning methods. We propose\na set of visual-reasoning primitives which, when composed, manifest as a model\ncapable of performing complex reasoning tasks in an explicitly-interpretable\nmanner. The fidelity and interpretability of the primitives' outputs enable an\nunparalleled ability to diagnose the strengths and weaknesses of the resulting\nmodel. Critically, we show that these primitives are highly performant,\nachieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show\nthat our model is able to effectively learn generalized representations when\nprovided a small amount of data containing novel object attributes. Using the\nCoGenT generalization task, we show more than a 20 percentage point improvement\nover the current state of the art.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-03-14T13:33:06Z",
    "updated": "2018-07-02T18:48:31Z",
    "doi": "10.1109/CVPR.2018.00519"
  },
  "1708.07747": {
    "id": "http://arxiv.org/abs/1708.07747v2",
    "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n  Algorithms",
    "authors": [
      "Han Xiao",
      "Kashif Rasul",
      "Roland Vollgraf"
    ],
    "abstract": "  We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images\nof 70,000 fashion products from 10 categories, with 7,000 images per category.\nThe training set has 60,000 images and the test set has 10,000 images.\nFashion-MNIST is intended to serve as a direct drop-in replacement for the\noriginal MNIST dataset for benchmarking machine learning algorithms, as it\nshares the same image size, data format and the structure of training and\ntesting splits. The dataset is freely available at\nhttps://github.com/zalandoresearch/fashion-mnist\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-08-25T14:01:29Z",
    "updated": "2017-09-15T21:29:49Z",
    "doi": null
  },
  "1606.09549": {
    "id": "http://arxiv.org/abs/1606.09549v3",
    "title": "Fully-Convolutional Siamese Networks for Object Tracking",
    "authors": [
      "Luca Bertinetto",
      "Jack Valmadre",
      "Jo\u00e3o F. Henriques",
      "Andrea Vedaldi",
      "Philip H. S. Torr"
    ],
    "abstract": "  The problem of arbitrary object tracking has traditionally been tackled by\nlearning a model of the object's appearance exclusively online, using as sole\ntraining data the video itself. Despite the success of these methods, their\nonline-only approach inherently limits the richness of the model they can\nlearn. Recently, several attempts have been made to exploit the expressive\npower of deep convolutional networks. However, when the object to track is not\nknown beforehand, it is necessary to perform Stochastic Gradient Descent online\nto adapt the weights of the network, severely compromising the speed of the\nsystem. In this paper we equip a basic tracking algorithm with a novel\nfully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset\nfor object detection in video. Our tracker operates at frame-rates beyond\nreal-time and, despite its extreme simplicity, achieves state-of-the-art\nperformance in multiple benchmarks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-06-30T16:00:43Z",
    "updated": "2021-12-01T19:21:43Z",
    "doi": null
  },
  "2303.17076": {
    "id": "http://arxiv.org/abs/2303.17076v1",
    "title": "DiffCollage: Parallel Generation of Large Content with Diffusion Models",
    "authors": [
      "Qinsheng Zhang",
      "Jiaming Song",
      "Xun Huang",
      "Yongxin Chen",
      "Ming-Yu Liu"
    ],
    "abstract": "  We present DiffCollage, a compositional diffusion model that can generate\nlarge content by leveraging diffusion models trained on generating pieces of\nthe large content. Our approach is based on a factor graph representation where\neach factor node represents a portion of the content and a variable node\nrepresents their overlap. This representation allows us to aggregate\nintermediate outputs from diffusion models defined on individual nodes to\ngenerate content of arbitrary size and shape in parallel without resorting to\nan autoregressive generation procedure. We apply DiffCollage to various tasks,\nincluding infinite image generation, panorama image generation, and\nlong-duration text-guided motion generation. Extensive experimental results\nwith a comparison to strong autoregressive baselines verify the effectiveness\nof our approach.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-30T00:51:12Z",
    "updated": "2023-03-30T00:51:12Z",
    "doi": null
  },
  "2205.11423": {
    "id": "http://arxiv.org/abs/2205.11423v1",
    "title": "Decoder Denoising Pretraining for Semantic Segmentation",
    "authors": [
      "Emmanuel Brempong Asiedu",
      "Simon Kornblith",
      "Ting Chen",
      "Niki Parmar",
      "Matthias Minderer",
      "Mohammad Norouzi"
    ],
    "abstract": "  Semantic segmentation labels are expensive and time consuming to acquire.\nHence, pretraining is commonly used to improve the label-efficiency of\nsegmentation models. Typically, the encoder of a segmentation model is\npretrained as a classifier and the decoder is randomly initialized. Here, we\nargue that random initialization of the decoder can be suboptimal, especially\nwhen few labeled examples are available. We propose a decoder pretraining\napproach based on denoising, which can be combined with supervised pretraining\nof the encoder. We find that decoder denoising pretraining on the ImageNet\ndataset strongly outperforms encoder-only supervised pretraining. Despite its\nsimplicity, decoder denoising pretraining achieves state-of-the-art results on\nlabel-efficient semantic segmentation and offers considerable gains on the\nCityscapes, Pascal Context, and ADE20K datasets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.4.6; I.5.4; I.2.10",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-23T16:08:31Z",
    "updated": "2022-05-23T16:08:31Z",
    "doi": null
  },
  "1603.08575": {
    "id": "http://arxiv.org/abs/1603.08575v3",
    "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
    "authors": [
      "S. M. Ali Eslami",
      "Nicolas Heess",
      "Theophane Weber",
      "Yuval Tassa",
      "David Szepesvari",
      "Koray Kavukcuoglu",
      "Geoffrey E. Hinton"
    ],
    "abstract": "  We present a framework for efficient inference in structured image models\nthat explicitly reason about objects. We achieve this by performing\nprobabilistic inference using a recurrent neural network that attends to scene\nelements and processes them one at a time. Crucially, the model itself learns\nto choose the appropriate number of inference steps. We use this scheme to\nlearn to perform inference in partially specified 2D models (variable-sized\nvariational auto-encoders) and fully specified 3D models (probabilistic\nrenderers). We show that such models learn to identify multiple objects -\ncounting, locating and classifying the elements of a scene - without any\nsupervision, e.g., decomposing 3D images with various numbers of objects in a\nsingle forward pass of a neural network. We further show that the networks\nproduce accurate inferences when compared to supervised counterparts, and that\ntheir structure leads to improved generalization.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-03-28T21:59:08Z",
    "updated": "2016-08-12T16:05:08Z",
    "doi": null
  },
  "2111.10701": {
    "id": "http://arxiv.org/abs/2111.10701v1",
    "title": "Self-Supervised Point Cloud Completion via Inpainting",
    "authors": [
      "Himangi Mittal",
      "Brian Okorn",
      "Arpit Jangid",
      "David Held"
    ],
    "abstract": "  When navigating in urban environments, many of the objects that need to be\ntracked and avoided are heavily occluded. Planning and tracking using these\npartial scans can be challenging. The aim of this work is to learn to complete\nthese partial point clouds, giving us a full understanding of the object's\ngeometry using only partial observations. Previous methods achieve this with\nthe help of complete, ground-truth annotations of the target objects, which are\navailable only for simulated datasets. However, such ground truth is\nunavailable for real-world LiDAR data. In this work, we present a\nself-supervised point cloud completion algorithm, PointPnCNet, which is trained\nonly on partial scans without assuming access to complete, ground-truth\nannotations. Our method achieves this via inpainting. We remove a portion of\nthe input data and train the network to complete the missing region. As it is\ndifficult to determine which regions were occluded in the initial cloud and\nwhich were synthetically removed, our network learns to complete the full\ncloud, including the missing regions in the initial partial cloud. We show that\nour method outperforms previous unsupervised and weakly-supervised methods on\nboth the synthetic dataset, ShapeNet, and real-world LiDAR dataset, Semantic\nKITTI.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-21T00:03:11Z",
    "updated": "2021-11-21T00:03:11Z",
    "doi": null
  },
  "2309.03895": {
    "id": "http://arxiv.org/abs/2309.03895v1",
    "title": "InstructDiffusion: A Generalist Modeling Interface for Vision Tasks",
    "authors": [
      "Zigang Geng",
      "Binxin Yang",
      "Tiankai Hang",
      "Chen Li",
      "Shuyang Gu",
      "Ting Zhang",
      "Jianmin Bao",
      "Zheng Zhang",
      "Han Hu",
      "Dong Chen",
      "Baining Guo"
    ],
    "abstract": "  We present InstructDiffusion, a unifying and generic framework for aligning\ncomputer vision tasks with human instructions. Unlike existing approaches that\nintegrate prior knowledge and pre-define the output space (e.g., categories and\ncoordinates) for each vision task, we cast diverse vision tasks into a\nhuman-intuitive image-manipulating process whose output space is a flexible and\ninteractive pixel space. Concretely, the model is built upon the diffusion\nprocess and is trained to predict pixels according to user instructions, such\nas encircling the man's left shoulder in red or applying a blue mask to the\nleft car. InstructDiffusion could handle a variety of vision tasks, including\nunderstanding tasks (such as segmentation and keypoint detection) and\ngenerative tasks (such as editing and enhancement). It even exhibits the\nability to handle unseen tasks and outperforms prior methods on novel datasets.\nThis represents a significant step towards a generalist modeling interface for\nvision tasks, advancing artificial general intelligence in the field of\ncomputer vision.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-07T17:56:57Z",
    "updated": "2023-09-07T17:56:57Z",
    "doi": null
  },
  "2305.06500": {
    "id": "http://arxiv.org/abs/2305.06500v2",
    "title": "InstructBLIP: Towards General-purpose Vision-Language Models with\n  Instruction Tuning",
    "authors": [
      "Wenliang Dai",
      "Junnan Li",
      "Dongxu Li",
      "Anthony Meng Huat Tiong",
      "Junqi Zhao",
      "Weisheng Wang",
      "Boyang Li",
      "Pascale Fung",
      "Steven Hoi"
    ],
    "abstract": "  Large-scale pre-training and instruction tuning have been successful at\ncreating general-purpose language models with broad competence. However,\nbuilding general-purpose vision-language models is challenging due to the rich\ninput distributions and task diversity resulting from the additional visual\ninput. Although vision-language pretraining has been widely studied,\nvision-language instruction tuning remains under-explored. In this paper, we\nconduct a systematic and comprehensive study on vision-language instruction\ntuning based on the pretrained BLIP-2 models. We gather 26 publicly available\ndatasets, covering a wide variety of tasks and capabilities, and transform them\ninto instruction tuning format. Additionally, we introduce an instruction-aware\nQuery Transformer, which extracts informative features tailored to the given\ninstruction. Trained on 13 held-in datasets, InstructBLIP attains\nstate-of-the-art zero-shot performance across all 13 held-out datasets,\nsubstantially outperforming BLIP-2 and larger Flamingo models. Our models also\nlead to state-of-the-art performance when finetuned on individual downstream\ntasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts).\nFurthermore, we qualitatively demonstrate the advantages of InstructBLIP over\nconcurrent multimodal models. All InstructBLIP models are open-sourced at\nhttps://github.com/salesforce/LAVIS/tree/main/projects/instructblip.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-11T00:38:10Z",
    "updated": "2023-06-15T08:00:18Z",
    "doi": null
  },
  "2005.08575": {
    "id": "http://arxiv.org/abs/2005.08575v5",
    "title": "Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio\n  Representation",
    "authors": [
      "Po-Han Chi",
      "Pei-Hung Chung",
      "Tsung-Han Wu",
      "Chun-Cheng Hsieh",
      "Yen-Hao Chen",
      "Shang-Wen Li",
      "Hung-yi Lee"
    ],
    "abstract": "  For self-supervised speech processing, it is crucial to use pretrained models\nas speech representation extractors. In recent works, increasing the size of\nthe model has been utilized in acoustic model training in order to achieve\nbetter performance. In this paper, we propose Audio ALBERT, a lite version of\nthe self-supervised speech representation model. We use the representations\nwith two downstream tasks, speaker identification, and phoneme classification.\nWe show that Audio ALBERT is capable of achieving competitive performance with\nthose huge models in the downstream tasks while utilizing 91\\% fewer\nparameters. Moreover, we use some simple probing models to measure how much the\ninformation of the speaker and phoneme is encoded in latent representations. In\nprobing experiments, we find that the latent representations encode richer\ninformation of both phoneme and speaker than that of the last layer.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-05-18T10:42:44Z",
    "updated": "2021-05-03T09:33:31Z",
    "doi": null
  },
  "1509.06825": {
    "id": "http://arxiv.org/abs/1509.06825v1",
    "title": "Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700\n  Robot Hours",
    "authors": [
      "Lerrel Pinto",
      "Abhinav Gupta"
    ],
    "abstract": "  Current learning-based robot grasping approaches exploit human-labeled\ndatasets for training the models. However, there are two problems with such a\nmethodology: (a) since each object can be grasped in multiple ways, manually\nlabeling grasp locations is not a trivial task; (b) human labeling is biased by\nsemantics. While there have been attempts to train robots using trial-and-error\nexperiments, the amount of data used in such experiments remains substantially\nlow and hence makes the learner prone to over-fitting. In this paper, we take\nthe leap of increasing the available training data to 40 times more than prior\nwork, leading to a dataset size of 50K data points collected over 700 hours of\nrobot grasping attempts. This allows us to train a Convolutional Neural Network\n(CNN) for the task of predicting grasp locations without severe overfitting. In\nour formulation, we recast the regression problem to an 18-way binary\nclassification over image patches. We also present a multi-stage learning\napproach where a CNN trained in one stage is used to collect hard negatives in\nsubsequent stages. Our experiments clearly show the benefit of using\nlarge-scale datasets (and multi-stage training) for the task of grasping. We\nalso compare to several baselines and show state-of-the-art performance on\ngeneralization to unseen objects for grasping.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2015-09-23T02:08:02Z",
    "updated": "2015-09-23T02:08:02Z",
    "doi": null
  },
  "2406.04324": {
    "id": "http://arxiv.org/abs/2406.04324v1",
    "title": "SF-V: Single Forward Video Generation Model",
    "authors": [
      "Zhixing Zhang",
      "Yanyu Li",
      "Yushu Wu",
      "Yanwu Xu",
      "Anil Kag",
      "Ivan Skorokhodov",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Junli Cao",
      "Dimitris Metaxas",
      "Sergey Tulyakov",
      "Jian Ren"
    ],
    "abstract": "  Diffusion-based video generation models have demonstrated remarkable success\nin obtaining high-fidelity videos through the iterative denoising process.\nHowever, these models require multiple denoising steps during sampling,\nresulting in high computational costs. In this work, we propose a novel\napproach to obtain single-step video generation models by leveraging\nadversarial training to fine-tune pre-trained video diffusion models. We show\nthat, through the adversarial training, the multi-steps video diffusion model,\ni.e., Stable Video Diffusion (SVD), can be trained to perform single forward\npass to synthesize high-quality videos, capturing both temporal and spatial\ndependencies in the video data. Extensive experiments demonstrate that our\nmethod achieves competitive generation quality of synthesized videos with\nsignificantly reduced computational overhead for the denoising process (i.e.,\naround $23\\times$ speedup compared with SVD and $6\\times$ speedup compared with\nexisting works, with even better generation quality), paving the way for\nreal-time video synthesis and editing. More visualization results are made\npublicly available at https://snap-research.github.io/SF-V.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-06T17:58:27Z",
    "updated": "2024-06-06T17:58:27Z",
    "doi": null
  },
  "2305.06161": {
    "id": "http://arxiv.org/abs/2305.06161v2",
    "title": "StarCoder: may the source be with you!",
    "authors": [
      "Raymond Li",
      "Loubna Ben Allal",
      "Yangtian Zi",
      "Niklas Muennighoff",
      "Denis Kocetkov",
      "Chenghao Mou",
      "Marc Marone",
      "Christopher Akiki",
      "Jia Li",
      "Jenny Chim",
      "Qian Liu",
      "Evgenii Zheltonozhskii",
      "Terry Yue Zhuo",
      "Thomas Wang",
      "Olivier Dehaene",
      "Mishig Davaadorj",
      "Joel Lamy-Poirier",
      "Jo\u00e3o Monteiro",
      "Oleh Shliazhko",
      "Nicolas Gontier",
      "Nicholas Meade",
      "Armel Zebaze",
      "Ming-Ho Yee",
      "Logesh Kumar Umapathi",
      "Jian Zhu",
      "Benjamin Lipkin",
      "Muhtasham Oblokulov",
      "Zhiruo Wang",
      "Rudra Murthy",
      "Jason Stillerman",
      "Siva Sankalp Patel",
      "Dmitry Abulkhanov",
      "Marco Zocca",
      "Manan Dey",
      "Zhihan Zhang",
      "Nour Fahmy",
      "Urvashi Bhattacharyya",
      "Wenhao Yu",
      "Swayam Singh",
      "Sasha Luccioni",
      "Paulo Villegas",
      "Maxim Kunakov",
      "Fedor Zhdanov",
      "Manuel Romero",
      "Tony Lee",
      "Nadav Timor",
      "Jennifer Ding",
      "Claire Schlesinger",
      "Hailey Schoelkopf",
      "Jan Ebert",
      "Tri Dao",
      "Mayank Mishra",
      "Alex Gu",
      "Jennifer Robinson",
      "Carolyn Jane Anderson",
      "Brendan Dolan-Gavitt",
      "Danish Contractor",
      "Siva Reddy",
      "Daniel Fried",
      "Dzmitry Bahdanau",
      "Yacine Jernite",
      "Carlos Mu\u00f1oz Ferrandis",
      "Sean Hughes",
      "Thomas Wolf",
      "Arjun Guha",
      "Leandro von Werra",
      "Harm de Vries"
    ],
    "abstract": "  The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-09T08:16:42Z",
    "updated": "2023-12-13T14:44:10Z",
    "doi": null
  },
  "2210.14868": {
    "id": "http://arxiv.org/abs/2210.14868v3",
    "title": "Multi-lingual Evaluation of Code Generation Models",
    "authors": [
      "Ben Athiwaratkun",
      "Sanjay Krishna Gouda",
      "Zijian Wang",
      "Xiaopeng Li",
      "Yuchen Tian",
      "Ming Tan",
      "Wasi Uddin Ahmad",
      "Shiqi Wang",
      "Qing Sun",
      "Mingyue Shang",
      "Sujan Kumar Gonugondla",
      "Hantian Ding",
      "Varun Kumar",
      "Nathan Fulton",
      "Arash Farahani",
      "Siddhartha Jain",
      "Robert Giaquinto",
      "Haifeng Qian",
      "Murali Krishna Ramanathan",
      "Ramesh Nallapati",
      "Baishakhi Ray",
      "Parminder Bhatia",
      "Sudipta Sengupta",
      "Dan Roth",
      "Bing Xiang"
    ],
    "abstract": "  We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-26T17:17:06Z",
    "updated": "2023-03-28T19:02:34Z",
    "doi": null
  },
  "2408.12340": {
    "id": "http://arxiv.org/abs/2408.12340v2",
    "title": "VTON-HandFit: Virtual Try-on for Arbitrary Hand Pose Guided by Hand\n  Priors Embedding",
    "authors": [
      "Yujie Liang",
      "Xiaobin Hu",
      "Boyuan Jiang",
      "Donghao Luo",
      "Kai WU",
      "Wenhui Han",
      "Taisong Jin",
      "Chengjie Wang"
    ],
    "abstract": "  Although diffusion-based image virtual try-on has made considerable progress,\nemerging approaches still struggle to effectively address the issue of hand\nocclusion (i.e., clothing regions occluded by the hand part), leading to a\nnotable degradation of the try-on performance. To tackle this issue widely\nexisting in real-world scenarios, we propose VTON-HandFit, leveraging the power\nof hand priors to reconstruct the appearance and structure for hand occlusion\ncases. Firstly, we tailor a Handpose Aggregation Net using the ControlNet-based\nstructure explicitly and adaptively encoding the global hand and pose priors.\nBesides, to fully exploit the hand-related structure and appearance\ninformation, we propose Hand-feature Disentanglement Embedding module to\ndisentangle the hand priors into the hand structure-parametric and\nvisual-appearance features, and customize a masked cross attention for further\ndecoupled feature embedding. Lastly, we customize a hand-canny constraint loss\nto better learn the structure edge knowledge from the hand template of model\nimage. VTON-HandFit outperforms the baselines in qualitative and quantitative\nevaluations on the public dataset and our self-collected hand-occlusion\nHandfit-3K dataset particularly for the arbitrary hand pose occlusion cases in\nreal-world scenarios. The Code and dataset will be available at\n\\url{https://github.com/VTON-HandFit/VTON-HandFit}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-22T12:36:10Z",
    "updated": "2024-08-27T02:53:37Z",
    "doi": null
  },
  "2406.04806": {
    "id": "http://arxiv.org/abs/2406.04806v4",
    "title": "Streaming Diffusion Policy: Fast Policy Synthesis with Variable Noise\n  Diffusion Models",
    "authors": [
      "Sigmund H. H\u00f8eg",
      "Yilun Du",
      "Olav Egeland"
    ],
    "abstract": "  Diffusion models have seen rapid adoption in robotic imitation learning,\nenabling autonomous execution of complex dexterous tasks. However, action\nsynthesis is often slow, requiring many steps of iterative denoising, limiting\nthe extent to which models can be used in tasks that require fast reactive\npolicies. To sidestep this, recent works have explored how the distillation of\nthe diffusion process can be used to accelerate policy synthesis. However,\ndistillation is computationally expensive and can hurt both the accuracy and\ndiversity of synthesized actions. We propose SDP (Streaming Diffusion Policy),\nan alternative method to accelerate policy synthesis, leveraging the insight\nthat generating a partially denoised action trajectory is substantially faster\nthan a full output action trajectory. At each observation, our approach outputs\na partially denoised action trajectory with variable levels of noise\ncorruption, where the immediate action to execute is noise-free, with\nsubsequent actions having increasing levels of noise and uncertainty. The\npartially denoised action trajectory for a new observation can then be quickly\ngenerated by applying a few steps of denoising to the previously predicted\nnoisy action trajectory (rolled over by one timestep). We illustrate the\nefficacy of this approach, dramatically speeding up policy synthesis while\npreserving performance across both simulated and real-world settings.\n",
    "categories": [
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-07T10:13:44Z",
    "updated": "2024-10-11T16:04:49Z",
    "doi": null
  },
  "2408.12483": {
    "id": "http://arxiv.org/abs/2408.12483v1",
    "title": "Not All Samples Should Be Utilized Equally: Towards Understanding and\n  Improving Dataset Distillation",
    "authors": [
      "Shaobo Wang",
      "Yantai Yang",
      "Qilong Wang",
      "Kaixin Li",
      "Linfeng Zhang",
      "Junchi Yan"
    ],
    "abstract": "  Dataset Distillation (DD) aims to synthesize a small dataset capable of\nperforming comparably to the original dataset. Despite the success of numerous\nDD methods, theoretical exploration of this area remains unaddressed. In this\npaper, we take an initial step towards understanding various matching-based DD\nmethods from the perspective of sample difficulty. We begin by empirically\nexamining sample difficulty, measured by gradient norm, and observe that\ndifferent matching-based methods roughly correspond to specific difficulty\ntendencies. We then extend the neural scaling laws of data pruning to DD to\ntheoretically explain these matching-based methods. Our findings suggest that\nprioritizing the synthesis of easier samples from the original dataset can\nenhance the quality of distilled datasets, especially in low IPC\n(image-per-class) settings. Based on our empirical observations and theoretical\nanalysis, we introduce the Sample Difficulty Correction (SDC) approach,\ndesigned to predominantly generate easier samples to achieve higher dataset\nquality. Our SDC can be seamlessly integrated into existing methods as a plugin\nwith minimal code adjustments. Experimental results demonstrate that adding SDC\ngenerates higher-quality distilled datasets across 7 distillation methods and 6\ndatasets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-22T15:20:32Z",
    "updated": "2024-08-22T15:20:32Z",
    "doi": null
  },
  "2306.02741": {
    "id": "http://arxiv.org/abs/2306.02741v1",
    "title": "ZIGNeRF: Zero-shot 3D Scene Representation with Invertible Generative\n  Neural Radiance Fields",
    "authors": [
      "Kanghyeok Ko",
      "Minhyeok Lee"
    ],
    "abstract": "  Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable\nproficiency in synthesizing multi-view images by learning the distribution of a\nset of unposed images. Despite the aptitude of existing generative NeRFs in\ngenerating 3D-consistent high-quality random samples within data distribution,\nthe creation of a 3D representation of a singular input image remains a\nformidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative\nmodel that executes zero-shot Generative Adversarial Network (GAN) inversion\nfor the generation of multi-view images from a single out-of-domain image. The\nmodel is underpinned by a novel inverter that maps out-of-domain images into\nthe latent code of the generator manifold. Notably, ZIGNeRF is capable of\ndisentangling the object from the background and executing 3D operations such\nas 360-degree rotation or depth and horizontal translation. The efficacy of our\nmodel is validated using multiple real-image datasets: Cats, AFHQ, CelebA,\nCelebA-HQ, and CompCars.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-05T09:41:51Z",
    "updated": "2023-06-05T09:41:51Z",
    "doi": null
  },
  "2307.00522": {
    "id": "http://arxiv.org/abs/2307.00522v1",
    "title": "LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance",
    "authors": [
      "Linoy Tsaban",
      "Apolin\u00e1rio Passos"
    ],
    "abstract": "  Recent large-scale text-guided diffusion models provide powerful\nimage-generation capabilities. Currently, a significant effort is given to\nenable the modification of these images using text only as means to offer\nintuitive and versatile editing. However, editing proves to be difficult for\nthese generative models due to the inherent nature of editing techniques, which\ninvolves preserving certain content from the original image. Conversely, in\ntext-based models, even minor modifications to the text prompt frequently\nresult in an entirely distinct result, making attaining one-shot generation\nthat accurately corresponds to the users intent exceedingly challenging. In\naddition, to edit a real image using these state-of-the-art tools, one must\nfirst invert the image into the pre-trained models domain - adding another\nfactor affecting the edit quality, as well as latency. In this exploratory\nreport, we propose LEDITS - a combined lightweight approach for real-image\nediting, incorporating the Edit Friendly DDPM inversion technique with Semantic\nGuidance, thus extending Semantic Guidance to real image editing, while\nharnessing the editing capabilities of DDPM inversion as well. This approach\nachieves versatile edits, both subtle and extensive as well as alterations in\ncomposition and style, while requiring no optimization nor extensions to the\narchitecture.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-02T09:11:09Z",
    "updated": "2023-07-02T09:11:09Z",
    "doi": null
  },
  "2404.17774": {
    "id": "http://arxiv.org/abs/2404.17774v2",
    "title": "High-quality Surface Reconstruction using Gaussian Surfels",
    "authors": [
      "Pinxuan Dai",
      "Jiamin Xu",
      "Wenxiang Xie",
      "Xinguo Liu",
      "Huamin Wang",
      "Weiwei Xu"
    ],
    "abstract": "  We propose a novel point-based representation, Gaussian surfels, to combine\nthe advantages of the flexible optimization procedure in 3D Gaussian points and\nthe surface alignment property of surfels. This is achieved by directly setting\nthe z-scale of 3D Gaussian points to 0, effectively flattening the original 3D\nellipsoid into a 2D ellipse. Such a design provides clear guidance to the\noptimizer. By treating the local z-axis as the normal direction, it greatly\nimproves optimization stability and surface alignment. While the derivatives to\nthe local z-axis computed from the covariance matrix are zero in this setting,\nwe design a self-supervised normal-depth consistency loss to remedy this issue.\nMonocular normal priors and foreground masks are incorporated to enhance the\nquality of the reconstruction, mitigating issues related to highlights and\nbackground. We propose a volumetric cutting method to aggregate the information\nof Gaussian surfels so as to remove erroneous points in depth maps generated by\nalpha blending. Finally, we apply screened Poisson reconstruction method to the\nfused depth maps to extract the surface mesh. Experimental results show that\nour method demonstrates superior performance in surface reconstruction compared\nto state-of-the-art neural volume rendering and point-based rendering methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-27T04:13:39Z",
    "updated": "2024-04-30T01:53:27Z",
    "doi": null
  },
  "2404.05674": {
    "id": "http://arxiv.org/abs/2404.05674v1",
    "title": "MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation",
    "authors": [
      "Kunpeng Song",
      "Yizhe Zhu",
      "Bingchen Liu",
      "Qing Yan",
      "Ahmed Elgammal",
      "Xiao Yang"
    ],
    "abstract": "  In this paper, we present MoMA: an open-vocabulary, training-free\npersonalized image model that boasts flexible zero-shot capabilities. As\nfoundational text-to-image models rapidly evolve, the demand for robust\nimage-to-image translation grows. Addressing this need, MoMA specializes in\nsubject-driven personalized image generation. Utilizing an open-source,\nMultimodal Large Language Model (MLLM), we train MoMA to serve a dual role as\nboth a feature extractor and a generator. This approach effectively synergizes\nreference image and text prompt information to produce valuable image features,\nfacilitating an image diffusion model. To better leverage the generated\nfeatures, we further introduce a novel self-attention shortcut method that\nefficiently transfers image features to an image diffusion model, improving the\nresemblance of the target object in generated images. Remarkably, as a\ntuning-free plug-and-play module, our model requires only a single reference\nimage and outperforms existing methods in generating images with high detail\nfidelity, enhanced identity-preservation and prompt faithfulness. Our work is\nopen-source, thereby providing universal access to these advancements.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-08T16:55:49Z",
    "updated": "2024-04-08T16:55:49Z",
    "doi": null
  },
  "2108.06152": {
    "id": "http://arxiv.org/abs/2108.06152v3",
    "title": "Conditional DETR for Fast Training Convergence",
    "authors": [
      "Depu Meng",
      "Xiaokang Chen",
      "Zejia Fan",
      "Gang Zeng",
      "Houqiang Li",
      "Yuhui Yuan",
      "Lei Sun",
      "Jingdong Wang"
    ],
    "abstract": "  The recently-developed DETR approach applies the transformer encoder and\ndecoder architecture to object detection and achieves promising performance. In\nthis paper, we handle the critical issue, slow training convergence, and\npresent a conditional cross-attention mechanism for fast DETR training. Our\napproach is motivated by that the cross-attention in DETR relies highly on the\ncontent embeddings for localizing the four extremities and predicting the box,\nwhich increases the need for high-quality content embeddings and thus the\ntraining difficulty. Our approach, named conditional DETR, learns a conditional\nspatial query from the decoder embedding for decoder multi-head\ncross-attention. The benefit is that through the conditional spatial query,\neach cross-attention head is able to attend to a band containing a distinct\nregion, e.g., one object extremity or a region inside the object box. This\nnarrows down the spatial range for localizing the distinct regions for object\nclassification and box regression, thus relaxing the dependence on the content\nembeddings and easing the training. Empirical results show that conditional\nDETR converges 6.7x faster for the backbones R50 and R101 and 10x faster for\nstronger backbones DC5-R50 and DC5-R101. Code is available at\nhttps://github.com/Atten4Vis/ConditionalDETR.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-08-13T10:07:46Z",
    "updated": "2023-09-29T13:21:57Z",
    "doi": null
  },
  "2207.10075": {
    "id": "http://arxiv.org/abs/2207.10075v2",
    "title": "Is an Object-Centric Video Representation Beneficial for Transfer?",
    "authors": [
      "Chuhan Zhang",
      "Ankush Gupta",
      "Andrew Zisserman"
    ],
    "abstract": "  The objective of this work is to learn an object-centric video\nrepresentation, with the aim of improving transferability to novel tasks, i.e.,\ntasks different from the pre-training task of action classification. To this\nend, we introduce a new object-centric video recognition model based on a\ntransformer architecture. The model learns a set of object-centric summary\nvectors for the video, and uses these vectors to fuse the visual and\nspatio-temporal trajectory 'modalities' of the video clip. We also introduce a\nnovel trajectory contrast loss to further enhance objectness in these summary\nvectors. With experiments on four datasets -- SomethingSomething-V2,\nSomethingElse, Action Genome and EpicKitchens -- we show that the\nobject-centric model outperforms prior video representations (both\nobject-agnostic and object-aware), when: (1) classifying actions on unseen\nobjects and unseen environments; (2) low-shot learning of novel classes; (3)\nlinear probe to other downstream tasks; as well as (4) for standard action\nclassification.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-20T17:59:44Z",
    "updated": "2022-10-08T18:19:42Z",
    "doi": null
  },
  "2210.04802": {
    "id": "http://arxiv.org/abs/2210.04802v2",
    "title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in\n  Fine-tuned Source Code Models",
    "authors": [
      "Hossein Hajipour",
      "Ning Yu",
      "Cristian-Alexandru Staicu",
      "Mario Fritz"
    ],
    "abstract": "  Large code datasets have become increasingly accessible for pre-training\nsource code models. However, for the fine-tuning phase, obtaining\nrepresentative training data that fully covers the code distribution for\nspecific downstream tasks remains challenging due to the task-specific nature\nand limited labeling resources. Moreover, fine-tuning pretrained models can\nresult in forgetting previously acquired pre-training knowledge. These lead to\nout-of-distribution (OOD) generalization issues with unexpected model inference\nbehaviors that have not been systematically studied yet. In this paper, we\ncontribute the first systematic approach that simulates various OOD scenarios\nalong different dimensions of source code data properties and study the\nfine-tuned model behaviors in such scenarios. We investigate the behaviors of\nmodels under different fine-tuning methodologies, including full fine-tuning\nand Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis,\nconducted on four state-of-the-art pretrained models and applied to two code\ngeneration tasks, exposes multiple failure modes attributed to OOD\ngeneralization issues. Additionally, our analysis uncovers that LoRA\nfine-tuning consistently exhibits significantly better OOD generalization\nperformance than full fine-tuning across various scenarios.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-10T16:07:24Z",
    "updated": "2023-10-30T14:25:57Z",
    "doi": null
  },
  "2110.09408": {
    "id": "http://arxiv.org/abs/2110.09408v3",
    "title": "HRFormer: High-Resolution Transformer for Dense Prediction",
    "authors": [
      "Yuhui Yuan",
      "Rao Fu",
      "Lang Huang",
      "Weihong Lin",
      "Chao Zhang",
      "Xilin Chen",
      "Jingdong Wang"
    ],
    "abstract": "  We present a High-Resolution Transformer (HRFormer) that learns\nhigh-resolution representations for dense prediction tasks, in contrast to the\noriginal Vision Transformer that produces low-resolution representations and\nhas high memory and computational cost. We take advantage of the\nmulti-resolution parallel design introduced in high-resolution convolutional\nnetworks (HRNet), along with local-window self-attention that performs\nself-attention over small non-overlapping image windows, for improving the\nmemory and computation efficiency. In addition, we introduce a convolution into\nthe FFN to exchange information across the disconnected image windows. We\ndemonstrate the effectiveness of the High-Resolution Transformer on both human\npose estimation and semantic segmentation tasks, e.g., HRFormer outperforms\nSwin transformer by $1.3$ AP on COCO pose estimation with $50\\%$ fewer\nparameters and $30\\%$ fewer FLOPs. Code is available at:\nhttps://github.com/HRNet/HRFormer.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-18T15:37:58Z",
    "updated": "2021-11-07T14:39:41Z",
    "doi": null
  },
  "1907.03395": {
    "id": "http://arxiv.org/abs/1907.03395v2",
    "title": "Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and\n  Graph Attention Networks",
    "authors": [
      "Vineet Kosaraju",
      "Amir Sadeghian",
      "Roberto Mart\u00edn-Mart\u00edn",
      "Ian Reid",
      "S. Hamid Rezatofighi",
      "Silvio Savarese"
    ],
    "abstract": "  Predicting the future trajectories of multiple interacting agents in a scene\nhas become an increasingly important problem for many different applications\nranging from control of autonomous vehicles and social robots to security and\nsurveillance. This problem is compounded by the presence of social interactions\nbetween humans and their physical interactions with the scene. While the\nexisting literature has explored some of these cues, they mainly ignored the\nmultimodal nature of each human's future trajectory. In this paper, we present\nSocial-BiGAT, a graph-based generative adversarial network that generates\nrealistic, multimodal trajectory predictions by better modelling the social\ninteractions of pedestrians in a scene. Our method is based on a graph\nattention network (GAT) that learns reliable feature representations that\nencode the social interactions between humans in the scene, and a recurrent\nencoder-decoder architecture that is trained adversarially to predict, based on\nthe features, the humans' paths. We explicitly account for the multimodal\nnature of the prediction problem by forming a reversible transformation between\neach scene and its latent noise vector, as in Bicycle-GAN. We show that our\nframework achieves state-of-the-art performance comparing it to several\nbaselines on existing trajectory forecasting benchmarks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-07-04T23:48:07Z",
    "updated": "2019-07-17T01:05:26Z",
    "doi": null
  },
  "1809.03627": {
    "id": "http://arxiv.org/abs/1809.03627v2",
    "title": "ClusterGAN : Latent Space Clustering in Generative Adversarial Networks",
    "authors": [
      "Sudipto Mukherjee",
      "Himanshu Asnani",
      "Eugene Lin",
      "Sreeram Kannan"
    ],
    "abstract": "  Generative Adversarial networks (GANs) have obtained remarkable success in\nmany unsupervised learning tasks and unarguably, clustering is an important\nunsupervised learning problem. While one can potentially exploit the\nlatent-space back-projection in GANs to cluster, we demonstrate that the\ncluster structure is not retained in the GAN latent space.\n  In this paper, we propose ClusterGAN as a new mechanism for clustering using\nGANs. By sampling latent variables from a mixture of one-hot encoded variables\nand continuous latent variables, coupled with an inverse network (which\nprojects the data to the latent space) trained jointly with a clustering\nspecific loss, we are able to achieve clustering in the latent space. Our\nresults show a remarkable phenomenon that GANs can preserve latent space\ninterpolation across categories, even though the discriminator is never exposed\nto such vectors. We compare our results with various clustering baselines and\ndemonstrate superior performance on both synthetic and real datasets.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-09-10T23:00:37Z",
    "updated": "2019-01-26T23:28:35Z",
    "doi": null
  },
  "2101.00529": {
    "id": "http://arxiv.org/abs/2101.00529v2",
    "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
    "authors": [
      "Pengchuan Zhang",
      "Xiujun Li",
      "Xiaowei Hu",
      "Jianwei Yang",
      "Lei Zhang",
      "Lijuan Wang",
      "Yejin Choi",
      "Jianfeng Gao"
    ],
    "abstract": "  This paper presents a detailed study of improving visual representations for\nvision language (VL) tasks and develops an improved object detection model to\nprovide object-centric representations of images. Compared to the most widely\nused \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new\nmodel is bigger, better-designed for VL tasks, and pre-trained on much larger\ntraining corpora that combine multiple public annotated object detection\ndatasets. Therefore, it can generate representations of a richer collection of\nvisual objects and concepts. While previous VL research focuses mainly on\nimproving the vision-language fusion model and leaves the object detection\nmodel improvement untouched, we show that visual features matter significantly\nin VL models. In our experiments we feed the visual features generated by the\nnew object detection model into a Transformer-based VL fusion model \\oscar\n\\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the\nVL model and fine-tune it on a wide range of downstream VL tasks. Our results\nshow that the new visual features significantly improve the performance across\nall VL tasks, creating new state-of-the-art results on seven public benchmarks.\nWe will release the new object detection model to public.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-01-02T23:35:27Z",
    "updated": "2021-03-10T01:27:16Z",
    "doi": null
  },
  "2106.11251": {
    "id": "http://arxiv.org/abs/2106.11251v2",
    "title": "Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval",
    "authors": [
      "Xiao Wang",
      "Craig Macdonald",
      "Nicola Tonellotto",
      "Iadh Ounis"
    ],
    "abstract": "  Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,\nhave shown the usefulness of expanding and reweighting the users' initial\nqueries using information occurring in an initial set of retrieved documents,\nknown as the pseudo-relevant set. Recently, dense retrieval -- through the use\nof neural contextual language models such as BERT for analysing the documents'\nand queries' contents and computing their relevance scores -- has shown a\npromising performance on several information retrieval tasks still relying on\nthe traditional inverted index for identifying documents relevant to a query.\nTwo different dense retrieval families have emerged: the use of single embedded\nrepresentations for each passage and query (e.g. using BERT's [CLS] token), or\nvia multiple representations (e.g. using an embedding for each token of the\nquery and document). In this work, we conduct the first study into the\npotential for multiple representation dense retrieval to be enhanced using\npseudo-relevance feedback. In particular, based on the pseudo-relevant set of\ndocuments identified using a first-pass dense retrieval, we extract\nrepresentative feedback embeddings (using KMeans clustering) -- while ensuring\nthat these embeddings discriminate among passages (based on IDF) -- which are\nthen added to the query representation. These additional feedback embeddings\nare shown to both enhance the effectiveness of a reranking as well as an\nadditional dense retrieval operation. Indeed, experiments on the MSMARCO\npassage ranking dataset show that MAP can be improved by upto 26% on the TREC\n2019 query set and 10% on the TREC 2020 query set by the application of our\nproposed ColBERT-PRF method on a ColBERT dense retrieval approach.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-21T16:49:03Z",
    "updated": "2021-07-01T14:02:56Z",
    "doi": "10.1145/3471158.3472250"
  },
  "2105.09377": {
    "id": "http://arxiv.org/abs/2105.09377v1",
    "title": "Pure Tensor Program Rewriting via Access Patterns (Representation Pearl)",
    "authors": [
      "Gus Henry Smith",
      "Andrew Liu",
      "Steven Lyubomirsky",
      "Scott Davidson",
      "Joseph McMahan",
      "Michael Taylor",
      "Luis Ceze",
      "Zachary Tatlock"
    ],
    "abstract": "  Tensor kernels in machine learning (ML) often correspond to pure mathematical\nexpressions, making term rewriting an attractive strategy for optimization and\nmapping to specialized hardware accelerators. However, existing ML intermediate\nrepresentations (IRs) tend to either be \\textit{pure but high-level}, making\nlow-level rewrites to hardware targets inexpressible, or \\textit{low-level but\nimpure}, hampering the use of term rewriting altogether. This paper introduces\nGlenside, a pure IR whose core abstraction -- the \\textit{access pattern} --\nenables low-level, layout-aware, hardware-centric program rewrites.\n  We demonstrate how term rewriting in Glenside can be used to map program\nfragments to hardware accelerator invocations and automatically discover\nclassic data layout transformations like \\texttt{im2col}. Glenside establishes\na new foundation for exploring further term rewriting techniques in optimizing\nlow-level tensor programs.\n",
    "categories": [
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-05-19T19:56:44Z",
    "updated": "2021-05-19T19:56:44Z",
    "doi": "10.1145/3460945.3464953"
  },
  "2201.13433": {
    "id": "http://arxiv.org/abs/2201.13433v1",
    "title": "Third Time's the Charm? Image and Video Editing with StyleGAN3",
    "authors": [
      "Yuval Alaluf",
      "Or Patashnik",
      "Zongze Wu",
      "Asif Zamir",
      "Eli Shechtman",
      "Dani Lischinski",
      "Daniel Cohen-Or"
    ],
    "abstract": "  StyleGAN is arguably one of the most intriguing and well-studied generative\nmodels, demonstrating impressive performance in image generation, inversion,\nand manipulation. In this work, we explore the recent StyleGAN3 architecture,\ncompare it to its predecessor, and investigate its unique advantages, as well\nas drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained\non unaligned data, one can still use aligned data for training, without\nhindering the ability to generate unaligned imagery. Next, our analysis of the\ndisentanglement of the different latent spaces of StyleGAN3 indicates that the\ncommonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts,\nunderscoring the benefits of using the StyleSpace for fine-grained editing.\nConsidering image inversion, we observe that existing encoder-based techniques\nstruggle when trained on unaligned data. We therefore propose an encoding\nscheme trained solely on aligned data, yet can still invert unaligned images.\nFinally, we introduce a novel video inversion and editing workflow that\nleverages the capabilities of a fine-tuned StyleGAN3 generator to reduce\ntexture sticking and expand the field of view of the edited video.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-01-31T18:44:59Z",
    "updated": "2022-01-31T18:44:59Z",
    "doi": null
  },
  "2201.00112": {
    "id": "http://arxiv.org/abs/2201.00112v1",
    "title": "SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface\n  Discriminators",
    "authors": [
      "Andrew Luo",
      "Tianqin Li",
      "Wen-Hao Zhang",
      "Tai Sing Lee"
    ],
    "abstract": "  Recent advances in deep generative models have led to immense progress in 3D\nshape synthesis. While existing models are able to synthesize shapes\nrepresented as voxels, point-clouds, or implicit functions, these methods only\nindirectly enforce the plausibility of the final 3D shape surface. Here we\npresent a 3D shape synthesis framework (SurfGen) that directly applies\nadversarial training to the object surface. Our approach uses a differentiable\nspherical projection layer to capture and represent the explicit zero\nisosurface of an implicit 3D generator as functions defined on the unit sphere.\nBy processing the spherical representation of 3D object surfaces with a\nspherical CNN in an adversarial setting, our generator can better learn the\nstatistics of natural shape surfaces. We evaluate our model on large-scale\nshape datasets, and demonstrate that the end-to-end trained model is capable of\ngenerating high fidelity 3D shapes with diverse topology.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-01-01T04:44:42Z",
    "updated": "2022-01-01T04:44:42Z",
    "doi": null
  },
  "2305.13657": {
    "id": "http://arxiv.org/abs/2305.13657v1",
    "title": "ChatGPT as your Personal Data Scientist",
    "authors": [
      "Md Mahadi Hassan",
      "Alex Knipper",
      "Shubhra Kanti Karmaker Santu"
    ],
    "abstract": "  The rise of big data has amplified the need for efficient, user-friendly\nautomated machine learning (AutoML) tools. However, the intricacy of\nunderstanding domain-specific data and defining prediction tasks necessitates\nhuman intervention making the process time-consuming while preventing full\nautomation. Instead, envision an intelligent agent capable of assisting users\nin conducting AutoML tasks through intuitive, natural conversations without\nrequiring in-depth knowledge of the underlying machine learning (ML) processes.\nThis agent's key challenge is to accurately comprehend the user's prediction\ngoals and, consequently, formulate precise ML tasks, adjust data sets and model\nparameters accordingly, and articulate results effectively. In this paper, we\ntake a pioneering step towards this ambitious goal by introducing a\nChatGPT-based conversational data-science framework to act as a \"personal data\nscientist\". Precisely, we utilize Large Language Models (ChatGPT) to build a\nnatural interface between the users and the ML models (Scikit-Learn), which in\nturn, allows us to approach this ambitious problem with a realistic solution.\n  Our model pivots around four dialogue states: Data Visualization, Task\nFormulation, Prediction Engineering, and Result Summary and Recommendation.\nEach state marks a unique conversation phase, impacting the overall user-system\ninteraction. Multiple LLM instances, serving as \"micro-agents\", ensure a\ncohesive conversation flow, granting us granular control over the\nconversation's progression. In summary, we developed an end-to-end system that\nnot only proves the viability of the novel concept of conversational data\nscience but also underscores the potency of LLMs in solving complex tasks.\nInterestingly, its development spotlighted several critical weaknesses in the\ncurrent LLMs (ChatGPT) and highlighted substantial opportunities for\nimprovement.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-23T04:00:16Z",
    "updated": "2023-05-23T04:00:16Z",
    "doi": null
  },
  "1709.00513": {
    "id": "http://arxiv.org/abs/1709.00513v2",
    "title": "Training Shallow and Thin Networks for Acceleration via Knowledge\n  Distillation with Conditional Adversarial Networks",
    "authors": [
      "Zheng Xu",
      "Yen-Chang Hsu",
      "Jiawei Huang"
    ],
    "abstract": "  There is an increasing interest on accelerating neural networks for real-time\napplications. We study the student-teacher strategy, in which a small and fast\nstudent network is trained with the auxiliary information learned from a large\nand accurate teacher network. We propose to use conditional adversarial\nnetworks to learn the loss function to transfer knowledge from teacher to\nstudent. The proposed method is particularly effective for relatively small\nstudent networks. Moreover, experimental results show the effect of network\nsize when the modern networks are used as student. We empirically study the\ntrade-off between inference time and classification accuracy, and provide\nsuggestions on choosing a proper student network.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-09-02T01:03:08Z",
    "updated": "2018-04-16T18:42:13Z",
    "doi": null
  },
  "2209.06899": {
    "id": "http://arxiv.org/abs/2209.06899v1",
    "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
    "authors": [
      "Lisa P. Argyle",
      "Ethan C. Busby",
      "Nancy Fulda",
      "Joshua Gubler",
      "Christopher Rytting",
      "David Wingate"
    ],
    "abstract": "  We propose and explore the possibility that language models can be studied as\neffective proxies for specific human sub-populations in social science\nresearch. Practical and research applications of artificial intelligence tools\nhave sometimes been limited by problematic biases (such as racism or sexism),\nwhich are often treated as uniform properties of the models. We show that the\n\"algorithmic bias\" within one such tool -- the GPT-3 language model -- is\ninstead both fine-grained and demographically correlated, meaning that proper\nconditioning will cause it to accurately emulate response distributions from a\nwide variety of human subgroups. We term this property \"algorithmic fidelity\"\nand explore its extent in GPT-3. We create \"silicon samples\" by conditioning\nthe model on thousands of socio-demographic backstories from real human\nparticipants in multiple large surveys conducted in the United States. We then\ncompare the silicon and human samples to demonstrate that the information\ncontained in GPT-3 goes far beyond surface similarity. It is nuanced,\nmultifaceted, and reflects the complex interplay between ideas, attitudes, and\nsocio-cultural context that characterize human attitudes. We suggest that\nlanguage models with sufficient algorithmic fidelity thus constitute a novel\nand powerful tool to advance understanding of humans and society across a\nvariety of disciplines.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-14T19:53:32Z",
    "updated": "2022-09-14T19:53:32Z",
    "doi": "10.1017/pan.2023.2"
  },
  "2309.14564": {
    "id": "http://arxiv.org/abs/2309.14564v4",
    "title": "Generative Escher Meshes",
    "authors": [
      "Noam Aigerman",
      "Thibault Groueix"
    ],
    "abstract": "  This paper proposes a fully-automatic, text-guided generative method for\nproducing perfectly-repeating, periodic, tile-able 2D imagery, such as the one\nseen on floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to\nsquare texture images that are seamless when tiled, our method generates\nnon-square tilings which comprise solely of repeating copies of the same\nobject. It achieves this by optimizing both geometry and texture of a 2D mesh,\nyielding a non-square tile in the shape and appearance of the desired object,\nwith close to no additional background details, that can tile the plane without\ngaps nor overlaps. We enable optimization of the tile's shape by an\nunconstrained, differentiable parameterization of the space of all valid\ntileable meshes for given boundary conditions stemming from a symmetry group.\nNamely, we construct a differentiable family of linear systems derived from a\n2D mesh-mapping technique - Orbifold Tutte Embedding - by considering the\nmesh's Laplacian matrix as differentiable parameters. We prove that the\nsolution space of these linear systems is exactly all possible valid tiling\nconfigurations, thereby providing an end-to-end differentiable representation\nfor the entire space of valid tiles. We render the textured mesh via a\ndifferentiable renderer, and leverage a pre-trained image diffusion model to\ninduce a loss on the resulting image, updating the mesh's parameters so as to\nmake its appearance match the text prompt. We show our method is able to\nproduce plausible, appealing results, with non-trivial tiles, for a variety of\ndifferent periodic tiling patterns.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-25T22:24:02Z",
    "updated": "2024-06-17T23:57:57Z",
    "doi": null
  },
  "1803.08475": {
    "id": "http://arxiv.org/abs/1803.08475v3",
    "title": "Attention, Learn to Solve Routing Problems!",
    "authors": [
      "Wouter Kool",
      "Herke van Hoof",
      "Max Welling"
    ],
    "abstract": "  The recently presented idea to learn heuristics for combinatorial\noptimization problems is promising as it can save costly development. However,\nto push this idea towards practical implementation, we need better models and\nbetter ways of training. We contribute in both directions: we propose a model\nbased on attention layers with benefits over the Pointer Network and we show\nhow to train this model using REINFORCE with a simple baseline based on a\ndeterministic greedy rollout, which we find is more efficient than using a\nvalue function. We significantly improve over recent learned heuristics for the\nTravelling Salesman Problem (TSP), getting close to optimal results for\nproblems up to 100 nodes. With the same hyperparameters, we learn strong\nheuristics for two variants of the Vehicle Routing Problem (VRP), the\nOrienteering Problem (OP) and (a stochastic variant of) the Prize Collecting\nTSP (PCTSP), outperforming a wide range of baselines and getting results close\nto highly optimized and specialized algorithms.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-03-22T17:22:24Z",
    "updated": "2019-02-07T09:10:08Z",
    "doi": null
  },
  "2007.08508": {
    "id": "http://arxiv.org/abs/2007.08508v1",
    "title": "RepPoints V2: Verification Meets Regression for Object Detection",
    "authors": [
      "Yihong Chen",
      "Zheng Zhang",
      "Yue Cao",
      "Liwei Wang",
      "Stephen Lin",
      "Han Hu"
    ],
    "abstract": "  Verification and regression are two general methodologies for prediction in\nneural networks. Each has its own strengths: verification can be easier to\ninfer accurately, and regression is more efficient and applicable to continuous\ntarget variables. Hence, it is often beneficial to carefully combine them to\ntake advantage of their benefits. In this paper, we take this philosophy to\nimprove state-of-the-art object detection, specifically by RepPoints. Though\nRepPoints provides high performance, we find that its heavy reliance on\nregression for object localization leaves room for improvement. We introduce\nverification tasks into the localization prediction of RepPoints, producing\nRepPoints v2, which provides consistent improvements of about 2.0 mAP over the\noriginal RepPoints on the COCO object detection benchmark using different\nbackbones and training methods. RepPoints v2 also achieves 52.1 mAP on COCO\n\\texttt{test-dev} by a single model. Moreover, we show that the proposed\napproach can more generally elevate other object detection frameworks as well\nas applications such as instance segmentation. The code is available at\nhttps://github.com/Scalsol/RepPointsV2.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-16T17:57:08Z",
    "updated": "2020-07-16T17:57:08Z",
    "doi": null
  },
  "2408.04631": {
    "id": "http://arxiv.org/abs/2408.04631v1",
    "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior\n  for Part-Level Dynamics",
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "  We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-08T17:59:38Z",
    "updated": "2024-08-08T17:59:38Z",
    "doi": null
  },
  "2212.10535": {
    "id": "http://arxiv.org/abs/2212.10535v2",
    "title": "A Survey of Deep Learning for Mathematical Reasoning",
    "authors": [
      "Pan Lu",
      "Liang Qiu",
      "Wenhao Yu",
      "Sean Welleck",
      "Kai-Wei Chang"
    ],
    "abstract": "  Mathematical reasoning is a fundamental aspect of human intelligence and is\napplicable in various fields, including science, engineering, finance, and\neveryday life. The development of artificial intelligence (AI) systems capable\nof solving math problems and proving theorems has garnered significant interest\nin the fields of machine learning and natural language processing. For example,\nmathematics serves as a testbed for aspects of reasoning that are challenging\nfor powerful deep learning models, driving new algorithmic and modeling\nadvances. On the other hand, recent advances in large-scale neural language\nmodels have opened up new benchmarks and opportunities to use deep learning for\nmathematical reasoning. In this survey paper, we review the key tasks,\ndatasets, and methods at the intersection of mathematical reasoning and deep\nlearning over the past decade. We also evaluate existing benchmarks and\nmethods, and discuss future research directions in this domain.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-20T18:46:16Z",
    "updated": "2023-06-22T01:37:02Z",
    "doi": null
  },
  "2311.18799": {
    "id": "http://arxiv.org/abs/2311.18799v2",
    "title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning",
    "authors": [
      "Artemis Panagopoulou",
      "Le Xue",
      "Ning Yu",
      "Junnan Li",
      "Dongxu Li",
      "Shafiq Joty",
      "Ran Xu",
      "Silvio Savarese",
      "Caiming Xiong",
      "Juan Carlos Niebles"
    ],
    "abstract": "  Recent research has achieved significant advancements in visual reasoning\ntasks through learning image-to-language projections and leveraging the\nimpressive reasoning abilities of Large Language Models (LLMs). This paper\nintroduces an efficient and effective framework that integrates multiple\nmodalities (images, 3D, audio and video) to a frozen LLM and demonstrates an\nemergent ability for cross-modal reasoning (2+ modality inputs). Our approach\nexplores two distinct projection mechanisms: Q-Formers and Linear Projections\n(LPs). Through extensive experimentation across all four modalities on 16\nbenchmarks, we explore both methods and assess their adaptability in integrated\nand separate cross-modal reasoning. The Q-Former projection demonstrates\nsuperior performance in single modality scenarios and adaptability in joint\nversus discriminative reasoning involving two or more modalities. However, it\nexhibits lower generalization capabilities than linear projection in contexts\nwhere task-modality data are limited. To enable this framework, we devise a\nscalable pipeline that automatically generates high-quality, instruction-tuning\ndatasets from readily available captioning data across different modalities,\nand contribute 24K QA data for audio and 250K QA data for 3D. To facilitate\nfurther research in cross-modal reasoning, we introduce the DisCRn\n(Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-30T18:43:51Z",
    "updated": "2024-09-09T16:00:04Z",
    "doi": null
  },
  "2207.13861": {
    "id": "http://arxiv.org/abs/2207.13861v2",
    "title": "DnSwin: Toward Real-World Denoising via Continuous Wavelet\n  Sliding-Transformer",
    "authors": [
      "Hao Li",
      "Zhijing Yang",
      "Xiaobin Hong",
      "Ziying Zhao",
      "Junyang Chen",
      "Yukai Shi",
      "Jinshan Pan"
    ],
    "abstract": "  Real-world image denoising is a practical image restoration problem that aims\nto obtain clean images from in-the-wild noisy inputs. Recently, the Vision\nTransformer (ViT) has exhibited a strong ability to capture long-range\ndependencies, and many researchers have attempted to apply the ViT to image\ndenoising tasks. However, a real-world image is an isolated frame that makes\nthe ViT build long-range dependencies based on the internal patches, which\ndivides images into patches, disarranges noise patterns and damages gradient\ncontinuity. In this article, we propose to resolve this issue by using a\ncontinuous Wavelet Sliding-Transformer that builds frequency correspondences\nunder real-world scenes, called DnSwin. Specifically, we first extract the\nbottom features from noisy input images by using a convolutional neural network\n(CNN) encoder. The key to DnSwin is to extract high-frequency and low-frequency\ninformation from the observed features and build frequency dependencies. To\nthis end, we propose a Wavelet Sliding-Window Transformer (WSWT) that utilizes\nthe discrete wavelet transform (DWT), self-attention and the inverse DWT (IDWT)\nto extract deep features. Finally, we reconstruct the deep features into\ndenoised images using a CNN decoder. Both quantitative and qualitative\nevaluations conducted on real-world denoising benchmarks demonstrate that the\nproposed DnSwin performs favorably against the state-of-the-art methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-28T02:33:57Z",
    "updated": "2022-09-13T05:14:07Z",
    "doi": "10.1016/j.knosys.2022.109815"
  },
  "1807.09251": {
    "id": "http://arxiv.org/abs/1807.09251v2",
    "title": "GANimation: Anatomically-aware Facial Animation from a Single Image",
    "authors": [
      "Albert Pumarola",
      "Antonio Agudo",
      "Aleix M. Martinez",
      "Alberto Sanfeliu",
      "Francesc Moreno-Noguer"
    ],
    "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have shown\nimpressive results for task of facial expression synthesis. The most successful\narchitecture is StarGAN, that conditions GANs generation process with images of\na specific domain, namely a set of images of persons sharing the same\nexpression. While effective, this approach can only generate a discrete number\nof expressions, determined by the content of the dataset. To address this\nlimitation, in this paper, we introduce a novel GAN conditioning scheme based\non Action Units (AU) annotations, which describes in a continuous manifold the\nanatomical facial movements defining a human expression. Our approach allows\ncontrolling the magnitude of activation of each AU and combine several of them.\nAdditionally, we propose a fully unsupervised strategy to train the model, that\nonly requires images annotated with their activated AUs, and exploit attention\nmechanisms that make our network robust to changing backgrounds and lighting\nconditions. Extensive evaluation show that our approach goes beyond competing\nconditional generators both in the capability to synthesize a much wider range\nof expressions ruled by anatomically feasible muscle movements, as in the\ncapacity of dealing with images in the wild.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-07-24T17:47:09Z",
    "updated": "2018-08-28T23:46:23Z",
    "doi": null
  },
  "2402.12204": {
    "id": "http://arxiv.org/abs/2402.12204v1",
    "title": "Enhancing Multilingual Capabilities of Large Language Models through\n  Self-Distillation from Resource-Rich Languages",
    "authors": [
      "Yuanchi Zhang",
      "Yile Wang",
      "Zijun Liu",
      "Shuo Wang",
      "Xiaolong Wang",
      "Peng Li",
      "Maosong Sun",
      "Yang Liu"
    ],
    "abstract": "  While large language models (LLMs) have been pre-trained on multilingual\ncorpora, their performance still lags behind in most languages compared to a\nfew resource-rich languages. One common approach to mitigate this issue is to\ntranslate training data from resource-rich languages into other languages and\nthen continue training. However, using the data obtained solely relying on\ntranslation while ignoring the original capabilities of LLMs across languages\nis not always effective, which we show will limit the performance of\ncross-lingual knowledge transfer. In this work, we propose SDRRL, a method\nbased on Self-Distillation from Resource-Rich Languages that effectively\nimprove multilingual performance by leveraging the internal capabilities of\nLLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and\nSeaLLM) and source languages across various comprehension and generation tasks,\nexperimental results demonstrate that SDRRL can significantly enhance\nmultilingual capabilities while minimizing the impact on original performance\nin resource-rich languages.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-19T15:07:32Z",
    "updated": "2024-02-19T15:07:32Z",
    "doi": null
  },
  "2010.10952": {
    "id": "http://arxiv.org/abs/2010.10952v4",
    "title": "A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels",
    "authors": [
      "Leon Lang",
      "Maurice Weiler"
    ],
    "abstract": "  Group equivariant convolutional networks (GCNNs) endow classical\nconvolutional networks with additional symmetry priors, which can lead to a\nconsiderably improved performance. Recent advances in the theoretical\ndescription of GCNNs revealed that such models can generally be understood as\nperforming convolutions with G-steerable kernels, that is, kernels that satisfy\nan equivariance constraint themselves. While the G-steerability constraint has\nbeen derived, it has to date only been solved for specific use cases - a\ngeneral characterization of G-steerable kernel spaces is still missing. This\nwork provides such a characterization for the practically relevant case of G\nbeing any compact group. Our investigation is motivated by a striking analogy\nbetween the constraints underlying steerable kernels on the one hand and\nspherical tensor operators from quantum mechanics on the other hand. By\ngeneralizing the famous Wigner-Eckart theorem for spherical tensor operators,\nwe prove that steerable kernel spaces are fully understood and parameterized in\nterms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan\ncoefficients, and 3) harmonic basis functions on homogeneous spaces.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-10-21T12:42:23Z",
    "updated": "2021-01-21T10:00:28Z",
    "doi": null
  },
  "2111.10023": {
    "id": "http://arxiv.org/abs/2111.10023v1",
    "title": "UFO: A UniFied TransfOrmer for Vision-Language Representation Learning",
    "authors": [
      "Jianfeng Wang",
      "Xiaowei Hu",
      "Zhe Gan",
      "Zhengyuan Yang",
      "Xiyang Dai",
      "Zicheng Liu",
      "Yumao Lu",
      "Lijuan Wang"
    ],
    "abstract": "  In this paper, we propose a single UniFied transfOrmer (UFO), which is\ncapable of processing either unimodal inputs (e.g., image or language) or\nmultimodal inputs (e.g., the concatenation of the image and the question), for\nvision-language (VL) representation learning. Existing approaches typically\ndesign an individual network for each modality and/or a specific fusion network\nfor multimodal tasks. To simplify the network architecture, we use a single\ntransformer network and enforce multi-task learning during VL pre-training,\nwhich includes the image-text contrastive loss, image-text matching loss, and\nmasked language modeling loss based on the bidirectional and the seq2seq\nattention mask. The same transformer network is used as the image encoder, the\ntext encoder, or the fusion network in different pre-training tasks.\nEmpirically, we observe less conflict among different tasks and achieve new\nstate of the arts on visual question answering, COCO image captioning\n(cross-entropy optimization) and nocaps (in SPICE). On other downstream tasks,\ne.g., image-text retrieval, we also achieve competitive performance.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-19T03:23:10Z",
    "updated": "2021-11-19T03:23:10Z",
    "doi": null
  },
  "2312.05133": {
    "id": "http://arxiv.org/abs/2312.05133v2",
    "title": "GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization",
    "authors": [
      "Yahao Shi",
      "Yanmin Wu",
      "Chenming Wu",
      "Xing Liu",
      "Chen Zhao",
      "Haocheng Feng",
      "Jian Zhang",
      "Bin Zhou",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "abstract": "  This paper presents a 3D Gaussian Inverse Rendering (GIR) method, employing\n3D Gaussian representations to effectively factorize the scene into material\nproperties, light, and geometry. The key contributions lie in three-fold. We\ncompute the normal of each 3D Gaussian using the shortest eigenvector, with a\ndirectional masking scheme forcing accurate normal estimation without external\nsupervision. We adopt an efficient voxel-based indirect illumination tracing\nscheme that stores direction-aware outgoing radiance in each 3D Gaussian to\ndisentangle secondary illumination for approximating multi-bounce light\ntransport. To further enhance the illumination disentanglement, we represent a\nhigh-resolution environmental map with a learnable low-resolution map and a\nlightweight, fully convolutional network. Our method achieves state-of-the-art\nperformance in both relighting and novel view synthesis tasks among the\nrecently proposed inverse rendering methods while achieving real-time\nrendering. This substantiates our proposed method's efficacy and broad\napplicability, highlighting its potential as an influential tool in various\nreal-time interactive graphics applications such as material editing and\nrelighting. The code will be released at https://github.com/guduxiaolang/GIR.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-08T16:05:15Z",
    "updated": "2024-08-15T15:40:48Z",
    "doi": null
  },
  "1701.02547": {
    "id": "http://arxiv.org/abs/1701.02547v4",
    "title": "A Convenient Category for Higher-Order Probability Theory",
    "authors": [
      "Chris Heunen",
      "Ohad Kammar",
      "Sam Staton",
      "Hongseok Yang"
    ],
    "abstract": "  Higher-order probabilistic programming languages allow programmers to write\nsophisticated models in machine learning and statistics in a succinct and\nstructured way, but step outside the standard measure-theoretic formalization\nof probability theory. Programs may use both higher-order functions and\ncontinuous distributions, or even define a probability distribution on\nfunctions. But standard probability theory does not handle higher-order\nfunctions well: the category of measurable spaces is not cartesian closed.\n  Here we introduce quasi-Borel spaces. We show that these spaces: form a new\nformalization of probability theory replacing measurable spaces; form a\ncartesian closed category and so support higher-order functions; form a\nwell-pointed category and so support good proof principles for equational\nreasoning; and support continuous probability distributions. We demonstrate the\nuse of quasi-Borel spaces for higher-order functions and probability by:\nshowing that a well-known construction of probability theory involving random\nfunctions gains a cleaner expression; and generalizing de Finetti's theorem,\nthat is a crucial theorem in probability theory, to quasi-Borel spaces.\n",
    "categories": [
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LO",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.CT",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.PR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-01-10T12:19:05Z",
    "updated": "2020-11-20T08:56:11Z",
    "doi": "10.1109/LICS.2017.8005137"
  },
  "2211.05781": {
    "id": "http://arxiv.org/abs/2211.05781v2",
    "title": "Demystify Transformers & Convolutions in Modern Image Deep Networks",
    "authors": [
      "Xiaowei Hu",
      "Min Shi",
      "Weiyun Wang",
      "Sitong Wu",
      "Linjie Xing",
      "Wenhai Wang",
      "Xizhou Zhu",
      "Lewei Lu",
      "Jie Zhou",
      "Xiaogang Wang",
      "Yu Qiao",
      "Jifeng Dai"
    ],
    "abstract": "  Vision transformers have gained popularity recently, leading to the\ndevelopment of new vision backbones with improved features and consistent\nperformance gains. However, these advancements are not solely attributable to\nnovel feature transformation designs; certain benefits also arise from advanced\nnetwork-level and block-level architectures. This paper aims to identify the\nreal gains of popular convolution and attention operators through a detailed\nstudy. We find that the key difference among these feature transformation\nmodules, such as attention or convolution, lies in their spatial feature\naggregation approach, known as the \"spatial token mixer\" (STM). To facilitate\nan impartial comparison, we introduce a unified architecture to neutralize the\nimpact of divergent network-level and block-level designs. Subsequently,\nvarious STMs are integrated into this unified framework for comprehensive\ncomparative analysis. Our experiments on various tasks and an analysis of\ninductive bias show a significant performance boost due to advanced\nnetwork-level and block-level designs, but performance differences persist\namong different STMs. Our detailed analysis also reveals various findings about\ndifferent STMs, such as effective receptive fields and invariance tests. All\nmodels and codes used in this study are publicly available at\n\\url{https://github.com/OpenGVLab/STM-Evaluation}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-10T18:59:43Z",
    "updated": "2023-12-01T08:00:51Z",
    "doi": null
  },
  "2404.00604": {
    "id": "http://arxiv.org/abs/2404.00604v1",
    "title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment",
    "authors": [
      "Xiao Liu",
      "Xixuan Song",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "  Reinforcement learning from human feedback (RLHF) has been a central\ntechnique for recent large language model (LLM) alignment. However, its heavy\ndependence on costly human or LLM-as-Judge preference feedback could stymie its\nwider applications. In this work, we introduce Self-Contrast, a feedback-free\nlarge language model alignment method via exploiting extensive self-generated\nnegatives. With only supervised fine-tuning (SFT) targets, Self-Contrast\nleverages the LLM itself to generate massive diverse candidates, and harnesses\na pre-trained embedding model to filter multiple negatives according to text\nsimilarity. Theoretically, we illustrate that in this setting, merely scaling\nnegative responses can still effectively approximate situations with more\nbalanced positive and negative preference annotations. Our experiments with\ndirect preference optimization (DPO) on three datasets show that, Self-Contrast\ncould consistently outperform SFT and standard DPO training by large margins.\nAnd as the number of self-generated negatives increases, the performance of\nSelf-Contrast continues to grow. Code and data are available at\nhttps://github.com/THUDM/Self-Contrast.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-31T08:30:15Z",
    "updated": "2024-03-31T08:30:15Z",
    "doi": null
  },
  "2305.08296": {
    "id": "http://arxiv.org/abs/2305.08296v1",
    "title": "Neural Face Rigging for Animating and Retargeting Facial Meshes in the\n  Wild",
    "authors": [
      "Dafei Qin",
      "Jun Saito",
      "Noam Aigerman",
      "Thibault Groueix",
      "Taku Komura"
    ],
    "abstract": "  We propose an end-to-end deep-learning approach for automatic rigging and\nretargeting of 3D models of human faces in the wild. Our approach, called\nNeural Face Rigging (NFR), holds three key properties:\n  (i) NFR's expression space maintains human-interpretable editing parameters\nfor artistic controls;\n  (ii) NFR is readily applicable to arbitrary facial meshes with different\nconnectivity and expressions;\n  (iii) NFR can encode and produce fine-grained details of complex expressions\nperformed by arbitrary subjects.\n  To the best of our knowledge, NFR is the first approach to provide realistic\nand controllable deformations of in-the-wild facial meshes, without the manual\ncreation of blendshapes or correspondence. We design a deformation autoencoder\nand train it through a multi-dataset training scheme, which benefits from the\nunique advantages of two data sources: a linear 3DMM with interpretable control\nparameters as in FACS, and 4D captures of real faces with fine-grained details.\nThrough various experiments, we show NFR's ability to automatically produce\nrealistic and accurate facial deformations across a wide range of existing\ndatasets as well as noisy facial scans in-the-wild, while providing\nartist-controlled, editable parameters.\n",
    "categories": [
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-15T01:51:08Z",
    "updated": "2023-05-15T01:51:08Z",
    "doi": null
  },
  "2202.13562": {
    "id": "http://arxiv.org/abs/2202.13562v2",
    "title": "Name Your Style: An Arbitrary Artist-aware Image Style Transfer",
    "authors": [
      "Zhi-Song Liu",
      "Li-Wen Wang",
      "Wan-Chi Siu",
      "Vicky Kalogeiton"
    ],
    "abstract": "  Image style transfer has attracted widespread attention in the past few\nyears. Despite its remarkable results, it requires additional style images\navailable as references, making it less flexible and inconvenient. Using text\nis the most natural way to describe the style. More importantly, text can\ndescribe implicit abstract styles, like styles of specific artists or art\nmovements. In this paper, we propose a text-driven image style transfer (TxST)\nthat leverages advanced image-text encoders to control arbitrary style\ntransfer. We introduce a contrastive training strategy to effectively extract\nstyle descriptions from the image-text model (i.e., CLIP), which aligns\nstylization with the text description. To this end, we also propose a novel and\nefficient attention module that explores cross-attentions to fuse style and\ncontent features. Finally, we achieve an arbitrary artist-aware image style\ntransfer to learn and transfer specific artistic characters such as Picasso,\noil painting, or a rough sketch. Extensive experiments demonstrate that our\napproach outperforms the state-of-the-art methods on both image and textual\nstyles. Moreover, it can mimic the styles of one or many artists to achieve\nattractive results, thus highlighting a promising direction in image style\ntransfer.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-02-28T06:21:38Z",
    "updated": "2022-03-05T00:47:30Z",
    "doi": null
  },
  "2204.04788": {
    "id": "http://arxiv.org/abs/2204.04788v2",
    "title": "Representation Learning by Detecting Incorrect Location Embeddings",
    "authors": [
      "Sepehr Sameni",
      "Simon Jenni",
      "Paolo Favaro"
    ],
    "abstract": "  In this paper, we introduce a novel self-supervised learning (SSL) loss for\nimage representation learning. There is a growing belief that generalization in\ndeep neural networks is linked to their ability to discriminate object shapes.\nSince object shape is related to the location of its parts, we propose to\ndetect those that have been artificially misplaced. We represent object parts\nwith image tokens and train a ViT to detect which token has been combined with\nan incorrect positional embedding. We then introduce sparsity in the inputs to\nmake the model more robust to occlusions and to speed up the training. We call\nour method DILEMMA, which stands for Detection of Incorrect Location EMbeddings\nwith MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an\nimprovement in their performance of respectively 4.41%, 3.97%, and 0.5% under\nthe same training time and with a linear probing transfer on ImageNet-1K. We\nalso show full fine-tuning improvements of MAE combined with our method on\nImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks.\nMoreover, we show that when downstream tasks are strongly reliant on shape\n(such as in the YOGA-82 pose dataset), our pre-trained features yield a\nsignificant gain over prior work.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-10T22:58:02Z",
    "updated": "2023-03-13T10:13:00Z",
    "doi": null
  },
  "2305.15399": {
    "id": "http://arxiv.org/abs/2305.15399v2",
    "title": "Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape",
    "authors": [
      "Rundi Wu",
      "Ruoshi Liu",
      "Carl Vondrick",
      "Changxi Zheng"
    ],
    "abstract": "  Synthesizing novel 3D models that resemble the input example has long been\npursued by graphics artists and machine learning researchers. In this paper, we\npresent Sin3DM, a diffusion model that learns the internal patch distribution\nfrom a single 3D textured shape and generates high-quality variations with fine\ngeometry and texture details. Training a diffusion model directly in 3D would\ninduce large memory and computational cost. Therefore, we first compress the\ninput into a lower-dimensional latent space and then train a diffusion model on\nit. Specifically, we encode the input 3D textured shape into triplane feature\nmaps that represent the signed distance and texture fields of the input. The\ndenoising network of our diffusion model has a limited receptive field to avoid\noverfitting, and uses triplane-aware 2D convolution blocks to improve the\nresult quality. Aside from randomly generating new samples, our model also\nfacilitates applications such as retargeting, outpainting and local editing.\nThrough extensive qualitative and quantitative evaluation, we show that our\nmethod outperforms prior methods in generation quality of 3D shapes.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-24T17:57:15Z",
    "updated": "2024-02-21T01:25:36Z",
    "doi": null
  },
  "2311.01410": {
    "id": "http://arxiv.org/abs/2311.01410v2",
    "title": "The Blessing of Randomness: SDE Beats ODE in General Diffusion-based\n  Image Editing",
    "authors": [
      "Shen Nie",
      "Hanzhong Allan Guo",
      "Cheng Lu",
      "Yuhao Zhou",
      "Chenyu Zheng",
      "Chongxuan Li"
    ],
    "abstract": "  We present a unified probabilistic formulation for diffusion-based image\nediting, where a latent variable is edited in a task-specific manner and\ngenerally deviates from the corresponding marginal distribution induced by the\noriginal stochastic or ordinary differential equation (SDE or ODE). Instead, it\ndefines a corresponding SDE or ODE for editing. In the formulation, we prove\nthat the Kullback-Leibler divergence between the marginal distributions of the\ntwo SDEs gradually decreases while that for the ODEs remains as the time\napproaches zero, which shows the promise of SDE in image editing. Inspired by\nit, we provide the SDE counterparts for widely used ODE baselines in various\ntasks including inpainting and image-to-image translation, where SDE shows a\nconsistent and substantial improvement. Moreover, we propose SDE-Drag -- a\nsimple yet effective method built upon the SDE formulation for point-based\ncontent dragging. We build a challenging benchmark (termed DragBench) with\nopen-set natural, art, and AI-generated images for evaluation. A user study on\nDragBench indicates that SDE-Drag significantly outperforms our ODE baseline,\nexisting diffusion-based methods, and the renowned DragGAN. Our results\ndemonstrate the superiority and versatility of SDE in image editing and push\nthe boundary of diffusion-based editing methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-02T17:23:14Z",
    "updated": "2024-02-29T14:31:13Z",
    "doi": null
  },
  "2407.06642": {
    "id": "http://arxiv.org/abs/2407.06642v2",
    "title": "Powerful and Flexible: Personalized Text-to-Image Generation via\n  Reinforcement Learning",
    "authors": [
      "Fanyue Wei",
      "Wei Zeng",
      "Zhenyang Li",
      "Dawei Yin",
      "Lixin Duan",
      "Wen Li"
    ],
    "abstract": "  Personalized text-to-image models allow users to generate varied styles of\nimages (specified with a sentence) for an object (specified with a set of\nreference images). While remarkable results have been achieved using\ndiffusion-based generation models, the visual structure and details of the\nobject are often unexpectedly changed during the diffusion process. One major\nreason is that these diffusion-based approaches typically adopt a simple\nreconstruction objective during training, which can hardly enforce appropriate\nstructural consistency between the generated and the reference images. To this\nend, in this paper, we design a novel reinforcement learning framework by\nutilizing the deterministic policy gradient method for personalized\ntext-to-image generation, with which various objectives, differential or even\nnon-differential, can be easily incorporated to supervise the diffusion models\nto improve the quality of the generated images. Experimental results on\npersonalized text-to-image generation benchmark datasets demonstrate that our\nproposed approach outperforms existing state-of-the-art methods by a large\nmargin on visual fidelity while maintaining text-alignment. Our code is\navailable at: \\url{https://github.com/wfanyue/DPG-T2I-Personalization}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-09T08:11:53Z",
    "updated": "2024-07-18T15:34:04Z",
    "doi": null
  },
  "2205.05131": {
    "id": "http://arxiv.org/abs/2205.05131v3",
    "title": "UL2: Unifying Language Learning Paradigms",
    "authors": [
      "Yi Tay",
      "Mostafa Dehghani",
      "Vinh Q. Tran",
      "Xavier Garcia",
      "Jason Wei",
      "Xuezhi Wang",
      "Hyung Won Chung",
      "Siamak Shakeri",
      "Dara Bahri",
      "Tal Schuster",
      "Huaixiu Steven Zheng",
      "Denny Zhou",
      "Neil Houlsby",
      "Donald Metzler"
    ],
    "abstract": "  Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized & unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 & GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-10T19:32:20Z",
    "updated": "2023-02-28T17:20:36Z",
    "doi": null
  },
  "1909.05372": {
    "id": "http://arxiv.org/abs/1909.05372v1",
    "title": "Overton: A Data System for Monitoring and Improving Machine-Learned\n  Products",
    "authors": [
      "Christopher R\u00e9",
      "Feng Niu",
      "Pallavi Gudipati",
      "Charles Srisuwananukorn"
    ],
    "abstract": "  We describe a system called Overton, whose main design goal is to support\nengineers in building, monitoring, and improving production machine learning\nsystems. Key challenges engineers face are monitoring fine-grained quality,\ndiagnosing errors in sophisticated applications, and handling contradictory or\nincomplete supervision data. Overton automates the life cycle of model\nconstruction, deployment, and monitoring by providing a set of novel\nhigh-level, declarative abstractions. Overton's vision is to shift developers\nto these higher-level tasks instead of lower-level machine learning tasks. In\nfact, using Overton, engineers can build deep-learning-based applications\nwithout writing any code in frameworks like TensorFlow. For over a year,\nOverton has been used in production to support multiple applications in both\nnear-real-time applications and back-of-house processing. In that time,\nOverton-based applications have answered billions of queries in multiple\nlanguages and processed trillions of records reducing errors 1.7-2.9 times\nversus production systems.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DB",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-09-07T03:51:13Z",
    "updated": "2019-09-07T03:51:13Z",
    "doi": null
  },
  "2211.11679": {
    "id": "http://arxiv.org/abs/2211.11679v3",
    "title": "Mean Shift Mask Transformer for Unseen Object Instance Segmentation",
    "authors": [
      "Yangxiao Lu",
      "Yuqiao Chen",
      "Nicholas Ruozzi",
      "Yu Xiang"
    ],
    "abstract": "  Segmenting unseen objects from images is a critical perception skill that a\nrobot needs to acquire. In robot manipulation, it can facilitate a robot to\ngrasp and manipulate unseen objects. Mean shift clustering is a widely used\nmethod for image segmentation tasks. However, the traditional mean shift\nclustering algorithm is not differentiable, making it difficult to integrate it\ninto an end-to-end neural network training framework. In this work, we propose\nthe Mean Shift Mask Transformer (MSMFormer), a new transformer architecture\nthat simulates the von Mises-Fisher (vMF) mean shift clustering algorithm,\nallowing for the joint training and inference of both the feature extractor and\nthe clustering. Its central component is a hypersphere attention mechanism,\nwhich updates object queries on a hypersphere. To illustrate the effectiveness\nof our method, we apply MSMFormer to unseen object instance segmentation. Our\nexperiments show that MSMFormer achieves competitive performance compared to\nstate-of-the-art methods for unseen object instance segmentation. The project\npage, appendix, video, and code are available at\nhttps://irvlutd.github.io/MSMFormer\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-21T17:47:48Z",
    "updated": "2023-09-21T23:04:42Z",
    "doi": null
  },
  "2311.18834": {
    "id": "http://arxiv.org/abs/2311.18834v1",
    "title": "ART$\\boldsymbol{\\cdot}$V: Auto-Regressive Text-to-Video Generation with\n  Diffusion Models",
    "authors": [
      "Wenming Weng",
      "Ruoyu Feng",
      "Yanhui Wang",
      "Qi Dai",
      "Chunyu Wang",
      "Dacheng Yin",
      "Zhiyuan Zhao",
      "Kai Qiu",
      "Jianmin Bao",
      "Yuhui Yuan",
      "Chong Luo",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ],
    "abstract": "  We present ART$\\boldsymbol{\\cdot}$V, an efficient framework for\nauto-regressive video generation with diffusion models. Unlike existing methods\nthat generate entire videos in one-shot, ART$\\boldsymbol{\\cdot}$V generates a\nsingle frame at a time, conditioned on the previous ones. The framework offers\nthree distinct advantages. First, it only learns simple continual motions\nbetween adjacent frames, therefore avoiding modeling complex long-range motions\nthat require huge training data. Second, it preserves the high-fidelity\ngeneration ability of the pre-trained image diffusion models by making only\nminimal network modifications. Third, it can generate arbitrarily long videos\nconditioned on a variety of prompts such as text, image or their combinations,\nmaking it highly versatile and flexible. To combat the common drifting issue in\nAR models, we propose masked diffusion model which implicitly learns which\ninformation can be drawn from reference images rather than network predictions,\nin order to reduce the risk of generating inconsistent appearances that cause\ndrifting. Moreover, we further enhance generation coherence by conditioning it\non the initial frame, which typically contains minimal noise. This is\nparticularly useful for long video generation. When trained for only two weeks\non four GPUs, ART$\\boldsymbol{\\cdot}$V already can generate videos with natural\nmotions, rich details and a high level of aesthetic quality. Besides, it\nenables various appealing applications, e.g., composing a long video from\nmultiple text prompts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-30T18:59:47Z",
    "updated": "2023-11-30T18:59:47Z",
    "doi": null
  },
  "2007.08509": {
    "id": "http://arxiv.org/abs/2007.08509v1",
    "title": "World-Consistent Video-to-Video Synthesis",
    "authors": [
      "Arun Mallya",
      "Ting-Chun Wang",
      "Karan Sapra",
      "Ming-Yu Liu"
    ],
    "abstract": "  Video-to-video synthesis (vid2vid) aims for converting high-level semantic\ninputs to photorealistic videos. While existing vid2vid methods can achieve\nshort-term temporal consistency, they fail to ensure the long-term one. This is\nbecause they lack knowledge of the 3D world being rendered and generate each\nframe only based on the past few frames. To address the limitation, we\nintroduce a novel vid2vid framework that efficiently and effectively utilizes\nall past generated frames during rendering. This is achieved by condensing the\n3D world rendered so far into a physically-grounded estimate of the current\nframe, which we call the guidance image. We further propose a novel neural\nnetwork architecture to take advantage of the information stored in the\nguidance images. Extensive experimental results on several challenging datasets\nverify the effectiveness of our approach in achieving world consistency - the\noutput video is consistent within the entire rendered 3D world.\n  https://nvlabs.github.io/wc-vid2vid/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-16T17:58:13Z",
    "updated": "2020-07-16T17:58:13Z",
    "doi": null
  },
  "2206.01720": {
    "id": "http://arxiv.org/abs/2206.01720v1",
    "title": "Revisiting the \"Video\" in Video-Language Understanding",
    "authors": [
      "Shyamal Buch",
      "Crist\u00f3bal Eyzaguirre",
      "Adrien Gaidon",
      "Jiajun Wu",
      "Li Fei-Fei",
      "Juan Carlos Niebles"
    ],
    "abstract": "  What makes a video task uniquely suited for videos, beyond what can be\nunderstood from a single image? Building on recent progress in self-supervised\nimage-language models, we revisit this question in the context of video and\nlanguage tasks. We propose the atemporal probe (ATP), a new model for\nvideo-language analysis which provides a stronger bound on the baseline\naccuracy of multimodal models constrained by image-level understanding. By\napplying this model to standard discriminative video and language tasks, such\nas video question answering and text-to-video retrieval, we characterize the\nlimitations and potential of current video-language benchmarks. We find that\nunderstanding of event temporality is often not necessary to achieve strong or\nstate-of-the-art performance, even compared with recent large-scale\nvideo-language models and in contexts intended to benchmark deeper video-level\nunderstanding. We also demonstrate how ATP can improve both video-language\ndataset and model design. We describe a technique for leveraging ATP to better\ndisentangle dataset subsets with a higher concentration of temporally\nchallenging data, improving benchmarking efficacy for causal and temporal\nunderstanding. Further, we show that effectively integrating ATP into full\nvideo-level temporal models can improve efficiency and state-of-the-art\naccuracy.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-03T17:57:33Z",
    "updated": "2022-06-03T17:57:33Z",
    "doi": null
  },
  "1811.07605": {
    "id": "http://arxiv.org/abs/1811.07605v3",
    "title": "Adversarial Autoencoders for Compact Representations of 3D Point Clouds",
    "authors": [
      "Maciej Zamorski",
      "Maciej Zi\u0119ba",
      "Piotr Klukowski",
      "Rafa\u0142 Nowak",
      "Karol Kurach",
      "Wojciech Stokowiec",
      "Tomasz Trzci\u0144ski"
    ],
    "abstract": "  Deep generative architectures provide a way to model not only images but also\ncomplex, 3-dimensional objects, such as point clouds. In this work, we present\na novel method to obtain meaningful representations of 3D shapes that can be\nused for challenging tasks including 3D points generation, reconstruction,\ncompression, and clustering. Contrary to existing methods for 3D point cloud\ngeneration that train separate decoupled models for representation learning and\ngeneration, our approach is the first end-to-end solution that allows to\nsimultaneously learn a latent space of representation and generate 3D shape out\nof it. Moreover, our model is capable of learning meaningful compact binary\ndescriptors with adversarial training conducted on a latent space. To achieve\nthis goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D\ninput and create 3D output. Thanks to our end-to-end training regime, the\nresulting method called 3D Adversarial Autoencoder (3dAAE) obtains either\nbinary or continuous latent space that covers a much wider portion of training\ndata distribution. Finally, our quantitative evaluation shows that 3dAAE\nprovides state-of-the-art results for 3D points clustering and 3D object\nretrieval.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-11-19T10:51:09Z",
    "updated": "2019-05-01T19:22:36Z",
    "doi": null
  },
  "2203.03014": {
    "id": "http://arxiv.org/abs/2203.03014v2",
    "title": "Learnable Irrelevant Modality Dropout for Multimodal Action Recognition\n  on Modality-Specific Annotated Videos",
    "authors": [
      "Saghir Alfasly",
      "Jian Lu",
      "Chen Xu",
      "Yuru Zou"
    ],
    "abstract": "  With the assumption that a video dataset is multimodality annotated in which\nauditory and visual modalities both are labeled or class-relevant, current\nmultimodal methods apply modality fusion or cross-modality attention. However,\neffectively leveraging the audio modality in vision-specific annotated videos\nfor action recognition is of particular challenge. To tackle this challenge, we\npropose a novel audio-visual framework that effectively leverages the audio\nmodality in any solely vision-specific annotated dataset. We adopt the language\nmodels (e.g., BERT) to build a semantic audio-video label dictionary (SAVLD)\nthat maps each video label to its most K-relevant audio labels in which SAVLD\nserves as a bridge between audio and video datasets. Then, SAVLD along with a\npretrained audio multi-label model are used to estimate the audio-visual\nmodality relevance during the training phase. Accordingly, a novel learnable\nirrelevant modality dropout (IMD) is proposed to completely drop out the\nirrelevant audio modality and fuse only the relevant modalities. Moreover, we\npresent a new two-stream video Transformer for efficiently modeling the visual\nmodalities. Results on several vision-specific annotated datasets including\nKinetics400 and UCF-101 validated our framework as it outperforms most relevant\naction recognition methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-06T17:31:06Z",
    "updated": "2022-03-27T03:26:40Z",
    "doi": null
  },
  "2308.15930": {
    "id": "http://arxiv.org/abs/2308.15930v3",
    "title": "LLaSM: Large Language and Speech Model",
    "authors": [
      "Yu Shu",
      "Siwei Dong",
      "Guangyao Chen",
      "Wenhao Huang",
      "Ruihua Zhang",
      "Daochen Shi",
      "Qiqi Xiang",
      "Yemin Shi"
    ],
    "abstract": "  Multi-modal large language models have garnered significant interest\nrecently. Though, most of the works focus on vision-language multi-modal models\nproviding strong capabilities in following vision-and-language instructions.\nHowever, we claim that speech is also an important modality through which\nhumans interact with the world. Hence, it is crucial for a general-purpose\nassistant to be able to follow multi-modal speech-and-language instructions. In\nthis work, we propose Large Language and Speech Model (LLaSM). LLaSM is an\nend-to-end trained large multi-modal speech-language model with cross-modal\nconversational abilities, capable of following speech-and-language\ninstructions. Our early experiments show that LLaSM demonstrates a more\nconvenient and natural way for humans to interact with artificial intelligence.\nSpecifically, we also release a large Speech Instruction Following dataset\nLLaSM-Audio-Instructions. Code and demo are available at\nhttps://github.com/LinkSoul-AI/LLaSM and\nhttps://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions\ndataset is available at\nhttps://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-30T10:12:39Z",
    "updated": "2023-09-16T06:14:54Z",
    "doi": null
  },
  "2409.00786": {
    "id": "http://arxiv.org/abs/2409.00786v1",
    "title": "Zero-Shot Paragraph-level Handwriting Imitation with Latent Diffusion\n  Models",
    "authors": [
      "Martin Mayr",
      "Marcel Dreier",
      "Florian Kordon",
      "Mathias Seuret",
      "Jochen Z\u00f6llner",
      "Fei Wu",
      "Andreas Maier",
      "Vincent Christlein"
    ],
    "abstract": "  The imitation of cursive handwriting is mainly limited to generating\nhandwritten words or lines. Multiple synthetic outputs must be stitched\ntogether to create paragraphs or whole pages, whereby consistency and layout\ninformation are lost. To close this gap, we propose a method for imitating\nhandwriting at the paragraph level that also works for unseen writing styles.\nTherefore, we introduce a modified latent diffusion model that enriches the\nencoder-decoder mechanism with specialized loss functions that explicitly\npreserve the style and content. We enhance the attention mechanism of the\ndiffusion model with adaptive 2D positional encoding and the conditioning\nmechanism to work with two modalities simultaneously: a style image and the\ntarget text. This significantly improves the realism of the generated\nhandwriting. Our approach sets a new benchmark in our comprehensive evaluation.\nIt outperforms all existing imitation methods at both line and paragraph\nlevels, considering combined style and content preservation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-09-01T17:33:31Z",
    "updated": "2024-09-01T17:33:31Z",
    "doi": null
  },
  "2407.14245": {
    "id": "http://arxiv.org/abs/2407.14245v1",
    "title": "Dataset Distillation by Automatic Training Trajectories",
    "authors": [
      "Dai Liu",
      "Jindong Gu",
      "Hu Cao",
      "Carsten Trinitis",
      "Martin Schulz"
    ],
    "abstract": "  Dataset Distillation is used to create a concise, yet informative, synthetic\ndataset that can replace the original dataset for training purposes. Some\nleading methods in this domain prioritize long-range matching, involving the\nunrolling of training trajectories with a fixed number of steps (NS) on the\nsynthetic dataset to align with various expert training trajectories. However,\ntraditional long-range matching methods possess an overfitting-like problem,\nthe fixed step size NS forces synthetic dataset to distortedly conform seen\nexpert training trajectories, resulting in a loss of generality-especially to\nthose from unencountered architecture. We refer to this as the Accumulated\nMismatching Problem (AMP), and propose a new approach, Automatic Training\nTrajectories (ATT), which dynamically and adaptively adjusts trajectory length\nNS to address the AMP. Our method outperforms existing methods particularly in\ntests involving cross-architectures. Moreover, owing to its adaptive nature, it\nexhibits enhanced stability in the face of parameter variations.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-19T12:27:11Z",
    "updated": "2024-07-19T12:27:11Z",
    "doi": null
  },
  "2302.04973": {
    "id": "http://arxiv.org/abs/2302.04973v2",
    "title": "Invariant Slot Attention: Object Discovery with Slot-Centric Reference\n  Frames",
    "authors": [
      "Ondrej Biza",
      "Sjoerd van Steenkiste",
      "Mehdi S. M. Sajjadi",
      "Gamaleldin F. Elsayed",
      "Aravindh Mahendran",
      "Thomas Kipf"
    ],
    "abstract": "  Automatically discovering composable abstractions from raw perceptual data is\na long-standing challenge in machine learning. Recent slot-based neural\nnetworks that learn about objects in a self-supervised manner have made\nexciting progress in this direction. However, they typically fall short at\nadequately capturing spatial symmetries present in the visual world, which\nleads to sample inefficiency, such as when entangling object appearance and\npose. In this paper, we present a simple yet highly effective method for\nincorporating spatial symmetries via slot-centric reference frames. We\nincorporate equivariance to per-object pose transformations into the attention\nand generation mechanism of Slot Attention by translating, scaling, and\nrotating position encodings. These changes result in little computational\noverhead, are easy to implement, and can result in large gains in terms of data\nefficiency and overall improvements to object discovery. We evaluate our method\non a wide range of synthetic object discovery benchmarks namely CLEVR,\nTetrominoes, CLEVRTex, Objects Room and MultiShapeNet, and show promising\nimprovements on the challenging real-world Waymo Open dataset.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-09T23:25:28Z",
    "updated": "2023-07-21T01:40:31Z",
    "doi": null
  },
  "2204.14095": {
    "id": "http://arxiv.org/abs/2204.14095v2",
    "title": "PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model\n  Pretraining",
    "authors": [
      "Yuting Gao",
      "Jinfeng Liu",
      "Zihan Xu",
      "Jun Zhang",
      "Ke Li",
      "Rongrong Ji",
      "Chunhua Shen"
    ],
    "abstract": "  Large-scale vision-language pre-training has achieved promising results on\ndownstream tasks. Existing methods highly rely on the assumption that the\nimage-text pairs crawled from the Internet are in perfect one-to-one\ncorrespondence. However, in real scenarios, this assumption can be difficult to\nhold: the text description, obtained by crawling the affiliated metadata of the\nimage, often suffers from the semantic mismatch and the mutual compatibility.\nTo address these issues, we introduce PyramidCLIP, which constructs an input\npyramid with different semantic levels for each modality, and aligns visual\nelements and linguistic elements in the form of hierarchy via peer-level\nsemantics alignment and cross-level relation alignment. Furthermore, we soften\nthe loss of negative samples (unpaired samples) so as to weaken the strict\nconstraint during the pre-training stage, thus mitigating the risk of forcing\nthe model to distinguish compatible negative pairs. Experiments on five\ndownstream tasks demonstrate the effectiveness of the proposed PyramidCLIP. In\nparticular, with the same amount of 15 million pre-training image-text pairs,\nPyramidCLIP exceeds CLIP on ImageNet zero-shot classification top-1 accuracy by\n10.6%/13.2%/10.0% with ResNet50/ViT-B32/ViT-B16 based image encoder\nrespectively. When scaling to larger datasets, PyramidCLIP achieves the\nstate-of-the-art results on several downstream tasks. In particular, the\nresults of PyramidCLIP-ResNet50 trained on 143M image-text pairs surpass that\nof CLIP using 400M data on ImageNet zero-shot classification task,\nsignificantly improving the data efficiency of CLIP.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-29T13:38:42Z",
    "updated": "2022-05-28T08:52:58Z",
    "doi": null
  },
  "2009.12559": {
    "id": "http://arxiv.org/abs/2009.12559v1",
    "title": "Affinity Space Adaptation for Semantic Segmentation Across Domains",
    "authors": [
      "Wei Zhou",
      "Yukang Wang",
      "Jiajia Chu",
      "Jiehua Yang",
      "Xiang Bai",
      "Yongchao Xu"
    ],
    "abstract": "  Semantic segmentation with dense pixel-wise annotation has achieved excellent\nperformance thanks to deep learning. However, the generalization of semantic\nsegmentation in the wild remains challenging. In this paper, we address the\nproblem of unsupervised domain adaptation (UDA) in semantic segmentation.\nMotivated by the fact that source and target domain have invariant semantic\nstructures, we propose to exploit such invariance across domains by leveraging\nco-occurring patterns between pairwise pixels in the output of structured\nsemantic segmentation. This is different from most existing approaches that\nattempt to adapt domains based on individual pixel-wise information in image,\nfeature, or output level. Specifically, we perform domain adaptation on the\naffinity relationship between adjacent pixels termed affinity space of source\nand target domain. To this end, we develop two affinity space adaptation\nstrategies: affinity space cleaning and adversarial affinity space alignment.\nExtensive experiments demonstrate that the proposed method achieves superior\nperformance against some state-of-the-art methods on several challenging\nbenchmarks for semantic segmentation across domains. The code is available at\nhttps://github.com/idealwei/ASANet.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-09-26T10:28:11Z",
    "updated": "2020-09-26T10:28:11Z",
    "doi": "10.1109/TIP.2020.3018221"
  },
  "2207.01887": {
    "id": "http://arxiv.org/abs/2207.01887v2",
    "title": "Open-Vocabulary Multi-Label Classification via Multi-Modal Knowledge\n  Transfer",
    "authors": [
      "Sunan He",
      "Taian Guo",
      "Tao Dai",
      "Ruizhi Qiao",
      "Bo Ren",
      "Shu-Tao Xia"
    ],
    "abstract": "  Real-world recognition system often encounters the challenge of unseen\nlabels. To identify such unseen labels, multi-label zero-shot learning (ML-ZSL)\nfocuses on transferring knowledge by a pre-trained textual label embedding\n(e.g., GloVe). However, such methods only exploit single-modal knowledge from a\nlanguage model, while ignoring the rich semantic information inherent in\nimage-text pairs. Instead, recently developed open-vocabulary (OV) based\nmethods succeed in exploiting such information of image-text pairs in object\ndetection, and achieve impressive performance. Inspired by the success of\nOV-based methods, we propose a novel open-vocabulary framework, named\nmulti-modal knowledge transfer (MKT), for multi-label classification.\nSpecifically, our method exploits multi-modal knowledge of image-text pairs\nbased on a vision and language pre-training (VLP) model. To facilitate\ntransferring the image-text matching ability of VLP model, knowledge\ndistillation is employed to guarantee the consistency of image and label\nembeddings, along with prompt tuning to further update the label embeddings. To\nfurther enable the recognition of multiple objects, a simple but effective\ntwo-stream module is developed to capture both local and global features.\nExtensive experimental results show that our method significantly outperforms\nstate-of-the-art methods on public benchmark datasets. The source code is\navailable at https://github.com/sunanhe/MKT.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-05T08:32:18Z",
    "updated": "2023-02-01T10:59:03Z",
    "doi": null
  },
  "2406.19680": {
    "id": "http://arxiv.org/abs/2406.19680v1",
    "title": "MimicMotion: High-Quality Human Motion Video Generation with\n  Confidence-aware Pose Guidance",
    "authors": [
      "Yuang Zhang",
      "Jiaxi Gu",
      "Li-Wen Wang",
      "Han Wang",
      "Junqi Cheng",
      "Yuefeng Zhu",
      "Fangyuan Zou"
    ],
    "abstract": "  In recent years, generative artificial intelligence has achieved significant\nadvancements in the field of image generation, spawning a variety of\napplications. However, video generation still faces considerable challenges in\nvarious aspects, such as controllability, video length, and richness of\ndetails, which hinder the application and popularization of this technology. In\nthis work, we propose a controllable video generation framework, dubbed\nMimicMotion, which can generate high-quality videos of arbitrary length\nmimicking specific motion guidance. Compared with previous methods, our\napproach has several highlights. Firstly, we introduce confidence-aware pose\nguidance that ensures high frame quality and temporal smoothness. Secondly, we\nintroduce regional loss amplification based on pose confidence, which\nsignificantly reduces image distortion. Lastly, for generating long and smooth\nvideos, we propose a progressive latent fusion strategy. By this means, we can\nproduce videos of arbitrary length with acceptable resource consumption. With\nextensive experiments and user studies, MimicMotion demonstrates significant\nimprovements over previous approaches in various aspects. Detailed results and\ncomparisons are available on our project page:\nhttps://tencent.github.io/MimicMotion .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-28T06:40:53Z",
    "updated": "2024-06-28T06:40:53Z",
    "doi": null
  },
  "2209.12325": {
    "id": "http://arxiv.org/abs/2209.12325v1",
    "title": "An Empirical Study on Cross-X Transfer for Legal Judgment Prediction",
    "authors": [
      "Joel Niklaus",
      "Matthias St\u00fcrmer",
      "Ilias Chalkidis"
    ],
    "abstract": "  Cross-lingual transfer learning has proven useful in a variety of Natural\nLanguage Processing (NLP) tasks, but it is understudied in the context of legal\nNLP, and not at all in Legal Judgment Prediction (LJP). We explore transfer\nlearning techniques on LJP using the trilingual Swiss-Judgment-Prediction\ndataset, including cases written in three languages. We find that cross-lingual\ntransfer improves the overall results across languages, especially when we use\nadapter-based fine-tuning. Finally, we further improve the model's performance\nby augmenting the training dataset with machine-translated versions of the\noriginal documents, using a 3x larger training corpus. Further on, we perform\nan analysis exploring the effect of cross-domain and cross-regional transfer,\ni.e., train a model across domains (legal areas), or regions. We find that in\nboth settings (legal areas, origin regions), models trained across all groups\nperform overall better, while they also have improved results in the worst-case\nscenarios. Finally, we report improved results when we ambitiously apply\ncross-jurisdiction transfer, where we further augment our dataset with Indian\nlegal cases.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68T50",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.2",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-25T21:41:56Z",
    "updated": "2022-09-25T21:41:56Z",
    "doi": null
  },
  "1806.02920": {
    "id": "http://arxiv.org/abs/1806.02920v1",
    "title": "GAIN: Missing Data Imputation using Generative Adversarial Nets",
    "authors": [
      "Jinsung Yoon",
      "James Jordon",
      "Mihaela van der Schaar"
    ],
    "abstract": "  We propose a novel method for imputing missing data by adapting the\nwell-known Generative Adversarial Nets (GAN) framework. Accordingly, we call\nour method Generative Adversarial Imputation Nets (GAIN). The generator (G)\nobserves some components of a real data vector, imputes the missing components\nconditioned on what is actually observed, and outputs a completed vector. The\ndiscriminator (D) then takes a completed vector and attempts to determine which\ncomponents were actually observed and which were imputed. To ensure that D\nforces G to learn the desired distribution, we provide D with some additional\ninformation in the form of a hint vector. The hint reveals to D partial\ninformation about the missingness of the original sample, which is used by D to\nfocus its attention on the imputation quality of particular components. This\nhint ensures that G does in fact learn to generate according to the true data\ndistribution. We tested our method on various datasets and found that GAIN\nsignificantly outperforms state-of-the-art imputation methods.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-06-07T22:57:16Z",
    "updated": "2018-06-07T22:57:16Z",
    "doi": null
  },
  "2402.11487": {
    "id": "http://arxiv.org/abs/2402.11487v2",
    "title": "Visual Concept-driven Image Generation with Text-to-Image Diffusion\n  Model",
    "authors": [
      "Tanzila Rahman",
      "Shweta Mahajan",
      "Hsin-Ying Lee",
      "Jian Ren",
      "Sergey Tulyakov",
      "Leonid Sigal"
    ],
    "abstract": "  Text-to-image (TTI) diffusion models have demonstrated impressive results in\ngenerating high-resolution images of complex and imaginative scenes. Recent\napproaches have further extended these methods with personalization techniques\nthat allow them to integrate user-illustrated concepts (e.g., the user\nhim/herself) using a few sample image illustrations. However, the ability to\ngenerate images with multiple interacting concepts, such as human subjects, as\nwell as concepts that may be entangled in one, or across multiple, image\nillustrations remains illusive. In this work, we propose a concept-driven TTI\npersonalization framework that addresses these core challenges. We build on\nexisting works that learn custom tokens for user-illustrated concepts, allowing\nthose to interact with existing text tokens in the TTI model. However,\nimportantly, to disentangle and better learn the concepts in question, we\njointly learn (latent) segmentation masks that disentangle these concepts in\nuser-provided image illustrations. We do so by introducing an Expectation\nMaximization (EM)-like optimization procedure where we alternate between\nlearning the custom tokens and estimating (latent) masks encompassing\ncorresponding concepts in user-supplied images. We obtain these masks based on\ncross-attention, from within the U-Net parameterized latent diffusion model and\nsubsequent DenseCRF optimization. We illustrate that such joint alternating\nrefinement leads to the learning of better tokens for concepts and, as a\nby-product, latent masks. We illustrate the benefits of the proposed approach\nqualitatively and quantitatively with several examples and use cases that can\ncombine three or more entangled concepts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-18T07:28:37Z",
    "updated": "2024-07-17T01:47:16Z",
    "doi": null
  },
  "2406.11132": {
    "id": "http://arxiv.org/abs/2406.11132v1",
    "title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language\n  Models Agents",
    "authors": [
      "Weizhe Chen",
      "Sven Koenig",
      "Bistra Dilkina"
    ],
    "abstract": "  In this past year, large language models (LLMs) have had remarkable success\nin domains outside the traditional natural language processing, and people are\nstarting to explore the usage of LLMs in more general and close to application\ndomains like code generation, travel planning, and robot controls. Connecting\nthese LLMs with great capacity and external tools, people are building the\nso-called LLM agents, which are supposed to help people do all kinds of work in\neveryday life. In all these domains, the prompt to the LLMs has been shown to\nmake a big difference in what the LLM would generate and thus affect the\nperformance of the LLM agents. Therefore, automatic prompt engineering has\nbecome an important question for many researchers and users of LLMs. In this\npaper, we propose a novel method, \\textsc{RePrompt}, which does \"gradient\ndescent\" to optimize the step-by-step instructions in the prompt of the LLM\nagents based on the chat history obtained from interactions with LLM agents. By\noptimizing the prompt, the LLM will learn how to plan in specific domains. We\nhave used experiments in PDDL generation and travel planning to show that our\nmethod could generally improve the performance for different reasoning tasks\nwhen using the updated prompt as the initial prompt.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-17T01:23:11Z",
    "updated": "2024-06-17T01:23:11Z",
    "doi": null
  },
  "2312.06742": {
    "id": "http://arxiv.org/abs/2312.06742v2",
    "title": "Honeybee: Locality-enhanced Projector for Multimodal LLM",
    "authors": [
      "Junbum Cha",
      "Wooyoung Kang",
      "Jonghwan Mun",
      "Byungseok Roh"
    ],
    "abstract": "  In Multimodal Large Language Models (MLLMs), a visual projector plays a\ncrucial role in bridging pre-trained vision encoders with LLMs, enabling\nprofound visual understanding while harnessing the LLMs' robust capabilities.\nDespite the importance of the visual projector, it has been relatively less\nexplored. In this study, we first identify two essential projector properties:\n(i) flexibility in managing the number of visual tokens, crucial for MLLMs'\noverall efficiency, and (ii) preservation of local context from visual\nfeatures, vital for spatial understanding. Based on these findings, we propose\na novel projector design that is both flexible and locality-enhanced,\neffectively satisfying the two desirable properties. Additionally, we present\ncomprehensive strategies to effectively utilize multiple and multifaceted\ninstruction datasets. Through extensive experiments, we examine the impact of\nindividual design choices. Finally, our proposed MLLM, Honeybee, remarkably\noutperforms previous state-of-the-art methods across various benchmarks,\nincluding MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly\nhigher efficiency. Code and models are available at\nhttps://github.com/kakaobrain/honeybee.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-11T18:59:06Z",
    "updated": "2024-04-01T03:00:06Z",
    "doi": null
  },
  "2402.09966": {
    "id": "http://arxiv.org/abs/2402.09966v1",
    "title": "Textual Localization: Decomposing Multi-concept Images for\n  Subject-Driven Text-to-Image Generation",
    "authors": [
      "Junjie Shentu",
      "Matthew Watson",
      "Noura Al Moubayed"
    ],
    "abstract": "  Subject-driven text-to-image diffusion models empower users to tailor the\nmodel to new concepts absent in the pre-training dataset using a few sample\nimages. However, prevalent subject-driven models primarily rely on\nsingle-concept input images, facing challenges in specifying the target concept\nwhen dealing with multi-concept input images. To this end, we introduce a\ntextual localized text-to-image model (Texual Localization) to handle\nmulti-concept input images. During fine-tuning, our method incorporates a novel\ncross-attention guidance to decompose multiple concepts, establishing distinct\nconnections between the visual representation of the target concept and the\nidentifier token in the text prompt. Experimental results reveal that our\nmethod outperforms or performs comparably to the baseline models in terms of\nimage fidelity and image-text alignment on multi-concept input images. In\ncomparison to Custom Diffusion, our method with hard guidance achieves CLIP-I\nscores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85%\nhigher in single-concept and multi-concept generation, respectively. Notably,\nour method generates cross-attention maps consistent with the target concept in\nthe generated images, a capability absent in existing models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-15T14:19:42Z",
    "updated": "2024-02-15T14:19:42Z",
    "doi": null
  },
  "2104.08718": {
    "id": "http://arxiv.org/abs/2104.08718v3",
    "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
    "authors": [
      "Jack Hessel",
      "Ari Holtzman",
      "Maxwell Forbes",
      "Ronan Le Bras",
      "Yejin Choi"
    ],
    "abstract": "  Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n  In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-18T05:00:29Z",
    "updated": "2022-03-23T19:47:21Z",
    "doi": null
  },
  "2312.06635": {
    "id": "http://arxiv.org/abs/2312.06635v6",
    "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
    "authors": [
      "Songlin Yang",
      "Bailin Wang",
      "Yikang Shen",
      "Rameswar Panda",
      "Yoon Kim"
    ],
    "abstract": "  Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-11T18:51:59Z",
    "updated": "2024-08-27T01:27:29Z",
    "doi": null
  },
  "2008.05221": {
    "id": "http://arxiv.org/abs/2008.05221v4",
    "title": "Compression of Deep Learning Models for Text: A Survey",
    "authors": [
      "Manish Gupta",
      "Puneet Agrawal"
    ],
    "abstract": "  In recent years, the fields of natural language processing (NLP) and\ninformation retrieval (IR) have made tremendous progress thanksto deep learning\nmodels like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and\nLong Short-Term Memory (LSTMs)networks, and Transformer [120] based models like\nBidirectional Encoder Representations from Transformers (BERT) [24],\nGenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network\n(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer\ntransformer (T5) [95], T-NLG [98] and GShard [63]. But these models are\nhumongous in size. On the other hand,real world applications demand small model\nsize, low response times and low computational power wattage. In this survey,\nwediscuss six different types of methods (Pruning, Quantization, Knowledge\nDistillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic\nTransformer based methods) for compression of such models to enable their\ndeployment in real industry NLP projects.Given the critical need of building\napplications with efficient and small models, and the large amount of recently\npublished work inthis area, we believe that this survey organizes the plethora\nof work done by the 'deep learning for NLP' community in the past fewyears and\npresents it as a coherent story.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-08-12T10:42:14Z",
    "updated": "2021-06-13T17:47:28Z",
    "doi": null
  },
  "2309.17179": {
    "id": "http://arxiv.org/abs/2309.17179v2",
    "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and\n  Training",
    "authors": [
      "Xidong Feng",
      "Ziyu Wan",
      "Muning Wen",
      "Stephen Marcus McAleer",
      "Ying Wen",
      "Weinan Zhang",
      "Jun Wang"
    ],
    "abstract": "  Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim\nto augment the reasoning capabilities of LLMs by using tree-search algorithms\nto guide multi-step reasoning. These methods rely on prompting a pre-trained\nmodel to serve as a value function and focus on problems with low search depth.\nAs a result, these methods will not work in domains where the pre-trained LLM\ndoes not have enough knowledge to serve as an effective value function or in\ndomains that require long-horizon planning. To address these limitations, we\npresent an AlphaZero-like tree-search learning framework for LLMs (termed\nTS-LLM), systematically illustrating how tree-search with a learned value\nfunction can guide LLM decoding. TS-LLM distinguishes itself in two key ways.\n(1) Leveraging a learned value function and AlphaZero-like algorithms, our\napproach can be generally adaptable to a wide range of tasks, language models\nof any size, and tasks of varying search depths. (2) Our approach can guide\nLLMs during both inference and training, iteratively improving the LLM.\nEmpirical results across reasoning, planning, alignment, and decision-making\ntasks show that TS-LLM outperforms existing approaches and can handle trees\nwith a depth of 64.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-29T12:20:19Z",
    "updated": "2024-02-09T00:13:46Z",
    "doi": null
  },
  "1909.01066": {
    "id": "http://arxiv.org/abs/1909.01066v2",
    "title": "Language Models as Knowledge Bases?",
    "authors": [
      "Fabio Petroni",
      "Tim Rockt\u00e4schel",
      "Patrick Lewis",
      "Anton Bakhtin",
      "Yuxiang Wu",
      "Alexander H. Miller",
      "Sebastian Riedel"
    ],
    "abstract": "  Recent progress in pretraining language models on large textual corpora led\nto a surge of improvements for downstream NLP tasks. Whilst learning linguistic\nknowledge, these models may also be storing relational knowledge present in the\ntraining data, and may be able to answer queries structured as\n\"fill-in-the-blank\" cloze statements. Language models have many advantages over\nstructured knowledge bases: they require no schema engineering, allow\npractitioners to query about an open class of relations, are easy to extend to\nmore data, and require no human supervision to train. We present an in-depth\nanalysis of the relational knowledge already present (without fine-tuning) in a\nwide range of state-of-the-art pretrained language models. We find that (i)\nwithout fine-tuning, BERT contains relational knowledge competitive with\ntraditional NLP methods that have some access to oracle knowledge, (ii) BERT\nalso does remarkably well on open-domain question answering against a\nsupervised baseline, and (iii) certain types of factual knowledge are learned\nmuch more readily than others by standard language model pretraining\napproaches. The surprisingly strong ability of these models to recall factual\nknowledge without any fine-tuning demonstrates their potential as unsupervised\nopen-domain QA systems. The code to reproduce our analysis is available at\nhttps://github.com/facebookresearch/LAMA.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-09-03T11:11:08Z",
    "updated": "2019-09-04T09:33:20Z",
    "doi": null
  },
  "2111.07991": {
    "id": "http://arxiv.org/abs/2111.07991v3",
    "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning",
    "authors": [
      "Xiaohua Zhai",
      "Xiao Wang",
      "Basil Mustafa",
      "Andreas Steiner",
      "Daniel Keysers",
      "Alexander Kolesnikov",
      "Lucas Beyer"
    ],
    "abstract": "  This paper presents contrastive-tuning, a simple method employing contrastive\ntraining to align image and text models while still taking advantage of their\npre-training. In our empirical study we find that locked pre-trained image\nmodels with unlocked text models work best. We call this instance of\ncontrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model\nto read out good representations from a pre-trained image model for new tasks.\nA LiT model gains the capability of zero-shot transfer to new vision tasks,\nsuch as image classification or retrieval. The proposed LiT is widely\napplicable; it works reliably with multiple pre-training methods (supervised\nand unsupervised) and across diverse architectures (ResNet, Vision Transformers\nand MLP-Mixer) using three different image-text datasets. With the\ntransformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2%\nzero-shot transfer accuracy on the ImageNet test set, and 82.5% on the\nchallenging out-of-distribution ObjectNet test set.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-15T18:53:48Z",
    "updated": "2022-06-22T14:43:02Z",
    "doi": null
  },
  "2207.04324": {
    "id": "http://arxiv.org/abs/2207.04324v2",
    "title": "Video Coding Using Learned Latent GAN Compression",
    "authors": [
      "Mustafa Shukor",
      "Bharath Bhushan Damodaran",
      "Xu Yao",
      "Pierre Hellier"
    ],
    "abstract": "  We propose in this paper a new paradigm for facial video compression. We\nleverage the generative capacity of GANs such as StyleGAN to represent and\ncompress a video, including intra and inter compression. Each frame is inverted\nin the latent space of StyleGAN, from which the optimal compression is learned.\nTo do so, a diffeomorphic latent representation is learned using a normalizing\nflows model, where an entropy model can be optimized for image coding. In\naddition, we propose a new perceptual loss that is more efficient than other\ncounterparts. Finally, an entropy model for video inter coding with residual is\nalso learned in the previously constructed latent representation. Our method\n(SGANC) is simple, faster to train, and achieves better results for image and\nvideo coding compared to state-of-the-art codecs such as VTM, AV1, and recent\ndeep learning techniques. In particular, it drastically minimizes perceptual\ndistortion at low bit rates.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-09T19:07:43Z",
    "updated": "2022-07-12T21:17:39Z",
    "doi": null
  },
  "1709.01509": {
    "id": "http://arxiv.org/abs/1709.01509v1",
    "title": "Linking Generative Adversarial Learning and Binary Classification",
    "authors": [
      "Akshay Balsubramani"
    ],
    "abstract": "  In this note, we point out a basic link between generative adversarial (GA)\ntraining and binary classification -- any powerful discriminator essentially\ncomputes an (f-)divergence between real and generated samples. The result,\nrepeatedly re-derived in decision theory, has implications for GA Networks\n(GANs), providing an alternative perspective on training f-GANs by designing\nthe discriminator loss function.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-09-05T17:55:59Z",
    "updated": "2017-09-05T17:55:59Z",
    "doi": null
  },
  "2306.00914": {
    "id": "http://arxiv.org/abs/2306.00914v3",
    "title": "Conditioning Diffusion Models via Attributes and Semantic Masks for Face\n  Generation",
    "authors": [
      "Nico Giambi",
      "Giuseppe Lisanti"
    ],
    "abstract": "  Deep generative models have shown impressive results in generating realistic\nimages of faces. GANs managed to generate high-quality, high-fidelity images\nwhen conditioned on semantic masks, but they still lack the ability to\ndiversify their output. Diffusion models partially solve this problem and are\nable to generate diverse samples given the same condition. In this paper, we\npropose a multi-conditioning approach for diffusion models via cross-attention\nexploiting both attributes and semantic masks to generate high-quality and\ncontrollable face images. We also studied the impact of applying\nperceptual-focused loss weighting into the latent space instead of the pixel\nspace. Our method extends the previous approaches by introducing conditioning\non more than one set of features, guaranteeing a more fine-grained control over\nthe generated face images. We evaluate our approach on the CelebA-HQ dataset,\nand we show that it can generate realistic and diverse samples while allowing\nfor fine-grained control over multiple attributes and semantic regions.\nAdditionally, we perform an ablation study to evaluate the impact of different\nconditioning strategies on the quality and diversity of the generated images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-01T17:16:37Z",
    "updated": "2023-09-27T18:13:12Z",
    "doi": null
  },
  "2401.14361": {
    "id": "http://arxiv.org/abs/2401.14361v2",
    "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
    "authors": [
      "Leyang Xue",
      "Yao Fu",
      "Zhan Lu",
      "Luo Mai",
      "Mahesh Marina"
    ],
    "abstract": "  This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PF",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-25T18:07:50Z",
    "updated": "2024-08-01T13:21:24Z",
    "doi": null
  },
  "2301.13622": {
    "id": "http://arxiv.org/abs/2301.13622v2",
    "title": "Learning Data Representations with Joint Diffusion Models",
    "authors": [
      "Kamil Deja",
      "Tomasz Trzcinski",
      "Jakub M. Tomczak"
    ],
    "abstract": "  Joint machine learning models that allow synthesizing and classifying data\noften offer uneven performance between those tasks or are unstable to train. In\nthis work, we depart from a set of empirical observations that indicate the\nusefulness of internal representations built by contemporary deep\ndiffusion-based generative models not only for generating but also predicting.\nWe then propose to extend the vanilla diffusion model with a classifier that\nallows for stable joint end-to-end training with shared parameterization\nbetween those objectives. The resulting joint diffusion model outperforms\nrecent state-of-the-art hybrid methods in terms of both classification and\ngeneration quality on all evaluated benchmarks. On top of our joint training\napproach, we present how we can directly benefit from shared generative and\ndiscriminative representations by introducing a method for visual\ncounterfactual explanations.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-31T13:29:19Z",
    "updated": "2023-04-05T13:09:54Z",
    "doi": null
  },
  "2302.14859": {
    "id": "http://arxiv.org/abs/2302.14859v2",
    "title": "BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis",
    "authors": [
      "Lior Yariv",
      "Peter Hedman",
      "Christian Reiser",
      "Dor Verbin",
      "Pratul P. Srinivasan",
      "Richard Szeliski",
      "Jonathan T. Barron",
      "Ben Mildenhall"
    ],
    "abstract": "  We present a method for reconstructing high-quality meshes of large unbounded\nreal-world scenes suitable for photorealistic novel view synthesis. We first\noptimize a hybrid neural volume-surface scene representation designed to have\nwell-behaved level sets that correspond to surfaces in the scene. We then bake\nthis representation into a high-quality triangle mesh, which we equip with a\nsimple and fast view-dependent appearance model based on spherical Gaussians.\nFinally, we optimize this baked representation to best reproduce the captured\nviewpoints, resulting in a model that can leverage accelerated polygon\nrasterization pipelines for real-time view synthesis on commodity hardware. Our\napproach outperforms previous scene representations for real-time rendering in\nterms of accuracy, speed, and power consumption, and produces high quality\nmeshes that enable applications such as appearance editing and physical\nsimulation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-28T18:58:03Z",
    "updated": "2023-05-16T15:01:42Z",
    "doi": null
  },
  "2211.00593": {
    "id": "http://arxiv.org/abs/2211.00593v1",
    "title": "Interpretability in the Wild: a Circuit for Indirect Object\n  Identification in GPT-2 small",
    "authors": [
      "Kevin Wang",
      "Alexandre Variengien",
      "Arthur Conmy",
      "Buck Shlegeris",
      "Jacob Steinhardt"
    ],
    "abstract": "  Research in mechanistic interpretability seeks to explain behaviors of\nmachine learning models in terms of their internal components. However, most\nprevious work either focuses on simple behaviors in small models, or describes\ncomplicated behaviors in larger models with broad strokes. In this work, we\nbridge this gap by presenting an explanation for how GPT-2 small performs a\nnatural language task called indirect object identification (IOI). Our\nexplanation encompasses 26 attention heads grouped into 7 main classes, which\nwe discovered using a combination of interpretability approaches relying on\ncausal interventions. To our knowledge, this investigation is the largest\nend-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a\nlanguage model. We evaluate the reliability of our explanation using three\nquantitative criteria--faithfulness, completeness and minimality. Though these\ncriteria support our explanation, they also point to remaining gaps in our\nunderstanding. Our work provides evidence that a mechanistic understanding of\nlarge ML models is feasible, opening opportunities to scale our understanding\nto both larger models and more complex tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-01T17:08:44Z",
    "updated": "2022-11-01T17:08:44Z",
    "doi": null
  },
  "2106.00291": {
    "id": "http://arxiv.org/abs/2106.00291v1",
    "title": "Preview, Attend and Review: Schema-Aware Curriculum Learning for\n  Multi-Domain Dialog State Tracking",
    "authors": [
      "Yinpei Dai",
      "Hangyu Li",
      "Yongbin Li",
      "Jian Sun",
      "Fei Huang",
      "Luo Si",
      "Xiaodan Zhu"
    ],
    "abstract": "  Existing dialog state tracking (DST) models are trained with dialog data in a\nrandom order, neglecting rich structural information in a dataset. In this\npaper, we propose to use curriculum learning (CL) to better leverage both the\ncurriculum structure and schema structure for task-oriented dialogs.\nSpecifically, we propose a model-agnostic framework called Schema-aware\nCurriculum Learning for Dialog State Tracking (SaCLog), which consists of a\npreview module that pre-trains a DST model with schema information, a\ncurriculum module that optimizes the model with CL, and a review module that\naugments mispredicted data to reinforce the CL training. We show that our\nproposed approach improves DST performance over both a transformer-based and\nRNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art\nresults on WOZ2.0 and MultiWOZ2.1.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-01T07:52:35Z",
    "updated": "2021-06-01T07:52:35Z",
    "doi": null
  },
  "2210.12254": {
    "id": "http://arxiv.org/abs/2210.12254v2",
    "title": "Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models",
    "authors": [
      "Vikram Voleti",
      "Christopher Pal",
      "Adam Oberman"
    ],
    "abstract": "  Generative models based on denoising diffusion techniques have led to an\nunprecedented increase in the quality and diversity of imagery that is now\npossible to create with neural generative models. However, most contemporary\nstate-of-the-art methods are derived from a standard isotropic Gaussian\nformulation. In this work we examine the situation where non-isotropic Gaussian\ndistributions are used. We present the key mathematical derivations for\ncreating denoising diffusion models using an underlying non-isotropic Gaussian\nnoise model. We also provide initial experiments with the CIFAR-10 dataset to\nhelp verify empirically that this more general modeling approach can also yield\nhigh-quality samples.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-21T21:16:46Z",
    "updated": "2022-11-23T00:40:58Z",
    "doi": null
  },
  "2310.08785": {
    "id": "http://arxiv.org/abs/2310.08785v1",
    "title": "DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided\n  Image Editing",
    "authors": [
      "Yueming Lyu",
      "Kang Zhao",
      "Bo Peng",
      "Yue Jiang",
      "Yingya Zhang",
      "Jing Dong"
    ],
    "abstract": "  Text-guided image editing faces significant challenges to training and\ninference flexibility. Much literature collects large amounts of annotated\nimage-text pairs to train text-conditioned generative models from scratch,\nwhich is expensive and not efficient. After that, some approaches that leverage\npre-trained vision-language models are put forward to avoid data collection,\nbut they are also limited by either per text-prompt optimization or\ninference-time hyper-parameters tuning. To address these issues, we investigate\nand identify a specific space, referred to as CLIP DeltaSpace, where the CLIP\nvisual feature difference of two images is semantically aligned with the CLIP\ntextual feature difference of their corresponding text descriptions. Based on\nDeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP\nvisual feature differences to the latent space directions of a generative model\nduring the training phase, and predicts the latent space directions from the\nCLIP textual feature differences during the inference phase. And this design\nendows DeltaEdit with two advantages: (1) text-free training; (2)\ngeneralization to various text prompts for zero-shot inference. Extensive\nexperiments validate the effectiveness and versatility of DeltaEdit with\ndifferent generative models, including both the GAN model and the diffusion\nmodel, in achieving flexible text-guided image editing. Code is available at\nhttps://github.com/Yueming6568/DeltaEdit.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-12T15:43:12Z",
    "updated": "2023-10-12T15:43:12Z",
    "doi": null
  },
  "2106.15326": {
    "id": "http://arxiv.org/abs/2106.15326v1",
    "title": "Source-free Domain Adaptation via Avatar Prototype Generation and\n  Adaptation",
    "authors": [
      "Zhen Qiu",
      "Yifan Zhang",
      "Hongbin Lin",
      "Shuaicheng Niu",
      "Yanxia Liu",
      "Qing Du",
      "Mingkui Tan"
    ],
    "abstract": "  We study a practical domain adaptation task, called source-free unsupervised\ndomain adaptation (UDA) problem, in which we cannot access source domain data\ndue to data privacy issues but only a pre-trained source model and unlabeled\ntarget data are available. This task, however, is very difficult due to one key\nchallenge: the lack of source data and target domain labels makes model\nadaptation very challenging. To address this, we propose to mine the hidden\nknowledge in the source model and exploit it to generate source avatar\nprototypes (i.e., representative features for each source class) as well as\ntarget pseudo labels for domain alignment. To this end, we propose a\nContrastive Prototype Generation and Adaptation (CPGA) method. Specifically,\nCPGA consists of two stages: (1) prototype generation: by exploring the\nclassification boundary information of the source model, we train a prototype\ngenerator to generate avatar prototypes via contrastive learning. (2) prototype\nadaptation: based on the generated source prototypes and target pseudo labels,\nwe develop a new robust contrastive prototype adaptation strategy to align each\npseudo-labeled target data to the corresponding source prototypes. Extensive\nexperiments on three UDA benchmark datasets demonstrate the effectiveness and\nsuperiority of the proposed method.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-18T08:30:54Z",
    "updated": "2021-06-18T08:30:54Z",
    "doi": null
  },
  "1712.02514": {
    "id": "http://arxiv.org/abs/1712.02514v1",
    "title": "TV-GAN: Generative Adversarial Network Based Thermal to Visible Face\n  Recognition",
    "authors": [
      "Teng Zhang",
      "Arnold Wiliem",
      "Siqi Yang",
      "Brian C. Lovell"
    ],
    "abstract": "  This work tackles the face recognition task on images captured using thermal\ncamera sensors which can operate in the non-light environment. While it can\ngreatly increase the scope and benefits of the current security surveillance\nsystems, performing such a task using thermal images is a challenging problem\ncompared to face recognition task in the Visible Light Domain (VLD). This is\npartly due to the much smaller amount number of thermal imagery data collected\ncompared to the VLD data. Unfortunately, direct application of the existing\nvery strong face recognition models trained using VLD data into the thermal\nimagery data will not produce a satisfactory performance. This is due to the\nexistence of the domain gap between the thermal and VLD images. To this end, we\npropose a Thermal-to-Visible Generative Adversarial Network (TV-GAN) that is\nable to transform thermal face images into their corresponding VLD images\nwhilst maintaining identity information which is sufficient enough for the\nexisting VLD face recognition models to perform recognition. Some examples are\npresented in Figure 1. Unlike the previous methods, our proposed TV-GAN uses an\nexplicit closed-set face recognition loss to regularize the discriminator\nnetwork training. This information will then be conveyed into the generator\nnetwork in the forms of gradient loss. In the experiment, we show that by using\nthis additional explicit regularization for the discriminator network, the\nTV-GAN is able to preserve more identity information when translating a thermal\nimage of a person which is not seen before by the TV-GAN.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-12-07T07:06:39Z",
    "updated": "2017-12-07T07:06:39Z",
    "doi": null
  },
  "2103.02885": {
    "id": "http://arxiv.org/abs/2103.02885v1",
    "title": "Extract the Knowledge of Graph Neural Networks and Go Beyond it: An\n  Effective Knowledge Distillation Framework",
    "authors": [
      "Cheng Yang",
      "Jiawei Liu",
      "Chuan Shi"
    ],
    "abstract": "  Semi-supervised learning on graphs is an important problem in the machine\nlearning area. In recent years, state-of-the-art classification methods based\non graph neural networks (GNNs) have shown their superiority over traditional\nones such as label propagation. However, the sophisticated architectures of\nthese neural models will lead to a complex prediction mechanism, which could\nnot make full use of valuable prior knowledge lying in the data, e.g.,\nstructurally correlated nodes tend to have the same class. In this paper, we\npropose a framework based on knowledge distillation to address the above\nissues. Our framework extracts the knowledge of an arbitrary learned GNN model\n(teacher model), and injects it into a well-designed student model. The student\nmodel is built with two simple prediction mechanisms, i.e., label propagation\nand feature transformation, which naturally preserves structure-based and\nfeature-based prior knowledge, respectively. In specific, we design the student\nmodel as a trainable combination of parameterized label propagation and feature\ntransformation modules. As a result, the learned student can benefit from both\nprior knowledge and the knowledge in GNN teachers for more effective\npredictions. Moreover, the learned student model has a more interpretable\nprediction process than GNNs. We conduct experiments on five public benchmark\ndatasets and employ seven GNN models including GCN, GAT, APPNP, SAGE, SGC,\nGCNII and GLP as the teacher models. Experimental results show that the learned\nstudent model can consistently outperform its corresponding teacher model by\n1.4% - 4.7% on average. Code and data are available at\nhttps://github.com/BUPT-GAMMA/CPF\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-03-04T08:13:55Z",
    "updated": "2021-03-04T08:13:55Z",
    "doi": null
  },
  "2308.00356": {
    "id": "http://arxiv.org/abs/2308.00356v1",
    "title": "Deep Image Harmonization with Globally Guided Feature Transformation and\n  Relation Distillation",
    "authors": [
      "Li Niu",
      "Linfeng Tan",
      "Xinhao Tao",
      "Junyan Cao",
      "Fengjun Guo",
      "Teng Long",
      "Liqing Zhang"
    ],
    "abstract": "  Given a composite image, image harmonization aims to adjust the foreground\nillumination to be consistent with background. Previous methods have explored\ntransforming foreground features to achieve competitive performance. In this\nwork, we show that using global information to guide foreground feature\ntransformation could achieve significant improvement. Besides, we propose to\ntransfer the foreground-background relation from real images to composite\nimages, which can provide intermediate supervision for the transformed encoder\nfeatures. Additionally, considering the drawbacks of existing harmonization\ndatasets, we also contribute a ccHarmony dataset which simulates the natural\nillumination variation. Extensive experiments on iHarmony4 and our contributed\ndataset demonstrate the superiority of our method. Our ccHarmony dataset is\nreleased at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-01T07:53:25Z",
    "updated": "2023-08-01T07:53:25Z",
    "doi": null
  },
  "2210.10828": {
    "id": "http://arxiv.org/abs/2210.10828v1",
    "title": "Grounded Video Situation Recognition",
    "authors": [
      "Zeeshan Khan",
      "C. V. Jawahar",
      "Makarand Tapaswi"
    ],
    "abstract": "  Dense video understanding requires answering several questions such as who is\ndoing what to whom, with what, how, why, and where. Recently, Video Situation\nRecognition (VidSitu) is framed as a task for structured prediction of multiple\nevents, their relationships, and actions and various verb-role pairs attached\nto descriptive entities. This task poses several challenges in identifying,\ndisambiguating, and co-referencing entities across multiple verb-role pairs,\nbut also faces some challenges of evaluation. In this work, we propose the\naddition of spatio-temporal grounding as an essential component of the\nstructured prediction task in a weakly supervised setting, and present a novel\nthree stage Transformer model, VideoWhisperer, that is empowered to make joint\npredictions. In stage one, we learn contextualised embeddings for video\nfeatures in parallel with key objects that appear in the video clips to enable\nfine-grained spatio-temporal reasoning. The second stage sees verb-role queries\nattend and pool information from object embeddings, localising answers to\nquestions posed about the action. The final stage generates these answers as\ncaptions to describe each verb-role pair present in the video. Our model\noperates on a group of events (clips) simultaneously and predicts verbs,\nverb-role pairs, their nouns, and their grounding on-the-fly. When evaluated on\na grounding-augmented version of the VidSitu dataset, we observe a large\nimprovement in entity captioning accuracy, as well as the ability to localize\nverb-roles without grounding annotations at training time.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-19T18:38:10Z",
    "updated": "2022-10-19T18:38:10Z",
    "doi": null
  },
  "2304.01373": {
    "id": "http://arxiv.org/abs/2304.01373v2",
    "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and\n  Scaling",
    "authors": [
      "Stella Biderman",
      "Hailey Schoelkopf",
      "Quentin Anthony",
      "Herbie Bradley",
      "Kyle O'Brien",
      "Eric Hallahan",
      "Mohammad Aflah Khan",
      "Shivanshu Purohit",
      "USVSN Sai Prashanth",
      "Edward Raff",
      "Aviya Skowron",
      "Lintang Sutawika",
      "Oskar van der Wal"
    ],
    "abstract": "  How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\n\\url{https://github.com/EleutherAI/pythia}.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-03T20:58:15Z",
    "updated": "2023-05-31T17:54:07Z",
    "doi": null
  },
  "2211.02077": {
    "id": "http://arxiv.org/abs/2211.02077v1",
    "title": "Scaling Multimodal Pre-Training via Cross-Modality Gradient\n  Harmonization",
    "authors": [
      "Junru Wu",
      "Yi Liang",
      "Feng Han",
      "Hassan Akbari",
      "Zhangyang Wang",
      "Cong Yu"
    ],
    "abstract": "  Self-supervised pre-training recently demonstrates success on large-scale\nmultimodal data, and state-of-the-art contrastive learning methods often\nenforce the feature consistency from cross-modality inputs, such as video/audio\nor video/text pairs. Despite its convenience to formulate and leverage in\npractice, such cross-modality alignment (CMA) is only a weak and noisy\nsupervision, since two modalities can be semantically misaligned even they are\ntemporally aligned. For example, even in the commonly adopted instructional\nvideos, a speaker can sometimes refer to something that is not visually present\nin the current frame; and the semantic misalignment would only be more\nunpredictable for the raw videos from the internet. We conjecture that might\ncause conflicts and biases among modalities, and may hence prohibit CMA from\nscaling up to training with larger and more heterogeneous data. This paper\nfirst verifies our conjecture by observing that, even in the latest VATT\npre-training using only instructional videos, there exist strong gradient\nconflicts between different CMA losses within the same video, audio, text\ntriplet, indicating them as the noisy source of supervision. We then propose to\nharmonize such gradients, via two techniques: (i) cross-modality gradient\nrealignment: modifying different CMA loss gradients for each sample triplet, so\nthat their gradient directions are more aligned; and (ii) gradient-based\ncurriculum learning: leveraging the gradient conflict information on an\nindicator of sample noisiness, to develop a curriculum learning strategy to\nprioritize training on less noisy sample triplets. Applying those techniques to\npre-training VATT on the HowTo100M dataset, we consistently improve its\nperformance on different downstream tasks. Moreover, we are able to scale VATT\npre-training to more complicated non-narrative Youtube8M dataset to further\nimprove the state-of-the-arts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-03T18:12:32Z",
    "updated": "2022-11-03T18:12:32Z",
    "doi": null
  },
  "2406.02106": {
    "id": "http://arxiv.org/abs/2406.02106v1",
    "title": "MARS: Benchmarking the Metaphysical Reasoning Abilities of Language\n  Models with a Multi-task Evaluation Dataset",
    "authors": [
      "Weiqi Wang",
      "Yangqiu Song"
    ],
    "abstract": "  To enable Large Language Models (LLMs) to function as conscious agents with\ngeneralizable reasoning capabilities, it is crucial that they possess the\nreasoning ability to comprehend situational changes (transitions) in\ndistribution triggered by environmental factors or actions from other agents.\nDespite its fundamental significance, this ability remains underexplored due to\nthe complexity of modeling infinite possible changes in an event and their\nassociated distributions, coupled with the lack of benchmark data with\nsituational transitions. Addressing these gaps, we propose a novel formulation\nof reasoning with distributional changes as a three-step discriminative\nprocess, termed as MetAphysical ReaSoning. We then introduce the first-ever\nbenchmark, MARS, comprising three tasks corresponding to each step. These tasks\nsystematically assess LLMs' capabilities in reasoning the plausibility of (i)\nchanges in actions, (ii) states caused by changed actions, and (iii)\nsituational transitions driven by changes in action. Extensive evaluations with\n20 (L)LMs of varying sizes and methods indicate that all three tasks in this\nprocess pose significant challenges, even for state-of-the-art LLMs and LMs\nafter fine-tuning. Further analyses reveal potential causes for the\nunderperformance of LLMs and demonstrate that pre-training them on large-scale\nconceptualization taxonomies can potentially enhance their metaphysical\nreasoning capabilities. Our data and models are publicly accessible at\nhttps://github.com/HKUST-KnowComp/MARS.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-04T08:35:04Z",
    "updated": "2024-06-04T08:35:04Z",
    "doi": null
  },
  "1807.01622": {
    "id": "http://arxiv.org/abs/1807.01622v1",
    "title": "Neural Processes",
    "authors": [
      "Marta Garnelo",
      "Jonathan Schwarz",
      "Dan Rosenbaum",
      "Fabio Viola",
      "Danilo J. Rezende",
      "S. M. Ali Eslami",
      "Yee Whye Teh"
    ],
    "abstract": "  A neural network (NN) is a parameterised function that can be tuned via\ngradient descent to approximate a labelled collection of data with high\nprecision. A Gaussian process (GP), on the other hand, is a probabilistic model\nthat defines a distribution over possible functions, and is updated in light of\ndata via the rules of probabilistic inference. GPs are probabilistic,\ndata-efficient and flexible, however they are also computationally intensive\nand thus limited in their applicability. We introduce a class of neural latent\nvariable models which we call Neural Processes (NPs), combining the best of\nboth worlds. Like GPs, NPs define distributions over functions, are capable of\nrapid adaptation to new observations, and can estimate the uncertainty in their\npredictions. Like NNs, NPs are computationally efficient during training and\nevaluation but also learn to adapt their priors to data. We demonstrate the\nperformance of NPs on a range of learning tasks, including regression and\noptimisation, and compare and contrast with related models in the literature.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-07-04T14:49:46Z",
    "updated": "2018-07-04T14:49:46Z",
    "doi": null
  },
  "2112.11542": {
    "id": "http://arxiv.org/abs/2112.11542v1",
    "title": "MIA-Former: Efficient and Robust Vision Transformers via Multi-grained\n  Input-Adaptation",
    "authors": [
      "Zhongzhi Yu",
      "Yonggan Fu",
      "Sicheng Li",
      "Chaojian Li",
      "Yingyan Lin"
    ],
    "abstract": "  ViTs are often too computationally expensive to be fitted onto real-world\nresource-constrained devices, due to (1) their quadratically increased\ncomplexity with the number of input tokens and (2) their overparameterized\nself-attention heads and model depth. In parallel, different images are of\nvaried complexity and their different regions can contain various levels of\nvisual information, indicating that treating all regions/tokens equally in\nterms of model complexity is unnecessary while such opportunities for trimming\ndown ViTs' complexity have not been fully explored. To this end, we propose a\nMulti-grained Input-adaptive Vision Transformer framework dubbed MIA-Former\nthat can input-adaptively adjust the structure of ViTs at three\ncoarse-to-fine-grained granularities (i.e., model depth and the number of model\nheads/tokens). In particular, our MIA-Former adopts a low-cost network trained\nwith a hybrid supervised and reinforcement training method to skip unnecessary\nlayers, heads, and tokens in an input adaptive manner, reducing the overall\ncomputational cost. Furthermore, an interesting side effect of our MIA-Former\nis that its resulting ViTs are naturally equipped with improved robustness\nagainst adversarial attacks over their static counterparts, because\nMIA-Former's multi-grained dynamic control improves the model diversity similar\nto the effect of ensemble and thus increases the difficulty of adversarial\nattacks against all its sub-models. Extensive experiments and ablation studies\nvalidate that the proposed MIA-Former framework can effectively allocate\ncomputation budgets adaptive to the difficulty of input images meanwhile\nincrease robustness, achieving state-of-the-art (SOTA) accuracy-efficiency\ntrade-offs, e.g., 20% computation savings with the same or even a higher\naccuracy compared with SOTA dynamic transformer models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-12-21T22:06:24Z",
    "updated": "2021-12-21T22:06:24Z",
    "doi": null
  },
  "2204.13509": {
    "id": "http://arxiv.org/abs/2204.13509v2",
    "title": "On the Effect of Pretraining Corpora on In-context Learning by a\n  Large-scale Language Model",
    "authors": [
      "Seongjin Shin",
      "Sang-Woo Lee",
      "Hwijeen Ahn",
      "Sungdong Kim",
      "HyoungSeok Kim",
      "Boseop Kim",
      "Kyunghyun Cho",
      "Gichang Lee",
      "Woomyoung Park",
      "Jung-Woo Ha",
      "Nako Sung"
    ],
    "abstract": "  Many recent studies on large-scale language models have reported successful\nin-context zero- and few-shot learning ability. However, the in-depth analysis\nof when in-context learning occurs is still lacking. For example, it is unknown\nhow in-context learning performance changes as the training corpus varies.\nHere, we investigate the effects of the source and size of the pretraining\ncorpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From\nour in-depth investigation, we introduce the following observations: (1)\nin-context learning performance heavily depends on the corpus domain source,\nand the size of the pretraining corpus does not necessarily determine the\nemergence of in-context learning, (2) in-context learning ability can emerge\nwhen a language model is trained on a combination of multiple corpora, even\nwhen each corpus does not result in in-context learning on its own, (3)\npretraining with a corpus related to a downstream task does not always\nguarantee the competitive in-context learning performance of the downstream\ntask, especially in the few-shot setting, and (4) the relationship between\nlanguage modeling (measured in perplexity) and in-context learning does not\nalways correlate: e.g., low perplexity does not always imply high in-context\nfew-shot learning performance.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-28T13:59:54Z",
    "updated": "2022-05-08T06:36:19Z",
    "doi": null
  },
  "2206.00048": {
    "id": "http://arxiv.org/abs/2206.00048v2",
    "title": "PandA: Unsupervised Learning of Parts and Appearances in the Feature\n  Maps of GANs",
    "authors": [
      "James Oldfield",
      "Christos Tzelepis",
      "Yannis Panagakis",
      "Mihalis A. Nicolaou",
      "Ioannis Patras"
    ],
    "abstract": "  Recent advances in the understanding of Generative Adversarial Networks\n(GANs) have led to remarkable progress in visual editing and synthesis tasks,\ncapitalizing on the rich semantics that are embedded in the latent spaces of\npre-trained GANs. However, existing methods are often tailored to specific GAN\narchitectures and are limited to either discovering global semantic directions\nthat do not facilitate localized control, or require some form of supervision\nthrough manually provided regions or segmentation masks. In this light, we\npresent an architecture-agnostic approach that jointly discovers factors\nrepresenting spatial parts and their appearances in an entirely unsupervised\nfashion. These factors are obtained by applying a semi-nonnegative tensor\nfactorization on the feature maps, which in turn enables context-aware local\nimage editing with pixel-level control. In addition, we show that the\ndiscovered appearance factors correspond to saliency maps that localize\nconcepts of interest, without using any labels. Experiments on a wide range of\nGAN architectures and datasets show that, in comparison to the state of the\nart, our method is far more efficient in terms of training time and, most\nimportantly, provides much more accurate localized control. Our code is\navailable at: https://github.com/james-oldfield/PandA.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-31T18:28:39Z",
    "updated": "2023-02-06T15:33:18Z",
    "doi": null
  },
  "2310.09213": {
    "id": "http://arxiv.org/abs/2310.09213v2",
    "title": "Discovery and Expansion of New Domains within Diffusion Models",
    "authors": [
      "Ye Zhu",
      "Yu Wu",
      "Duo Xu",
      "Zhiwei Deng",
      "Yan Yan",
      "Olga Russakovsky"
    ],
    "abstract": "  In this work, we study the generalization properties of diffusion models in a\nfew-shot setup, introduce a novel tuning-free paradigm to synthesize the target\nout-of-domain (OOD) data, and demonstrate its advantages compared to existing\nmethods in data-sparse scenarios with large domain gaps. Specifically, given a\npre-trained model and a small set of images that are OOD relative to the\nmodel's training distribution, we explore whether the frozen model is able to\ngeneralize to this new domain. We begin by revealing that Denoising Diffusion\nProbabilistic Models (DDPMs) trained on single-domain images are already\nequipped with sufficient representation abilities to reconstruct arbitrary\nimages from the inverted latent encoding following bi-directional deterministic\ndiffusion and denoising trajectories. We then demonstrate through both\ntheoretical and empirical perspectives that the OOD images establish Gaussian\npriors in latent spaces of the given model, and the inverted latent modes are\nseparable from their initial training domain. We then introduce our novel\ntuning-free paradigm to synthesize new images of the target unseen domain by\ndiscovering qualified OOD latent encodings in the inverted noisy spaces. This\nis fundamentally different from the current paradigm that seeks to modify the\ndenoising trajectory to achieve the same goal by tuning the model parameters.\nExtensive cross-model and domain experiments show that our proposed method can\nexpand the latent space and generate unseen images via frozen DDPMs without\nimpairing the quality of generation of their original domain. We also showcase\na practical application of our proposed heuristic approach in dramatically\ndifferent domains using astrophysical data, revealing the great potential of\nsuch a generalization paradigm in data spare fields such as scientific\nexplorations.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-13T16:07:31Z",
    "updated": "2024-05-26T20:17:35Z",
    "doi": null
  },
  "2305.00633": {
    "id": "http://arxiv.org/abs/2305.00633v3",
    "title": "Self-Evaluation Guided Beam Search for Reasoning",
    "authors": [
      "Yuxi Xie",
      "Kenji Kawaguchi",
      "Yiran Zhao",
      "Xu Zhao",
      "Min-Yen Kan",
      "Junxian He",
      "Qizhe Xie"
    ],
    "abstract": "  Breaking down a problem into intermediate steps has demonstrated impressive\nperformance in Large Language Model (LLM) reasoning. However, the growth of the\nreasoning chain introduces uncertainty and error accumulation, making it\nchallenging to elicit accurate final results. To tackle this challenge of\nuncertainty in multi-step reasoning, we introduce a stepwise self-evaluation\nmechanism to guide and calibrate the reasoning process of LLMs. We propose a\ndecoding algorithm integrating the self-evaluation guidance via stochastic beam\nsearch. The self-evaluation guidance serves as a better-calibrated automatic\ncriterion, facilitating an efficient search in the reasoning space and\nresulting in superior prediction quality. Stochastic beam search balances\nexploitation and exploration of the search space with temperature-controlled\nrandomness. Our approach surpasses the corresponding Codex-backboned baselines\nin few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA,\nand StrategyQA benchmarks, respectively. Experiment results with Llama-2 on\narithmetic reasoning demonstrate the efficiency of our method in outperforming\nthe baseline methods with comparable computational budgets. Further analysis in\nmulti-step reasoning finds our self-evaluation guidance pinpoints logic\nfailures and leads to higher consistency and robustness. Our code is publicly\navailable at https://guideddecoding.github.io/.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-01T02:37:59Z",
    "updated": "2023-10-26T01:43:17Z",
    "doi": null
  },
  "2405.05749": {
    "id": "http://arxiv.org/abs/2405.05749v2",
    "title": "NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via\n  Generative Prior",
    "authors": [
      "Gihoon Kim",
      "Kwanggyoon Seo",
      "Sihun Cha",
      "Junyong Noh"
    ],
    "abstract": "  Audio-driven talking head generation is advancing from 2D to 3D content.\nNotably, Neural Radiance Field (NeRF) is in the spotlight as a means to\nsynthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based\napproach typically requires a large number of paired audio-visual data for each\nidentity, thereby limiting the scalability of the method. Although there have\nbeen attempts to generate audio-driven 3D talking head animations with a single\nimage, the results are often unsatisfactory due to insufficient information on\nobscured regions in the image. In this paper, we mainly focus on addressing the\noverlooked aspect of 3D consistency in the one-shot, audio-driven domain, where\nfacial animations are synthesized primarily in front-facing perspectives. We\npropose a novel method, NeRFFaceSpeech, which enables to produce high-quality\n3D-aware talking head. Using prior knowledge of generative models combined with\nNeRF, our method can craft a 3D-consistent facial feature space corresponding\nto a single image. Our spatial synchronization method employs audio-correlated\nvertex dynamics of a parametric face model to transform static image features\ninto dynamic visuals through ray deformation, ensuring realistic 3D facial\nmotion. Moreover, we introduce LipaintNet that can replenish the lacking\ninformation in the inner-mouth area, which can not be obtained from a given\nsingle image. The network is trained in a self-supervised manner by utilizing\nthe generative capabilities without additional data. The comprehensive\nexperiments demonstrate the superiority of our method in generating\naudio-driven talking heads from a single image with enhanced 3D consistency\ncompared to previous approaches. In addition, we introduce a quantitative way\nof measuring the robustness of a model against pose changes for the first time,\nwhich has been possible only qualitatively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-05-09T13:14:06Z",
    "updated": "2024-05-10T14:13:10Z",
    "doi": null
  },
  "2006.06466": {
    "id": "http://arxiv.org/abs/2006.06466v2",
    "title": "How Interpretable and Trustworthy are GAMs?",
    "authors": [
      "Chun-Hao Chang",
      "Sarah Tan",
      "Ben Lengerich",
      "Anna Goldenberg",
      "Rich Caruana"
    ],
    "abstract": "  Generalized additive models (GAMs) have become a leading modelclass for\ninterpretable machine learning. However, there are many algorithms for training\nGAMs, and these can learn different or even contradictory models, while being\nequally accurate. Which GAM should we trust? In this paper, we quantitatively\nand qualitatively investigate a variety of GAM algorithms on real and simulated\ndatasets. We find that GAMs with high feature sparsity (only using afew\nvariables to make predictions) can miss patterns in the data and be unfair to\nrare subpopulations. Our results suggest that inductive bias plays a crucial\nrole in what interpretable models learn and that tree-based GAMs represent the\nbest balance of sparsity, fidelity and accuracy and thus appear to be the most\ntrustworthy GAM.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-06-11T14:21:59Z",
    "updated": "2021-06-07T02:53:40Z",
    "doi": null
  },
  "2312.08885": {
    "id": "http://arxiv.org/abs/2312.08885v1",
    "title": "SceneWiz3D: Towards Text-guided 3D Scene Composition",
    "authors": [
      "Qihang Zhang",
      "Chaoyang Wang",
      "Aliaksandr Siarohin",
      "Peiye Zhuang",
      "Yinghao Xu",
      "Ceyuan Yang",
      "Dahua Lin",
      "Bolei Zhou",
      "Sergey Tulyakov",
      "Hsin-Ying Lee"
    ],
    "abstract": "  We are witnessing significant breakthroughs in the technology for generating\n3D objects from text. Existing approaches either leverage large text-to-image\nmodels to optimize a 3D representation or train 3D generators on object-centric\ndatasets. Generating entire scenes, however, remains very challenging as a\nscene contains multiple 3D objects, diverse and scattered. In this work, we\nintroduce SceneWiz3D, a novel approach to synthesize high-fidelity 3D scenes\nfrom text. We marry the locality of objects with globality of scenes by\nintroducing a hybrid 3D representation: explicit for objects and implicit for\nscenes. Remarkably, an object, being represented explicitly, can be either\ngenerated from text using conventional text-to-3D approaches, or provided by\nusers. To configure the layout of the scene and automatically place objects, we\napply the Particle Swarm Optimization technique during the optimization\nprocess. Furthermore, it is difficult for certain parts of the scene (e.g.,\ncorners, occlusion) to receive multi-view supervision, leading to inferior\ngeometry. We incorporate an RGBD panorama diffusion model to mitigate it,\nresulting in high-quality geometry. Extensive evaluation supports that our\napproach achieves superior quality over previous approaches, enabling the\ngeneration of detailed and view-consistent 3D scenes.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-13T18:59:30Z",
    "updated": "2023-12-13T18:59:30Z",
    "doi": null
  },
  "2311.10982": {
    "id": "http://arxiv.org/abs/2311.10982v1",
    "title": "Make Pixels Dance: High-Dynamic Video Generation",
    "authors": [
      "Yan Zeng",
      "Guoqiang Wei",
      "Jiani Zheng",
      "Jiaxin Zou",
      "Yang Wei",
      "Yuchen Zhang",
      "Hang Li"
    ],
    "abstract": "  Creating high-dynamic videos such as motion-rich actions and sophisticated\nvisual effects poses a significant challenge in the field of artificial\nintelligence. Unfortunately, current state-of-the-art video generation methods,\nprimarily focusing on text-to-video generation, tend to produce video clips\nwith minimal motions despite maintaining high fidelity. We argue that relying\nsolely on text instructions is insufficient and suboptimal for video\ngeneration. In this paper, we introduce PixelDance, a novel approach based on\ndiffusion models that incorporates image instructions for both the first and\nlast frames in conjunction with text instructions for video generation.\nComprehensive experimental results demonstrate that PixelDance trained with\npublic data exhibits significantly better proficiency in synthesizing videos\nwith complex scenes and intricate motions, setting a new standard for video\ngeneration.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-18T06:25:58Z",
    "updated": "2023-11-18T06:25:58Z",
    "doi": null
  },
  "2207.01696": {
    "id": "http://arxiv.org/abs/2207.01696v2",
    "title": "TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of\n  3D Human Motions and Texts",
    "authors": [
      "Chuan Guo",
      "Xinxin Zuo",
      "Sen Wang",
      "Li Cheng"
    ],
    "abstract": "  Inspired by the strong ties between vision and language, the two intimate\nhuman sensing and communication modalities, our paper aims to explore the\ngeneration of 3D human full-body motions from texts, as well as its reciprocal\ntask, shorthanded for text2motion and motion2text, respectively. To tackle the\nexisting challenges, especially to enable the generation of multiple distinct\nmotions from the same text, and to avoid the undesirable production of trivial\nmotionless pose sequences, we propose the use of motion token, a discrete and\ncompact motion representation. This provides one level playing ground when\nconsidering both motions and text signals, as the motion and text tokens,\nrespectively. Moreover, our motion2text module is integrated into the inverse\nalignment process of our text2motion training pipeline, where a significant\ndeviation of synthesized text from the input text would be penalized by a large\ntraining loss; empirically this is shown to effectively improve performance.\nFinally, the mappings in-between the two modalities of motions and texts are\nfacilitated by adapting the neural model for machine translation (NMT) to our\ncontext. This autoregressive modeling of the distribution over discrete motion\ntokens further enables non-deterministic production of pose sequences, of\nvariable lengths, from an input text. Our approach is flexible, could be used\nfor both text2motion and motion2text tasks. Empirical evaluations on two\nbenchmark datasets demonstrate the superior performance of our approach on both\ntasks over a variety of state-of-the-art methods. Project page:\nhttps://ericguo5513.github.io/TM2T/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-04T19:52:18Z",
    "updated": "2022-08-04T18:31:20Z",
    "doi": null
  },
  "1609.05158": {
    "id": "http://arxiv.org/abs/1609.05158v2",
    "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient\n  Sub-Pixel Convolutional Neural Network",
    "authors": [
      "Wenzhe Shi",
      "Jose Caballero",
      "Ferenc Husz\u00e1r",
      "Johannes Totz",
      "Andrew P. Aitken",
      "Rob Bishop",
      "Daniel Rueckert",
      "Zehan Wang"
    ],
    "abstract": "  Recently, several models based on deep neural networks have achieved great\nsuccess in terms of both reconstruction accuracy and computational performance\nfor single image super-resolution. In these methods, the low resolution (LR)\ninput image is upscaled to the high resolution (HR) space using a single\nfilter, commonly bicubic interpolation, before reconstruction. This means that\nthe super-resolution (SR) operation is performed in HR space. We demonstrate\nthat this is sub-optimal and adds computational complexity. In this paper, we\npresent the first convolutional neural network (CNN) capable of real-time SR of\n1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN\narchitecture where the feature maps are extracted in the LR space. In addition,\nwe introduce an efficient sub-pixel convolution layer which learns an array of\nupscaling filters to upscale the final LR feature maps into the HR output. By\ndoing so, we effectively replace the handcrafted bicubic filter in the SR\npipeline with more complex upscaling filters specifically trained for each\nfeature map, whilst also reducing the computational complexity of the overall\nSR operation. We evaluate the proposed approach using images and videos from\npublicly available datasets and show that it performs significantly better\n(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster\nthan previous CNN-based methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-09-16T17:58:14Z",
    "updated": "2016-09-23T17:16:37Z",
    "doi": null
  },
  "2302.05573": {
    "id": "http://arxiv.org/abs/2302.05573v1",
    "title": "3D Colored Shape Reconstruction from a Single RGB Image through\n  Diffusion",
    "authors": [
      "Bo Li",
      "Xiaolin Wei",
      "Fengwei Chen",
      "Bin Liu"
    ],
    "abstract": "  We propose a novel 3d colored shape reconstruction method from a single RGB\nimage through diffusion model. Diffusion models have shown great development\npotentials for high-quality 3D shape generation. However, most existing work\nbased on diffusion models only focus on geometric shape generation, they cannot\neither accomplish 3D reconstruction from a single image, or produce 3D\ngeometric shape with color information. In this work, we propose to reconstruct\na 3D colored shape from a single RGB image through a novel conditional\ndiffusion model. The reverse process of the proposed diffusion model is\nconsisted of three modules, shape prediction module, color prediction module\nand NeRF-like rendering module. In shape prediction module, the reference RGB\nimage is first encoded into a high-level shape feature and then the shape\nfeature is utilized as a condition to predict the reverse geometric noise in\ndiffusion model. Then the color of each 3D point updated in shape prediction\nmodule is predicted by color prediction module. Finally, a NeRF-like rendering\nmodule is designed to render the colored point cloud predicted by the former\ntwo modules to 2D image space to guide the training conditioned only on a\nreference image. As far as the authors know, the proposed method is the first\ndiffusion model for 3D colored shape reconstruction from a single RGB image.\nExperimental results demonstrate that the proposed method achieves competitive\nperformance on colored 3D shape reconstruction, and the ablation study\nvalidates the positive role of the color prediction module in improving the\nreconstruction quality of 3D geometric point cloud.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-11T02:15:00Z",
    "updated": "2023-02-11T02:15:00Z",
    "doi": null
  },
  "2208.08643": {
    "id": "http://arxiv.org/abs/2208.08643v2",
    "title": "Learning Program Representations with a Tree-Structured Transformer",
    "authors": [
      "Wenhan Wang",
      "Kechi Zhang",
      "Ge Li",
      "Shangqing Liu",
      "Anran Li",
      "Zhi Jin",
      "Yang Liu"
    ],
    "abstract": "  Learning vector representations for programs is a critical step in applying\ndeep learning techniques for program understanding tasks. Various neural\nnetwork models are proposed to learn from tree-structured program\nrepresentations, e.g., abstract syntax tree (AST) and concrete syntax tree\n(CST). However, most neural architectures either fail to capture long-range\ndependencies which are ubiquitous in programs, or cannot learn effective\nrepresentations for syntax tree nodes, making them incapable of performing the\nnode-level prediction tasks, e.g., bug localization. In this paper, we propose\nTree-Transformer, a novel recursive tree-structured neural network to learn the\nvector representations for source codes. We propose a multi-head attention\nmechanism to model the dependency between siblings and parent-children node\npairs. Moreover, we propose a bi-directional propagation strategy to allow node\ninformation passing in two directions, bottom-up and top-down along trees. In\nthis way, Tree-Transformer can learn the information of the node features as\nwell as the global contextual information. The extensive experimental results\nshow that our Tree-Transformer significantly outperforms the existing\ntree-based and graph-based program representation learning approaches in both\nthe tree-level and node-level prediction tasks.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-18T05:42:01Z",
    "updated": "2023-01-07T12:31:50Z",
    "doi": null
  },
  "2307.06507": {
    "id": "http://arxiv.org/abs/2307.06507v2",
    "title": "Improving Nonalcoholic Fatty Liver Disease Classification Performance\n  With Latent Diffusion Models",
    "authors": [
      "Romain Hardy",
      "Joe Klepich",
      "Ryan Mitchell",
      "Steve Hall",
      "Jericho Villareal",
      "Cornelia Ilin"
    ],
    "abstract": "  Integrating deep learning with clinical expertise holds great potential for\naddressing healthcare challenges and empowering medical professionals with\nimproved diagnostic tools. However, the need for annotated medical images is\noften an obstacle to leveraging the full power of machine learning models. Our\nresearch demonstrates that by combining synthetic images, generated using\ndiffusion models, with real images, we can enhance nonalcoholic fatty liver\ndisease (NAFLD) classification performance even in low-data regime settings. We\nevaluate the quality of the synthetic images by comparing two metrics:\nInception Score (IS) and Fr\\'{e}chet Inception Distance (FID), computed on\ndiffusion- and generative adversarial network (GAN)-generated images. Our\nresults show superior performance for the diffusion-generated images, with a\nmaximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score\nof $69.45$ compared to $100.05$ for GANs. Utilizing a partially frozen CNN\nbackbone (EfficientNet v1), our synthetic augmentation method achieves a\nmaximum image-level ROC AUC of $0.904$ on a NAFLD prediction task.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-13T01:14:08Z",
    "updated": "2023-11-15T02:24:21Z",
    "doi": null
  },
  "2403.14572": {
    "id": "http://arxiv.org/abs/2403.14572v2",
    "title": "Implicit Style-Content Separation using B-LoRA",
    "authors": [
      "Yarden Frenkel",
      "Yael Vinker",
      "Ariel Shamir",
      "Daniel Cohen-Or"
    ],
    "abstract": "  Image stylization involves manipulating the visual appearance and texture\n(style) of an image while preserving its underlying objects, structures, and\nconcepts (content). The separation of style and content is essential for\nmanipulating the image's style independently from its content, ensuring a\nharmonious and visually pleasing result. Achieving this separation requires a\ndeep understanding of both the visual and semantic characteristics of images,\noften necessitating the training of specialized models or employing heavy\noptimization. In this paper, we introduce B-LoRA, a method that leverages LoRA\n(Low-Rank Adaptation) to implicitly separate the style and content components\nof a single image, facilitating various image stylization tasks. By analyzing\nthe architecture of SDXL combined with LoRA, we find that jointly learning the\nLoRA weights of two specific blocks (referred to as B-LoRAs) achieves\nstyle-content separation that cannot be achieved by training each B-LoRA\nindependently. Consolidating the training into only two blocks and separating\nstyle and content allows for significantly improving style manipulation and\novercoming overfitting issues often associated with model fine-tuning. Once\ntrained, the two B-LoRAs can be used as independent components to allow various\nimage stylization tasks, including image style transfer, text-based image\nstylization, consistent style generation, and style-content mixing.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-21T17:20:21Z",
    "updated": "2024-09-22T12:42:39Z",
    "doi": null
  },
  "2305.14808": {
    "id": "http://arxiv.org/abs/2305.14808v1",
    "title": "SAGA: Summarization-Guided Assert Statement Generation",
    "authors": [
      "Yuwei Zhang",
      "Zhi Jin",
      "Zejun Wang",
      "Ying Xing",
      "Ge Li"
    ],
    "abstract": "  Generating meaningful assert statements is one of the key challenges in\nautomated test case generation, which requires understanding the intended\nfunctionality of the tested code. Recently, deep learning-based models have\nshown promise in improving the performance of assert statement generation.\nHowever, existing models only rely on the test prefixes along with their\ncorresponding focal methods, yet ignore the developer-written summarization.\nBased on our observations, the summarization contents usually express the\nintended program behavior or contain parameters that will appear directly in\nthe assert statement. Such information will help existing models address their\ncurrent inability to accurately predict assert statements. This paper presents\na novel summarization-guided approach for automatically generating assert\nstatements. To derive generic representations for natural language (i.e.,\nsummarization) and programming language (i.e., test prefixes and focal\nmethods), we leverage a pre-trained language model as the reference\narchitecture and fine-tune it on the task of assert statement generation. To\nthe best of our knowledge, the proposed approach makes the first attempt to\nleverage the summarization of focal methods as the guidance for making the\ngenerated assert statements more accurate. We demonstrate the effectiveness of\nour approach on two real-world datasets when compared with state-of-the-art\nmodels.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-24T07:03:21Z",
    "updated": "2023-05-24T07:03:21Z",
    "doi": "10.1007/s11390-023-2878-6"
  },
  "2301.12662": {
    "id": "http://arxiv.org/abs/2301.12662v1",
    "title": "SingSong: Generating musical accompaniments from singing",
    "authors": [
      "Chris Donahue",
      "Antoine Caillon",
      "Adam Roberts",
      "Ethan Manilow",
      "Philippe Esling",
      "Andrea Agostinelli",
      "Mauro Verzetti",
      "Ian Simon",
      "Olivier Pietquin",
      "Neil Zeghidour",
      "Jesse Engel"
    ],
    "abstract": "  We present SingSong, a system that generates instrumental music to accompany\ninput vocals, potentially offering musicians and non-musicians alike an\nintuitive new way to create music featuring their own voice. To accomplish\nthis, we build on recent developments in musical source separation and audio\ngeneration. Specifically, we apply a state-of-the-art source separation\nalgorithm to a large corpus of music audio to produce aligned pairs of vocals\nand instrumental sources. Then, we adapt AudioLM (Borsos et al., 2022) -- a\nstate-of-the-art approach for unconditional audio generation -- to be suitable\nfor conditional \"audio-to-audio\" generation tasks, and train it on the\nsource-separated (vocal, instrumental) pairs. In a pairwise comparison with the\nsame vocal inputs, listeners expressed a significant preference for\ninstrumentals generated by SingSong compared to those from a strong retrieval\nbaseline.\n  Sound examples at https://g.co/magenta/singsong\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-30T04:53:23Z",
    "updated": "2023-01-30T04:53:23Z",
    "doi": null
  },
  "2211.09590": {
    "id": "http://arxiv.org/abs/2211.09590v5",
    "title": "Hypergraph Transformer for Skeleton-based Action Recognition",
    "authors": [
      "Yuxuan Zhou",
      "Zhi-Qi Cheng",
      "Chao Li",
      "Yanwen Fang",
      "Yifeng Geng",
      "Xuansong Xie",
      "Margret Keuper"
    ],
    "abstract": "  Skeleton-based action recognition aims to recognize human actions given human\njoint coordinates with skeletal interconnections. By defining a graph with\njoints as vertices and their natural connections as edges, previous works\nsuccessfully adopted Graph Convolutional networks (GCNs) to model joint\nco-occurrences and achieved superior performance. More recently, a limitation\nof GCNs is identified, i.e., the topology is fixed after training. To relax\nsuch a restriction, Self-Attention (SA) mechanism has been adopted to make the\ntopology of GCNs adaptive to the input, resulting in the state-of-the-art\nhybrid models. Concurrently, attempts with plain Transformers have also been\nmade, but they still lag behind state-of-the-art GCN-based methods due to the\nlack of structural prior. Unlike hybrid models, we propose a more elegant\nsolution to incorporate the bone connectivity into Transformer via a graph\ndistance embedding. Our embedding retains the information of skeletal structure\nduring training, whereas GCNs merely use it for initialization. More\nimportantly, we reveal an underlying issue of graph models in general, i.e.,\npairwise aggregation essentially ignores the high-order kinematic dependencies\nbetween body joints. To fill this gap, we propose a new self-attention (SA)\nmechanism on hypergraph, termed Hypergraph Self-Attention (HyperSA), to\nincorporate intrinsic higher-order relations into the model. We name the\nresulting model Hyperformer, and it beats state-of-the-art graph models w.r.t.\naccuracy and efficiency on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA\ndatasets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.2.10",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-17T15:36:48Z",
    "updated": "2023-03-21T17:34:34Z",
    "doi": null
  },
  "1608.05343": {
    "id": "http://arxiv.org/abs/1608.05343v2",
    "title": "Decoupled Neural Interfaces using Synthetic Gradients",
    "authors": [
      "Max Jaderberg",
      "Wojciech Marian Czarnecki",
      "Simon Osindero",
      "Oriol Vinyals",
      "Alex Graves",
      "David Silver",
      "Koray Kavukcuoglu"
    ],
    "abstract": "  Training directed neural networks typically requires forward-propagating data\nthrough a computation graph, followed by backpropagating error signal, to\nproduce weight updates. All layers, or more generally, modules, of the network\nare therefore locked, in the sense that they must wait for the remainder of the\nnetwork to execute forwards and propagate error backwards before they can be\nupdated. In this work we break this constraint by decoupling modules by\nintroducing a model of the future computation of the network graph. These\nmodels predict what the result of the modelled subgraph will produce using only\nlocal information. In particular we focus on modelling error gradients: by\nusing the modelled synthetic gradient in place of true backpropagated error\ngradients we decouple subgraphs, and can update them independently and\nasynchronously i.e. we realise decoupled neural interfaces. We show results for\nfeed-forward models, where every layer is trained asynchronously, recurrent\nneural networks (RNNs) where predicting one's future gradient extends the time\nover which the RNN can effectively model, and also a hierarchical RNN system\nwith ticking at different timescales. Finally, we demonstrate that in addition\nto predicting gradients, the same framework can be used to predict inputs,\nresulting in models which are decoupled in both the forward and backwards pass\n-- amounting to independent networks which co-learn such that they can be\ncomposed into a single functioning corporation.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-08-18T17:29:09Z",
    "updated": "2017-07-03T10:52:04Z",
    "doi": null
  },
  "1608.04207": {
    "id": "http://arxiv.org/abs/1608.04207v3",
    "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction\n  Tasks",
    "authors": [
      "Yossi Adi",
      "Einat Kermany",
      "Yonatan Belinkov",
      "Ofer Lavi",
      "Yoav Goldberg"
    ],
    "abstract": "  There is a lot of research interest in encoding variable length sentences\ninto fixed length vectors, in a way that preserves the sentence meanings. Two\ncommon methods include representations based on averaging word vectors, and\nrepresentations based on the hidden states of recurrent neural networks such as\nLSTMs. The sentence vectors are used as features for subsequent machine\nlearning tasks or for pre-training in the context of deep learning. However,\nnot much is known about the properties that are encoded in these sentence\nrepresentations and about the language information they capture. We propose a\nframework that facilitates better understanding of the encoded representations.\nWe define prediction tasks around isolated aspects of sentence structure\n(namely sentence length, word content, and word order), and score\nrepresentations by the ability to train a classifier to solve each prediction\ntask when using the representation as input. We demonstrate the potential\ncontribution of the approach by analyzing different sentence representation\nmechanisms. The analysis sheds light on the relative strengths of different\nsentence embedding methods with respect to these low level prediction tasks,\nand on the effect of the encoded vector's dimensionality on the resulting\nrepresentations.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-08-15T08:51:38Z",
    "updated": "2017-02-09T06:58:50Z",
    "doi": null
  },
  "2310.09484": {
    "id": "http://arxiv.org/abs/2310.09484v3",
    "title": "Fast-DiM: Towards Fast Diffusion Morphs",
    "authors": [
      "Zander W. Blasingame",
      "Chen Liu"
    ],
    "abstract": "  Diffusion Morphs (DiM) are a recent state-of-the-art method for creating high\nquality face morphs; however, they require a high number of network function\nevaluations (NFE) to create the morphs. We propose a new DiM pipeline,\nFast-DiM, which can create morphs of a similar quality but with fewer NFE. We\ninvestigate the ODE solvers used to solve the Probability Flow ODE and the\nimpact they have on the the creation of face morphs. Additionally, we employ an\nalternative method for encoding images into the latent space of the Diffusion\nmodel by solving the Probability Flow ODE as time runs forwards. Our\nexperiments show that we can reduce the NFE by upwards of 85% in the encoding\nprocess while experiencing only 1.6\\% reduction in Mated Morph Presentation\nMatch Rate (MMPMR). Likewise, we showed we could cut NFE, in the sampling\nprocess, in half with only a maximal reduction of 0.23% in MMPMR.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-14T04:11:01Z",
    "updated": "2024-06-29T17:23:42Z",
    "doi": "10.1109/MSEC.2024.3410112"
  },
  "2301.13126": {
    "id": "http://arxiv.org/abs/2301.13126v3",
    "title": "LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain",
    "authors": [
      "Joel Niklaus",
      "Veton Matoshi",
      "Pooja Rani",
      "Andrea Galassi",
      "Matthias St\u00fcrmer",
      "Ilias Chalkidis"
    ],
    "abstract": "  Lately, propelled by the phenomenal advances around the transformer\narchitecture, the legal NLP field has enjoyed spectacular growth. To measure\nprogress, well curated and challenging benchmarks are crucial. However, most\nbenchmarks are English only and in legal NLP specifically there is no\nmultilingual benchmark available yet. Additionally, many benchmarks are\nsaturated, with the best models clearly outperforming the best humans and\nachieving near perfect scores. We survey the legal NLP literature and select 11\ndatasets covering 24 languages, creating LEXTREME. To provide a fair\ncomparison, we propose two aggregate scores, one based on the datasets and one\non the languages. The best baseline (XLM-R large) achieves both a dataset\naggregate score a language aggregate score of 61.3. This indicates that\nLEXTREME is still very challenging and leaves ample room for improvement. To\nmake it easy for researchers and practitioners to use, we release LEXTREME on\nhuggingface together with all the code required to evaluate models and a public\nWeights and Biases project with all the runs.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68T50",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.2",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-30T18:05:08Z",
    "updated": "2024-01-08T10:08:40Z",
    "doi": "10.18653/v1/2023.findings-emnlp.200"
  },
  "1907.08292": {
    "id": "http://arxiv.org/abs/1907.08292v1",
    "title": "Compositional Deep Learning",
    "authors": [
      "Bruno Gavranovi\u0107"
    ],
    "abstract": "  Neural networks have become an increasingly popular tool for solving many\nreal-world problems. They are a general framework for differentiable\noptimization which includes many other machine learning approaches as special\ncases. In this thesis we build a category-theoretic formalism around a class of\nneural networks exemplified by CycleGAN. CycleGAN is a collection of neural\nnetworks, closed under composition, whose inductive bias is increased by\nenforcing composition invariants, i.e. cycle-consistencies. Inspired by\nFunctorial Data Migration, we specify the interconnection of these networks\nusing a categorical schema, and network instances as set-valued functors on\nthis schema. We also frame neural network architectures, datasets, models, and\na number of other concepts in a categorical setting and thus show a special\nclass of functors, rather than functions, can be learned using gradient\ndescent. We use the category-theoretic framework to conceive a novel neural\nnetwork architecture whose goal is to learn the task of object insertion and\nobject deletion in images with unpaired data. We test the architecture on three\ndifferent datasets and obtain promising results.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.CT",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-07-16T10:21:15Z",
    "updated": "2019-07-16T10:21:15Z",
    "doi": null
  },
  "2208.01748": {
    "id": "http://arxiv.org/abs/2208.01748v1",
    "title": "A Fast Text-Driven Approach for Generating Artistic Content",
    "authors": [
      "Marian Lupascu",
      "Ryan Murdock",
      "Ionut Mironic\u0103",
      "Yijun Li"
    ],
    "abstract": "  In this work, we propose a complete framework that generates visual art.\nUnlike previous stylization methods that are not flexible with style parameters\n(i.e., they allow stylization with only one style image, a single stylization\ntext or stylization of a content image from a certain domain), our method has\nno such restriction. In addition, we implement an improved version that can\ngenerate a wide range of results with varying degrees of detail, style and\nstructure, with a boost in generation speed. To further enhance the results, we\ninsert an artistic super-resolution module in the generative pipeline. This\nmodule will bring additional details such as patterns specific to painters,\nslight brush marks, and so on.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-22T14:34:59Z",
    "updated": "2022-06-22T14:34:59Z",
    "doi": null
  },
  "1807.06358": {
    "id": "http://arxiv.org/abs/1807.06358v2",
    "title": "IntroVAE: Introspective Variational Autoencoders for Photographic Image\n  Synthesis",
    "authors": [
      "Huaibo Huang",
      "Zhihang Li",
      "Ran He",
      "Zhenan Sun",
      "Tieniu Tan"
    ],
    "abstract": "  We present a novel introspective variational autoencoder (IntroVAE) model for\nsynthesizing high-resolution photographic images. IntroVAE is capable of\nself-evaluating the quality of its generated samples and improving itself\naccordingly. Its inference and generator models are jointly trained in an\nintrospective way. On one hand, the generator is required to reconstruct the\ninput images from the noisy outputs of the inference model as normal VAEs. On\nthe other hand, the inference model is encouraged to classify between the\ngenerated and real samples while the generator tries to fool it as GANs. These\ntwo famous generative frameworks are integrated in a simple yet efficient\nsingle-stream architecture that can be trained in a single stage. IntroVAE\npreserves the advantages of VAEs, such as stable training and nice latent\nmanifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires\nno extra discriminators, because the inference model itself serves as a\ndiscriminator to distinguish between the generated and real samples.\nExperiments demonstrate that our method produces high-resolution\nphoto-realistic images (e.g., CELEBA images at \\(1024^{2}\\)), which are\ncomparable to or better than the state-of-the-art GANs.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-07-17T11:37:31Z",
    "updated": "2018-10-27T13:46:18Z",
    "doi": null
  },
  "2405.07178": {
    "id": "http://arxiv.org/abs/2405.07178v1",
    "title": "Hologram: Realtime Holographic Overlays via LiDAR Augmented\n  Reconstruction",
    "authors": [
      "Ekansh Agrawal"
    ],
    "abstract": "  Guided by the hologram technology of the infamous Star Wars franchise, I\npresent an application that creates real-time holographic overlays using LiDAR\naugmented 3D reconstruction. Prior attempts involve SLAM or NeRFs which either\nrequire highly calibrated scenes, incur steep computation costs, or fail to\nrender dynamic scenes. I propose 3 high-fidelity reconstruction tools that can\nrun on a portable device, such as a iPhone 14 Pro, which can allow for metric\naccurate facial reconstructions. My systems enable interactive and immersive\nholographic experiences that can be used for a wide range of applications,\nincluding augmented reality, telepresence, and entertainment.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-05-12T06:35:10Z",
    "updated": "2024-05-12T06:35:10Z",
    "doi": null
  },
  "2311.09265": {
    "id": "http://arxiv.org/abs/2311.09265v1",
    "title": "FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier",
    "authors": [
      "Zhongjie Duan",
      "Chengyu Wang",
      "Cen Chen",
      "Weining Qian",
      "Jun Huang",
      "Mingyi Jin"
    ],
    "abstract": "  With the emergence of diffusion models and rapid development in image\nprocessing, it has become effortless to generate fancy images in tasks such as\nstyle transfer and image editing. However, these impressive image processing\napproaches face consistency issues in video processing. In this paper, we\npropose a powerful model-free toolkit called FastBlend to address the\nconsistency problem for video processing. Based on a patch matching algorithm,\nwe design two inference modes, including blending and interpolation. In the\nblending mode, FastBlend eliminates video flicker by blending the frames within\na sliding window. Moreover, we optimize both computational efficiency and video\nquality according to different application scenarios. In the interpolation\nmode, given one or more keyframes rendered by diffusion models, FastBlend can\nrender the whole video. Since FastBlend does not modify the generation process\nof diffusion models, it exhibits excellent compatibility. Extensive experiments\nhave demonstrated the effectiveness of FastBlend. In the blending mode,\nFastBlend outperforms existing methods for video deflickering and video\nsynthesis. In the interpolation mode, FastBlend surpasses video interpolation\nand model-based video processing approaches. The source codes have been\nreleased on GitHub.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-15T08:28:28Z",
    "updated": "2023-11-15T08:28:28Z",
    "doi": null
  },
  "2308.11200": {
    "id": "http://arxiv.org/abs/2308.11200v1",
    "title": "SegRNN: Segment Recurrent Neural Network for Long-Term Time Series\n  Forecasting",
    "authors": [
      "Shengsheng Lin",
      "Weiwei Lin",
      "Wentai Wu",
      "Feiyu Zhao",
      "Ruichao Mo",
      "Haotong Zhang"
    ],
    "abstract": "  RNN-based methods have faced challenges in the Long-term Time Series\nForecasting (LTSF) domain when dealing with excessively long look-back windows\nand forecast horizons. Consequently, the dominance in this domain has shifted\ntowards Transformer, MLP, and CNN approaches. The substantial number of\nrecurrent iterations are the fundamental reasons behind the limitations of RNNs\nin LTSF. To address these issues, we propose two novel strategies to reduce the\nnumber of iterations in RNNs for LTSF tasks: Segment-wise Iterations and\nParallel Multi-step Forecasting (PMF). RNNs that combine these strategies,\nnamely SegRNN, significantly reduce the required recurrent iterations for LTSF,\nresulting in notable improvements in forecast accuracy and inference speed.\nExtensive experiments demonstrate that SegRNN not only outperforms SOTA\nTransformer-based models but also reduces runtime and memory usage by more than\n78%. These achievements provide strong evidence that RNNs continue to excel in\nLTSF tasks and encourage further exploration of this domain with more RNN-based\napproaches. The source code is coming soon.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-22T05:23:04Z",
    "updated": "2023-08-22T05:23:04Z",
    "doi": null
  },
  "2106.07447": {
    "id": "http://arxiv.org/abs/2106.07447v1",
    "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units",
    "authors": [
      "Wei-Ning Hsu",
      "Benjamin Bolte",
      "Yao-Hung Hubert Tsai",
      "Kushal Lakhotia",
      "Ruslan Salakhutdinov",
      "Abdelrahman Mohamed"
    ],
    "abstract": "  Self-supervised approaches for speech representation learning are challenged\nby three unique problems: (1) there are multiple sound units in each input\nutterance, (2) there is no lexicon of input sound units during the pre-training\nphase, and (3) sound units have variable lengths with no explicit segmentation.\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\napproach for self-supervised speech representation learning, which utilizes an\noffline clustering step to provide aligned target labels for a BERT-like\nprediction loss. A key ingredient of our approach is applying the prediction\nloss over the masked regions only, which forces the model to learn a combined\nacoustic and language model over the continuous inputs. HuBERT relies primarily\non the consistency of the unsupervised clustering step rather than the\nintrinsic quality of the assigned cluster labels. Starting with a simple\nk-means teacher of 100 clusters, and using two iterations of clustering, the\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\ndev-other and test-other evaluation subsets.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-14T14:14:28Z",
    "updated": "2021-06-14T14:14:28Z",
    "doi": null
  },
  "2407.16125": {
    "id": "http://arxiv.org/abs/2407.16125v1",
    "title": "Diffusion Prior-Based Amortized Variational Inference for Noisy Inverse\n  Problems",
    "authors": [
      "Sojin Lee",
      "Dogyun Park",
      "Inho Kong",
      "Hyunwoo J. Kim"
    ],
    "abstract": "  Recent studies on inverse problems have proposed posterior samplers that\nleverage the pre-trained diffusion models as powerful priors. These attempts\nhave paved the way for using diffusion models in a wide range of inverse\nproblems. However, the existing methods entail computationally demanding\niterative sampling procedures and optimize a separate solution for each\nmeasurement, which leads to limited scalability and lack of generalization\ncapability across unseen samples. To address these limitations, we propose a\nnovel approach, Diffusion prior-based Amortized Variational Inference (DAVI)\nthat solves inverse problems with a diffusion prior from an amortized\nvariational inference perspective. Specifically, instead of separate\nmeasurement-wise optimization, our amortized inference learns a function that\ndirectly maps measurements to the implicit posterior distributions of\ncorresponding clean data, enabling a single-step posterior sampling even for\nunseen measurements. Extensive experiments on image restoration tasks, e.g.,\nGaussian deblur, 4$\\times$ super-resolution, and box inpainting with two\nbenchmark datasets, demonstrate our approach's superior performance over strong\nbaselines. Code is available at https://github.com/mlvlab/DAVI.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-23T02:14:18Z",
    "updated": "2024-07-23T02:14:18Z",
    "doi": null
  },
  "1611.05009": {
    "id": "http://arxiv.org/abs/1611.05009v4",
    "title": "OctNet: Learning Deep 3D Representations at High Resolutions",
    "authors": [
      "Gernot Riegler",
      "Ali Osman Ulusoy",
      "Andreas Geiger"
    ],
    "abstract": "  We present OctNet, a representation for deep learning with sparse 3D data. In\ncontrast to existing models, our representation enables 3D convolutional\nnetworks which are both deep and high resolution. Towards this goal, we exploit\nthe sparsity in the input data to hierarchically partition the space using a\nset of unbalanced octrees where each leaf node stores a pooled feature\nrepresentation. This allows to focus memory allocation and computation to the\nrelevant dense regions and enables deeper networks without compromising\nresolution. We demonstrate the utility of our OctNet representation by\nanalyzing the impact of resolution on several 3D tasks including 3D object\nclassification, orientation estimation and point cloud labeling.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-11-15T20:05:45Z",
    "updated": "2017-04-10T08:46:56Z",
    "doi": null
  },
  "1712.00559": {
    "id": "http://arxiv.org/abs/1712.00559v3",
    "title": "Progressive Neural Architecture Search",
    "authors": [
      "Chenxi Liu",
      "Barret Zoph",
      "Maxim Neumann",
      "Jonathon Shlens",
      "Wei Hua",
      "Li-Jia Li",
      "Li Fei-Fei",
      "Alan Yuille",
      "Jonathan Huang",
      "Kevin Murphy"
    ],
    "abstract": "  We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-12-02T06:23:16Z",
    "updated": "2018-07-26T19:51:26Z",
    "doi": null
  },
  "2404.07762": {
    "id": "http://arxiv.org/abs/2404.07762v4",
    "title": "NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous\n  Driving",
    "authors": [
      "William Ljungbergh",
      "Adam Tonderski",
      "Joakim Johnander",
      "Holger Caesar",
      "Kalle \u00c5str\u00f6m",
      "Michael Felsberg",
      "Christoffer Petersson"
    ],
    "abstract": "  We present a versatile NeRF-based simulator for testing autonomous driving\n(AD) software systems, designed with a focus on sensor-realistic closed-loop\nevaluation and the creation of safety-critical scenarios. The simulator learns\nfrom sequences of real-world driving sensor data and enables reconfigurations\nand renderings of new, unseen scenarios. In this work, we use our simulator to\ntest the responses of AD models to safety-critical scenarios inspired by the\nEuropean New Car Assessment Programme (Euro NCAP). Our evaluation reveals that,\nwhile state-of-the-art end-to-end planners excel in nominal driving scenarios\nin an open-loop setting, they exhibit critical flaws when navigating our\nsafety-critical scenarios in a closed-loop setting. This highlights the need\nfor advancements in the safety and real-world usability of end-to-end planners.\nBy publicly releasing our simulator and scenarios as an easy-to-run evaluation\nsuite, we invite the research community to explore, refine, and validate their\nAD models in controlled, yet highly configurable and challenging\nsensor-realistic environments. Code and instructions can be found at\nhttps://github.com/atonderski/neuro-ncap\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-11T14:03:16Z",
    "updated": "2024-04-23T07:29:18Z",
    "doi": null
  },
  "2004.05675": {
    "id": "http://arxiv.org/abs/2004.05675v1",
    "title": "A Non-Parametric Test to Detect Data-Copying in Generative Models",
    "authors": [
      "Casey Meehan",
      "Kamalika Chaudhuri",
      "Sanjoy Dasgupta"
    ],
    "abstract": "  Detecting overfitting in generative models is an important challenge in\nmachine learning. In this work, we formalize a form of overfitting that we call\n{\\em{data-copying}} -- where the generative model memorizes and outputs\ntraining samples or small variations thereof. We provide a three sample\nnon-parametric test for detecting data-copying that uses the training set, a\nseparate sample from the target distribution, and a generated sample from the\nmodel, and study the performance of our test on several canonical models and\ndatasets.\n  For code \\& examples, visit https://github.com/casey-meehan/data-copying\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-12T18:59:29Z",
    "updated": "2020-04-12T18:59:29Z",
    "doi": null
  },
  "2206.02967": {
    "id": "http://arxiv.org/abs/2206.02967v2",
    "title": "Masked Unsupervised Self-training for Label-free Image Classification",
    "authors": [
      "Junnan Li",
      "Silvio Savarese",
      "Steven C. H. Hoi"
    ],
    "abstract": "  State-of-the-art computer vision models are mostly trained with supervised\nlearning using human-labeled images, which limits their scalability due to the\nexpensive annotation cost. While self-supervised representation learning has\nachieved impressive progress, it still requires a second stage of finetuning on\nlabeled data. On the other hand, models pre-trained with large-scale text-image\nsupervision (e.g., CLIP) have enabled zero-shot transfer to downstream image\nclassification tasks. However, the zero-shot performance of CLIP-like models\nare often insufficient for real-world adoption. In this paper, we aim to\nleverage the abundant unlabeled data from a target domain to improve the\nperformance of a pre-trained zero-shot classifier, by unsupervised finetuning\nof the pre-trained model. We propose Masked Unsupervised Self-Training (MUST),\na new unsupervised adaptation method which leverages two different and\ncomplementary sources of training signals: pseudo-labels and raw images. MUST\njointly optimizes three objectives to learn both class-level global feature and\npixel-level local feature and enforces a regularization between the two. We\ndemonstrate the efficacy of MUST on a variety of downstream tasks, where it\nimproves upon CLIP by a large margin. MUST also outperforms supervised few-shot\nadaptation methods. It achieves a top-1 accuracy of 77.7% on ImageNet using\nViT-B, +9.4% higher than CLIP, and +6.2% higher than 16-shot CLIP adaptation.\nOur code is available at https://github.com/salesforce/MUST.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-07T02:03:06Z",
    "updated": "2023-03-10T01:15:56Z",
    "doi": null
  },
  "2205.12654": {
    "id": "http://arxiv.org/abs/2205.12654v1",
    "title": "Bitext Mining Using Distilled Sentence Representations for Low-Resource\n  Languages",
    "authors": [
      "Kevin Heffernan",
      "Onur \u00c7elebi",
      "Holger Schwenk"
    ],
    "abstract": "  Scaling multilingual representation learning beyond the hundred most frequent\nlanguages is challenging, in particular to cover the long tail of low-resource\nlanguages. A promising approach has been to train one-for-all multilingual\nmodels capable of cross-lingual transfer, but these models often suffer from\ninsufficient capacity and interference between unrelated languages. Instead, we\nmove away from this approach and focus on training multiple language (family)\nspecific representations, but most prominently enable all languages to still be\nencoded in the same representational space. To achieve this, we focus on\nteacher-student training, allowing all encoders to be mutually compatible for\nbitext mining, and enabling fast learning of new languages. We introduce a new\nteacher-student training scheme which combines supervised and self-supervised\ntraining, allowing encoders to take advantage of monolingual training data,\nwhich is valuable in the low-resource setting.\n  Our approach significantly outperforms the original LASER encoder. We study\nvery low-resource languages and handle 50 African languages, many of which are\nnot covered by any other model. For these languages, we train sentence\nencoders, mine bitexts, and validate the bitexts by training NMT systems.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-25T10:53:24Z",
    "updated": "2022-05-25T10:53:24Z",
    "doi": null
  },
  "2312.14125": {
    "id": "http://arxiv.org/abs/2312.14125v4",
    "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
    "authors": [
      "Dan Kondratyuk",
      "Lijun Yu",
      "Xiuye Gu",
      "Jos\u00e9 Lezama",
      "Jonathan Huang",
      "Grant Schindler",
      "Rachel Hornung",
      "Vighnesh Birodkar",
      "Jimmy Yan",
      "Ming-Chang Chiu",
      "Krishna Somandepalli",
      "Hassan Akbari",
      "Yair Alon",
      "Yong Cheng",
      "Josh Dillon",
      "Agrim Gupta",
      "Meera Hahn",
      "Anja Hauth",
      "David Hendon",
      "Alonso Martinez",
      "David Minnen",
      "Mikhail Sirotenko",
      "Kihyuk Sohn",
      "Xuan Yang",
      "Hartwig Adam",
      "Ming-Hsuan Yang",
      "Irfan Essa",
      "Huisheng Wang",
      "David A. Ross",
      "Bryan Seybold",
      "Lu Jiang"
    ],
    "abstract": "  We present VideoPoet, a language model capable of synthesizing high-quality\nvideo, with matching audio, from a large variety of conditioning signals.\nVideoPoet employs a decoder-only transformer architecture that processes\nmultimodal inputs -- including images, videos, text, and audio. The training\nprotocol follows that of Large Language Models (LLMs), consisting of two\nstages: pretraining and task-specific adaptation. During pretraining, VideoPoet\nincorporates a mixture of multimodal generative objectives within an\nautoregressive Transformer framework. The pretrained LLM serves as a foundation\nthat can be adapted for a range of video generation tasks. We present empirical\nresults demonstrating the model's state-of-the-art capabilities in zero-shot\nvideo generation, specifically highlighting VideoPoet's ability to generate\nhigh-fidelity motions. Project page: http://sites.research.google/videopoet/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-21T18:46:41Z",
    "updated": "2024-06-04T17:25:20Z",
    "doi": null
  },
  "2310.14804": {
    "id": "http://arxiv.org/abs/2310.14804v2",
    "title": "Large Language Models can Share Images, Too!",
    "authors": [
      "Young-Jun Lee",
      "Dokyong Lee",
      "Joo Won Sung",
      "Jonghwan Hyeon",
      "Ho-Jin Choi"
    ],
    "abstract": "  This paper explores the image-sharing capability of Large Language Models\n(LLMs), such as GPT-4 and LLaMA 2, in a zero-shot setting. To facilitate a\ncomprehensive evaluation of LLMs, we introduce the PhotoChat++ dataset, which\nincludes enriched annotations (i.e., intent, triggering sentence, image\ndescription, and salient information). Furthermore, we present the\ngradient-free and extensible Decide, Describe, and Retrieve (DribeR) framework.\nWith extensive experiments, we unlock the image-sharing capability of DribeR\nequipped with LLMs in zero-shot prompting, with ChatGPT achieving the best\nperformance. Our findings also reveal the emergent image-sharing ability in\nLLMs under zero-shot conditions, validating the effectiveness of DribeR. We use\nthis framework to demonstrate its practicality and effectiveness in two\nreal-world scenarios: (1) human-bot interaction and (2) dataset augmentation.\nTo the best of our knowledge, this is the first study to assess the\nimage-sharing ability of various LLMs in a zero-shot setting. We make our\nsource code and dataset publicly available at\nhttps://github.com/passing2961/DribeR.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-23T10:59:21Z",
    "updated": "2024-07-04T13:55:33Z",
    "doi": null
  },
  "2307.04349": {
    "id": "http://arxiv.org/abs/2307.04349v2",
    "title": "RLTF: Reinforcement Learning from Unit Test Feedback",
    "authors": [
      "Jiate Liu",
      "Yiqin Zhu",
      "Kaiwen Xiao",
      "Qiang Fu",
      "Xiao Han",
      "Wei Yang",
      "Deheng Ye"
    ],
    "abstract": "  The goal of program synthesis, or code generation, is to generate executable\ncode based on given descriptions. Recently, there has been an increasing number\nof studies employing reinforcement learning (RL) to improve the performance of\nlarge language models (LLMs) for code. However, current representative works\neither rely solely on offline frameworks, limiting the exploration of new\nsample spaces, or fall short in the utilization of unit test signals, not\naccounting for specific error locations within the code. To address these\nissues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,\na novel online RL framework with unit test feedback of multi-granularity for\nrefining code LLMs. Our approach generates data in real-time during training\nand simultaneously utilizes fine-grained feedback signals to guide the model\ntowards producing higher-quality code. Extensive experiments show that RLTF\nachieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our\ncode is available at: https://github.com/Zyq-scut/RLTF.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-10T05:18:18Z",
    "updated": "2023-11-13T03:49:27Z",
    "doi": null
  },
  "2406.02820": {
    "id": "http://arxiv.org/abs/2406.02820v1",
    "title": "ORACLE: Leveraging Mutual Information for Consistent Character\n  Generation with LoRAs in Diffusion Models",
    "authors": [
      "Kiymet Akdemir",
      "Pinar Yanardag"
    ],
    "abstract": "  Text-to-image diffusion models have recently taken center stage as pivotal\ntools in promoting visual creativity across an array of domains such as comic\nbook artistry, children's literature, game development, and web design. These\nmodels harness the power of artificial intelligence to convert textual\ndescriptions into vivid images, thereby enabling artists and creators to bring\ntheir imaginative concepts to life with unprecedented ease. However, one of the\nsignificant hurdles that persist is the challenge of maintaining consistency in\ncharacter generation across diverse contexts. Variations in textual prompts,\neven if minor, can yield vastly different visual outputs, posing a considerable\nproblem in projects that require a uniform representation of characters\nthroughout. In this paper, we introduce a novel framework designed to produce\nconsistent character representations from a single text prompt across diverse\nsettings. Through both quantitative and qualitative analyses, we demonstrate\nthat our framework outperforms existing methods in generating characters with\nconsistent visual identities, underscoring its potential to transform creative\nindustries. By addressing the critical challenge of character consistency, we\nnot only enhance the practical utility of these models but also broaden the\nhorizons for artistic and creative expression.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-04T23:39:08Z",
    "updated": "2024-06-04T23:39:08Z",
    "doi": null
  },
  "2309.00952": {
    "id": "http://arxiv.org/abs/2309.00952v1",
    "title": "Bridge Diffusion Model: bridge non-English language-native text-to-image\n  diffusion model with English communities",
    "authors": [
      "Shanyuan Liu",
      "Dawei Leng",
      "Yuhui Yin"
    ],
    "abstract": "  Text-to-Image generation (TTI) technologies are advancing rapidly, especially\nin the English language communities. However, English-native TTI models\ninherently carry biases from English world centric training data, which creates\na dilemma for development of other language-native TTI models. One common\nchoice is fine-tuning the English-native TTI model with translated samples from\nnon-English communities. It falls short of fully addressing the model bias\nproblem. Alternatively, training non-English language native models from\nscratch can effectively resolve the English world bias, but diverges from the\nEnglish TTI communities, thus not able to utilize the strides continuously\ngaining in the English TTI communities any more. To build non-English language\nnative TTI model meanwhile keep compatability with the English TTI communities,\nwe propose a novel model structure referred as \"Bridge Diffusion Model\" (BDM).\nThe proposed BDM employs a backbone-branch network structure to learn the\nnon-English language semantics while keep the latent space compatible with the\nEnglish-native TTI backbone, in an end-to-end manner. The unique advantages of\nthe proposed BDM are that it's not only adept at generating images that\nprecisely depict non-English language semantics, but also compatible with\nvarious English-native TTI plugins, such as different checkpoints, LoRA,\nControlNet, Dreambooth, and Textual Inversion, etc. Moreover, BDM can\nconcurrently generate content seamlessly combining both non-English native and\nEnglish-native semantics within a single image, fostering cultural interaction.\nWe verify our method by applying BDM to build a Chinese-native TTI model,\nwhereas the method is generic and applicable to any other language.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-02T14:30:56Z",
    "updated": "2023-09-02T14:30:56Z",
    "doi": null
  },
  "2402.07995": {
    "id": "http://arxiv.org/abs/2402.07995v2",
    "title": "How the Galaxy-Halo Connection Depends on Large-Scale Environment",
    "authors": [
      "John F. Wu",
      "Christian Kragh Jespersen",
      "Risa H. Wechsler"
    ],
    "abstract": "  We investigate the connection between galaxies, dark matter halos, and their\nlarge-scale environments at $z=0$ with Illustris TNG300 hydrodynamic simulation\ndata. We predict stellar masses from subhalo properties to test two types of\nmachine learning (ML) models: Explainable Boosting Machines (EBMs) with simple\ngalaxy environment features and $\\mathbb{E}(3)$-invariant graph neural networks\n(GNNs). The best-performing EBM models leverage spherically averaged\noverdensity features on $3$ Mpc scales. Interpretations via SHapley Additive\nexPlanations (SHAP) also suggest that, in the context of the TNG300 galaxy-halo\nconnection, simple spherical overdensity on $\\sim 3$ Mpc scales is more\nimportant than cosmic web distance features measured using the DisPerSE\nalgorithm. Meanwhile, a GNN with connectivity defined by a fixed linking\nlength, $L$, outperforms the EBM models by a significant margin. As we increase\nthe linking length scale, GNNs learn important environmental contributions up\nto the largest scales we probe ($L=10$ Mpc). We conclude that $3$ Mpc distance\nscales are most critical for describing the TNG galaxy-halo connection using\nthe spherical overdensity parameterization but that information on larger\nscales, which is not captured by simple environmental parameters or cosmic web\nfeatures, can further augment these models. Our study highlights the benefits\nof using interpretable ML algorithms to explain models of astrophysical\nphenomena, and the power of using GNNs to flexibly learn complex relationships\ndirectly from data while imposing constraints from physical symmetries.\n",
    "categories": [
      {
        "@term": "astro-ph.GA",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "astro-ph.CO",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "astro-ph.IM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-12T19:01:57Z",
    "updated": "2024-10-03T20:56:38Z",
    "doi": "10.3847/1538-4357/ad7bb3"
  },
  "2403.10701": {
    "id": "http://arxiv.org/abs/2403.10701v1",
    "title": "IMPRINT: Generative Object Compositing by Learning Identity-Preserving\n  Representation",
    "authors": [
      "Yizhi Song",
      "Zhifei Zhang",
      "Zhe Lin",
      "Scott Cohen",
      "Brian Price",
      "Jianming Zhang",
      "Soo Ye Kim",
      "He Zhang",
      "Wei Xiong",
      "Daniel Aliaga"
    ],
    "abstract": "  Generative object compositing emerges as a promising new avenue for\ncompositional image editing. However, the requirement of object identity\npreservation poses a significant challenge, limiting practical usage of most\nexisting methods. In response, this paper introduces IMPRINT, a novel\ndiffusion-based generative model trained with a two-stage learning framework\nthat decouples learning of identity preservation from that of compositing. The\nfirst stage is targeted for context-agnostic, identity-preserving pretraining\nof the object encoder, enabling the encoder to learn an embedding that is both\nview-invariant and conducive to enhanced detail preservation. The subsequent\nstage leverages this representation to learn seamless harmonization of the\nobject composited to the background. In addition, IMPRINT incorporates a\nshape-guidance mechanism offering user-directed control over the compositing\nprocess. Extensive experiments demonstrate that IMPRINT significantly\noutperforms existing methods and various baselines on identity preservation and\ncomposition quality.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-15T21:37:04Z",
    "updated": "2024-03-15T21:37:04Z",
    "doi": null
  },
  "2403.05518": {
    "id": "http://arxiv.org/abs/2403.05518v1",
    "title": "Bias-Augmented Consistency Training Reduces Biased Reasoning in\n  Chain-of-Thought",
    "authors": [
      "James Chua",
      "Edward Rees",
      "Hunar Batra",
      "Samuel R. Bowman",
      "Julian Michael",
      "Ethan Perez",
      "Miles Turpin"
    ],
    "abstract": "  While chain-of-thought prompting (CoT) has the potential to improve the\nexplainability of language model reasoning, it can systematically misrepresent\nthe factors influencing models' behavior--for example, rationalizing answers in\nline with a user's opinion without mentioning this bias. To mitigate this\nbiased reasoning problem, we introduce bias-augmented consistency training\n(BCT), an unsupervised fine-tuning scheme that trains models to give consistent\nreasoning across prompts with and without biasing features. We construct a\nsuite testing nine forms of biased reasoning on seven question-answering tasks,\nand find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of\nbiased reasoning by 86% on held-out tasks. Moreover, this model generalizes to\nother forms of bias, reducing biased reasoning on held-out biases by an average\nof 37%. As BCT generalizes to held-out biases and does not require gold labels,\nthis method may hold promise for reducing biased reasoning from as-of-yet\nunknown biases and on tasks where supervision for ground truth reasoning is\nunavailable.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-08T18:41:42Z",
    "updated": "2024-03-08T18:41:42Z",
    "doi": null
  },
  "2207.01821": {
    "id": "http://arxiv.org/abs/2207.01821v2",
    "title": "Toward Explainable and Fine-Grained 3D Grounding through Referring\n  Textual Phrases",
    "authors": [
      "Zhihao Yuan",
      "Xu Yan",
      "Zhuo Li",
      "Xuhao Li",
      "Yao Guo",
      "Shuguang Cui",
      "Zhen Li"
    ],
    "abstract": "  Recent progress in 3D scene understanding has explored visual grounding\n(3DVG) to localize a target object through a language description. However,\nexisting methods only consider the dependency between the entire sentence and\nthe target object, ignoring fine-grained relationships between contexts and\nnon-target ones. In this paper, we extend 3DVG to a more fine-grained and\ninterpretable task, called 3D Phrase Aware Grounding (3DPAG). The 3DPAG task\naims to localize the target objects in a 3D scene by explicitly identifying all\nphrase-related objects and then conducting the reasoning according to\ncontextual phrases. To tackle this problem, we manually labeled about 227K\nphrase-level annotations using a self-developed platform, from 88K sentences of\nwidely used 3DVG datasets, i.e., Nr3D, Sr3D and ScanRefer. By tapping on our\ndatasets, we can extend previous 3DVG methods to the fine-grained phrase-aware\nscenario. It is achieved through the proposed novel phrase-object alignment\noptimization and phrase-specific pre-training, boosting conventional 3DVG\nperformance as well. Extensive results confirm significant improvements, i.e.,\nprevious state-of-the-art method achieves 3.9%, 3.5% and 4.6% overall accuracy\ngains on Nr3D, Sr3D and ScanRefer respectively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-05T05:50:12Z",
    "updated": "2023-05-27T10:03:34Z",
    "doi": null
  },
  "2402.16021": {
    "id": "http://arxiv.org/abs/2402.16021v1",
    "title": "TMT: Tri-Modal Translation between Speech, Image, and Text by Processing\n  Different Modalities as Different Languages",
    "authors": [
      "Minsu Kim",
      "Jee-weon Jung",
      "Hyeongseop Rha",
      "Soumi Maiti",
      "Siddhant Arora",
      "Xuankai Chang",
      "Shinji Watanabe",
      "Yong Man Ro"
    ],
    "abstract": "  The capability to jointly process multi-modal information is becoming an\nessential task. However, the limited number of paired multi-modal data and the\nlarge computational requirements in multi-modal learning hinder the\ndevelopment. We propose a novel Tri-Modal Translation (TMT) model that\ntranslates between arbitrary modalities spanning speech, image, and text. We\nintroduce a novel viewpoint, where we interpret different modalities as\ndifferent languages, and treat multi-modal translation as a well-established\nmachine translation problem. To this end, we tokenize speech and image data\ninto discrete tokens, which provide a unified interface across modalities and\nsignificantly decrease the computational cost. In the proposed TMT, a\nmulti-modal encoder-decoder conducts the core translation, whereas\nmodality-specific processing is conducted only within the tokenization and\ndetokenization stages. We evaluate the proposed TMT on all six modality\ntranslation tasks. TMT outperforms single model counterparts consistently,\ndemonstrating that unifying tasks is beneficial not only for practicality but\nalso for performance.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-25T07:46:57Z",
    "updated": "2024-02-25T07:46:57Z",
    "doi": null
  },
  "2206.00800": {
    "id": "http://arxiv.org/abs/2206.00800v1",
    "title": "CcHarmony: Color-checker based Image Harmonization Dataset",
    "authors": [
      "Haoxu Huang",
      "Li Niu"
    ],
    "abstract": "  Image harmonization targets at adjusting the foreground in a composite image\nto make it compatible with the background, producing a more realistic and\nharmonious image. Training deep image harmonization network requires abundant\ntraining data, but it is extremely difficult to acquire training pairs of\ncomposite images and ground-truth harmonious images. Therefore, existing works\nturn to adjust the foreground appearance in a real image to create a synthetic\ncomposite image. However, such adjustment may not faithfully reflect the\nnatural illumination change of foreground. In this work, we explore a novel\ntransitive way to construct image harmonization dataset. Specifically, based on\nthe existing datasets with recorded illumination information, we first convert\nthe foreground in a real image to the standard illumination condition, and then\nconvert it to another illumination condition, which is combined with the\noriginal background to form a synthetic composite image. In this manner, we\nconstruct an image harmonization dataset called ccHarmony, which is named after\ncolor checker (cc). The dataset is available at\nhttps://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-01T23:57:16Z",
    "updated": "2022-06-01T23:57:16Z",
    "doi": null
  },
  "2301.13856": {
    "id": "http://arxiv.org/abs/2301.13856v2",
    "title": "Simplex Random Features",
    "authors": [
      "Isaac Reid",
      "Krzysztof Choromanski",
      "Valerii Likhosherstov",
      "Adrian Weller"
    ],
    "abstract": "  We present Simplex Random Features (SimRFs), a new random feature (RF)\nmechanism for unbiased approximation of the softmax and Gaussian kernels by\ngeometrical correlation of random projection vectors. We prove that SimRFs\nprovide the smallest possible mean square error (MSE) on unbiased estimates of\nthese kernels among the class of weight-independent geometrically-coupled\npositive random feature (PRF) mechanisms, substantially outperforming the\npreviously most accurate Orthogonal Random Features at no observable extra\ncost. We present a more computationally expensive SimRFs+ variant, which we\nprove is asymptotically optimal in the broader family of weight-dependent\ngeometrical coupling schemes (which permit correlations between random vector\ndirections and norms). In extensive empirical studies, we show consistent gains\nprovided by SimRFs in settings including pointwise kernel estimation,\nnonparametric classification and scalable Transformers.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-31T18:53:39Z",
    "updated": "2023-10-07T15:55:57Z",
    "doi": null
  },
  "2303.15060": {
    "id": "http://arxiv.org/abs/2303.15060v1",
    "title": "TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using\n  Differentiable Rendering",
    "authors": [
      "Jaehoon Choi",
      "Dongki Jung",
      "Taejae Lee",
      "Sangwook Kim",
      "Youngdong Jung",
      "Dinesh Manocha",
      "Donghwan Lee"
    ],
    "abstract": "  We present a new pipeline for acquiring a textured mesh in the wild with a\nsingle smartphone which offers access to images, depth maps, and valid poses.\nOur method first introduces an RGBD-aided structure from motion, which can\nyield filtered depth maps and refines camera poses guided by corresponding\ndepth. Then, we adopt the neural implicit surface reconstruction method, which\nallows for high-quality mesh and develops a new training process for applying a\nregularization provided by classical multi-view stereo methods. Moreover, we\napply a differentiable rendering to fine-tune incomplete texture maps and\ngenerate textures which are perceptually closer to the original scene. Our\npipeline can be applied to any common objects in the real world without the\nneed for either in-the-lab environments or accurate mask images. We demonstrate\nresults of captured objects with complex shapes and validate our method\nnumerically against existing 3D reconstruction and texture mapping methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-27T10:07:52Z",
    "updated": "2023-03-27T10:07:52Z",
    "doi": null
  },
  "2211.10658": {
    "id": "http://arxiv.org/abs/2211.10658v2",
    "title": "EDGE: Editable Dance Generation From Music",
    "authors": [
      "Jonathan Tseng",
      "Rodrigo Castellon",
      "C. Karen Liu"
    ],
    "abstract": "  Dance is an important human art form, but creating new dances can be\ndifficult and time-consuming. In this work, we introduce Editable Dance\nGEneration (EDGE), a state-of-the-art method for editable dance generation that\nis capable of creating realistic, physically-plausible dances while remaining\nfaithful to the input music. EDGE uses a transformer-based diffusion model\npaired with Jukebox, a strong music feature extractor, and confers powerful\nediting capabilities well-suited to dance, including joint-wise conditioning,\nand in-betweening. We introduce a new metric for physical plausibility, and\nevaluate dance quality generated by our method extensively through (1) multiple\nquantitative metrics on physical plausibility, beat alignment, and diversity\nbenchmarks, and more importantly, (2) a large-scale user study, demonstrating a\nsignificant improvement over previous state-of-the-art methods. Qualitative\nsamples from our model can be found at our website.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-19T10:41:38Z",
    "updated": "2022-11-27T06:27:17Z",
    "doi": null
  },
  "2406.07522": {
    "id": "http://arxiv.org/abs/2406.07522v1",
    "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling",
    "authors": [
      "Liliang Ren",
      "Yang Liu",
      "Yadong Lu",
      "Yelong Shen",
      "Chen Liang",
      "Weizhu Chen"
    ],
    "abstract": "  Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-11T17:50:51Z",
    "updated": "2024-06-11T17:50:51Z",
    "doi": null
  },
  "1503.03585": {
    "id": "http://arxiv.org/abs/1503.03585v8",
    "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
    "authors": [
      "Jascha Sohl-Dickstein",
      "Eric A. Weiss",
      "Niru Maheswaranathan",
      "Surya Ganguli"
    ],
    "abstract": "  A central problem in machine learning involves modeling complex data-sets\nusing highly flexible families of probability distributions in which learning,\nsampling, inference, and evaluation are still analytically or computationally\ntractable. Here, we develop an approach that simultaneously achieves both\nflexibility and tractability. The essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly destroy structure in a\ndata distribution through an iterative forward diffusion process. We then learn\na reverse diffusion process that restores structure in data, yielding a highly\nflexible and tractable generative model of the data. This approach allows us to\nrapidly learn, sample from, and evaluate probabilities in deep generative\nmodels with thousands of layers or time steps, as well as to compute\nconditional and posterior probabilities under the learned model. We\nadditionally release an open source reference implementation of the algorithm.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cond-mat.dis-nn",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "q-bio.NC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2015-03-12T04:51:37Z",
    "updated": "2015-11-18T21:50:51Z",
    "doi": null
  },
  "2210.16788": {
    "id": "http://arxiv.org/abs/2210.16788v1",
    "title": "Image-free Domain Generalization via CLIP for 3D Hand Pose Estimation",
    "authors": [
      "Seongyeong Lee",
      "Hansoo Park",
      "Dong Uk Kim",
      "Jihyeon Kim",
      "Muhammadjon Boboev",
      "Seungryul Baek"
    ],
    "abstract": "  RGB-based 3D hand pose estimation has been successful for decades thanks to\nlarge-scale databases and deep learning. However, the hand pose estimation\nnetwork does not operate well for hand pose images whose characteristics are\nfar different from the training data. This is caused by various factors such as\nilluminations, camera angles, diverse backgrounds in the input images, etc.\nMany existing methods tried to solve it by supplying additional large-scale\nunconstrained/target domain images to augment data space; however collecting\nsuch large-scale images takes a lot of labors. In this paper, we present a\nsimple image-free domain generalization approach for the hand pose estimation\nframework that uses only source domain data. We try to manipulate the image\nfeatures of the hand pose estimation network by adding the features from text\ndescriptions using the CLIP (Contrastive Language-Image Pre-training) model.\nThe manipulated image features are then exploited to train the hand pose\nestimation network via the contrastive learning framework. In experiments with\nSTB and RHD datasets, our algorithm shows improved performance over the\nstate-of-the-art domain generalization approaches.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-30T09:32:37Z",
    "updated": "2022-10-30T09:32:37Z",
    "doi": null
  },
  "2210.01033": {
    "id": "http://arxiv.org/abs/2210.01033v2",
    "title": "LPT: Long-tailed Prompt Tuning for Image Classification",
    "authors": [
      "Bowen Dong",
      "Pan Zhou",
      "Shuicheng Yan",
      "Wangmeng Zuo"
    ],
    "abstract": "  For long-tailed classification, most works often pretrain a big model on a\nlarge-scale dataset, and then fine-tune the whole model for adapting to\nlong-tailed data. Though promising, fine-tuning the whole pretrained model\ntends to suffer from high cost in computation and deployment of different\nmodels for different tasks, as well as weakened generalization ability for\noverfitting to certain features of long-tailed data. To alleviate these issues,\nwe propose an effective Long-tailed Prompt Tuning method for long-tailed\nclassification. LPT introduces several trainable prompts into a frozen\npretrained model to adapt it to long-tailed data. For better effectiveness, we\ndivide prompts into two groups: 1) a shared prompt for the whole long-tailed\ndataset to learn general features and to adapt a pretrained model into target\ndomain; and 2) group-specific prompts to gather group-specific features for the\nsamples which have similar features and also to empower the pretrained model\nwith discrimination ability. Then we design a two-phase training paradigm to\nlearn these prompts. In phase 1, we train the shared prompt via supervised\nprompt tuning to adapt a pretrained model to the desired long-tailed domain. In\nphase 2, we use the learnt shared prompt as query to select a small best\nmatched set for a group of similar samples from the group-specific prompt set\nto dig the common features of these similar samples, then optimize these\nprompts with dual sampling strategy and asymmetric GCL loss. By only\nfine-tuning a few prompts while fixing the pretrained model, LPT can reduce\ntraining and deployment cost by storing a few prompts, and enjoys a strong\ngeneralization ability of the pretrained model. Experiments show that on\nvarious long-tailed benchmarks, with only ~1.1% extra parameters, LPT achieves\ncomparable performance than previous whole model fine-tuning methods, and is\nmore robust to domain-shift.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-03T15:47:02Z",
    "updated": "2023-03-28T10:16:03Z",
    "doi": null
  },
  "2203.04904": {
    "id": "http://arxiv.org/abs/2203.04904v3",
    "title": "Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning",
    "authors": [
      "Zhenhailong Wang",
      "Hang Yu",
      "Manling Li",
      "Han Zhao",
      "Heng Ji"
    ],
    "abstract": "  Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models still fall short of few-shot transfer ability on\ndomain-specific problems. Classical fine-tuning often fails to prevent highly\nexpressive models from exploiting spurious correlations. Although\nmodel-agnostic meta-learning (MAML) presents as a natural alternative for\nfew-shot transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use on large-scale vision-language models\nsuch as CLIP. While much literature has been devoted to exploring alternative\noptimization strategies, we identify another essential aspect towards effective\nfew-shot transfer learning, task sampling, which is previously only be viewed\nas part of data pre-processing in MAML. To show the impact of task sampling, we\npropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which\ndifferentiates classical fine-tuning only on uniformly sampling multiple tasks.\nDespite its simplicity, we show that MAMF consistently outperforms classical\nfine-tuning on five few-shot vision-language classification tasks. We further\nshow that the effectiveness of the bi-level optimization in MAML is highly\nsensitive to the zero-shot performance of a task in the context of few-shot\nvision-language classification. The goal of this paper is to provide new\ninsights on what makes few-shot learning work, and encourage more research into\ninvestigating better task sampling strategies.\n",
    "categories": [
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-09T17:26:53Z",
    "updated": "2022-07-15T04:16:21Z",
    "doi": null
  },
  "2205.01859": {
    "id": "http://arxiv.org/abs/2205.01859v1",
    "title": "DEAR: A Novel Deep Learning-based Approach for Automated Program Repair",
    "authors": [
      "Yi Li",
      "Shaohua Wang",
      "Tien N. Nguyen"
    ],
    "abstract": "  The existing deep learning (DL)-based automated program repair (APR) models\nare limited in fixing general software defects. % We present {\\tool}, a\nDL-based approach that supports fixing for the general bugs that require\ndependent changes at once to one or multiple consecutive statements in one or\nmultiple hunks of code. % We first design a novel fault localization (FL)\ntechnique for multi-hunk, multi-statement fixes that combines traditional\nspectrum-based (SB) FL with deep learning and data-flow analysis. It takes the\nbuggy statements returned by the SBFL model, detects the buggy hunks to be\nfixed at once, and expands a buggy statement $s$ in a hunk to include other\nsuspicious statements around $s$. We design a two-tier, tree-based LSTM model\nthat incorporates cycle training and uses a divide-and-conquer strategy to\nlearn proper code transformations for fixing multiple statements in the\nsuitable fixing context consisting of surrounding subtrees. We conducted\nseveral experiments to evaluate {\\tool} on three datasets: Defects4J (395\nbugs), BigFix (+26k bugs), and CPatMiner (+44k bugs). On Defects4J dataset,\n{\\tool} outperforms the baselines from 42\\%--683\\% in terms of the number of\nauto-fixed bugs with only the top-1 patches. On BigFix dataset, it fixes\n31--145 more bugs than existing DL-based APR models with the top-1 patches. On\nCPatMiner dataset, among 667 fixed bugs, there are 169 (25.3\\%)\nmulti-hunk/multi-statement bugs. {\\tool} fixes 71 and 164 more bugs, including\n52 and 61 more multi-hunk/multi-statement bugs, than the state-of-the-art,\nDL-based APR models.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-04T02:29:40Z",
    "updated": "2022-05-04T02:29:40Z",
    "doi": null
  },
  "2401.00909": {
    "id": "http://arxiv.org/abs/2401.00909v2",
    "title": "Taming Mode Collapse in Score Distillation for Text-to-3D Generation",
    "authors": [
      "Peihao Wang",
      "Dejia Xu",
      "Zhiwen Fan",
      "Dilin Wang",
      "Sreyas Mohan",
      "Forrest Iandola",
      "Rakesh Ranjan",
      "Yilei Li",
      "Qiang Liu",
      "Zhangyang Wang",
      "Vikas Chandra"
    ],
    "abstract": "  Despite the remarkable performance of score distillation in text-to-3D\ngeneration, such techniques notoriously suffer from view inconsistency issues,\nalso known as \"Janus\" artifact, where the generated objects fake each view with\nmultiple front faces. Although empirically effective methods have approached\nthis problem via score debiasing or prompt engineering, a more rigorous\nperspective to explain and tackle this problem remains elusive. In this paper,\nwe reveal that the existing score distillation-based text-to-3D generation\nframeworks degenerate to maximal likelihood seeking on each view independently\nand thus suffer from the mode collapse problem, manifesting as the Janus\nartifact in practice. To tame mode collapse, we improve score distillation by\nre-establishing the entropy term in the corresponding variational objective,\nwhich is applied to the distribution of rendered images. Maximizing the entropy\nencourages diversity among different views in generated 3D assets, thereby\nmitigating the Janus problem. Based on this new objective, we derive a new\nupdate rule for 3D score distillation, dubbed Entropic Score Distillation\n(ESD). We theoretically reveal that ESD can be simplified and implemented by\njust adopting the classifier-free guidance trick upon variational score\ndistillation. Although embarrassingly straightforward, our extensive\nexperiments successfully demonstrate that ESD can be an effective treatment for\nJanus artifacts in score distillation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-31T22:47:06Z",
    "updated": "2024-03-29T18:04:37Z",
    "doi": null
  },
  "2403.14472": {
    "id": "http://arxiv.org/abs/2403.14472v5",
    "title": "Detoxifying Large Language Models via Knowledge Editing",
    "authors": [
      "Mengru Wang",
      "Ningyu Zhang",
      "Ziwen Xu",
      "Zekun Xi",
      "Shumin Deng",
      "Yunzhi Yao",
      "Qishen Zhang",
      "Linyi Yang",
      "Jindong Wang",
      "Huajun Chen"
    ],
    "abstract": "  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments with several\nknowledge editing approaches, indicating that knowledge editing has the\npotential to detoxify LLMs with a limited impact on general performance\nefficiently. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxifying\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.HC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-21T15:18:30Z",
    "updated": "2024-05-28T09:11:25Z",
    "doi": null
  },
  "2408.13413": {
    "id": "http://arxiv.org/abs/2408.13413v1",
    "title": "TVG: A Training-free Transition Video Generation Method with Diffusion\n  Models",
    "authors": [
      "Rui Zhang",
      "Yaosen Chen",
      "Yuegen Liu",
      "Wei Wang",
      "Xuming Wen",
      "Hongxia Wang"
    ],
    "abstract": "  Transition videos play a crucial role in media production, enhancing the flow\nand coherence of visual narratives. Traditional methods like morphing often\nlack artistic appeal and require specialized skills, limiting their\neffectiveness. Recent advances in diffusion model-based video generation offer\nnew possibilities for creating transitions but face challenges such as poor\ninter-frame relationship modeling and abrupt content changes. We propose a\nnovel training-free Transition Video Generation (TVG) approach using\nvideo-level diffusion models that addresses these limitations without\nadditional training. Our method leverages Gaussian Process Regression\n($\\mathcal{GPR}$) to model latent representations, ensuring smooth and dynamic\ntransitions between frames. Additionally, we introduce interpolation-based\nconditional controls and a Frequency-aware Bidirectional Fusion (FBiF)\narchitecture to enhance temporal control and transition reliability.\nEvaluations of benchmark datasets and custom image pairs demonstrate the\neffectiveness of our approach in generating high-quality smooth transition\nvideos. The code are provided in https://sobeymil.github.io/tvg.com.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-24T00:33:14Z",
    "updated": "2024-08-24T00:33:14Z",
    "doi": null
  },
  "1709.04326": {
    "id": "http://arxiv.org/abs/1709.04326v4",
    "title": "Learning with Opponent-Learning Awareness",
    "authors": [
      "Jakob N. Foerster",
      "Richard Y. Chen",
      "Maruan Al-Shedivat",
      "Shimon Whiteson",
      "Pieter Abbeel",
      "Igor Mordatch"
    ],
    "abstract": "  Multi-agent settings are quickly gathering importance in machine learning.\nThis includes a plethora of recent work on deep multi-agent reinforcement\nlearning, but also can be extended to hierarchical RL, generative adversarial\nnetworks and decentralised optimisation. In all these settings the presence of\nmultiple learning agents renders the training problem non-stationary and often\nleads to unstable training or undesired final results. We present Learning with\nOpponent-Learning Awareness (LOLA), a method in which each agent shapes the\nanticipated learning of the other agents in the environment. The LOLA learning\nrule includes a term that accounts for the impact of one agent's policy on the\nanticipated parameter update of the other agents. Results show that the\nencounter of two LOLA agents leads to the emergence of tit-for-tat and\ntherefore cooperation in the iterated prisoners' dilemma, while independent\nlearning does not. In this domain, LOLA also receives higher payouts compared\nto a naive learner, and is robust against exploitation by higher order\ngradient-based methods. Applied to repeated matching pennies, LOLA agents\nconverge to the Nash equilibrium. In a round robin tournament we show that LOLA\nagents successfully shape the learning of a range of multi-agent learning\nalgorithms from literature, resulting in the highest average returns on the\nIPD. We also show that the LOLA update rule can be efficiently calculated using\nan extension of the policy gradient estimator, making the method suitable for\nmodel-free RL. The method thus scales to large parameter and input spaces and\nnonlinear function approximators. We apply LOLA to a grid world task with an\nembedded social dilemma using recurrent policies and opponent modelling. By\nexplicitly considering the learning of the other agent, LOLA agents learn to\ncooperate out of self-interest. The code is at github.com/alshedivat/lola.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GT",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-09-13T13:42:15Z",
    "updated": "2018-09-19T19:22:48Z",
    "doi": null
  },
  "2210.10253": {
    "id": "http://arxiv.org/abs/2210.10253v1",
    "title": "On the Adversarial Robustness of Mixture of Experts",
    "authors": [
      "Joan Puigcerver",
      "Rodolphe Jenatton",
      "Carlos Riquelme",
      "Pranjal Awasthi",
      "Srinadh Bhojanapalli"
    ],
    "abstract": "  Adversarial robustness is a key desirable property of neural networks. It has\nbeen empirically shown to be affected by their sizes, with larger networks\nbeing typically more robust. Recently, Bubeck and Sellke proved a lower bound\non the Lipschitz constant of functions that fit the training data in terms of\ntheir number of parameters. This raises an interesting open question, do -- and\ncan -- functions with more parameters, but not necessarily more computational\ncost, have better robustness? We study this question for sparse Mixture of\nExpert models (MoEs), that make it possible to scale up the model size for a\nroughly constant computational cost. We theoretically show that under certain\nconditions on the routing and the structure of the data, MoEs can have\nsignificantly smaller Lipschitz constants than their dense counterparts. The\nrobustness of MoEs can suffer when the highest weighted experts for an input\nimplement sufficiently different functions. We next empirically evaluate the\nrobustness of MoEs on ImageNet using adversarial attacks and show they are\nindeed more robust than dense models with the same computational cost. We make\nkey observations showing the robustness of MoEs to the choice of experts,\nhighlighting the redundancy of experts in models trained in practice.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-19T02:24:57Z",
    "updated": "2022-10-19T02:24:57Z",
    "doi": null
  },
  "2205.11083": {
    "id": "http://arxiv.org/abs/2205.11083v3",
    "title": "Deep Digging into the Generalization of Self-Supervised Monocular Depth\n  Estimation",
    "authors": [
      "Jinwoo Bae",
      "Sungho Moon",
      "Sunghoon Im"
    ],
    "abstract": "  Self-supervised monocular depth estimation has been widely studied recently.\nMost of the work has focused on improving performance on benchmark datasets,\nsuch as KITTI, but has offered a few experiments on generalization performance.\nIn this paper, we investigate the backbone networks (e.g. CNNs, Transformers,\nand CNN-Transformer hybrid models) toward the generalization of monocular depth\nestimation. We first evaluate state-of-the-art models on diverse public\ndatasets, which have never been seen during the network training. Next, we\ninvestigate the effects of texture-biased and shape-biased representations\nusing the various texture-shifted datasets that we generated. We observe that\nTransformers exhibit a strong shape bias and CNNs do a strong texture-bias. We\nalso find that shape-biased models show better generalization performance for\nmonocular depth estimation compared to texture-biased models. Based on these\nobservations, we newly design a CNN-Transformer hybrid network with a\nmulti-level adaptive feature fusion module, called MonoFormer. The design\nintuition behind MonoFormer is to increase shape bias by employing Transformers\nwhile compensating for the weak locality bias of Transformers by adaptively\nfusing multi-level representations. Extensive experiments show that the\nproposed method achieves state-of-the-art performance with various public\ndatasets. Our method also shows the best generalization ability among the\ncompetitive methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-23T06:56:25Z",
    "updated": "2023-03-20T03:52:42Z",
    "doi": null
  },
  "2105.09755": {
    "id": "http://arxiv.org/abs/2105.09755v1",
    "title": "Generalized Wasserstein barycenters between probability measures living\n  on different subspaces",
    "authors": [
      "Julie Delon",
      "Natha\u00ebl Gozlan",
      "Alexandre Saint-Dizier"
    ],
    "abstract": "  In this paper, we introduce a generalization of the Wasserstein barycenter,\nto a case where the initial probability measures live on different subspaces of\nR^d. We study the existence and uniqueness of this barycenter, we show how it\nis related to a larger multi-marginal optimal transport problem, and we propose\na dual formulation. Finally, we explain how to compute numerically this\ngeneralized barycenter on discrete distributions, and we propose an explicit\nsolution for Gaussian distributions.\n",
    "categories": [
      {
        "@term": "math.PR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.FA",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-05-20T14:01:51Z",
    "updated": "2021-05-20T14:01:51Z",
    "doi": null
  },
  "2310.03295": {
    "id": "http://arxiv.org/abs/2310.03295v1",
    "title": "Can pre-trained models assist in dataset distillation?",
    "authors": [
      "Yao Lu",
      "Xuguang Chen",
      "Yuchen Zhang",
      "Jianyang Gu",
      "Tianle Zhang",
      "Yifan Zhang",
      "Xiaoniu Yang",
      "Qi Xuan",
      "Kai Wang",
      "Yang You"
    ],
    "abstract": "  Dataset Distillation (DD) is a prominent technique that encapsulates\nknowledge from a large-scale original dataset into a small synthetic dataset\nfor efficient training. Meanwhile, Pre-trained Models (PTMs) function as\nknowledge repositories, containing extensive information from the original\ndataset. This naturally raises a question: Can PTMs effectively transfer\nknowledge to synthetic datasets, guiding DD accurately? To this end, we conduct\npreliminary experiments, confirming the contribution of PTMs to DD. Afterwards,\nwe systematically study different options in PTMs, including initialization\nparameters, model architecture, training epoch and domain knowledge, revealing\nthat: 1) Increasing model diversity enhances the performance of synthetic\ndatasets; 2) Sub-optimal models can also assist in DD and outperform\nwell-trained ones in certain cases; 3) Domain-specific PTMs are not mandatory\nfor DD, but a reasonable domain match is crucial. Finally, by selecting optimal\noptions, we significantly improve the cross-architecture generalization over\nbaseline DD methods. We hope our work will facilitate researchers to develop\nbetter DD techniques. Our code is available at\nhttps://github.com/yaolu-zjut/DDInterpreter.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-05T03:51:21Z",
    "updated": "2023-10-05T03:51:21Z",
    "doi": null
  },
  "2302.14503": {
    "id": "http://arxiv.org/abs/2302.14503v1",
    "title": "Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?",
    "authors": [
      "Hyemin Ahn",
      "Esteve Valls Mascaro",
      "Dongheui Lee"
    ],
    "abstract": "  After many researchers observed fruitfulness from the recent diffusion\nprobabilistic model, its effectiveness in image generation is actively studied\nthese days. In this paper, our objective is to evaluate the potential of\ndiffusion probabilistic models for 3D human motion-related tasks. To this end,\nthis paper presents a study of employing diffusion probabilistic models to\npredict future 3D human motion(s) from the previously observed motion. Based on\nthe Human 3.6M and HumanEva-I datasets, our results show that diffusion\nprobabilistic models are competitive for both single (deterministic) and\nmultiple (stochastic) 3D motion prediction tasks, after finishing a single\ntraining process. In addition, we find out that diffusion probabilistic models\ncan offer an attractive compromise, since they can strike the right balance\nbetween the likelihood and diversity of the predicted future motions. Our code\nis publicly available on the project website:\nhttps://sites.google.com/view/diffusion-motion-prediction.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-28T11:34:55Z",
    "updated": "2023-02-28T11:34:55Z",
    "doi": null
  },
  "2302.03548": {
    "id": "http://arxiv.org/abs/2302.03548v1",
    "title": "PhysFormer++: Facial Video-based Physiological Measurement with SlowFast\n  Temporal Difference Transformer",
    "authors": [
      "Zitong Yu",
      "Yuming Shen",
      "Jingang Shi",
      "Hengshuang Zhao",
      "Yawen Cui",
      "Jiehua Zhang",
      "Philip Torr",
      "Guoying Zhao"
    ],
    "abstract": "  Remote photoplethysmography (rPPG), which aims at measuring heart activities\nand physiological signals from facial video without any contact, has great\npotential in many applications (e.g., remote healthcare and affective\ncomputing). Recent deep learning approaches focus on mining subtle rPPG clues\nusing convolutional neural networks with limited spatio-temporal receptive\nfields, which neglect the long-range spatio-temporal perception and interaction\nfor rPPG modeling. In this paper, we propose two end-to-end video transformer\nbased architectures, namely PhysFormer and PhysFormer++, to adaptively\naggregate both local and global spatio-temporal features for rPPG\nrepresentation enhancement. As key modules in PhysFormer, the temporal\ndifference transformers first enhance the quasi-periodic rPPG features with\ntemporal difference guided global attention, and then refine the local\nspatio-temporal representation against interference. To better exploit the\ntemporal contextual and periodic rPPG clues, we also extend the PhysFormer to\nthe two-pathway SlowFast based PhysFormer++ with temporal difference periodic\nand cross-attention transformers. Furthermore, we propose the label\ndistribution learning and a curriculum learning inspired dynamic constraint in\nfrequency domain, which provide elaborate supervisions for PhysFormer and\nPhysFormer++ and alleviate overfitting. Comprehensive experiments are performed\non four benchmark datasets to show our superior performance on both intra- and\ncross-dataset testings. Unlike most transformer networks needed pretraining\nfrom large-scale datasets, the proposed PhysFormer family can be easily trained\nfrom scratch on rPPG datasets, which makes it promising as a novel transformer\nbaseline for the rPPG community.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-07T15:56:03Z",
    "updated": "2023-02-07T15:56:03Z",
    "doi": null
  },
  "2203.00242": {
    "id": "http://arxiv.org/abs/2203.00242v1",
    "title": "Unsupervised Vision-and-Language Pre-training via Retrieval-based\n  Multi-Granular Alignment",
    "authors": [
      "Mingyang Zhou",
      "Licheng Yu",
      "Amanpreet Singh",
      "Mengjiao Wang",
      "Zhou Yu",
      "Ning Zhang"
    ],
    "abstract": "  Vision-and-Language (V+L) pre-training models have achieved tremendous\nsuccess in recent years on various multi-modal benchmarks. However, the\nmajority of existing models require pre-training on a large set of parallel\nimage-text data, which is costly to collect, compared to image-only or\ntext-only data. In this paper, we explore unsupervised Vision-and-Language\npre-training (UVLP) to learn the cross-modal representation from non-parallel\nimage and text datasets. We found two key factors that lead to good\nunsupervised V+L pre-training without parallel data: (i) joint image-and-text\ninput (ii) overall image-text alignment (even for non-parallel data).\nAccordingly, we propose a novel unsupervised V+L pre-training curriculum for\nnon-parallel texts and images. We first construct a weakly aligned image-text\ncorpus via a retrieval-based approach, then apply a set of multi-granular\nalignment pre-training tasks, including region-to-tag, region-to-phrase, and\nimage-to-sentence alignment, to bridge the gap between the two modalities. A\ncomprehensive ablation study shows each granularity is helpful to learn a\nstronger pre-trained model. We adapt our pre-trained model to a set of V+L\ndownstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. Our\nmodel achieves the state-of-art performance in all these tasks under the\nunsupervised setting.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-01T05:34:01Z",
    "updated": "2022-03-01T05:34:01Z",
    "doi": null
  },
  "1703.01488": {
    "id": "http://arxiv.org/abs/1703.01488v1",
    "title": "Autoencoding Variational Inference For Topic Models",
    "authors": [
      "Akash Srivastava",
      "Charles Sutton"
    ],
    "abstract": "  Topic models are one of the most popular methods for learning representations\nof text, but a major challenge is that any change to the topic model requires\nmathematically deriving a new inference algorithm. A promising approach to\naddress this problem is autoencoding variational Bayes (AEVB), but it has\nproven diffi- cult to apply to topic models in practice. We present what is to\nour knowledge the first effective AEVB based inference method for latent\nDirichlet allocation (LDA), which we call Autoencoded Variational Inference For\nTopic Model (AVITM). This model tackles the problems caused for AEVB by the\nDirichlet prior and by component collapsing. We find that AVITM matches\ntraditional methods in accuracy with much better inference time. Indeed,\nbecause of the inference network, we find that it is unnecessary to pay the\ncomputational cost of running variational optimization on test data. Because\nAVITM is black box, it is readily applied to new topic models. As a dramatic\nillustration of this, we present a new topic model called ProdLDA, that\nreplaces the mixture model in LDA with a product of experts. By changing only\none line of code from LDA, we find that ProdLDA yields much more interpretable\ntopics, even if LDA is trained via collapsed Gibbs sampling.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-03-04T16:28:15Z",
    "updated": "2017-03-04T16:28:15Z",
    "doi": null
  },
  "1909.13387": {
    "id": "http://arxiv.org/abs/1909.13387v2",
    "title": "FaSNet: Low-latency Adaptive Beamforming for Multi-microphone Audio\n  Processing",
    "authors": [
      "Yi Luo",
      "Enea Ceolini",
      "Cong Han",
      "Shih-Chii Liu",
      "Nima Mesgarani"
    ],
    "abstract": "  Beamforming has been extensively investigated for multi-channel audio\nprocessing tasks. Recently, learning-based beamforming methods, sometimes\ncalled \\textit{neural beamformers}, have achieved significant improvements in\nboth signal quality (e.g. signal-to-noise ratio (SNR)) and speech recognition\n(e.g. word error rate (WER)). Such systems are generally non-causal and require\na large context for robust estimation of inter-channel features, which is\nimpractical in applications requiring low-latency responses. In this paper, we\npropose filter-and-sum network (FaSNet), a time-domain, filter-based\nbeamforming approach suitable for low-latency scenarios. FaSNet has a two-stage\nsystem design that first learns frame-level time-domain adaptive beamforming\nfilters for a selected reference channel, and then calculate the filters for\nall remaining channels. The filtered outputs at all channels are summed to\ngenerate the final output. Experiments show that despite its small model size,\nFaSNet is able to outperform several traditional oracle beamformers with\nrespect to scale-invariant signal-to-noise ratio (SI-SNR) in reverberant speech\nenhancement and separation tasks. Moreover, when trained with a\nfrequency-domain objective function on the CHiME-3 dataset, FaSNet achieves\n14.3\\% relative word error rate reduction (RWERR) compared with the baseline\nmodel. These results show the efficacy of FaSNet particularly in reverberant\nand noisy signal conditions.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.SP",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-09-29T22:26:06Z",
    "updated": "2019-10-01T02:07:14Z",
    "doi": null
  },
  "2301.11798": {
    "id": "http://arxiv.org/abs/2301.11798v2",
    "title": "MedSegDiff-V2: Diffusion based Medical Image Segmentation with\n  Transformer",
    "authors": [
      "Junde Wu",
      "Wei Ji",
      "Huazhu Fu",
      "Min Xu",
      "Yueming Jin",
      "Yanwu Xu"
    ],
    "abstract": "  The Diffusion Probabilistic Model (DPM) has recently gained popularity in the\nfield of computer vision, thanks to its image generation applications, such as\nImagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated\nimpressive capabilities and sparked much discussion within the community.\nRecent investigations have further unveiled the utility of DPM in the domain of\nmedical image analysis, as underscored by the commendable performance exhibited\nby the medical image segmentation model across various tasks. Although these\nmodels were originally underpinned by a UNet architecture, there exists a\npotential avenue for enhancing their performance through the integration of\nvision transformer mechanisms. However, we discovered that simply combining\nthese two models resulted in subpar performance. To effectively integrate these\ntwo cutting-edge techniques for the Medical image segmentation, we propose a\nnovel Transformer-based Diffusion framework, called MedSegDiff-V2. We verify\nits effectiveness on 20 medical image segmentation tasks with different image\nmodalities. Through comprehensive evaluation, our approach demonstrates\nsuperiority over prior state-of-the-art (SOTA) methodologies. Code is released\nat https://github.com/KidsWithTokens/MedSegDiff\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-19T03:42:36Z",
    "updated": "2023-12-24T03:02:12Z",
    "doi": null
  },
  "2111.15666": {
    "id": "http://arxiv.org/abs/2111.15666v2",
    "title": "HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing",
    "authors": [
      "Yuval Alaluf",
      "Omer Tov",
      "Ron Mokady",
      "Rinon Gal",
      "Amit H. Bermano"
    ],
    "abstract": "  The inversion of real images into StyleGAN's latent space is a well-studied\nproblem. Nevertheless, applying existing approaches to real-world scenarios\nremains an open challenge, due to an inherent trade-off between reconstruction\nand editability: latent space regions which can accurately represent real\nimages typically suffer from degraded semantic control. Recent work proposes to\nmitigate this trade-off by fine-tuning the generator to add the target image to\nwell-behaved, editable regions of the latent space. While promising, this\nfine-tuning scheme is impractical for prevalent use as it requires a lengthy\ntraining phase for each new image. In this work, we introduce this approach\ninto the realm of encoder-based inversion. We propose HyperStyle, a\nhypernetwork that learns to modulate StyleGAN's weights to faithfully express a\ngiven image in editable regions of the latent space. A naive modulation\napproach would require training a hypernetwork with over three billion\nparameters. Through careful network design, we reduce this to be in line with\nexisting encoders. HyperStyle yields reconstructions comparable to those of\noptimization techniques with the near real-time inference capabilities of\nencoders. Lastly, we demonstrate HyperStyle's effectiveness on several\napplications beyond the inversion task, including the editing of out-of-domain\nimages which were never seen during training.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-30T18:56:30Z",
    "updated": "2022-03-29T16:11:11Z",
    "doi": null
  },
  "2408.08495": {
    "id": "http://arxiv.org/abs/2408.08495v1",
    "title": "Achieving Complex Image Edits via Function Aggregation with Diffusion\n  Models",
    "authors": [
      "Mohammadreza Samadi",
      "Fred X. Han",
      "Mohammad Salameh",
      "Hao Wu",
      "Fengyu Sun",
      "Chunhua Zhou",
      "Di Niu"
    ],
    "abstract": "  Diffusion models have demonstrated strong performance in generative tasks,\nmaking them ideal candidates for image editing. Recent studies highlight their\nability to apply desired edits effectively by following textual instructions,\nyet two key challenges persist. First, these models struggle to apply multiple\nedits simultaneously, resulting in computational inefficiencies due to their\nreliance on sequential processing. Second, relying on textual prompts to\ndetermine the editing region can lead to unintended alterations in other parts\nof the image. In this work, we introduce FunEditor, an efficient diffusion\nmodel designed to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. FunEditor is 5 to 24 times faster inference\nthan existing methods on complex tasks like object movement. Our experiments\ndemonstrate that FunEditor significantly outperforms recent baselines,\nincluding both inference-time optimization methods and fine-tuned models,\nacross various metrics, such as image quality assessment (IQA) and\nobject-background consistency.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-16T02:33:55Z",
    "updated": "2024-08-16T02:33:55Z",
    "doi": null
  },
  "2212.11263": {
    "id": "http://arxiv.org/abs/2212.11263v1",
    "title": "3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions",
    "authors": [
      "Dale Decatur",
      "Itai Lang",
      "Rana Hanocka"
    ],
    "abstract": "  We present 3D Highlighter, a technique for localizing semantic regions on a\nmesh using text as input. A key feature of our system is the ability to\ninterpret \"out-of-domain\" localizations. Our system demonstrates the ability to\nreason about where to place non-obviously related concepts on an input 3D\nshape, such as adding clothing to a bare 3D animal model. Our method\ncontextualizes the text description using a neural field and colors the\ncorresponding region of the shape using a probability-weighted blend. Our\nneural optimization is guided by a pre-trained CLIP encoder, which bypasses the\nneed for any 3D datasets or 3D annotations. Thus, 3D Highlighter is highly\nflexible, general, and capable of producing localizations on a myriad of input\nshapes. Our code is publicly available at\nhttps://github.com/threedle/3DHighlighter.\n",
    "categories": [
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-21T18:54:47Z",
    "updated": "2022-12-21T18:54:47Z",
    "doi": null
  },
  "2112.02466": {
    "id": "http://arxiv.org/abs/2112.02466v2",
    "title": "Pose-guided Feature Disentangling for Occluded Person Re-identification\n  Based on Transformer",
    "authors": [
      "Tao Wang",
      "Hong Liu",
      "Pinhao Song",
      "Tianyu Guo",
      "Wei Shi"
    ],
    "abstract": "  Occluded person re-identification is a challenging task as human body parts\ncould be occluded by some obstacles (e.g. trees, cars, and pedestrians) in\ncertain scenes. Some existing pose-guided methods solve this problem by\naligning body parts according to graph matching, but these graph-based methods\nare not intuitive and complicated. Therefore, we propose a transformer-based\nPose-guided Feature Disentangling (PFD) method by utilizing pose information to\nclearly disentangle semantic components (e.g. human body or joint parts) and\nselectively match non-occluded parts correspondingly. First, Vision Transformer\n(ViT) is used to extract the patch features with its strong capability. Second,\nto preliminarily disentangle the pose information from patch information, the\nmatching and distributing mechanism is leveraged in Pose-guided Feature\nAggregation (PFA) module. Third, a set of learnable semantic views are\nintroduced in transformer decoder to implicitly enhance the disentangled body\npart features. However, those semantic views are not guaranteed to be related\nto the body without additional supervision. Therefore, Pose-View Matching (PVM)\nmodule is proposed to explicitly match visible body parts and automatically\nseparate occlusion features. Fourth, to better prevent the interference of\nocclusions, we design a Pose-guided Push Loss to emphasize the features of\nvisible body parts. Extensive experiments over five challenging datasets for\ntwo tasks (occluded and holistic Re-ID) demonstrate that our proposed PFD is\nsuperior promising, which performs favorably against state-of-the-art methods.\nCode is available at https://github.com/WangTaoAs/PFD_Net\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-12-05T03:23:31Z",
    "updated": "2021-12-11T08:04:37Z",
    "doi": null
  },
  "2403.05808": {
    "id": "http://arxiv.org/abs/2403.05808v2",
    "title": "Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with\n  Diffusion Model for Blind Image Super-Resolution",
    "authors": [
      "Junxiong Lin",
      "Yan Wang",
      "Zeng Tao",
      "Boyang Wang",
      "Qing Zhao",
      "Haorang Wang",
      "Xuan Tong",
      "Xinji Mai",
      "Yuxuan Lin",
      "Wei Song",
      "Jiawen Yu",
      "Shaoqi Yan",
      "Wenqiang Zhang"
    ],
    "abstract": "  Pre-trained diffusion models utilized for image generation encapsulate a\nsubstantial reservoir of a priori knowledge pertaining to intricate textures.\nHarnessing the potential of leveraging this a priori knowledge in the context\nof image super-resolution presents a compelling avenue. Nonetheless, prevailing\ndiffusion-based methodologies presently overlook the constraints imposed by\ndegradation information on the diffusion process. Furthermore, these methods\nfail to consider the spatial variability inherent in the estimated blur kernel,\nstemming from factors such as motion jitter and out-of-focus elements in\nopen-environment scenarios. This oversight results in a notable deviation of\nthe image super-resolution effect from fundamental realities. To address these\nconcerns, we introduce a framework known as Adaptive Multi-modal Fusion of\n\\textbf{S}patially Variant Kernel Refinement with Diffusion Model for Blind\nImage \\textbf{S}uper-\\textbf{R}esolution (SSR). Within the SSR framework, we\npropose a Spatially Variant Kernel Refinement (SVKR) module. SVKR estimates a\nDepth-Informed Kernel, which takes the depth information into account and is\nspatially variant. Additionally, SVKR enhance the accuracy of depth information\nacquired from LR images, allowing for mutual enhancement between the depth map\nand blur kernel estimates. Finally, we introduce the Adaptive Multi-Modal\nFusion (AMF) module to align the information from three modalities:\nlow-resolution images, depth maps, and blur kernels. This alignment can\nconstrain the diffusion model to generate more authentic SR results.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-09T06:01:25Z",
    "updated": "2024-07-09T15:42:48Z",
    "doi": null
  },
  "1804.08275": {
    "id": "http://arxiv.org/abs/1804.08275v1",
    "title": "Deep Semantic Hashing with Generative Adversarial Networks",
    "authors": [
      "Zhaofan Qiu",
      "Yingwei Pan",
      "Ting Yao",
      "Tao Mei"
    ],
    "abstract": "  Hashing has been a widely-adopted technique for nearest neighbor search in\nlarge-scale image retrieval tasks. Recent research has shown that leveraging\nsupervised information can lead to high quality hashing. However, the cost of\nannotating data is often an obstacle when applying supervised hashing to a new\ndomain. Moreover, the results can suffer from the robustness problem as the\ndata at training and test stage could come from similar but different\ndistributions. This paper studies the exploration of generating synthetic data\nthrough semi-supervised generative adversarial networks (GANs), which leverages\nlargely unlabeled and limited labeled training data to produce highly\ncompelling data with intrinsic invariance and global coherence, for better\nunderstanding statistical structures of natural data. We demonstrate that the\nabove two limitations can be well mitigated by applying the synthetic data for\nhashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\npresented, which mainly consists of four components: a deep convolution neural\nnetworks (CNN) for learning image representations, an adversary stream to\ndistinguish synthetic images from real ones, a hash stream for encoding image\nrepresentations to hash codes and a classification stream. The whole\narchitecture is trained end-to-end by jointly optimizing three losses, i.e.,\nadversarial loss to correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the input\nreal-synthetic triplets and classification loss to classify each sample\naccurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\nbenchmarks validate the capability of exploiting synthetic images for hashing.\nOur framework also achieves superior results when compared to state-of-the-art\ndeep hash models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-23T08:19:55Z",
    "updated": "2018-04-23T08:19:55Z",
    "doi": null
  },
  "1910.12638": {
    "id": "http://arxiv.org/abs/1910.12638v2",
    "title": "Mockingjay: Unsupervised Speech Representation Learning with Deep\n  Bidirectional Transformer Encoders",
    "authors": [
      "Andy T. Liu",
      "Shu-wen Yang",
      "Po-Han Chi",
      "Po-chun Hsu",
      "Hung-yi Lee"
    ],
    "abstract": "  We present Mockingjay as a new speech representation learning approach, where\nbidirectional Transformer encoders are pre-trained on a large amount of\nunlabeled speech. Previous speech representation methods learn through\nconditioning on past frames and predicting information about future frames.\nWhereas Mockingjay is designed to predict the current frame through jointly\nconditioning on both past and future contexts. The Mockingjay representation\nimproves performance for a wide range of downstream tasks, including phoneme\nclassification, speaker recognition, and sentiment classification on spoken\ncontent, while outperforming other approaches. Mockingjay is empirically\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\nfurther improve performance dramatically. In a low resource setting with only\n0.1% of labeled data, we outperform the result of Mel-features that uses all\n100% labeled data.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-10-25T01:55:12Z",
    "updated": "2020-02-02T15:08:39Z",
    "doi": "10.1109/ICASSP40776.2020.9054458"
  },
  "2402.12099": {
    "id": "http://arxiv.org/abs/2402.12099v2",
    "title": "Human Video Translation via Query Warping",
    "authors": [
      "Haiming Zhu",
      "Yangyang Xu",
      "Shengfeng He"
    ],
    "abstract": "  In this paper, we present QueryWarp, a novel framework for temporally\ncoherent human motion video translation. Existing diffusion-based video editing\napproaches that rely solely on key and value tokens to ensure temporal\nconsistency, which scarifies the preservation of local and structural regions.\nIn contrast, we aim to consider complementary query priors by constructing the\ntemporal correlations among query tokens from different frames. Initially, we\nextract appearance flows from source poses to capture continuous human\nforeground motion. Subsequently, during the denoising process of the diffusion\nmodel, we employ appearance flows to warp the previous frame's query token,\naligning it with the current frame's query. This query warping imposes explicit\nconstraints on the outputs of self-attention layers, effectively guaranteeing\ntemporally coherent translation. We perform experiments on various human motion\nvideo translation tasks, and the results demonstrate that our QueryWarp\nframework surpasses state-of-the-art methods both qualitatively and\nquantitatively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-19T12:28:45Z",
    "updated": "2024-05-21T04:04:01Z",
    "doi": null
  },
  "2208.04307": {
    "id": "http://arxiv.org/abs/2208.04307v1",
    "title": "PlaneFormers: From Sparse View Planes to 3D Reconstruction",
    "authors": [
      "Samir Agarwala",
      "Linyi Jin",
      "Chris Rockwell",
      "David F. Fouhey"
    ],
    "abstract": "  We present an approach for the planar surface reconstruction of a scene from\nimages with limited overlap. This reconstruction task is challenging since it\nrequires jointly reasoning about single image 3D reconstruction, correspondence\nbetween images, and the relative camera pose between images. Past work has\nproposed optimization-based approaches. We introduce a simpler approach, the\nPlaneFormer, that uses a transformer applied to 3D-aware plane tokens to\nperform 3D reasoning. Our experiments show that our approach is substantially\nmore effective than prior work, and that several 3D-specific design decisions\nare crucial for its success.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-08T17:58:13Z",
    "updated": "2022-08-08T17:58:13Z",
    "doi": null
  },
  "2208.11328": {
    "id": "http://arxiv.org/abs/2208.11328v2",
    "title": "K-Order Graph-oriented Transformer with GraAttention for 3D Pose and\n  Shape Estimation",
    "authors": [
      "Weixi Zhao",
      "Weiqiang Wang"
    ],
    "abstract": "  We propose a novel attention-based 2D-to-3D pose estimation network for\ngraph-structured data, named KOG-Transformer, and a 3D pose-to-shape estimation\nnetwork for hand data, named GASE-Net. Previous 3D pose estimation methods have\nfocused on various modifications to the graph convolution kernel, such as\nabandoning weight sharing or increasing the receptive field. Some of these\nmethods employ attention-based non-local modules as auxiliary modules. In order\nto better model the relationship between nodes in graph-structured data and\nfuse the information of different neighbor nodes in a differentiated way, we\nmake targeted modifications to the attention module and propose two modules\ndesigned for graph-structured data, graph relative positional encoding\nmulti-head self-attention (GR-MSA) and K-order graph-oriented multi-head\nself-attention (KOG-MSA). By stacking GR-MSA and KOG-MSA, we propose a novel\nnetwork KOG-Transformer for 2D-to-3D pose estimation. Furthermore, we propose a\nnetwork for shape estimation on hand data, called GraAttention shape estimation\nnetwork (GASE-Net), which takes a 3D pose as input and gradually models the\nshape of the hand from sparse to dense. We have empirically shown the\nsuperiority of KOG-Transformer through extensive experiments. Experimental\nresults show that KOG-Transformer significantly outperforms the previous\nstate-of-the-art methods on the benchmark dataset Human3.6M. We evaluate the\neffect of GASE-Net on two public available hand datasets, ObMan and\nInterHand2.6M. GASE-Net can predict the corresponding shape for input pose with\nstrong generalization ability.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-24T06:54:03Z",
    "updated": "2022-09-24T02:20:27Z",
    "doi": null
  },
  "1904.03468": {
    "id": "http://arxiv.org/abs/1904.03468v1",
    "title": "Deep Stacked Hierarchical Multi-patch Network for Image Deblurring",
    "authors": [
      "Hongguang Zhang",
      "Yuchao Dai",
      "Hongdong Li",
      "Piotr Koniusz"
    ],
    "abstract": "  Despite deep end-to-end learning methods have shown their superiority in\nremoving non-uniform motion blur, there still exist major challenges with the\ncurrent multi-scale and scale-recurrent models: 1) Deconvolution/upsampling\noperations in the coarse-to-fine scheme result in expensive runtime; 2) Simply\nincreasing the model depth with finer-scale levels cannot improve the quality\nof deblurring. To tackle the above problems, we present a deep hierarchical\nmulti-patch network inspired by Spatial Pyramid Matching to deal with blurry\nimages via a fine-to-coarse hierarchical representation. To deal with the\nperformance saturation w.r.t. depth, we propose a stacked version of our\nmulti-patch model. Our proposed basic multi-patch model achieves the\nstate-of-the-art performance on the GoPro dataset while enjoying a 40x faster\nruntime compared to current multi-scale methods. With 30ms to process an image\nat 1280x720 resolution, it is the first real-time deep motion deblurring model\nfor 720p images at 30fps. For stacked networks, significant improvements (over\n1.2dB) are achieved on the GoPro dataset by increasing the network depth.\nMoreover, by varying the depth of the stacked model, one can adapt the\nperformance and runtime of the same network for different application\nscenarios.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-04-06T15:15:36Z",
    "updated": "2019-04-06T15:15:36Z",
    "doi": null
  },
  "2002.02848": {
    "id": "http://arxiv.org/abs/2002.02848v1",
    "title": "Unsupervised pretraining transfers well across languages",
    "authors": [
      "Morgane Rivi\u00e8re",
      "Armand Joulin",
      "Pierre-Emmanuel Mazar\u00e9",
      "Emmanuel Dupoux"
    ],
    "abstract": "  Cross-lingual and multi-lingual training of Automatic Speech Recognition\n(ASR) has been extensively investigated in the supervised setting. This assumes\nthe existence of a parallel corpus of speech and orthographic transcriptions.\nRecently, contrastive predictive coding (CPC) algorithms have been proposed to\npretrain ASR systems with unlabelled data. In this work, we investigate whether\nunsupervised pretraining transfers well across languages. We show that a slight\nmodification of the CPC pretraining extracts features that transfer well to\nother languages, being on par or even outperforming supervised pretraining.\nThis shows the potential of unsupervised methods for languages with few\nlinguistic resources.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-02-07T15:34:53Z",
    "updated": "2020-02-07T15:34:53Z",
    "doi": null
  },
  "2301.03846": {
    "id": "http://arxiv.org/abs/2301.03846v1",
    "title": "Practitioners' Expectations on Code Completion",
    "authors": [
      "Chaozheng Wang",
      "Junhao Hu",
      "Cuiyun Gao",
      "Yu Jin",
      "Tao Xie",
      "Hailiang Huang",
      "Zhenyu Lei",
      "Yuetang Deng"
    ],
    "abstract": "  Code completion has become a common practice for programmers during their\ndaily programming activities. It aims at automatically predicting the next\ntokens or lines that the programmers tend to use. A good code completion tool\ncan substantially save keystrokes and improve the programming efficiency for\nprogrammers. Recently, various techniques for code completion have been\nproposed for usage in practice. However, it is still unclear what are\npractitioners' expectations on code completion and whether existing research\nhas met their demands. To fill the gap, we perform an empirical study by first\ninterviewing 15 practitioners and then surveying 599 practitioners from 18 IT\ncompanies about their expectations on code completion. We then compare the\npractitioners' demands with current research via conducting a literature review\nof papers on code completion published in premier publication venues from 2012\nto 2022. Based on the comparison, we highlight the directions desirable for\nresearchers to invest efforts towards developing code completion techniques for\nmeeting practitioners' expectations.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-10T08:30:37Z",
    "updated": "2023-01-10T08:30:37Z",
    "doi": null
  },
  "2406.04961": {
    "id": "http://arxiv.org/abs/2406.04961v1",
    "title": "Multiplane Prior Guided Few-Shot Aerial Scene Rendering",
    "authors": [
      "Zihan Gao",
      "Licheng Jiao",
      "Lingling Li",
      "Xu Liu",
      "Fang Liu",
      "Puhua Chen",
      "Yuwei Guo"
    ],
    "abstract": "  Neural Radiance Fields (NeRF) have been successfully applied in various\naerial scenes, yet they face challenges with sparse views due to limited\nsupervision. The acquisition of dense aerial views is often prohibitive, as\nunmanned aerial vehicles (UAVs) may encounter constraints in perspective range\nand energy constraints. In this work, we introduce Multiplane Prior guided NeRF\n(MPNeRF), a novel approach tailored for few-shot aerial scene rendering-marking\na pioneering effort in this domain. Our key insight is that the intrinsic\ngeometric regularities specific to aerial imagery could be leveraged to enhance\nNeRF in sparse aerial scenes. By investigating NeRF's and Multiplane Image\n(MPI)'s behavior, we propose to guide the training process of NeRF with a\nMultiplane Prior. The proposed Multiplane Prior draws upon MPI's benefits and\nincorporates advanced image comprehension through a SwinV2 Transformer,\npre-trained via SimMIM. Our extensive experiments demonstrate that MPNeRF\noutperforms existing state-of-the-art methods applied in non-aerial contexts,\nby tripling the performance in SSIM and LPIPS even with three views available.\nWe hope our work offers insights into the development of NeRF-based\napplications in aerial scenes with limited data.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-07T14:25:53Z",
    "updated": "2024-06-07T14:25:53Z",
    "doi": null
  },
  "1806.06923": {
    "id": "http://arxiv.org/abs/1806.06923v1",
    "title": "Implicit Quantile Networks for Distributional Reinforcement Learning",
    "authors": [
      "Will Dabney",
      "Georg Ostrovski",
      "David Silver",
      "R\u00e9mi Munos"
    ],
    "abstract": "  In this work, we build on recent advances in distributional reinforcement\nlearning to give a generally applicable, flexible, and state-of-the-art\ndistributional variant of DQN. We achieve this by using quantile regression to\napproximate the full quantile function for the state-action return\ndistribution. By reparameterizing a distribution over the sample space, this\nyields an implicitly defined return distribution and gives rise to a large\nclass of risk-sensitive policies. We demonstrate improved performance on the 57\nAtari 2600 games in the ALE, and use our algorithm's implicitly defined\ndistributions to study the effects of risk-sensitive policies in Atari games.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-06-14T14:28:37Z",
    "updated": "2018-06-14T14:28:37Z",
    "doi": null
  },
  "2203.01726": {
    "id": "http://arxiv.org/abs/2203.01726v3",
    "title": "Ensembles of Vision Transformers as a New Paradigm for Automated\n  Classification in Ecology",
    "authors": [
      "S. Kyathanahally",
      "T. Hardeman",
      "M. Reyes",
      "E. Merz",
      "T. Bulas",
      "P. Brun",
      "F. Pomati",
      "M. Baity-Jesi"
    ],
    "abstract": "  Monitoring biodiversity is paramount to manage and protect natural resources.\nCollecting images of organisms over large temporal or spatial scales is a\npromising practice to monitor the biodiversity of natural ecosystems, providing\nlarge amounts of data with minimal interference with the environment. Deep\nlearning models are currently used to automate classification of organisms into\ntaxonomic units. However, imprecision in these classifiers introduces a\nmeasurement noise that is difficult to control and can significantly hinder the\nanalysis and interpretation of data. {We overcome this limitation through\nensembles of Data-efficient image Transformers (DeiTs), which not only are easy\nto train and implement, but also significantly outperform} the previous state\nof the art (SOTA). We validate our results on ten ecological imaging datasets\nof diverse origin, ranging from plankton to birds. On all the datasets, we\nachieve a new SOTA, with a reduction of the error with respect to the previous\nSOTA ranging from 29.35% to 100.00%, and often achieving performances very\nclose to perfect classification. Ensembles of DeiTs perform better not because\nof superior single-model performances but rather due to smaller overlaps in the\npredictions by independent models and lower top-1 probabilities. This increases\nthe benefit of ensembling, especially when using geometric averages to combine\nindividual learners. While we only test our approach on biodiversity image\ndatasets, our approach is generic and can be applied to any kind of images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-03T14:16:22Z",
    "updated": "2022-09-29T12:15:31Z",
    "doi": "10.1038/s41598-022-21910-0"
  },
  "2111.10659": {
    "id": "http://arxiv.org/abs/2111.10659v2",
    "title": "Are Vision Transformers Robust to Patch Perturbations?",
    "authors": [
      "Jindong Gu",
      "Volker Tresp",
      "Yao Qin"
    ],
    "abstract": "  Recent advances in Vision Transformer (ViT) have demonstrated its impressive\nperformance in image classification, which makes it a promising alternative to\nConvolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image\nas a sequence of image patches. The patch-based input image representation\nmakes the following question interesting: How does ViT perform when individual\ninput image patches are perturbed with natural corruptions or adversarial\nperturbations, compared to CNNs? In this work, we study the robustness of ViT\nto patch-wise perturbations. Surprisingly, we find that ViTs are more robust to\nnaturally corrupted patches than CNNs, whereas they are more vulnerable to\nadversarial patches. Furthermore, we discover that the attention mechanism\ngreatly affects the robustness of vision transformers. Specifically, the\nattention module can help improve the robustness of ViT by effectively ignoring\nnatural corrupted patches. However, when ViTs are attacked by an adversary, the\nattention mechanism can be easily fooled to focus more on the adversarially\nperturbed patches and cause a mistake. Based on our analysis, we propose a\nsimple temperature-scaling based method to improve the robustness of ViT\nagainst adversarial patches. Extensive qualitative and quantitative experiments\nare performed to support our findings, understanding, and improvement of ViT\nrobustness to patch-wise perturbations across a set of transformer-based\narchitectures.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-20T19:00:51Z",
    "updated": "2022-07-18T17:24:18Z",
    "doi": null
  },
  "2207.10022": {
    "id": "http://arxiv.org/abs/2207.10022v2",
    "title": "Secrets of Event-Based Optical Flow",
    "authors": [
      "Shintaro Shiba",
      "Yoshimitsu Aoki",
      "Guillermo Gallego"
    ],
    "abstract": "  Event cameras respond to scene dynamics and offer advantages to estimate\nmotion. Following recent image-based deep-learning achievements, optical flow\nestimation methods for event cameras have rushed to combine those image-based\nmethods with event data. However, it requires several adaptations (data\nconversion, loss function, etc.) as they have very different properties. We\ndevelop a principled method to extend the Contrast Maximization framework to\nestimate optical flow from events alone. We investigate key elements: how to\ndesign the objective function to prevent overfitting, how to warp events to\ndeal better with occlusions, and how to improve convergence with multi-scale\nraw events. With these key elements, our method ranks first among unsupervised\nmethods on the MVSEC benchmark, and is competitive on the DSEC benchmark.\nMoreover, our method allows us to expose the issues of the ground truth flow in\nthose benchmarks, and produces remarkable results when it is transferred to\nunsupervised learning settings. Our code is available at\nhttps://github.com/tub-rip/event_based_optical_flow\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-20T16:40:38Z",
    "updated": "2022-07-21T17:26:51Z",
    "doi": "10.1007/978-3-031-19797-0_36"
  },
  "2208.11257": {
    "id": "http://arxiv.org/abs/2208.11257v1",
    "title": "3D-FM GAN: Towards 3D-Controllable Face Manipulation",
    "authors": [
      "Yuchen Liu",
      "Zhixin Shu",
      "Yijun Li",
      "Zhe Lin",
      "Richard Zhang",
      "S. Y. Kung"
    ],
    "abstract": "  3D-controllable portrait synthesis has significantly advanced, thanks to\nbreakthroughs in generative adversarial networks (GANs). However, it is still\nchallenging to manipulate existing face images with precise 3D control. While\nconcatenating GAN inversion and a 3D-aware, noise-to-image GAN is a\nstraight-forward solution, it is inefficient and may lead to noticeable drop in\nediting quality. To fill this gap, we propose 3D-FM GAN, a novel conditional\nGAN framework designed specifically for 3D-controllable face manipulation, and\ndoes not require any tuning after the end-to-end learning phase. By carefully\nencoding both the input face image and a physically-based rendering of 3D edits\ninto a StyleGAN's latent spaces, our image generator provides high-quality,\nidentity-preserved, 3D-controllable face manipulation. To effectively learn\nsuch novel framework, we develop two essential training strategies and a novel\nmultiplicative co-modulation architecture that improves significantly upon\nnaive schemes. With extensive evaluations, we show that our method outperforms\nthe prior arts on various tasks, with better editability, stronger identity\npreservation, and higher photo-realism. In addition, we demonstrate a better\ngeneralizability of our design on large pose editing and out-of-domain images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-24T01:33:13Z",
    "updated": "2022-08-24T01:33:13Z",
    "doi": null
  },
  "1811.05868": {
    "id": "http://arxiv.org/abs/1811.05868v2",
    "title": "Pitfalls of Graph Neural Network Evaluation",
    "authors": [
      "Oleksandr Shchur",
      "Maximilian Mumme",
      "Aleksandar Bojchevski",
      "Stephan G\u00fcnnemann"
    ],
    "abstract": "  Semi-supervised node classification in graphs is a fundamental problem in\ngraph mining, and the recently proposed graph neural networks (GNNs) have\nachieved unparalleled results on this task. Due to their massive success, GNNs\nhave attracted a lot of attention, and many novel architectures have been put\nforward. In this paper we show that existing evaluation strategies for GNN\nmodels have serious shortcomings. We show that using the same\ntrain/validation/test splits of the same datasets, as well as making\nsignificant changes to the training procedure (e.g. early stopping criteria)\nprecludes a fair comparison of different architectures. We perform a thorough\nempirical evaluation of four prominent GNN models and show that considering\ndifferent splits of the data leads to dramatically different rankings of\nmodels. Even more importantly, our findings suggest that simpler GNN\narchitectures are able to outperform the more sophisticated ones if the\nhyperparameters and the training procedure are tuned fairly for all models.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-11-14T15:53:19Z",
    "updated": "2019-06-18T13:15:39Z",
    "doi": null
  },
  "2301.04272": {
    "id": "http://arxiv.org/abs/2301.04272v2",
    "title": "Data Distillation: A Survey",
    "authors": [
      "Noveen Sachdeva",
      "Julian McAuley"
    ],
    "abstract": "  The popularity of deep learning has led to the curation of a vast number of\nmassive and multifarious datasets. Despite having close-to-human performance on\nindividual tasks, training parameter-hungry models on large datasets poses\nmulti-faceted problems such as (a) high model-training time; (b) slow research\niteration; and (c) poor eco-sustainability. As an alternative, data\ndistillation approaches aim to synthesize terse data summaries, which can serve\nas effective drop-in replacements of the original dataset for scenarios like\nmodel training, inference, architecture search, etc. In this survey, we present\na formal framework for data distillation, along with providing a detailed\ntaxonomy of existing approaches. Additionally, we cover data distillation\napproaches for different data modalities, namely images, graphs, and user-item\ninteractions (recommender systems), while also identifying current challenges\nand future research directions.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-11T02:25:10Z",
    "updated": "2023-09-26T04:43:31Z",
    "doi": null
  },
  "2005.08144": {
    "id": "http://arxiv.org/abs/2005.08144v1",
    "title": "High-dimensional Convolutional Networks for Geometric Pattern\n  Recognition",
    "authors": [
      "Christopher Choy",
      "Junha Lee",
      "Rene Ranftl",
      "Jaesik Park",
      "Vladlen Koltun"
    ],
    "abstract": "  Many problems in science and engineering can be formulated in terms of\ngeometric patterns in high-dimensional spaces. We present high-dimensional\nconvolutional networks (ConvNets) for pattern recognition problems that arise\nin the context of geometric registration. We first study the effectiveness of\nconvolutional networks in detecting linear subspaces in high-dimensional spaces\nwith up to 32 dimensions: much higher dimensionality than prior applications of\nConvNets. We then apply high-dimensional ConvNets to 3D registration under\nrigid motions and image correspondence estimation. Experiments indicate that\nour high-dimensional ConvNets outperform prior approaches that relied on deep\nnetworks based on global pooling operators.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-05-17T01:46:12Z",
    "updated": "2020-05-17T01:46:12Z",
    "doi": null
  },
  "1602.02410": {
    "id": "http://arxiv.org/abs/1602.02410v2",
    "title": "Exploring the Limits of Language Modeling",
    "authors": [
      "Rafal Jozefowicz",
      "Oriol Vinyals",
      "Mike Schuster",
      "Noam Shazeer",
      "Yonghui Wu"
    ],
    "abstract": "  In this work we explore recent advances in Recurrent Neural Networks for\nlarge scale Language Modeling, a task central to language understanding. We\nextend current models to deal with two key challenges present in this task:\ncorpora and vocabulary sizes, and complex, long term structure of language. We\nperform an exhaustive study on techniques such as character Convolutional\nNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.\nOur best single model significantly improves state-of-the-art perplexity from\n51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),\nwhile an ensemble of models sets a new record by improving perplexity from 41.0\ndown to 23.7. We also release these models for the NLP and ML community to\nstudy and improve upon.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-02-07T19:11:17Z",
    "updated": "2016-02-11T23:01:48Z",
    "doi": null
  },
  "2204.07955": {
    "id": "http://arxiv.org/abs/2204.07955v2",
    "title": "Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment\n  Analysis",
    "authors": [
      "Yan Ling",
      "Jianfei Yu",
      "Rui Xia"
    ],
    "abstract": "  As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment\nAnalysis (MABSA) has attracted increasing attention in recent years. However,\nprevious approaches either (i) use separately pre-trained visual and textual\nmodels, which ignore the crossmodal alignment or (ii) use vision-language\nmodels pre-trained with general pre-training tasks, which are inadequate to\nidentify finegrained aspects, opinions, and their alignments across modalities.\nTo tackle these limitations, we propose a task-specific Vision-Language\nPre-training framework for MABSA (VLPMABSA), which is a unified multimodal\nencoder-decoder architecture for all the pretraining and downstream tasks. We\nfurther design three types of task-specific pre-training tasks from the\nlanguage, vision, and multimodal modalities, respectively. Experimental results\nshow that our approach generally outperforms the state-of-the-art approaches on\nthree MABSA subtasks. Further analysis demonstrates the effectiveness of each\npretraining task. The source code is publicly released at\nhttps://github.com/NUSTM/VLP-MABSA.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-17T08:44:00Z",
    "updated": "2022-04-21T12:46:38Z",
    "doi": null
  },
  "2401.02142": {
    "id": "http://arxiv.org/abs/2401.02142v2",
    "title": "GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion\n  Generation",
    "authors": [
      "Xuehao Gao",
      "Yang Yang",
      "Zhenyu Xie",
      "Shaoyi Du",
      "Zhongqian Sun",
      "Yang Wu"
    ],
    "abstract": "  In this paper, we propose a novel cascaded diffusion-based generative\nframework for text-driven human motion synthesis, which exploits a strategy\nnamed GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy\nsets up generation objectives by grouping body joints of detailed skeletons in\nclose semantic proximity together and then replacing each of such joint group\nwith a single body-part node. Such an operation recursively abstracts a human\npose to coarser and coarser skeletons at multiple granularity levels. With\ngradually increasing the abstraction level, human motion becomes more and more\nconcise and stable, significantly benefiting the cross-modal motion synthesis\ntask. The whole text-driven human motion synthesis problem is then divided into\nmultiple abstraction levels and solved with a multi-stage generation framework\nwith a cascaded latent diffusion model: an initial generator first generates\nthe coarsest human motion guess from a given text description; then, a series\nof successive generators gradually enrich the motion details based on the\ntextual description and the previous synthesized results. Notably, we further\nintegrate GUESS with the proposed dynamic multi-condition fusion mechanism to\ndynamically balance the cooperative effects of the given textual condition and\nsynthesized coarse motion prompt in different generation stages. Extensive\nexperiments on large-scale datasets verify that GUESS outperforms existing\nstate-of-the-art methods by large margins in terms of accuracy, realisticness,\nand diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-04T08:48:21Z",
    "updated": "2024-01-06T03:17:55Z",
    "doi": null
  },
  "2311.16945": {
    "id": "http://arxiv.org/abs/2311.16945v2",
    "title": "UC-NeRF: Neural Radiance Field for Under-Calibrated Multi-view Cameras\n  in Autonomous Driving",
    "authors": [
      "Kai Cheng",
      "Xiaoxiao Long",
      "Wei Yin",
      "Jin Wang",
      "Zhiqiang Wu",
      "Yuexin Ma",
      "Kaixuan Wang",
      "Xiaozhi Chen",
      "Xuejin Chen"
    ],
    "abstract": "  Multi-camera setups find widespread use across various applications, such as\nautonomous driving, as they greatly expand sensing capabilities. Despite the\nfast development of Neural radiance field (NeRF) techniques and their wide\napplications in both indoor and outdoor scenes, applying NeRF to multi-camera\nsystems remains very challenging. This is primarily due to the inherent\nunder-calibration issues in multi-camera setup, including inconsistent imaging\neffects stemming from separately calibrated image signal processing units in\ndiverse cameras, and system errors arising from mechanical vibrations during\ndriving that affect relative camera poses. In this paper, we present UC-NeRF, a\nnovel method tailored for novel view synthesis in under-calibrated multi-view\ncamera systems. Firstly, we propose a layer-based color correction to rectify\nthe color inconsistency in different image regions. Second, we propose virtual\nwarping to generate more viewpoint-diverse but color-consistent virtual views\nfor color correction and 3D recovery. Finally, a spatiotemporally constrained\npose refinement is designed for more robust and accurate pose calibration in\nmulti-camera systems. Our method not only achieves state-of-the-art performance\nof novel view synthesis in multi-camera setups, but also effectively\nfacilitates depth estimation in large-scale outdoor scenes with the synthesized\nnovel views.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-28T16:47:59Z",
    "updated": "2023-12-11T03:17:13Z",
    "doi": null
  },
  "2205.09753": {
    "id": "http://arxiv.org/abs/2205.09753v2",
    "title": "HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory\n  Prediction via Scene Encoding",
    "authors": [
      "Xiaosong Jia",
      "Penghao Wu",
      "Li Chen",
      "Yu Liu",
      "Hongyang Li",
      "Junchi Yan"
    ],
    "abstract": "  Encoding a driving scene into vector representations has been an essential\ntask for autonomous driving that can benefit downstream tasks e.g. trajectory\nprediction. The driving scene often involves heterogeneous elements such as the\ndifferent types of objects (agents, lanes, traffic signs) and the semantic\nrelations between objects are rich and diverse. Meanwhile, there also exist\nrelativity across elements, which means that the spatial relation is a relative\nconcept and need be encoded in a ego-centric manner instead of in a global\ncoordinate system. Based on these observations, we propose Heterogeneous\nDriving Graph Transformer (HDGT), a backbone modelling the driving scene as a\nheterogeneous graph with different types of nodes and edges. For heterogeneous\ngraph construction, we connect different types of nodes according to diverse\nsemantic relations. For spatial relation encoding, the coordinates of the node\nas well as its in-edges are in the local node-centric coordinate system. For\nthe aggregation module in the graph neural network (GNN), we adopt the\ntransformer structure in a hierarchical way to fit the heterogeneous nature of\ninputs. Experimental results show that HDGT achieves state-of-the-art\nperformance for the task of trajectory prediction, on INTERACTION Prediction\nChallenge and Waymo Open Motion Challenge.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-30T07:08:30Z",
    "updated": "2023-07-20T08:41:46Z",
    "doi": null
  },
  "2307.00781": {
    "id": "http://arxiv.org/abs/2307.00781v1",
    "title": "ACDMSR: Accelerated Conditional Diffusion Models for Single Image\n  Super-Resolution",
    "authors": [
      "Axi Niu",
      "Pham Xuan Trung",
      "Kang Zhang",
      "Jinqiu Sun",
      "Yu Zhu",
      "In So Kweon",
      "Yanning Zhang"
    ],
    "abstract": "  Diffusion models have gained significant popularity in the field of\nimage-to-image translation. Previous efforts applying diffusion models to image\nsuper-resolution (SR) have demonstrated that iteratively refining pure Gaussian\nnoise using a U-Net architecture trained on denoising at various noise levels\ncan yield satisfactory high-resolution images from low-resolution inputs.\nHowever, this iterative refinement process comes with the drawback of low\ninference speed, which strongly limits its applications. To speed up inference\nand further enhance the performance, our research revisits diffusion models in\nimage super-resolution and proposes a straightforward yet significant diffusion\nmodel-based super-resolution method called ACDMSR (accelerated conditional\ndiffusion model for image super-resolution). Specifically, our method adapts\nthe standard diffusion model to perform super-resolution through a\ndeterministic iterative denoising process. Our study also highlights the\neffectiveness of using a pre-trained SR model to provide the conditional image\nof the given low-resolution (LR) image to achieve superior high-resolution\nresults. We demonstrate that our method surpasses previous attempts in\nqualitative and quantitative results through extensive experiments conducted on\nbenchmark datasets such as Set5, Set14, Urban100, BSD100, and Manga109.\nMoreover, our approach generates more visually realistic counterparts for\nlow-resolution images, emphasizing its effectiveness in practical scenarios.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-03T06:49:04Z",
    "updated": "2023-07-03T06:49:04Z",
    "doi": null
  },
  "2302.02257": {
    "id": "http://arxiv.org/abs/2302.02257v4",
    "title": "Multi-Source Diffusion Models for Simultaneous Music Generation and\n  Separation",
    "authors": [
      "Giorgio Mariani",
      "Irene Tallini",
      "Emilian Postolache",
      "Michele Mancusi",
      "Luca Cosmo",
      "Emanuele Rodol\u00e0"
    ],
    "abstract": "  In this work, we define a diffusion-based generative model capable of both\nmusic synthesis and source separation by learning the score of the joint\nprobability density of sources sharing a context. Alongside the classic total\ninference tasks (i.e., generating a mixture, separating the sources), we also\nintroduce and experiment on the partial generation task of source imputation,\nwhere we generate a subset of the sources given the others (e.g., play a piano\ntrack that goes well with the drums). Additionally, we introduce a novel\ninference method for the separation task based on Dirac likelihood functions.\nWe train our model on Slakh2100, a standard dataset for musical source\nseparation, provide qualitative results in the generation settings, and\nshowcase competitive quantitative results in the source separation setting. Our\nmethod is the first example of a single model that can handle both generation\nand separation tasks, thus representing a step toward general audio models.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-04T23:18:36Z",
    "updated": "2024-03-18T11:39:29Z",
    "doi": null
  },
  "2212.07075": {
    "id": "http://arxiv.org/abs/2212.07075v1",
    "title": "Cross-Modal Similarity-Based Curriculum Learning for Image Captioning",
    "authors": [
      "Hongkuan Zhang",
      "Saku Sugawara",
      "Akiko Aizawa",
      "Lei Zhou",
      "Ryohei Sasano",
      "Koichi Takeda"
    ],
    "abstract": "  Image captioning models require the high-level generalization ability to\ndescribe the contents of various images in words. Most existing approaches\ntreat the image-caption pairs equally in their training without considering the\ndifferences in their learning difficulties. Several image captioning approaches\nintroduce curriculum learning methods that present training data with\nincreasing levels of difficulty. However, their difficulty measurements are\neither based on domain-specific features or prior model training. In this\npaper, we propose a simple yet efficient difficulty measurement for image\ncaptioning using cross-modal similarity calculated by a pretrained\nvision-language model. Experiments on the COCO and Flickr30k datasets show that\nour proposed approach achieves superior performance and competitive convergence\nspeed to baselines without requiring heuristics or incurring additional\ntraining costs. Moreover, the higher model performance on difficult examples\nand unseen data also demonstrates the generalization ability.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-14T07:52:36Z",
    "updated": "2022-12-14T07:52:36Z",
    "doi": null
  },
  "2410.02681": {
    "id": "http://arxiv.org/abs/2410.02681v1",
    "title": "Understanding and Mitigating Miscalibration in Prompt Tuning for\n  Vision-Language Models",
    "authors": [
      "Shuoyuan Wang",
      "Yixuan Li",
      "Hongxin Wei"
    ],
    "abstract": "  Confidence calibration is critical for the safe deployment of machine\nlearning models in the real world. However, such issue in vision-language\nmodels like CLIP, particularly after fine-tuning, has not been fully addressed.\nIn this work, we demonstrate that existing prompt tuning methods usually lead\nto a trade-off of calibration between base and new classes: the cross-entropy\nloss in CoOp causes overconfidence in new classes by increasing textual label\ndivergence, whereas the regularization of KgCoOp maintains the confidence level\nbut results in underconfidence in base classes due to the improved accuracy.\nInspired by the observations, we introduce Dynamic Outlier Regularization (DOR)\nto ensure the confidence calibration on both base and new classes after\nfine-tuning. In particular, we propose to minimize the feature deviation of\nnovel textual labels (instead of base classes) sampled from a large vocabulary.\nIn effect, DOR prevents the increase in textual divergence for new labels while\neasing restrictions on base classes. Extensive experiments demonstrate that DOR\ncan enhance the calibration performance of current fine-tuning methods on base\nand new classes.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-10-03T17:06:21Z",
    "updated": "2024-10-03T17:06:21Z",
    "doi": null
  },
  "2109.04144": {
    "id": "http://arxiv.org/abs/2109.04144v1",
    "title": "Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning",
    "authors": [
      "Prasetya Ajie Utama",
      "Nafise Sadat Moosavi",
      "Victor Sanh",
      "Iryna Gurevych"
    ],
    "abstract": "  Recent prompt-based approaches allow pretrained language models to achieve\nstrong performances on few-shot finetuning by reformulating downstream tasks as\na language modeling problem. In this work, we demonstrate that, despite its\nadvantages on low data regimes, finetuned prompt-based models for sentence pair\nclassification tasks still suffer from a common pitfall of adopting inference\nheuristics based on lexical overlap, e.g., models incorrectly assuming a\nsentence pair is of the same meaning because they consist of the same set of\nwords. Interestingly, we find that this particular inference heuristic is\nsignificantly less present in the zero-shot evaluation of the prompt-based\nmodel, indicating how finetuning can be destructive to useful knowledge learned\nduring the pretraining. We then show that adding a regularization that\npreserves pretraining weights is effective in mitigating this destructive\ntendency of few-shot finetuning. Our evaluation on three datasets demonstrates\npromising improvements on the three corresponding challenge datasets used to\ndiagnose the inference heuristics.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-09T10:10:29Z",
    "updated": "2021-09-09T10:10:29Z",
    "doi": null
  },
  "1609.09475": {
    "id": "http://arxiv.org/abs/1609.09475v3",
    "title": "Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the\n  Amazon Picking Challenge",
    "authors": [
      "Andy Zeng",
      "Kuan-Ting Yu",
      "Shuran Song",
      "Daniel Suo",
      "Ed Walker Jr.",
      "Alberto Rodriguez",
      "Jianxiong Xiao"
    ],
    "abstract": "  Robot warehouse automation has attracted significant interest in recent\nyears, perhaps most visibly in the Amazon Picking Challenge (APC). A fully\nautonomous warehouse pick-and-place system requires robust vision that reliably\nrecognizes and locates objects amid cluttered environments, self-occlusions,\nsensor noise, and a large variety of objects. In this paper we present an\napproach that leverages multi-view RGB-D data and self-supervised, data-driven\nlearning to overcome those difficulties. The approach was part of the\nMIT-Princeton Team system that took 3rd- and 4th- place in the stowing and\npicking tasks, respectively at APC 2016. In the proposed approach, we segment\nand label multiple views of a scene with a fully convolutional neural network,\nand then fit pre-scanned 3D object models to the resulting segmentation to get\nthe 6D object pose. Training a deep neural network for segmentation typically\nrequires a large amount of training data. We propose a self-supervised method\nto generate a large labeled dataset without tedious manual segmentation. We\ndemonstrate that our system can reliably estimate the 6D pose of objects under\na variety of scenarios. All code, data, and benchmarks are available at\nhttp://apc.cs.princeton.edu/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-09-29T19:39:13Z",
    "updated": "2017-05-07T20:12:55Z",
    "doi": null
  },
  "2006.04768": {
    "id": "http://arxiv.org/abs/2006.04768v3",
    "title": "Linformer: Self-Attention with Linear Complexity",
    "authors": [
      "Sinong Wang",
      "Belinda Z. Li",
      "Madian Khabsa",
      "Han Fang",
      "Hao Ma"
    ],
    "abstract": "  Large transformer models have shown extraordinary success in achieving\nstate-of-the-art results in many natural language processing applications.\nHowever, training and deploying these models can be prohibitively costly for\nlong sequences, as the standard self-attention mechanism of the Transformer\nuses $O(n^2)$ time and space with respect to sequence length. In this paper, we\ndemonstrate that the self-attention mechanism can be approximated by a low-rank\nmatrix. We further exploit this finding to propose a new self-attention\nmechanism, which reduces the overall self-attention complexity from $O(n^2)$ to\n$O(n)$ in both time and space. The resulting linear transformer, the\n\\textit{Linformer}, performs on par with standard Transformer models, while\nbeing much more memory- and time-efficient.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-06-08T17:37:52Z",
    "updated": "2020-06-14T08:15:54Z",
    "doi": null
  },
  "2305.05189": {
    "id": "http://arxiv.org/abs/2305.05189v4",
    "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with\n  Large Language Models",
    "authors": [
      "Shanshan Zhong",
      "Zhongzhan Huang",
      "Wushao Wen",
      "Jinghui Qin",
      "Liang Lin"
    ],
    "abstract": "  Diffusion models, which have emerged to become popular text-to-image\ngeneration models, can produce high-quality and content-rich images guided by\ntextual prompts. However, there are limitations to semantic understanding and\ncommonsense reasoning in existing models when the input prompts are concise\nnarrative, resulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-efficient\nfine-tuning approach called the Semantic Understanding and Reasoning adapter\n(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first\ncollect and annotate a new dataset SURD which consists of more than 57,000\nsemantically corrected multi-modal samples. Each sample contains a simple\nnarrative prompt, a complex keyword-based prompt, and a high-quality image.\nThen, we align the semantic representation of narrative prompts to the complex\nprompts and transfer knowledge of large language models (LLMs) to our\nSUR-adapter via knowledge distillation so that it can acquire the powerful\nsemantic understanding and reasoning capabilities to build a high-quality\ntextual semantic representation for text-to-image generation. We conduct\nexperiments by integrating multiple LLMs and popular pre-trained diffusion\nmodels to show the effectiveness of our approach in enabling diffusion models\nto understand and reason concise natural language without image quality\ndegradation. Our approach can make text-to-image diffusion models easier to use\nwith better user experience, which demonstrates our approach has the potential\nfor further advancing the development of user-friendly text-to-image generation\nmodels by bridging the semantic gap between simple narrative prompts and\ncomplex keyword-based prompts. The code is released at\nhttps://github.com/Qrange-group/SUR-adapter.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-09T05:48:38Z",
    "updated": "2023-11-29T08:18:14Z",
    "doi": null
  },
  "2209.13816": {
    "id": "http://arxiv.org/abs/2209.13816v3",
    "title": "Revisiting Few-Shot Learning from a Causal Perspective",
    "authors": [
      "Guoliang Lin",
      "Yongheng Xu",
      "Hanjiang Lai",
      "Jian Yin"
    ],
    "abstract": "  Few-shot learning with $N$-way $K$-shot scheme is an open challenge in\nmachine learning. Many metric-based approaches have been proposed to tackle\nthis problem, e.g., the Matching Networks and CLIP-Adapter. Despite that these\napproaches have shown significant progress, the mechanism of why these methods\nsucceed has not been well explored. In this paper, we try to interpret these\nmetric-based few-shot learning methods via causal mechanism. We show that the\nexisting approaches can be viewed as specific forms of front-door adjustment,\nwhich can alleviate the effect of spurious correlations and thus learn the\ncausality. This causal interpretation could provide us a new perspective to\nbetter understand these existing metric-based methods. Further, based on this\ncausal interpretation, we simply introduce two causal methods for metric-based\nfew-shot learning, which considers not only the relationship between examples\nbut also the diversity of representations. Experimental results demonstrate the\nsuperiority of our proposed methods in few-shot classification on various\nbenchmark datasets. Code is available in\nhttps://github.com/lingl1024/causalFewShot.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-28T03:46:02Z",
    "updated": "2024-05-07T02:27:42Z",
    "doi": null
  },
  "2111.11591": {
    "id": "http://arxiv.org/abs/2111.11591v2",
    "title": "Efficient Video Transformers with Spatial-Temporal Token Selection",
    "authors": [
      "Junke Wang",
      "Xitong Yang",
      "Hengduo Li",
      "Li Liu",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "  Video transformers have achieved impressive results on major video\nrecognition benchmarks, which however suffer from high computational cost. In\nthis paper, we present STTS, a token selection framework that dynamically\nselects a few informative tokens in both temporal and spatial dimensions\nconditioned on input video samples. Specifically, we formulate token selection\nas a ranking problem, which estimates the importance of each token through a\nlightweight scorer network and only those with top scores will be used for\ndownstream evaluation. In the temporal dimension, we keep the frames that are\nmost relevant to the action categories, while in the spatial dimension, we\nidentify the most discriminative region in feature maps without affecting the\nspatial context used in a hierarchical way in most video transformers. Since\nthe decision of token selection is non-differentiable, we employ a\nperturbed-maximum based differentiable Top-K operator for end-to-end training.\nWe mainly conduct extensive experiments on Kinetics-400 with a recently\nintroduced video transformer backbone, MViT. Our framework achieves similar\nresults while requiring 20% less computation. We also demonstrate our approach\nis generic for different transformer architectures and video datasets. Code is\navailable at https://github.com/wangjk666/STTS.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-23T00:35:58Z",
    "updated": "2022-07-16T09:15:15Z",
    "doi": null
  },
  "2401.03630": {
    "id": "http://arxiv.org/abs/2401.03630v2",
    "title": "Why Solving Multi-agent Path Finding with Large Language Model has not\n  Succeeded Yet",
    "authors": [
      "Weizhe Chen",
      "Sven Koenig",
      "Bistra Dilkina"
    ],
    "abstract": "  With the explosive influence caused by the success of large language models\n(LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work\nshowing that foundation models can be used to solve a large variety of tasks.\nHowever, there is very limited work that shares insights on multi-agent\nplanning. Multi-agent planning is different from other domains by combining the\ndifficulty of multi-agent coordination and planning, and making it hard to\nleverage external tools to facilitate the reasoning needed. In this paper, we\nfocus on the problem of multi-agent path finding (MAPF), which is also known as\nmulti-robot route planning, and study the performance of solving MAPF with\nLLMs. We first show the motivating success on an empty room map without\nobstacles, then the failure to plan on the harder room map and maze map of the\nstandard MAPF benchmark. We present our position on why directly solving MAPF\nwith LLMs has not been successful yet, and we use various experiments to\nsupport our hypothesis. Based on our results, we discussed how researchers with\ndifferent backgrounds could help with this problem from different perspectives.\n",
    "categories": [
      {
        "@term": "cs.MA",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-08T02:22:04Z",
    "updated": "2024-02-09T17:48:19Z",
    "doi": null
  },
  "2209.06040": {
    "id": "http://arxiv.org/abs/2209.06040v1",
    "title": "DMTNet: Dynamic Multi-scale Network for Dual-pixel Images Defocus\n  Deblurring with Transformer",
    "authors": [
      "Dafeng Zhang",
      "Xiaobing Wang"
    ],
    "abstract": "  Recent works achieve excellent results in defocus deblurring task based on\ndual-pixel data using convolutional neural network (CNN), while the scarcity of\ndata limits the exploration and attempt of vision transformer in this task. In\naddition, the existing works use fixed parameters and network architecture to\ndeblur images with different distribution and content information, which also\naffects the generalization ability of the model. In this paper, we propose a\ndynamic multi-scale network, named DMTNet, for dual-pixel images defocus\ndeblurring. DMTNet mainly contains two modules: feature extraction module and\nreconstruction module. The feature extraction module is composed of several\nvision transformer blocks, which uses its powerful feature extraction\ncapability to obtain richer features and improve the robustness of the model.\nThe reconstruction module is composed of several Dynamic Multi-scale\nSub-reconstruction Module (DMSSRM). DMSSRM can restore images by adaptively\nassigning weights to features from different scales according to the blur\ndistribution and content information of the input images. DMTNet combines the\nadvantages of transformer and CNN, in which the vision transformer improves the\nperformance ceiling of CNN, and the inductive bias of CNN enables transformer\nto extract more robust features without relying on a large amount of data.\nDMTNet might be the first attempt to use vision transformer to restore the\nblurring images to clarity. By combining with CNN, the vision transformer may\nachieve better performance on small datasets. Experimental results on the\npopular benchmarks demonstrate that our DMTNet significantly outperforms\nstate-of-the-art methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-13T14:47:09Z",
    "updated": "2022-09-13T14:47:09Z",
    "doi": "10.1109/ICME52920.2022.9859631"
  },
  "2111.12707": {
    "id": "http://arxiv.org/abs/2111.12707v4",
    "title": "MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation",
    "authors": [
      "Wenhao Li",
      "Hong Liu",
      "Hao Tang",
      "Pichao Wang",
      "Luc Van Gool"
    ],
    "abstract": "  Estimating 3D human poses from monocular videos is a challenging task due to\ndepth ambiguity and self-occlusion. Most existing works attempt to solve both\nissues by exploiting spatial and temporal relationships. However, those works\nignore the fact that it is an inverse problem where multiple feasible solutions\n(i.e., hypotheses) exist. To relieve this limitation, we propose a\nMulti-Hypothesis Transformer (MHFormer) that learns spatio-temporal\nrepresentations of multiple plausible pose hypotheses. In order to effectively\nmodel multi-hypothesis dependencies and build strong relationships across\nhypothesis features, the task is decomposed into three stages: (i) Generate\nmultiple initial hypothesis representations; (ii) Model self-hypothesis\ncommunication, merge multiple hypotheses into a single converged representation\nand then partition it into several diverged hypotheses; (iii) Learn\ncross-hypothesis communication and aggregate the multi-hypothesis features to\nsynthesize the final 3D pose. Through the above processes, the final\nrepresentation is enhanced and the synthesized pose is much more accurate.\nExtensive experiments show that MHFormer achieves state-of-the-art results on\ntwo challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and\nwhistles, its performance surpasses the previous best result by a large margin\nof 3% on Human3.6M. Code and models are available at\n\\url{https://github.com/Vegetebird/MHFormer}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-24T18:59:02Z",
    "updated": "2022-06-28T15:02:02Z",
    "doi": null
  },
  "2403.02460": {
    "id": "http://arxiv.org/abs/2403.02460v4",
    "title": "MagicClay: Sculpting Meshes With Generative Neural Fields",
    "authors": [
      "Amir Barda",
      "Vladimir G. Kim",
      "Noam Aigerman",
      "Amit H. Bermano",
      "Thibault Groueix"
    ],
    "abstract": "  The recent developments in neural fields have brought phenomenal capabilities\nto the field of shape generation, but they lack crucial properties, such as\nincremental control - a fundamental requirement for artistic work. Triangular\nmeshes, on the other hand, are the representation of choice for most geometry\nrelated tasks, offering efficiency and intuitive control, but do not lend\nthemselves to neural optimization. To support downstream tasks, previous art\ntypically proposes a two-step approach, where first a shape is generated using\nneural fields, and then a mesh is extracted for further processing. Instead, in\nthis paper we introduce a hybrid approach that maintains both a mesh and a\nSigned Distance Field (SDF) representations consistently. Using this\nrepresentation, we introduce MagicClay - an artist friendly tool for sculpting\nregions of a mesh according to textual prompts while keeping other regions\nuntouched. Our framework carefully and efficiently balances consistency between\nthe representations and regularizations in every step of the shape\noptimization; Relying on the mesh representation, we show how to render the SDF\nat higher resolutions and faster. In addition, we employ recent work in\ndifferentiable mesh reconstruction to adaptively allocate triangles in the mesh\nwhere required, as indicated by the SDF. Using an implemented prototype, we\ndemonstrate superior generated geometry compared to the state-of-the-art, and\nnovel consistent control, allowing sequential prompt-based edits to the same\nmesh for the first time.\n",
    "categories": [
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-04T20:20:14Z",
    "updated": "2024-10-09T17:52:09Z",
    "doi": null
  },
  "2302.03540": {
    "id": "http://arxiv.org/abs/2302.03540v1",
    "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal\n  Supervision",
    "authors": [
      "Eugene Kharitonov",
      "Damien Vincent",
      "Zal\u00e1n Borsos",
      "Rapha\u00ebl Marinier",
      "Sertan Girgin",
      "Olivier Pietquin",
      "Matt Sharifi",
      "Marco Tagliasacchi",
      "Neil Zeghidour"
    ],
    "abstract": "  We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can\nbe trained with minimal supervision. By combining two types of discrete speech\nrepresentations, we cast TTS as a composition of two sequence-to-sequence\ntasks: from text to high-level semantic tokens (akin to \"reading\") and from\nsemantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two\ntasks enables training of the \"speaking\" module using abundant audio-only data,\nand unlocks the highly efficient combination of pretraining and backtranslation\nto reduce the need for parallel data when training the \"reading\" component. To\ncontrol the speaker identity, we adopt example prompting, which allows\nSPEAR-TTS to generalize to unseen speakers using only a short sample of 3\nseconds, without any explicit speaker representation or speaker-id labels. Our\nexperiments demonstrate that SPEAR-TTS achieves a character error rate that is\ncompetitive with state-of-the-art methods using only 15 minutes of parallel\ndata, while matching ground-truth speech in terms of naturalness and acoustic\nquality, as measured in subjective tests.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-07T15:48:31Z",
    "updated": "2023-02-07T15:48:31Z",
    "doi": null
  },
  "2001.08361": {
    "id": "http://arxiv.org/abs/2001.08361v1",
    "title": "Scaling Laws for Neural Language Models",
    "authors": [
      "Jared Kaplan",
      "Sam McCandlish",
      "Tom Henighan",
      "Tom B. Brown",
      "Benjamin Chess",
      "Rewon Child",
      "Scott Gray",
      "Alec Radford",
      "Jeffrey Wu",
      "Dario Amodei"
    ],
    "abstract": "  We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-01-23T03:59:20Z",
    "updated": "2020-01-23T03:59:20Z",
    "doi": null
  },
  "2105.06070": {
    "id": "http://arxiv.org/abs/2105.06070v1",
    "title": "GAN Prior Embedded Network for Blind Face Restoration in the Wild",
    "authors": [
      "Tao Yang",
      "Peiran Ren",
      "Xuansong Xie",
      "Lei Zhang"
    ],
    "abstract": "  Blind face restoration (BFR) from severely degraded face images in the wild\nis a very challenging problem. Due to the high illness of the problem and the\ncomplex unknown degradation, directly training a deep neural network (DNN)\nusually cannot lead to acceptable results. Existing generative adversarial\nnetwork (GAN) based methods can produce better results but tend to generate\nover-smoothed restorations. In this work, we propose a new method by first\nlearning a GAN for high-quality face image generation and embedding it into a\nU-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN\nwith a set of synthesized low-quality face images. The GAN blocks are designed\nto ensure that the latent code and noise input to the GAN can be respectively\ngenerated from the deep and shallow features of the DNN, controlling the global\nface structure, local face details and background of the reconstructed image.\nThe proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can\ngenerate visually photo-realistic results. Our experiments demonstrated that\nthe proposed GPEN achieves significantly superior results to state-of-the-art\nBFR methods both quantitatively and qualitatively, especially for the\nrestoration of severely degraded face images in the wild. The source code and\nmodels can be found at https://github.com/yangxy/GPEN.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-05-13T04:14:00Z",
    "updated": "2021-05-13T04:14:00Z",
    "doi": null
  },
  "2312.13308": {
    "id": "http://arxiv.org/abs/2312.13308v2",
    "title": "SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting",
    "authors": [
      "Richard Shaw",
      "Michal Nazarczuk",
      "Jifei Song",
      "Arthur Moreau",
      "Sibi Catley-Chandar",
      "Helisa Dhamo",
      "Eduardo Perez-Pellitero"
    ],
    "abstract": "  Novel view synthesis has shown rapid progress recently, with methods capable\nof producing increasingly photorealistic results. 3D Gaussian Splatting has\nemerged as a promising method, producing high-quality renderings of scenes and\nenabling interactive viewing at real-time frame rates. However, it is limited\nto static scenes. In this work, we extend 3D Gaussian Splatting to reconstruct\ndynamic scenes. We model a scene's dynamics using dynamic MLPs, learning\ndeformations from temporally-local canonical representations to per-frame 3D\nGaussians. To disentangle static and dynamic regions, tuneable parameters weigh\neach Gaussian's respective MLP parameters, improving the dynamics modelling of\nimbalanced scenes. We introduce a sliding window training strategy that\npartitions the sequence into smaller manageable windows to handle arbitrary\nlength scenes while maintaining high rendering quality. We propose an adaptive\nsampling strategy to determine appropriate window size hyperparameters based on\nthe scene's motion, balancing training overhead with visual quality. Training a\nseparate dynamic 3D Gaussian model for each sliding window allows the canonical\nrepresentation to change, enabling the reconstruction of scenes with\nsignificant geometric changes. Temporal consistency is enforced using a\nfine-tuning step with self-supervising consistency loss on randomly sampled\nnovel views. As a result, our method produces high-quality renderings of\ngeneral dynamic scenes with competitive quantitative performance, which can be\nviewed in real-time in our dynamic interactive viewer.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-20T03:54:03Z",
    "updated": "2024-07-18T10:18:51Z",
    "doi": null
  },
  "2404.02101": {
    "id": "http://arxiv.org/abs/2404.02101v1",
    "title": "CameraCtrl: Enabling Camera Control for Text-to-Video Generation",
    "authors": [
      "Hao He",
      "Yinghao Xu",
      "Yuwei Guo",
      "Gordon Wetzstein",
      "Bo Dai",
      "Hongsheng Li",
      "Ceyuan Yang"
    ],
    "abstract": "  Controllability plays a crucial role in video generation since it allows\nusers to create desired content. However, existing models largely overlooked\nthe precise control of camera pose that serves as a cinematic language to\nexpress deeper narrative nuances. To alleviate this issue, we introduce\nCameraCtrl, enabling accurate camera pose control for text-to-video(T2V)\nmodels. After precisely parameterizing the camera trajectory, a plug-and-play\ncamera module is then trained on a T2V model, leaving others untouched.\nAdditionally, a comprehensive study on the effect of various datasets is also\nconducted, suggesting that videos with diverse camera distribution and similar\nappearances indeed enhance controllability and generalization. Experimental\nresults demonstrate the effectiveness of CameraCtrl in achieving precise and\ndomain-adaptive camera control, marking a step forward in the pursuit of\ndynamic and customized video storytelling from textual and camera pose inputs.\nOur project website is at: https://hehao13.github.io/projects-CameraCtrl/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-02T16:52:41Z",
    "updated": "2024-04-02T16:52:41Z",
    "doi": null
  },
  "2202.08455": {
    "id": "http://arxiv.org/abs/2202.08455v1",
    "title": "Transformer for Graphs: An Overview from Architecture Perspective",
    "authors": [
      "Erxue Min",
      "Runfa Chen",
      "Yatao Bian",
      "Tingyang Xu",
      "Kangfei Zhao",
      "Wenbing Huang",
      "Peilin Zhao",
      "Junzhou Huang",
      "Sophia Ananiadou",
      "Yu Rong"
    ],
    "abstract": "  Recently, Transformer model, which has achieved great success in many\nartificial intelligence fields, has demonstrated its great potential in\nmodeling graph-structured data. Till now, a great variety of Transformers has\nbeen proposed to adapt to the graph-structured data. However, a comprehensive\nliterature review and systematical evaluation of these Transformer variants for\ngraphs are still unavailable. It's imperative to sort out the existing\nTransformer models for graphs and systematically investigate their\neffectiveness on various graph tasks. In this survey, we provide a\ncomprehensive review of various Graph Transformer models from the architectural\ndesign perspective. We first disassemble the existing models and conclude three\ntypical ways to incorporate the graph information into the vanilla Transformer:\n1) GNNs as Auxiliary Modules, 2) Improved Positional Embedding from Graphs, and\n3) Improved Attention Matrix from Graphs. Furthermore, we implement the\nrepresentative components in three groups and conduct a comprehensive\ncomparison on various kinds of famous graph data benchmarks to investigate the\nreal performance gain of each component. Our experiments confirm the benefits\nof current graph-specific modules on Transformer and reveal their advantages on\ndifferent kinds of graph tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-02-17T06:02:06Z",
    "updated": "2022-02-17T06:02:06Z",
    "doi": null
  },
  "2212.09100": {
    "id": "http://arxiv.org/abs/2212.09100v3",
    "title": "SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input\n  Images",
    "authors": [
      "Abdullah Hamdi",
      "Bernard Ghanem",
      "Matthias Nie\u00dfner"
    ],
    "abstract": "  Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel\nview synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels\nfor efficient and fast rendering (plenoxels,InstantNGP). In order to leverage\nmachine learning and adoption of SRFs as a 3D representation, we present SPARF,\na large-scale ShapeNet-based synthetic dataset for novel view synthesis\nconsisting of $\\sim$ 17 million images rendered from nearly 40,000 shapes at\nhigh resolution (400 X 400 pixels). The dataset is orders of magnitude larger\nthan existing synthetic datasets for novel view synthesis and includes more\nthan one million 3D-optimized radiance fields with multiple voxel resolutions.\nFurthermore, we propose a novel pipeline (SuRFNet) that learns to generate\nsparse voxel radiance fields from only few views. This is done by using the\ndensely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs\npartial SRFs from few/one images and a specialized SRF loss to learn to\ngenerate high-quality sparse voxel radiance fields that can be rendered from\nnovel views. Our approach achieves state-of-the-art results in the task of\nunconstrained novel view synthesis based on few views on ShapeNet as compared\nto recent baselines. The SPARF dataset is made public with the code and models\non the project website https://abdullahamdi.com/sparf/ .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68T45",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-18T14:56:22Z",
    "updated": "2023-08-21T12:53:09Z",
    "doi": "10.1109/ICCVW60793.2023.00315"
  },
  "2406.02549": {
    "id": "http://arxiv.org/abs/2406.02549v1",
    "title": "Dreamguider: Improved Training free Diffusion-based Conditional\n  Generation",
    "authors": [
      "Nithin Gopalakrishnan Nair",
      "Vishal M Patel"
    ],
    "abstract": "  Diffusion models have emerged as a formidable tool for training-free\nconditional generation.However, a key hurdle in inference-time guidance\ntechniques is the need for compute-heavy backpropagation through the diffusion\nnetwork for estimating the guidance direction. Moreover, these techniques often\nrequire handcrafted parameter tuning on a case-by-case basis. Although some\nrecent works have introduced minimal compute methods for linear inverse\nproblems, a generic lightweight guidance solution to both linear and non-linear\nguidance problems is still missing. To this end, we propose Dreamguider, a\nmethod that enables inference-time guidance without compute-heavy\nbackpropagation through the diffusion network. The key idea is to regulate the\ngradient flow through a time-varying factor. Moreover, we propose an empirical\nguidance scale that works for a wide variety of tasks, hence removing the need\nfor handcrafted parameter tuning. We further introduce an effective lightweight\naugmentation strategy that significantly boosts the performance during\ninference-time guidance. We present experiments using Dreamguider on multiple\ntasks across multiple datasets and models to show the effectiveness of the\nproposed modules. To facilitate further research, we will make the code public\nafter the review process.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-04T17:59:32Z",
    "updated": "2024-06-04T17:59:32Z",
    "doi": null
  },
  "1308.3052": {
    "id": "http://arxiv.org/abs/1308.3052v2",
    "title": "Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual\n  Image Quality Index",
    "authors": [
      "Wufeng Xue",
      "Lei Zhang",
      "Xuanqin Mou",
      "Alan C. Bovik"
    ],
    "abstract": "  It is an important task to faithfully evaluate the perceptual quality of\noutput images in many applications such as image compression, image restoration\nand multimedia streaming. A good image quality assessment (IQA) model should\nnot only deliver high quality prediction accuracy but also be computationally\nefficient. The efficiency of IQA metrics is becoming particularly important due\nto the increasing proliferation of high-volume visual data in high-speed\nnetworks. We present a new effective and efficient IQA model, called gradient\nmagnitude similarity deviation (GMSD). The image gradients are sensitive to\nimage distortions, while different local structures in a distorted image suffer\ndifferent degrees of degradations. This motivates us to explore the use of\nglobal variation of gradient based local quality map for overall image quality\nprediction. We find that the pixel-wise gradient magnitude similarity (GMS)\nbetween the reference and distorted images combined with a novel pooling\nstrategy the standard deviation of the GMS map can predict accurately\nperceptual image quality. The resulting GMSD algorithm is much faster than most\nstate-of-the-art IQA methods, and delivers highly competitive prediction\naccuracy.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2013-08-14T07:25:10Z",
    "updated": "2013-11-26T03:55:01Z",
    "doi": "10.1109/TIP.2013.2293423"
  },
  "2309.11715": {
    "id": "http://arxiv.org/abs/2309.11715v3",
    "title": "Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow\n  removal",
    "authors": [
      "Xiao Feng Zhang",
      "Tian Yi Song",
      "Jia Wei Yao"
    ],
    "abstract": "  Segment Anything (SAM), an advanced universal image segmentation model\ntrained on an expansive visual dataset, has set a new benchmark in image\nsegmentation and computer vision. However, it faced challenges when it came to\ndistinguishing between shadows and their backgrounds. To address this, we\ndeveloped Deshadow-Anything, considering the generalization of large-scale\ndatasets, and we performed Fine-tuning on large-scale datasets to achieve image\nshadow removal. The diffusion model can diffuse along the edges and textures of\nan image, helping to remove shadows while preserving the details of the image.\nFurthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input\nperturbation (DDPM-AIP) to accelerate the iterative training speed of\ndiffusion. Experiments on shadow removal tasks demonstrate that these methods\ncan effectively improve image restoration performance.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-21T01:35:13Z",
    "updated": "2024-01-03T02:01:09Z",
    "doi": null
  },
  "2408.00211": {
    "id": "http://arxiv.org/abs/2408.00211v1",
    "title": "Penzai + Treescope: A Toolkit for Interpreting, Visualizing, and Editing\n  Models As Data",
    "authors": [
      "Daniel D. Johnson"
    ],
    "abstract": "  Much of today's machine learning research involves interpreting, modifying or\nvisualizing models after they are trained. I present Penzai, a neural network\nlibrary designed to simplify model manipulation by representing models as\nsimple data structures, and Treescope, an interactive pretty-printer and array\nvisualizer that can visualize both model inputs/outputs and the models\nthemselves. Penzai models are built using declarative combinators that expose\nthe model forward pass in the structure of the model object itself, and use\nnamed axes to ensure each operation is semantically meaningful. With Penzai's\ntree-editing selector system, users can both insert and replace model\ncomponents, allowing them to intervene on intermediate values or make other\nedits to the model structure. Users can then get immediate feedback by\nvisualizing the modified model with Treescope. I describe the motivation and\nmain features of Penzai and Treescope, and discuss how treating the model as\ndata enables a variety of analyses and interventions to be implemented as\ndata-structure transformations, without requiring model designers to add\nexplicit hooks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-01T00:45:37Z",
    "updated": "2024-08-01T00:45:37Z",
    "doi": null
  },
  "2210.14862": {
    "id": "http://arxiv.org/abs/2210.14862v2",
    "title": "Visual Semantic Parsing: From Images to Abstract Meaning Representation",
    "authors": [
      "Mohamed Ashraf Abdelsalam",
      "Zhan Shi",
      "Federico Fancellu",
      "Kalliopi Basioti",
      "Dhaivat J. Bhatt",
      "Vladimir Pavlovic",
      "Afsaneh Fazly"
    ],
    "abstract": "  The success of scene graphs for visual scene understanding has brought\nattention to the benefits of abstracting a visual input (e.g., image) into a\nstructured representation, where entities (people and objects) are nodes\nconnected by edges specifying their relations. Building these representations,\nhowever, requires expensive manual annotation in the form of images paired with\ntheir scene graphs or frames. These formalisms remain limited in the nature of\nentities and relations they can capture. In this paper, we propose to leverage\na widely-used meaning representation in the field of natural language\nprocessing, the Abstract Meaning Representation (AMR), to address these\nshortcomings. Compared to scene graphs, which largely emphasize spatial\nrelationships, our visual AMR graphs are more linguistically informed, with a\nfocus on higher-level semantic concepts extrapolated from visual input.\nMoreover, they allow us to generate meta-AMR graphs to unify information\ncontained in multiple image descriptions under one representation. Through\nextensive experimentation and analysis, we demonstrate that we can re-purpose\nan existing text-to-AMR parser to parse images into AMRs. Our findings point to\nimportant future research directions for improved scene understanding.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-26T17:06:42Z",
    "updated": "2022-10-27T15:54:50Z",
    "doi": null
  },
  "2211.01095": {
    "id": "http://arxiv.org/abs/2211.01095v2",
    "title": "DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic\n  Models",
    "authors": [
      "Cheng Lu",
      "Yuhao Zhou",
      "Fan Bao",
      "Jianfei Chen",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "abstract": "  Diffusion probabilistic models (DPMs) have achieved impressive success in\nhigh-resolution image synthesis, especially in recent large-scale text-to-image\ngeneration applications. An essential technique for improving the sample\nquality of DPMs is guided sampling, which usually needs a large guidance scale\nto obtain the best sample quality. The commonly-used fast sampler for guided\nsampling is DDIM, a first-order diffusion ODE solver that generally needs 100\nto 250 steps for high-quality samples. Although recent works propose dedicated\nhigh-order solvers and achieve a further speedup for sampling without guidance,\ntheir effectiveness for guided sampling has not been well-tested before. In\nthis work, we demonstrate that previous high-order fast samplers suffer from\ninstability issues, and they even become slower than DDIM when the guidance\nscale grows large. To further speed up guided sampling, we propose\nDPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++\nsolves the diffusion ODE with the data prediction model and adopts thresholding\nmethods to keep the solution matches training data distribution. We further\npropose a multistep variant of DPM-Solver++ to address the instability issue by\nreducing the effective step size. Experiments show that DPM-Solver++ can\ngenerate high-quality samples within only 15 to 20 steps for guided sampling by\npixel-space and latent-space DPMs.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-02T13:14:30Z",
    "updated": "2023-05-06T17:15:37Z",
    "doi": null
  },
  "2403.09413": {
    "id": "http://arxiv.org/abs/2403.09413v2",
    "title": "Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting",
    "authors": [
      "Jaewoo Jung",
      "Jisang Han",
      "Honggyu An",
      "Jiwon Kang",
      "Seonghoon Park",
      "Seungryong Kim"
    ],
    "abstract": "  3D Gaussian splatting (3DGS) has recently demonstrated impressive\ncapabilities in real-time novel view synthesis and 3D reconstruction. However,\n3DGS heavily depends on the accurate initialization derived from\nStructure-from-Motion (SfM) methods. When the quality of the initial point\ncloud deteriorates, such as in the presence of noise or when using randomly\ninitialized point cloud, 3DGS often undergoes large performance drops. To\naddress this limitation, we propose a novel optimization strategy dubbed\nRAIN-GS (Relaing Accurate Initialization Constraint for 3D Gaussian Splatting).\nOur approach is based on an in-depth analysis of the original 3DGS optimization\nscheme and the analysis of the SfM initialization in the frequency domain.\nLeveraging simple modifications based on our analyses, RAIN-GS successfully\ntrains 3D Gaussians from sub-optimal point cloud (e.g., randomly initialized\npoint cloud), effectively relaxing the need for accurate initialization. We\ndemonstrate the efficacy of our strategy through quantitative and qualitative\ncomparisons on multiple datasets, where RAIN-GS trained with random point cloud\nachieves performance on-par with or even better than 3DGS trained with accurate\nSfM point cloud. Our project page and code can be found at\nhttps://ku-cvlab.github.io/RAIN-GS.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-14T14:04:21Z",
    "updated": "2024-05-28T14:14:16Z",
    "doi": null
  },
  "2406.05000": {
    "id": "http://arxiv.org/abs/2406.05000v1",
    "title": "AttnDreamBooth: Towards Text-Aligned Personalized Text-to-Image\n  Generation",
    "authors": [
      "Lianyu Pang",
      "Jian Yin",
      "Baoquan Zhao",
      "Feize Wu",
      "Fu Lee Wang",
      "Qing Li",
      "Xudong Mao"
    ],
    "abstract": "  Recent advances in text-to-image models have enabled high-quality\npersonalized image synthesis of user-provided concepts with flexible textual\ncontrol. In this work, we analyze the limitations of two primary techniques in\ntext-to-image personalization: Textual Inversion and DreamBooth. When\nintegrating the learned concept into new prompts, Textual Inversion tends to\noverfit the concept, while DreamBooth often overlooks it. We attribute these\nissues to the incorrect learning of the embedding alignment for the concept. We\nintroduce AttnDreamBooth, a novel approach that addresses these issues by\nseparately learning the embedding alignment, the attention map, and the subject\nidentity in different training stages. We also introduce a cross-attention map\nregularization term to enhance the learning of the attention map. Our method\ndemonstrates significant improvements in identity preservation and text\nalignment compared to the baseline methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-07T15:12:26Z",
    "updated": "2024-06-07T15:12:26Z",
    "doi": null
  },
  "1805.10180": {
    "id": "http://arxiv.org/abs/1805.10180v3",
    "title": "Pyramid Attention Network for Semantic Segmentation",
    "authors": [
      "Hanchao Li",
      "Pengfei Xiong",
      "Jie An",
      "Lingxue Wang"
    ],
    "abstract": "  A Pyramid Attention Network(PAN) is proposed to exploit the impact of global\ncontextual information in semantic segmentation. Different from most existing\nworks, we combine attention mechanism and spatial pyramid to extract precise\ndense features for pixel labeling instead of complicated dilated convolution\nand artificially designed decoder networks. Specifically, we introduce a\nFeature Pyramid Attention module to perform spatial pyramid attention structure\non high-level output and combining global pooling to learn a better feature\nrepresentation, and a Global Attention Upsample module on each decoder layer to\nprovide global context as a guidance of low-level features to select category\nlocalization details. The proposed approach achieves state-of-the-art\nperformance on PASCAL VOC 2012 and Cityscapes benchmarks with a new record of\nmIoU accuracy 84.0% on PASCAL VOC 2012, while training without COCO dataset.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-05-25T14:40:14Z",
    "updated": "2018-11-25T11:46:47Z",
    "doi": null
  },
  "2204.05562": {
    "id": "http://arxiv.org/abs/2204.05562v5",
    "title": "FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient\n  Package for Federated Graph Learning",
    "authors": [
      "Zhen Wang",
      "Weirui Kuang",
      "Yuexiang Xie",
      "Liuyi Yao",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "abstract": "  The incredible development of federated learning (FL) has benefited various\ntasks in the domains of computer vision and natural language processing, and\nthe existing frameworks such as TFF and FATE has made the deployment easy in\nreal-world applications. However, federated graph learning (FGL), even though\ngraph data are prevalent, has not been well supported due to its unique\ncharacteristics and requirements. The lack of FGL-related framework increases\nthe efforts for accomplishing reproducible research and deploying in real-world\napplications. Motivated by such strong demand, in this paper, we first discuss\nthe challenges in creating an easy-to-use FGL package and accordingly present\nour implemented package FederatedScope-GNN (FS-G), which provides (1) a unified\nview for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo\nand ModelZoo for out-of-the-box FGL capability; (3) an efficient model\nauto-tuning component; and (4) off-the-shelf privacy attack and defense\nabilities. We validate the effectiveness of FS-G by conducting extensive\nexperiments, which simultaneously gains many valuable insights about FGL for\nthe community. Moreover, we employ FS-G to serve the FGL application in\nreal-world E-commerce scenarios, where the attained improvements indicate great\npotential business benefits. We publicly release FS-G, as submodules of\nFederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL's\nresearch and enable broad applications that would otherwise be infeasible due\nto the lack of a dedicated package.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-12T06:48:06Z",
    "updated": "2022-08-01T15:50:24Z",
    "doi": null
  },
  "2312.12729": {
    "id": "http://arxiv.org/abs/2312.12729v1",
    "title": "Segment Anything Model Meets Image Harmonization",
    "authors": [
      "Haoxing Chen",
      "Yaohui Li",
      "Zhangxuan Gu",
      "Zhuoer Xu",
      "Jun Lan",
      "Huaxiong Li"
    ],
    "abstract": "  Image harmonization is a crucial technique in image composition that aims to\nseamlessly match the background by adjusting the foreground of composite\nimages. Current methods adopt either global-level or pixel-level feature\nmatching. Global-level feature matching ignores the proximity prior, treating\nforeground and background as separate entities. On the other hand, pixel-level\nfeature matching loses contextual information. Therefore, it is necessary to\nuse the information from semantic maps that describe different objects to guide\nharmonization. In this paper, we propose Semantic-guided Region-aware Instance\nNormalization (SRIN) that can utilize the semantic segmentation maps output by\na pre-trained Segment Anything Model (SAM) to guide the visual consistency\nlearning of foreground and background features. Abundant experiments\ndemonstrate the superiority of our method for image harmonization over\nstate-of-the-art methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-20T02:57:21Z",
    "updated": "2023-12-20T02:57:21Z",
    "doi": null
  },
  "2402.00631": {
    "id": "http://arxiv.org/abs/2402.00631v2",
    "title": "Beyond Inserting: Learning Identity Embedding for Semantic-Fidelity\n  Personalized Diffusion Generation",
    "authors": [
      "Yang Li",
      "Songlin Yang",
      "Wei Wang",
      "Jing Dong"
    ],
    "abstract": "  Advanced diffusion-based Text-to-Image (T2I) models, such as the Stable\nDiffusion Model, have made significant progress in generating diverse and\nhigh-quality images using text prompts alone. However, when non-famous users\nrequire personalized image generation for their identities (IDs), the T2I\nmodels fail to accurately generate their ID-related images. The main problem is\nthat pre-trained T2I models do not learn the mapping between the new ID prompts\nand their corresponding visual content. The previous methods either failed to\naccurately fit the face region or lost the interactive generative ability with\nother existing concepts in T2I models. In other words, they are unable to\ngenerate T2I-aligned and semantic-fidelity images for the given prompts with\nother concepts such as scenes (``Eiffel Tower''), actions (``holding a\nbasketball''), and facial attributes (``eyes closed''). In this paper, we focus\non inserting accurate and interactive ID embedding into the Stable Diffusion\nModel for semantic-fidelity personalized generation. We address this challenge\nfrom two perspectives: face-wise region fitting and semantic-fidelity token\noptimization. Specifically, we first visualize the attention overfit problem\nand propose a face-wise attention loss to fit the face region instead of\nentangling ID-unrelated information, such as face layout and background. This\nkey trick significantly enhances the ID accuracy and interactive generative\nability with other existing concepts. Then, we optimize one ID representation\nas multiple per-stage tokens where each token contains two disentangled\nfeatures. This expansion of the textual conditioning space improves\nsemantic-fidelity control. Extensive experiments validate that our results\nexhibit superior ID accuracy, text-based manipulation ability, and\ngeneralization compared to previous methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-31T11:52:33Z",
    "updated": "2024-03-22T14:00:37Z",
    "doi": null
  },
  "2205.02151": {
    "id": "http://arxiv.org/abs/2205.02151v1",
    "title": "Dual Cross-Attention Learning for Fine-Grained Visual Categorization and\n  Object Re-Identification",
    "authors": [
      "Haowei Zhu",
      "Wenjing Ke",
      "Dong Li",
      "Ji Liu",
      "Lu Tian",
      "Yi Shan"
    ],
    "abstract": "  Recently, self-attention mechanisms have shown impressive performance in\nvarious NLP and CV tasks, which can help capture sequential characteristics and\nderive global information. In this work, we explore how to extend\nself-attention modules to better learn subtle feature embeddings for\nrecognizing fine-grained objects, e.g., different bird species or person\nidentities. To this end, we propose a dual cross-attention learning (DCAL)\nalgorithm to coordinate with self-attention learning. First, we propose\nglobal-local cross-attention (GLCA) to enhance the interactions between global\nimages and local high-response regions, which can help reinforce the\nspatial-wise discriminative clues for recognition. Second, we propose pair-wise\ncross-attention (PWCA) to establish the interactions between image pairs. PWCA\ncan regularize the attention learning of an image by treating another image as\ndistractor and will be removed during inference. We observe that DCAL can\nreduce misleading attentions and diffuse the attention response to discover\nmore complementary parts for recognition. We conduct extensive evaluations on\nfine-grained visual categorization and object re-identification. Experiments\ndemonstrate that DCAL performs on par with state-of-the-art methods and\nconsistently improves multiple self-attention baselines, e.g., surpassing\nDeiT-Tiny and ViT-Base by 2.8% and 2.4% mAP on MSMT17, respectively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-04T16:14:26Z",
    "updated": "2022-05-04T16:14:26Z",
    "doi": null
  },
  "2407.04604": {
    "id": "http://arxiv.org/abs/2407.04604v2",
    "title": "PartCraft: Crafting Creative Objects by Parts",
    "authors": [
      "Kam Woh Ng",
      "Xiatian Zhu",
      "Yi-Zhe Song",
      "Tao Xiang"
    ],
    "abstract": "  This paper propels creative control in generative visual AI by allowing users\nto \"select\". Departing from traditional text or sketch-based methods, we for\nthe first time allow users to choose visual concepts by parts for their\ncreative endeavors. The outcome is fine-grained generation that precisely\ncaptures selected visual concepts, ensuring a holistically faithful and\nplausible result. To achieve this, we first parse objects into parts through\nunsupervised feature clustering. Then, we encode parts into text tokens and\nintroduce an entropy-based normalized attention loss that operates on them.\nThis loss design enables our model to learn generic prior topology knowledge\nabout object's part composition, and further generalize to novel part\ncompositions to ensure the generation looks holistically faithful. Lastly, we\nemploy a bottleneck encoder to project the part tokens. This not only enhances\nfidelity but also accelerates learning, by leveraging shared knowledge and\nfacilitating information exchange among instances. Visual results in the paper\nand supplementary material showcase the compelling power of PartCraft in\ncrafting highly customized, innovative creations, exemplified by the \"charming\"\nand creative birds. Code is released at https://github.com/kamwoh/partcraft.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-05T15:53:04Z",
    "updated": "2024-07-08T13:38:49Z",
    "doi": null
  },
  "2408.14506": {
    "id": "http://arxiv.org/abs/2408.14506v1",
    "title": "Distilling Long-tailed Datasets",
    "authors": [
      "Zhenghao Zhao",
      "Haoxuan Wang",
      "Yuzhang Shang",
      "Kai Wang",
      "Yan Yan"
    ],
    "abstract": "  Dataset distillation (DD) aims to distill a small, information-rich dataset\nfrom a larger one for efficient neural network training. However, existing DD\nmethods struggle with long-tailed datasets, which are prevalent in real-world\nscenarios. By investigating the reasons behind this unexpected result, we\nidentified two main causes: 1) Expert networks trained on imbalanced data\ndevelop biased gradients, leading to the synthesis of similarly imbalanced\ndistilled datasets. Parameter matching, a common technique in DD, involves\naligning the learning parameters of the distilled dataset with that of the\noriginal dataset. However, in the context of long-tailed datasets, matching\nbiased experts leads to inheriting the imbalance present in the original data,\ncausing the distilled dataset to inadequately represent tail classes. 2) The\nexperts trained on such datasets perform suboptimally on tail classes,\nresulting in misguided distillation supervision and poor-quality soft-label\ninitialization. To address these issues, we propose a novel long-tailed dataset\ndistillation method, Long-tailed Aware Dataset distillation (LAD).\nSpecifically, we propose Weight Mismatch Avoidance to avoid directly matching\nthe biased expert trajectories. It reduces the distance between the student and\nthe biased expert trajectories and prevents the tail class bias from being\ndistilled to the synthetic dataset. Moreover, we propose Adaptive Decoupled\nMatching, which jointly matches the decoupled backbone and classifier to\nimprove the tail class performance and initialize reliable soft labels. This\nwork pioneers the field of long-tailed dataset distillation (LTDD), marking the\nfirst effective effort to distill long-tailed datasets.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-24T15:36:36Z",
    "updated": "2024-08-24T15:36:36Z",
    "doi": null
  },
  "2305.06077": {
    "id": "http://arxiv.org/abs/2305.06077v2",
    "title": "Relightify: Relightable 3D Faces from a Single Image via Diffusion\n  Models",
    "authors": [
      "Foivos Paraperas Papantoniou",
      "Alexandros Lattas",
      "Stylianos Moschoglou",
      "Stefanos Zafeiriou"
    ],
    "abstract": "  Following the remarkable success of diffusion models on image generation,\nrecent works have also demonstrated their impressive ability to address a\nnumber of inverse problems in an unsupervised way, by properly constraining the\nsampling process based on a conditioning input. Motivated by this, in this\npaper, we present the first approach to use diffusion models as a prior for\nhighly accurate 3D facial BRDF reconstruction from a single image. We start by\nleveraging a high-quality UV dataset of facial reflectance (diffuse and\nspecular albedo and normals), which we render under varying illumination\nsettings to simulate natural RGB textures and, then, train an unconditional\ndiffusion model on concatenated pairs of rendered textures and reflectance\ncomponents. At test time, we fit a 3D morphable model to the given image and\nunwrap the face in a partial UV texture. By sampling from the diffusion model,\nwhile retaining the observed texture part intact, the model inpaints not only\nthe self-occluded areas but also the unknown reflectance components, in a\nsingle sequence of denoising steps. In contrast to existing methods, we\ndirectly acquire the observed texture from the input image, thus, resulting in\nmore faithful and consistent reflectance estimation. Through a series of\nqualitative and quantitative comparisons, we demonstrate superior performance\nin both texture completion as well as reflectance reconstruction tasks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-10T11:57:49Z",
    "updated": "2023-08-22T01:06:42Z",
    "doi": null
  },
  "1804.07007": {
    "id": "http://arxiv.org/abs/1804.07007v3",
    "title": "QuaSE: Accurate Text Style Transfer under Quantifiable Guidance",
    "authors": [
      "Yi Liao",
      "Lidong Bing",
      "Piji Li",
      "Shuming Shi",
      "Wai Lam",
      "Tong Zhang"
    ],
    "abstract": "  We propose the task of Quantifiable Sequence Editing (QuaSE): editing an\ninput sequence to generate an output sequence that satisfies a given numerical\noutcome value measuring a certain property of the sequence, with the\nrequirement of keeping the main content of the input sequence. For example, an\ninput sequence could be a word sequence, such as review sentence and\nadvertisement text. For a review sentence, the outcome could be the review\nrating; for an advertisement, the outcome could be the click-through rate. The\nmajor challenge in performing QuaSE is how to perceive the outcome-related\nwordings, and only edit them to change the outcome. In this paper, the proposed\nframework contains two latent factors, namely, outcome factor and content\nfactor, disentangled from the input sentence to allow convenient editing to\nchange the outcome and keep the content. Our framework explores the\npseudo-parallel sentences by modeling their content similarity and outcome\ndifferences to enable a better disentanglement of the latent factors, which\nallows generating an output to better satisfy the desired outcome and keep the\ncontent. The dual reconstruction structure further enhances the capability of\ngenerating expected output by exploiting the couplings of latent factors of\npseudo-parallel sentences. For evaluation, we prepared a dataset of Yelp review\nsentences with the ratings as outcome. Extensive experimental results are\nreported and discussed to elaborate the peculiarities of our framework.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-19T06:13:48Z",
    "updated": "2019-01-30T05:14:32Z",
    "doi": null
  },
  "2406.01014": {
    "id": "http://arxiv.org/abs/2406.01014v1",
    "title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective\n  Navigation via Multi-Agent Collaboration",
    "authors": [
      "Junyang Wang",
      "Haiyang Xu",
      "Haitao Jia",
      "Xi Zhang",
      "Ming Yan",
      "Weizhou Shen",
      "Ji Zhang",
      "Fei Huang",
      "Jitao Sang"
    ],
    "abstract": "  Mobile device operation tasks are increasingly becoming a popular multi-modal\nAI application scenario. Current Multi-modal Large Language Models (MLLMs),\nconstrained by their training data, lack the capability to function effectively\nas operation assistants. Instead, MLLM-based agents, which enhance capabilities\nthrough tool invocation, are gradually being applied to this scenario. However,\nthe two major navigation challenges in mobile device operation tasks, task\nprogress navigation and focus content navigation, are significantly complicated\nunder the single-agent architecture of existing work. This is due to the overly\nlong token sequences and the interleaved text-image data format, which limit\nperformance. To address these navigation challenges effectively, we propose\nMobile-Agent-v2, a multi-agent architecture for mobile device operation\nassistance. The architecture comprises three agents: planning agent, decision\nagent, and reflection agent. The planning agent generates task progress, making\nthe navigation of history operations more efficient. To retain focus content,\nwe design a memory unit that updates with task progress. Additionally, to\ncorrect erroneous operations, the reflection agent observes the outcomes of\neach operation and handles any mistakes accordingly. Experimental results\nindicate that Mobile-Agent-v2 achieves over a 30% improvement in task\ncompletion compared to the single-agent architecture of Mobile-Agent. The code\nis open-sourced at https://github.com/X-PLUG/MobileAgent.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-03T05:50:00Z",
    "updated": "2024-06-03T05:50:00Z",
    "doi": null
  },
  "1705.08741": {
    "id": "http://arxiv.org/abs/1705.08741v2",
    "title": "Train longer, generalize better: closing the generalization gap in large\n  batch training of neural networks",
    "authors": [
      "Elad Hoffer",
      "Itay Hubara",
      "Daniel Soudry"
    ],
    "abstract": "  Background: Deep learning models are typically trained using stochastic\ngradient descent or one of its variants. These methods update the weights using\ntheir gradient, estimated from a small fraction of the training data. It has\nbeen observed that when using large batch sizes there is a persistent\ndegradation in generalization performance - known as the \"generalization gap\"\nphenomena. Identifying the origin of this gap and closing it had remained an\nopen problem.\n  Contributions: We examine the initial high learning rate training phase. We\nfind that the weight distance from its initialization grows logarithmically\nwith the number of weight updates. We therefore propose a \"random walk on\nrandom landscape\" statistical model which is known to exhibit similar\n\"ultra-slow\" diffusion behavior. Following this hypothesis we conducted\nexperiments to show empirically that the \"generalization gap\" stems from the\nrelatively small number of updates rather than the batch size, and can be\ncompletely eliminated by adapting the training regime used. We further\ninvestigate different techniques to train models in the large-batch regime and\npresent a novel algorithm named \"Ghost Batch Normalization\" which enables\nsignificant decrease in the generalization gap without increasing the number of\nupdates. To validate our findings we conduct several additional experiments on\nMNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices\nand beliefs concerning training of deep models and suggest they may not be\noptimal to achieve good generalization.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-05-24T13:17:27Z",
    "updated": "2018-01-01T08:49:43Z",
    "doi": null
  },
  "1905.05460": {
    "id": "http://arxiv.org/abs/1905.05460v2",
    "title": "Cognitive Graph for Multi-Hop Reading Comprehension at Scale",
    "authors": [
      "Ming Ding",
      "Chang Zhou",
      "Qibin Chen",
      "Hongxia Yang",
      "Jie Tang"
    ],
    "abstract": "  We propose a new CogQA framework for multi-hop question answering in\nweb-scale documents. Inspired by the dual process theory in cognitive science,\nthe framework gradually builds a \\textit{cognitive graph} in an iterative\nprocess by coordinating an implicit extraction module (System 1) and an\nexplicit reasoning module (System 2). While giving accurate answers, our\nframework further provides explainable reasoning paths. Specifically, our\nimplementation based on BERT and graph neural network efficiently handles\nmillions of documents for multi-hop reasoning questions in the HotpotQA\nfullwiki dataset, achieving a winning joint $F_1$ score of 34.9 on the\nleaderboard, compared to 23.6 of the best competitor.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-05-14T08:55:04Z",
    "updated": "2019-06-04T09:44:29Z",
    "doi": null
  },
  "2404.14777": {
    "id": "http://arxiv.org/abs/2404.14777v2",
    "title": "ClinicalAgent: Clinical Trial Multi-Agent System with Large Language\n  Model-based Reasoning",
    "authors": [
      "Ling Yue",
      "Sixue Xing",
      "Jintai Chen",
      "Tianfan Fu"
    ],
    "abstract": "  Large Language Models (LLMs) and multi-agent systems have shown impressive\ncapabilities in natural language tasks but face challenges in clinical trial\napplications, primarily due to limited access to external knowledge.\nRecognizing the potential of advanced clinical trial tools that aggregate and\npredict based on the latest medical data, we propose an integrated solution to\nenhance their accessibility and utility. We introduce Clinical Agent System\n(ClinicalAgent), a clinical multi-agent system designed for clinical trial\ntasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct\nreasoning technology. This integration not only boosts LLM performance in\nclinical contexts but also introduces novel functionalities. The proposed\nmethod achieves competitive predictive performance in clinical trial outcome\nprediction (0.7908 PR-AUC), obtaining a 0.3326 improvement over the standard\nprompt Method. Publicly available code can be found at\nhttps://anonymous.4open.science/r/ClinicalAgent-6671.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-23T06:30:53Z",
    "updated": "2024-07-20T07:52:16Z",
    "doi": null
  },
  "2404.07990": {
    "id": "http://arxiv.org/abs/2404.07990v2",
    "title": "OpenBias: Open-set Bias Detection in Text-to-Image Generative Models",
    "authors": [
      "Moreno D'Inc\u00e0",
      "Elia Peruzzo",
      "Massimiliano Mancini",
      "Dejia Xu",
      "Vidit Goel",
      "Xingqian Xu",
      "Zhangyang Wang",
      "Humphrey Shi",
      "Nicu Sebe"
    ],
    "abstract": "  Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-11T17:59:56Z",
    "updated": "2024-08-05T12:55:47Z",
    "doi": null
  },
  "2301.07315": {
    "id": "http://arxiv.org/abs/2301.07315v1",
    "title": "Face Recognition in the age of CLIP & Billion image datasets",
    "authors": [
      "Aaditya Bhat",
      "Shrey Jain"
    ],
    "abstract": "  CLIP (Contrastive Language-Image Pre-training) models developed by OpenAI\nhave achieved outstanding results on various image recognition and retrieval\ntasks, displaying strong zero-shot performance. This means that they are able\nto perform effectively on tasks for which they have not been explicitly\ntrained. Inspired by the success of OpenAI CLIP, a new publicly available\ndataset called LAION-5B was collected which resulted in the development of open\nViT-H/14, ViT-G/14 models that outperform the OpenAI L/14 model. The LAION-5B\ndataset also released an approximate nearest neighbor index, with a web\ninterface for search & subset creation.\n  In this paper, we evaluate the performance of various CLIP models as\nzero-shot face recognizers. Our findings show that CLIP models perform well on\nface recognition tasks, but increasing the size of the CLIP model does not\nnecessarily lead to improved accuracy. Additionally, we investigate the\nrobustness of CLIP models against data poisoning attacks by testing their\nperformance on poisoned data. Through this analysis, we aim to understand the\npotential consequences and misuse of search engines built using CLIP models,\nwhich could potentially function as unintentional face recognition engines.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-18T05:34:57Z",
    "updated": "2023-01-18T05:34:57Z",
    "doi": null
  },
  "1804.04235": {
    "id": "http://arxiv.org/abs/1804.04235v1",
    "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
    "authors": [
      "Noam Shazeer",
      "Mitchell Stern"
    ],
    "abstract": "  In several recently proposed stochastic optimization methods (e.g. RMSProp,\nAdam, Adadelta), parameter updates are scaled by the inverse square roots of\nexponential moving averages of squared past gradients. Maintaining these\nper-parameter second-moment estimators requires memory equal to the number of\nparameters. For the case of neural network weight matrices, we propose\nmaintaining only the per-row and per-column sums of these moving averages, and\nestimating the per-parameter second moments based on these sums. We demonstrate\nempirically that this method produces similar results to the baseline.\nSecondly, we show that adaptive methods can produce larger-than-desired updates\nwhen the decay rate of the second moment accumulator is too slow. We propose\nupdate clipping and a gradually increasing decay rate scheme as remedies.\nCombining these methods and dropping momentum, we achieve comparable results to\nthe published Adam regime in training the Transformer model on the WMT 2014\nEnglish-German machine translation task, while using very little auxiliary\nstorage in the optimizer. Finally, we propose scaling the parameter updates\nbased on the scale of the parameters themselves.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-11T21:42:32Z",
    "updated": "2018-04-11T21:42:32Z",
    "doi": null
  },
  "2212.07597": {
    "id": "http://arxiv.org/abs/2212.07597v1",
    "title": "Triangulating Python Performance Issues with Scalene",
    "authors": [
      "Emery D. Berger",
      "Sam Stern",
      "Juan Altmayer Pizzorno"
    ],
    "abstract": "  This paper proposes Scalene, a profiler specialized for Python. Scalene\ncombines a suite of innovations to precisely and simultaneously profile CPU,\nmemory, and GPU usage, all with low overhead. Scalene's CPU and memory\nprofilers help Python programmers direct their optimization efforts by\ndistinguishing between inefficient Python and efficient native execution time\nand memory usage. Scalene's memory profiler employs a novel sampling algorithm\nthat lets it operate with low overhead yet high precision. It also incorporates\na novel algorithm that automatically pinpoints memory leaks, whether within\nPython or across the Python-native boundary. Scalene tracks a new metric called\ncopy volume, which highlights costly copying operations that can occur when\nPython silently converts between C and Python data representations, or between\nCPU and GPU. Since its introduction, Scalene has been widely adopted, with over\n500,000 downloads to date. We present experience reports from developers who\nused Scalene to achieve significant performance improvements and memory\nsavings.\n",
    "categories": [
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PF",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-15T02:56:25Z",
    "updated": "2022-12-15T02:56:25Z",
    "doi": null
  },
  "2403.10242": {
    "id": "http://arxiv.org/abs/2403.10242v1",
    "title": "FDGaussian: Fast Gaussian Splatting from Single Image via\n  Geometric-aware Diffusion Model",
    "authors": [
      "Qijun Feng",
      "Zhen Xing",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "  Reconstructing detailed 3D objects from single-view images remains a\nchallenging task due to the limited information available. In this paper, we\nintroduce FDGaussian, a novel two-stage framework for single-image 3D\nreconstruction. Recent methods typically utilize pre-trained 2D diffusion\nmodels to generate plausible novel views from the input image, yet they\nencounter issues with either multi-view inconsistency or lack of geometric\nfidelity. To overcome these challenges, we propose an orthogonal plane\ndecomposition mechanism to extract 3D geometric features from the 2D input,\nenabling the generation of consistent multi-view images. Moreover, we further\naccelerate the state-of-the-art Gaussian Splatting incorporating epipolar\nattention to fuse images from different viewpoints. We demonstrate that\nFDGaussian generates images with high consistency across different views and\nreconstructs high-quality 3D objects, both qualitatively and quantitatively.\nMore examples can be found at our website https://qjfeng.net/FDGaussian/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-15T12:24:36Z",
    "updated": "2024-03-15T12:24:36Z",
    "doi": null
  },
  "2002.06525": {
    "id": "http://arxiv.org/abs/2002.06525v1",
    "title": "Learning to Generate Multiple Style Transfer Outputs for an Input\n  Sentence",
    "authors": [
      "Kevin Lin",
      "Ming-Yu Liu",
      "Ming-Ting Sun",
      "Jan Kautz"
    ],
    "abstract": "  Text style transfer refers to the task of rephrasing a given text in a\ndifferent style. While various methods have been proposed to advance the state\nof the art, they often assume the transfer output follows a delta distribution,\nand thus their models cannot generate different style transfer results for a\ngiven input text. To address the limitation, we propose a one-to-many text\nstyle transfer framework. In contrast to prior works that learn a one-to-one\nmapping that converts an input sentence to one output sentence, our approach\nlearns a one-to-many mapping that can convert an input sentence to multiple\ndifferent output sentences, while preserving the input content. This is\nachieved by applying adversarial training with a latent decomposition scheme.\nSpecifically, we decompose the latent representation of the input sentence to a\nstyle code that captures the language style variation and a content code that\nencodes the language style-independent content. We then combine the content\ncode with the style code for generating a style transfer output. By combining\nthe same content code with a different style code, we generate a different\nstyle transfer output. Extensive experimental results with comparisons to\nseveral text style transfer approaches on multiple public datasets using a\ndiverse set of performance metrics validate effectiveness of the proposed\napproach.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-02-16T07:10:45Z",
    "updated": "2020-02-16T07:10:45Z",
    "doi": null
  },
  "2210.04287": {
    "id": "http://arxiv.org/abs/2210.04287v1",
    "title": "Learning to Decompose Visual Features with Latent Textual Prompts",
    "authors": [
      "Feng Wang",
      "Manling Li",
      "Xudong Lin",
      "Hairong Lv",
      "Alexander G. Schwing",
      "Heng Ji"
    ],
    "abstract": "  Recent advances in pre-training vision-language models like CLIP have shown\ngreat potential in learning transferable visual representations. Nonetheless,\nfor downstream inference, CLIP-like models suffer from either 1) degraded\naccuracy and robustness in the case of inaccurate text descriptions during\nretrieval-based inference (the challenge for zero-shot protocol); or 2)\nbreaking the well-established vision-language alignment (the challenge for\nlinear probing). To address them, we propose Decomposed Feature Prompting\n(DeFo). DeFo leverages a flexible number of learnable embeddings as textual\ninput while maintaining the vision-language dual-model architecture, which\nenables the model to learn decomposed visual features with the help of\nfeature-level textual prompts. We further use an additional linear layer to\nperform classification, allowing a scalable size of language inputs. Our\nempirical study shows DeFo's significance in improving the vision-language\nmodels. For example, DeFo obtains 73.2% test accuracy on ImageNet with a\nResNet-50 backbone without tuning any pretrained weights of both the vision and\nlanguage encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and\noutperforming state-of-the-art vision-language prompt tuning method by 7.6%.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-09T15:40:13Z",
    "updated": "2022-10-09T15:40:13Z",
    "doi": null
  },
  "2303.16491": {
    "id": "http://arxiv.org/abs/2303.16491v2",
    "title": "Implicit Diffusion Models for Continuous Super-Resolution",
    "authors": [
      "Sicheng Gao",
      "Xuhui Liu",
      "Bohan Zeng",
      "Sheng Xu",
      "Yanjing Li",
      "Xiaoyan Luo",
      "Jianzhuang Liu",
      "Xiantong Zhen",
      "Baochang Zhang"
    ],
    "abstract": "  Image super-resolution (SR) has attracted increasing attention due to its\nwide applications. However, current SR methods generally suffer from\nover-smoothing and artifacts, and most work only with fixed magnifications.\nThis paper introduces an Implicit Diffusion Model (IDM) for high-fidelity\ncontinuous image super-resolution. IDM integrates an implicit neural\nrepresentation and a denoising diffusion model in a unified end-to-end\nframework, where the implicit neural representation is adopted in the decoding\nprocess to learn continuous-resolution representation. Furthermore, we design a\nscale-controllable conditioning mechanism that consists of a low-resolution\n(LR) conditioning network and a scaling factor. The scaling factor regulates\nthe resolution and accordingly modulates the proportion of the LR information\nand generated features in the final output, which enables the model to\naccommodate the continuous-resolution requirement. Extensive experiments\nvalidate the effectiveness of our IDM and demonstrate its superior performance\nover prior arts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-29T07:02:20Z",
    "updated": "2023-09-03T10:19:55Z",
    "doi": null
  },
  "1807.10267": {
    "id": "http://arxiv.org/abs/1807.10267v3",
    "title": "Generating 3D faces using Convolutional Mesh Autoencoders",
    "authors": [
      "Anurag Ranjan",
      "Timo Bolkart",
      "Soubhik Sanyal",
      "Michael J. Black"
    ],
    "abstract": "  Learned 3D representations of human faces are useful for computer vision\nproblems such as 3D face tracking and reconstruction from images, as well as\ngraphics applications such as character generation and animation. Traditional\nmodels learn a latent representation of a face using linear subspaces or\nhigher-order tensor generalizations. Due to this linearity, they can not\ncapture extreme deformations and non-linear expressions. To address this, we\nintroduce a versatile model that learns a non-linear representation of a face\nusing spectral convolutions on a mesh surface. We introduce mesh sampling\noperations that enable a hierarchical mesh representation that captures\nnon-linear variations in shape and expression at multiple scales within the\nmodel. In a variational setting, our model samples diverse realistic 3D faces\nfrom a multivariate Gaussian distribution. Our training data consists of 20,466\nmeshes of extreme expressions captured over 12 different subjects. Despite\nlimited training data, our trained model outperforms state-of-the-art face\nmodels with 50% lower reconstruction error, while using 75% fewer parameters.\nWe also show that, replacing the expression space of an existing\nstate-of-the-art face model with our autoencoder, achieves a lower\nreconstruction error. Our data, model and code are available at\nhttp://github.com/anuragranj/coma\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-07-26T17:53:50Z",
    "updated": "2018-07-31T20:13:00Z",
    "doi": null
  },
  "2402.10200": {
    "id": "http://arxiv.org/abs/2402.10200v2",
    "title": "Chain-of-Thought Reasoning Without Prompting",
    "authors": [
      "Xuezhi Wang",
      "Denny Zhou"
    ],
    "abstract": "  In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding effectively\nelicits reasoning capabilities from language models, which were previously\nobscured by standard greedy decoding.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-15T18:55:41Z",
    "updated": "2024-05-23T20:53:59Z",
    "doi": null
  },
  "2207.02196": {
    "id": "http://arxiv.org/abs/2207.02196v4",
    "title": "Accelerating Score-based Generative Models with Preconditioned Diffusion\n  Sampling",
    "authors": [
      "Hengyuan Ma",
      "Li Zhang",
      "Xiatian Zhu",
      "Jianfeng Feng"
    ],
    "abstract": "  Score-based generative models (SGMs) have recently emerged as a promising\nclass of generative models. However, a fundamental limitation is that their\ninference is very slow due to a need for many (e.g., 2000) iterations of\nsequential computations. An intuitive acceleration method is to reduce the\nsampling iterations which however causes severe performance degradation. We\ninvestigate this problem by viewing the diffusion sampling process as a\nMetropolis adjusted Langevin algorithm, which helps reveal the underlying cause\nto be ill-conditioned curvature. Under this insight, we propose a\nmodel-agnostic preconditioned diffusion sampling (PDS) method that leverages\nmatrix preconditioning to alleviate the aforementioned problem. Crucially, PDS\nis proven theoretically to converge to the original target distribution of a\nSGM, no need for retraining. Extensive experiments on three image datasets with\na variety of resolutions and diversity validate that PDS consistently\naccelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In\nparticular, PDS can accelerate by up to 29x on more challenging high resolution\n(1024x1024) image generation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-05T17:55:42Z",
    "updated": "2022-12-05T13:22:07Z",
    "doi": null
  },
  "1810.08189": {
    "id": "http://arxiv.org/abs/1810.08189v2",
    "title": "Convolutional Collaborative Filter Network for Video Based\n  Recommendation Systems",
    "authors": [
      "Cheng-Kang Hsieh",
      "Miguel Campo",
      "Abhinav Taliyan",
      "Matt Nickens",
      "Mitkumar Pandya",
      "JJ Espinoza"
    ],
    "abstract": "  This analysis explores the temporal sequencing of objects in a movie trailer.\nTemporal sequencing of objects in a movie trailer (e.g., a long shot of an\nobject vs intermittent short shots) can convey information about the type of\nmovie, plot of the movie, role of the main characters, and the filmmakers\ncinematographic choices. When combined with historical customer data,\nsequencing analysis can be used to improve predictions of customer behavior.\nE.g., a customer buys tickets to a new movie and maybe the customer has seen\nmovies in the past that contained similar sequences. To explore object\nsequencing in movie trailers, we propose a video convolutional network to\ncapture actions and scenes that are predictive of customers' preferences. The\nmodel learns the specific nature of sequences for different types of objects\n(e.g., cars vs faces), and the role of sequences in predicting customer future\nbehavior. We show how such a temporal-aware model outperforms simple feature\npooling methods proposed in our previous works and, importantly, demonstrate\nthe additional model explain-ability allowed by such a model.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-10-18T17:57:58Z",
    "updated": "2018-10-22T20:43:16Z",
    "doi": null
  },
  "2407.05352": {
    "id": "http://arxiv.org/abs/2407.05352v1",
    "title": "Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model",
    "authors": [
      "Danni Yang",
      "Ruohan Dong",
      "Jiayi Ji",
      "Yiwei Ma",
      "Haowei Wang",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ],
    "abstract": "  Recently, diffusion models have increasingly demonstrated their capabilities\nin vision understanding. By leveraging prompt-based learning to construct\nsentences, these models have shown proficiency in classification and visual\ngrounding tasks. However, existing approaches primarily showcase their ability\nto perform sentence-level localization, leaving the potential for leveraging\ncontextual information for phrase-level understanding largely unexplored. In\nthis paper, we utilize Panoptic Narrative Grounding (PNG) as a proxy task to\ninvestigate this capability further. PNG aims to segment object instances\nmentioned by multiple noun phrases within a given narrative text. Specifically,\nwe introduce the DiffPNG framework, a straightforward yet effective approach\nthat fully capitalizes on the diffusion's architecture for segmentation by\ndecomposing the process into a sequence of localization, segmentation, and\nrefinement steps. The framework initially identifies anchor points using\ncross-attention mechanisms and subsequently performs segmentation with\nself-attention to achieve zero-shot PNG. Moreover, we introduce a refinement\nmodule based on SAM to enhance the quality of the segmentation masks. Our\nextensive experiments on the PNG dataset demonstrate that DiffPNG achieves\nstrong performance in the zero-shot PNG task setting, conclusively proving the\ndiffusion model's capability for context-aware, phrase-level understanding.\nSource code is available at \\url{https://github.com/nini0919/DiffPNG}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-07T13:06:34Z",
    "updated": "2024-07-07T13:06:34Z",
    "doi": null
  },
  "2211.12112": {
    "id": "http://arxiv.org/abs/2211.12112v1",
    "title": "Human Evaluation of Text-to-Image Models on a Multi-Task Benchmark",
    "authors": [
      "Vitali Petsiuk",
      "Alexander E. Siemenn",
      "Saisamrit Surbehera",
      "Zad Chin",
      "Keith Tyser",
      "Gregory Hunter",
      "Arvind Raghavan",
      "Yann Hicke",
      "Bryan A. Plummer",
      "Ori Kerret",
      "Tonio Buonassisi",
      "Kate Saenko",
      "Armando Solar-Lezama",
      "Iddo Drori"
    ],
    "abstract": "  We provide a new multi-task benchmark for evaluating text-to-image models. We\nperform a human evaluation comparing the most common open-source (Stable\nDiffusion) and commercial (DALL-E 2) models. Twenty computer science AI\ngraduate students evaluated the two models, on three tasks, at three difficulty\nlevels, across ten prompts each, providing 3,600 ratings. Text-to-image\ngeneration has seen rapid progress to the point that many recent models have\ndemonstrated their ability to create realistic high-resolution images for\nvarious prompts. However, current text-to-image methods and the broader body of\nresearch in vision-language understanding still struggle with intricate text\nprompts that contain many objects with multiple attributes and relationships.\nWe introduce a new text-to-image benchmark that contains a suite of thirty-two\ntasks over multiple applications that capture a model's ability to handle\ndifferent features of a text prompt. For example, asking a model to generate a\nvarying number of the same object to measure its ability to count or providing\na text prompt with several objects that each have a different attribute to\nidentify its ability to match objects and attributes correctly. Rather than\nsubjectively evaluating text-to-image results on a set of prompts, our new\nmulti-task benchmark consists of challenge tasks at three difficulty levels\n(easy, medium, and hard) and human ratings for each generated image.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-22T09:27:53Z",
    "updated": "2022-11-22T09:27:53Z",
    "doi": null
  },
  "1712.01026": {
    "id": "http://arxiv.org/abs/1712.01026v4",
    "title": "Wasserstein Divergence for GANs",
    "authors": [
      "Jiqing Wu",
      "Zhiwu Huang",
      "Janine Thoma",
      "Dinesh Acharya",
      "Luc Van Gool"
    ],
    "abstract": "  In many domains of computer vision, generative adversarial networks (GANs)\nhave achieved great success, among which the family of Wasserstein GANs (WGANs)\nis considered to be state-of-the-art due to the theoretical contributions and\ncompetitive qualitative performance. However, it is very challenging to\napproximate the $k$-Lipschitz constraint required by the Wasserstein-1\nmetric~(W-met). In this paper, we propose a novel Wasserstein\ndivergence~(W-div), which is a relaxed version of W-met and does not require\nthe $k$-Lipschitz constraint. As a concrete application, we introduce a\nWasserstein divergence objective for GANs~(WGAN-div), which can faithfully\napproximate W-div through optimization. Under various settings, including\nprogressive growing training, we demonstrate the stability of the proposed\nWGAN-div owing to its theoretical and practical advantages over WGANs. Also, we\nstudy the quantitative and visual performance of WGAN-div on standard image\nsynthesis benchmarks of computer vision, showing the superior performance of\nWGAN-div compared to the state-of-the-art methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-12-04T11:57:28Z",
    "updated": "2018-09-05T12:41:21Z",
    "doi": null
  },
  "2212.06593": {
    "id": "http://arxiv.org/abs/2212.06593v1",
    "title": "FastMIM: Expediting Masked Image Modeling Pre-training for Vision",
    "authors": [
      "Jianyuan Guo",
      "Kai Han",
      "Han Wu",
      "Yehui Tang",
      "Yunhe Wang",
      "Chang Xu"
    ],
    "abstract": "  The combination of transformers and masked image modeling (MIM) pre-training\nframework has shown great potential in various vision tasks. However, the\npre-training computational budget is too heavy and withholds the MIM from\nbecoming a practical training paradigm. This paper presents FastMIM, a simple\nand generic framework for expediting masked image modeling with the following\ntwo steps: (i) pre-training vision backbones with low-resolution input images;\nand (ii) reconstructing Histograms of Oriented Gradients (HOG) feature instead\nof original RGB values of the input images. In addition, we propose FastMIM-P\nto progressively enlarge the input resolution during pre-training stage to\nfurther enhance the transfer results of models with high capacity. We point out\nthat: (i) a wide range of input resolutions in pre-training phase can lead to\nsimilar performances in fine-tuning phase and downstream tasks such as\ndetection and segmentation; (ii) the shallow layers of encoder are more\nimportant during pre-training and discarding last several layers can speed up\nthe training stage with no harm to fine-tuning performance; (iii) the decoder\nshould match the size of selected network; and (iv) HOG is more stable than RGB\nvalues when resolution transfers;. Equipped with FastMIM, all kinds of vision\nbackbones can be pre-trained in an efficient way. For example, we can achieve\n83.8%/84.1% top-1 accuracy on ImageNet-1K with ViT-B/Swin-B as backbones.\nCompared to previous relevant approaches, we can achieve comparable or better\ntop-1 accuracy while accelerate the training procedure by $\\sim$5$\\times$. Code\ncan be found in https://github.com/ggjy/FastMIM.pytorch.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-13T14:09:32Z",
    "updated": "2022-12-13T14:09:32Z",
    "doi": null
  },
  "1806.02169": {
    "id": "http://arxiv.org/abs/1806.02169v2",
    "title": "StarGAN-VC: Non-parallel many-to-many voice conversion with star\n  generative adversarial networks",
    "authors": [
      "Hirokazu Kameoka",
      "Takuhiro Kaneko",
      "Kou Tanaka",
      "Nobukatsu Hojo"
    ],
    "abstract": "  This paper proposes a method that allows non-parallel many-to-many voice\nconversion (VC) by using a variant of a generative adversarial network (GAN)\ncalled StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it\n(1) requires no parallel utterances, transcriptions, or time alignment\nprocedures for speech generator training, (2) simultaneously learns\nmany-to-many mappings across different attribute domains using a single\ngenerator network, (3) is able to generate converted speech signals quickly\nenough to allow real-time implementations and (4) requires only several minutes\nof training examples to generate reasonably realistic-sounding speech.\nSubjective evaluation experiments on a non-parallel many-to-many speaker\nidentity conversion task revealed that the proposed method obtained higher\nsound quality and speaker similarity than a state-of-the-art method based on\nvariational autoencoding GANs.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-06-06T13:24:23Z",
    "updated": "2018-06-29T06:19:34Z",
    "doi": null
  },
  "2309.03044": {
    "id": "http://arxiv.org/abs/2309.03044v1",
    "title": "Method-Level Bug Severity Prediction using Source Code Metrics and LLMs",
    "authors": [
      "Ehsan Mashhadi",
      "Hossein Ahmadvand",
      "Hadi Hemmati"
    ],
    "abstract": "  In the past couple of decades, significant research efforts are devoted to\nthe prediction of software bugs. However, most existing work in this domain\ntreats all bugs the same, which is not the case in practice. It is important\nfor a defect prediction method to estimate the severity of the identified bugs\nso that the higher-severity ones get immediate attention. In this study, we\ninvestigate source code metrics, source code representation using large\nlanguage models (LLMs), and their combination in predicting bug severity labels\nof two prominent datasets. We leverage several source metrics at method-level\ngranularity to train eight different machine-learning models. Our results\nsuggest that Decision Tree and Random Forest models outperform other models\nregarding our several evaluation metrics. We then use the pre-trained CodeBERT\nLLM to study the source code representations' effectiveness in predicting bug\nseverity. CodeBERT finetuning improves the bug severity prediction results\nsignificantly in the range of 29%-140% for several evaluation metrics, compared\nto the best classic prediction model on source code metric. Finally, we\nintegrate source code metrics into CodeBERT as an additional input, using our\ntwo proposed architectures, which both enhance the CodeBERT model\neffectiveness.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-06T14:38:07Z",
    "updated": "2023-09-06T14:38:07Z",
    "doi": "10.5281/zenodo.8267597"
  },
  "2208.01753": {
    "id": "http://arxiv.org/abs/2208.01753v1",
    "title": "Two-Stream Transformer Architecture for Long Video Understanding",
    "authors": [
      "Edward Fish",
      "Jon Weinbren",
      "Andrew Gilbert"
    ],
    "abstract": "  Pure vision transformer architectures are highly effective for short video\nclassification and action recognition tasks. However, due to the quadratic\ncomplexity of self attention and lack of inductive bias, transformers are\nresource intensive and suffer from data inefficiencies. Long form video\nunderstanding tasks amplify data and memory efficiency problems in transformers\nmaking current approaches unfeasible to implement on data or memory restricted\ndomains. This paper introduces an efficient Spatio-Temporal Attention Network\n(STAN) which uses a two-stream transformer architecture to model dependencies\nbetween static image features and temporal contextual features. Our proposed\napproach can classify videos up to two minutes in length on a single GPU, is\ndata efficient, and achieves SOTA performance on several long video\nunderstanding tasks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-02T21:03:48Z",
    "updated": "2022-08-02T21:03:48Z",
    "doi": null
  },
  "1804.10253": {
    "id": "http://arxiv.org/abs/1804.10253v3",
    "title": "From Principal Subspaces to Principal Components with Linear\n  Autoencoders",
    "authors": [
      "Elad Plaut"
    ],
    "abstract": "  The autoencoder is an effective unsupervised learning model which is widely\nused in deep learning. It is well known that an autoencoder with a single\nfully-connected hidden layer, a linear activation function and a squared error\ncost function trains weights that span the same subspace as the one spanned by\nthe principal component loading vectors, but that they are not identical to the\nloading vectors. In this paper, we show how to recover the loading vectors from\nthe autoencoder weights.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-26T19:28:02Z",
    "updated": "2018-12-28T19:02:12Z",
    "doi": null
  },
  "2009.06520": {
    "id": "http://arxiv.org/abs/2009.06520v2",
    "title": "A Systematic Literature Review on the Use of Deep Learning in Software\n  Engineering Research",
    "authors": [
      "Cody Watson",
      "Nathan Cooper",
      "David Nader Palacio",
      "Kevin Moran",
      "Denys Poshyvanyk"
    ],
    "abstract": "  An increasingly popular set of techniques adopted by software engineering\n(SE) researchers to automate development tasks are those rooted in the concept\nof Deep Learning (DL). The popularity of such techniques largely stems from\ntheir automated feature engineering capabilities, which aid in modeling\nsoftware artifacts. However, due to the rapid pace at which DL techniques have\nbeen adopted, it is difficult to distill the current successes, failures, and\nopportunities of the current research landscape. In an effort to bring clarity\nto this crosscutting area of work, from its modern inception to the present,\nthis paper presents a systematic literature review of research at the\nintersection of SE & DL. The review canvases work appearing in the most\nprominent SE and DL conferences and journals and spans 128 papers across 23\nunique SE tasks. We center our analysis around the components of learning, a\nset of principles that govern the application of machine learning techniques\n(ML) to a given problem domain, discussing several aspects of the surveyed work\nat a granular level. The end result of our analysis is a research roadmap that\nboth delineates the foundations of DL techniques applied to SE research, and\nhighlights likely areas of fertile exploration for the future.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.NE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-09-14T15:28:28Z",
    "updated": "2021-09-23T18:11:47Z",
    "doi": null
  },
  "2305.16317": {
    "id": "http://arxiv.org/abs/2305.16317v3",
    "title": "Parallel Sampling of Diffusion Models",
    "authors": [
      "Andy Shih",
      "Suneel Belkhale",
      "Stefano Ermon",
      "Dorsa Sadigh",
      "Nima Anari"
    ],
    "abstract": "  Diffusion models are powerful generative models but suffer from slow\nsampling, often taking 1000 sequential denoising steps for one sample. As a\nresult, considerable efforts have been directed toward reducing the number of\ndenoising steps, but these methods hurt sample quality. Instead of reducing the\nnumber of denoising steps (trading quality for speed), in this paper we explore\nan orthogonal approach: can we run the denoising steps in parallel (trading\ncompute for speed)? In spite of the sequential nature of the denoising steps,\nwe show that surprisingly it is possible to parallelize sampling via Picard\niterations, by guessing the solution of future denoising steps and iteratively\nrefining until convergence. With this insight, we present ParaDiGMS, a novel\nmethod to accelerate the sampling of pretrained diffusion models by denoising\nmultiple steps in parallel. ParaDiGMS is the first diffusion sampling method\nthat enables trading compute for speed and is even compatible with existing\nfast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we\nimprove sampling speed by 2-4x across a range of robotics and image generation\nmodels, giving state-of-the-art sampling speeds of 0.2s on 100-step\nDiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable\ndegradation of task reward, FID score, or CLIP score.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-25T17:59:42Z",
    "updated": "2023-10-16T01:51:04Z",
    "doi": null
  },
  "2109.08306": {
    "id": "http://arxiv.org/abs/2109.08306v1",
    "title": "SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based\n  Sentiment Analysis",
    "authors": [
      "Chengxi Li",
      "Feiyu Gao",
      "Jiajun Bu",
      "Lu Xu",
      "Xiang Chen",
      "Yu Gu",
      "Zirui Shao",
      "Qi Zheng",
      "Ningyu Zhang",
      "Yongpan Wang",
      "Zhi Yu"
    ],
    "abstract": "  Aspect-based sentiment analysis (ABSA) is an emerging fine-grained sentiment\nanalysis task that aims to extract aspects, classify corresponding sentiment\npolarities and find opinions as the causes of sentiment. The latest research\ntends to solve the ABSA task in a unified way with end-to-end frameworks. Yet,\nthese frameworks get fine-tuned from downstream tasks without any task-adaptive\nmodification. Specifically, they do not use task-related knowledge well or\nexplicitly model relations between aspect and opinion terms, hindering them\nfrom better performance. In this paper, we propose SentiPrompt to use sentiment\nknowledge enhanced prompts to tune the language model in the unified framework.\nWe inject sentiment knowledge regarding aspects, opinions, and polarities into\nprompt and explicitly model term relations via constructing consistency and\npolarity judgment templates from the ground truth triplets. Experimental\nresults demonstrate that our approach can outperform strong baselines on\nTriplet Extraction, Pair Extraction, and Aspect Term Extraction with Sentiment\nClassification by a notable margin.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-17T01:56:06Z",
    "updated": "2021-09-17T01:56:06Z",
    "doi": null
  },
  "2204.02849": {
    "id": "http://arxiv.org/abs/2204.02849v2",
    "title": "KNN-Diffusion: Image Generation via Large-Scale Retrieval",
    "authors": [
      "Shelly Sheynin",
      "Oron Ashual",
      "Adam Polyak",
      "Uriel Singer",
      "Oran Gafni",
      "Eliya Nachmani",
      "Yaniv Taigman"
    ],
    "abstract": "  Recent text-to-image models have achieved impressive results. However, since\nthey require large-scale datasets of text-image pairs, it is impractical to\ntrain them on new domains where data is scarce or not labeled. In this work, we\npropose using large-scale retrieval methods, in particular, efficient\nk-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a\nsubstantially small and efficient text-to-image diffusion model without any\ntext, (2) generating out-of-distribution images by simply swapping the\nretrieval database at inference time, and (3) performing text-driven local\nsemantic manipulations while preserving object identity. To demonstrate the\nrobustness of our method, we apply our kNN approach on two state-of-the-art\ndiffusion backbones, and show results on several different datasets. As\nevaluated by human studies and automatic metrics, our method achieves\nstate-of-the-art results compared to existing approaches that train\ntext-to-image generation models using images only (without paired text data)\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-06T14:13:35Z",
    "updated": "2022-10-02T11:55:59Z",
    "doi": null
  },
  "2309.04828": {
    "id": "http://arxiv.org/abs/2309.04828v1",
    "title": "FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate\n  Representations",
    "authors": [
      "Changan Niu",
      "Chuanyi Li",
      "Vincent Ng",
      "David Lo",
      "Bin Luo"
    ],
    "abstract": "  While the majority of existing pre-trained models from code learn source code\nfeatures such as code tokens and abstract syntax trees, there are some other\nworks that focus on learning from compiler intermediate representations (IRs).\nExisting IR-based models typically utilize IR features such as instructions,\ncontrol and data flow graphs (CDFGs), call graphs, etc. However, these methods\nconfuse variable nodes and instruction nodes in a CDFG and fail to distinguish\ndifferent types of flows, and the neural networks they use fail to capture\nlong-distance dependencies and have over-smoothing and over-squashing problems.\nTo address these weaknesses, we propose FAIR, a Flow type-Aware pre-trained\nmodel for IR that involves employing (1) a novel input representation of IR\nprograms; (2) Graph Transformer to address over-smoothing, over-squashing and\nlong-dependencies problems; and (3) five pre-training tasks that we\nspecifically propose to enable FAIR to learn the semantics of IR tokens, flow\ntype information, and the overall representation of IR. Experimental results\nshow that FAIR can achieve state-of-the-art results on four code-related\ndownstream tasks.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-09T15:51:49Z",
    "updated": "2023-09-09T15:51:49Z",
    "doi": null
  },
  "2105.06597": {
    "id": "http://arxiv.org/abs/2105.06597v4",
    "title": "RetGen: A Joint framework for Retrieval and Grounded Text Generation\n  Modeling",
    "authors": [
      "Yizhe Zhang",
      "Siqi Sun",
      "Xiang Gao",
      "Yuwei Fang",
      "Chris Brockett",
      "Michel Galley",
      "Jianfeng Gao",
      "Bill Dolan"
    ],
    "abstract": "  Recent advances in large-scale pre-training such as GPT-3 allow seemingly\nhigh quality text to be generated from a given prompt. However, such generation\nsystems often suffer from problems of hallucinated facts, and are not\ninherently designed to incorporate useful external information. Grounded\ngeneration models appear to offer remedies, but their training typically relies\non rarely-available parallel data where information-relevant documents are\nprovided for context. We propose a framework that alleviates this data\nconstraint by jointly training a grounded generator and document retriever on\nthe language model signal. The model learns to reward retrieval of the\ndocuments with the highest utility in generation, and attentively combines them\nusing a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We\ndemonstrate that both generator and retriever can take advantage of this joint\ntraining and work synergistically to produce more informative and relevant text\nin both prose and dialogue generation.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-05-14T00:11:38Z",
    "updated": "2022-02-24T18:29:55Z",
    "doi": null
  },
  "2011.12026": {
    "id": "http://arxiv.org/abs/2011.12026v2",
    "title": "Adversarial Generation of Continuous Images",
    "authors": [
      "Ivan Skorokhodov",
      "Savva Ignatyev",
      "Mohamed Elhoseiny"
    ],
    "abstract": "  In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-11-24T11:06:40Z",
    "updated": "2021-06-28T09:00:05Z",
    "doi": null
  },
  "2309.00841": {
    "id": "http://arxiv.org/abs/2309.00841v1",
    "title": "LeanContext: Cost-Efficient Domain-Specific Question Answering Using\n  LLMs",
    "authors": [
      "Md Adnan Arefeen",
      "Biplob Debnath",
      "Srimat Chakradhar"
    ],
    "abstract": "  Question-answering (QA) is a significant application of Large Language Models\n(LLMs), shaping chatbot capabilities across healthcare, education, and customer\nservice. However, widespread LLM integration presents a challenge for small\nbusinesses due to the high expenses of LLM API usage. Costs rise rapidly when\ndomain-specific data (context) is used alongside queries for accurate\ndomain-specific LLM responses. One option is to summarize the context by using\nLLMs and reduce the context. However, this can also filter out useful\ninformation that is necessary to answer some domain-specific queries. In this\npaper, we shift from human-oriented summarizers to AI model-friendly summaries.\nOur approach, LeanContext, efficiently extracts $k$ key sentences from the\ncontext that are closely aligned with the query. The choice of $k$ is neither\nstatic nor random; we introduce a reinforcement learning technique that\ndynamically determines $k$ based on the query and context. The rest of the less\nimportant sentences are reduced using a free open source text reduction method.\nWe evaluate LeanContext against several recent query-aware and query-unaware\ncontext reduction approaches on prominent datasets (arxiv papers and BBC news\narticles). Despite cost reductions of $37.29\\%$ to $67.81\\%$, LeanContext's\nROUGE-1 score decreases only by $1.41\\%$ to $2.65\\%$ compared to a baseline\nthat retains the entire context (no summarization). Additionally, if free\npretrained LLM-based summarizers are used to reduce context (into human\nconsumable summaries), LeanContext can further modify the reduced context to\nenhance the accuracy (ROUGE-1 score) by $13.22\\%$ to $24.61\\%$.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-02T06:33:18Z",
    "updated": "2023-09-02T06:33:18Z",
    "doi": null
  },
  "2308.07732": {
    "id": "http://arxiv.org/abs/2308.07732v1",
    "title": "UniTR: A Unified and Efficient Multi-Modal Transformer for\n  Bird's-Eye-View Representation",
    "authors": [
      "Haiyang Wang",
      "Hao Tang",
      "Shaoshuai Shi",
      "Aoxue Li",
      "Zhenguo Li",
      "Bernt Schiele",
      "Liwei Wang"
    ],
    "abstract": "  Jointly processing information from multiple sensors is crucial to achieving\naccurate and robust perception for reliable autonomous driving systems.\nHowever, current 3D perception research follows a modality-specific paradigm,\nleading to additional computation overheads and inefficient collaboration\nbetween different sensor data. In this paper, we present an efficient\nmulti-modal backbone for outdoor 3D perception named UniTR, which processes a\nvariety of modalities with unified modeling and shared parameters. Unlike\nprevious works, UniTR introduces a modality-agnostic transformer encoder to\nhandle these view-discrepant sensor data for parallel modal-wise representation\nlearning and automatic cross-modal interaction without additional fusion steps.\nMore importantly, to make full use of these complementary sensor types, we\npresent a novel multi-modal integration strategy by both considering\nsemantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood\nrelations. UniTR is also a fundamentally task-agnostic backbone that naturally\nsupports different 3D perception tasks. It sets a new state-of-the-art\nperformance on the nuScenes benchmark, achieving +1.1 NDS higher for 3D object\ndetection and +12.0 higher mIoU for BEV map segmentation with lower inference\nlatency. Code will be available at https://github.com/Haiyang-W/UniTR .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-15T12:13:44Z",
    "updated": "2023-08-15T12:13:44Z",
    "doi": null
  },
  "2011.11961": {
    "id": "http://arxiv.org/abs/2011.11961v4",
    "title": "MODNet: Real-Time Trimap-Free Portrait Matting via Objective\n  Decomposition",
    "authors": [
      "Zhanghan Ke",
      "Jiayu Sun",
      "Kaican Li",
      "Qiong Yan",
      "Rynson W. H. Lau"
    ],
    "abstract": "  Existing portrait matting methods either require auxiliary inputs that are\ncostly to obtain or involve multiple stages that are computationally expensive,\nmaking them less suitable for real-time applications. In this work, we present\na light-weight matting objective decomposition network (MODNet) for portrait\nmatting in real-time with a single input image. The key idea behind our\nefficient design is by optimizing a series of sub-objectives simultaneously via\nexplicit constraints. In addition, MODNet includes two novel techniques for\nimproving model efficiency and robustness. First, an Efficient Atrous Spatial\nPyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for\nsemantic estimation. Second, a self-supervised sub-objectives consistency (SOC)\nstrategy is proposed to adapt MODNet to real-world data to address the domain\nshift problem common to trimap-free methods. MODNet is easy to be trained in an\nend-to-end manner. It is much faster than contemporaneous methods and runs at\n67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms\nprior trimap-free methods by a large margin on both Adobe Matting Dataset and a\ncarefully designed photographic portrait matting (PPM-100) benchmark proposed\nby us. Further, MODNet achieves remarkable results on daily photos and videos.\nOur code and models are available at https://github.com/ZHKKKe/MODNet, and the\nPPM-100 benchmark is released at https://github.com/ZHKKKe/PPM.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-11-24T08:38:36Z",
    "updated": "2022-03-18T04:49:53Z",
    "doi": null
  },
  "2005.00341": {
    "id": "http://arxiv.org/abs/2005.00341v1",
    "title": "Jukebox: A Generative Model for Music",
    "authors": [
      "Prafulla Dhariwal",
      "Heewoo Jun",
      "Christine Payne",
      "Jong Wook Kim",
      "Alec Radford",
      "Ilya Sutskever"
    ],
    "abstract": "  We introduce Jukebox, a model that generates music with singing in the raw\naudio domain. We tackle the long context of raw audio using a multi-scale\nVQ-VAE to compress it to discrete codes, and modeling those using\nautoregressive Transformers. We show that the combined model at scale can\ngenerate high-fidelity and diverse songs with coherence up to multiple minutes.\nWe can condition on artist and genre to steer the musical and vocal style, and\non unaligned lyrics to make the singing more controllable. We are releasing\nthousands of non cherry-picked samples at https://jukebox.openai.com, along\nwith model weights and code at https://github.com/openai/jukebox\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-30T09:02:45Z",
    "updated": "2020-04-30T09:02:45Z",
    "doi": null
  },
  "2409.08091": {
    "id": "http://arxiv.org/abs/2409.08091v2",
    "title": "EZIGen: Enhancing zero-shot subject-driven image generation with precise\n  subject encoding and decoupled guidance",
    "authors": [
      "Zicheng Duan",
      "Yuxuan Ding",
      "Chenhui Gou",
      "Ziqin Zhou",
      "Ethan Smith",
      "Lingqiao Liu"
    ],
    "abstract": "  Zero-shot subject-driven image generation aims to produce images that\nincorporate a subject from a given example image. The challenge lies in\npreserving the subject's identity while aligning with the text prompt which\noften requires modifying certain aspects of the subject's appearance. Despite\nadvancements in diffusion model based methods, existing approaches still\nstruggle to balance identity preservation with text prompt alignment. In this\nstudy, we conducted an in-depth investigation into this issue and uncovered key\ninsights for achieving effective identity preservation while maintaining a\nstrong balance. Our key findings include: (1) the design of the subject image\nencoder significantly impacts identity preservation quality, and (2) separating\ntext and subject guidance is crucial for both text alignment and identity\npreservation. Building on these insights, we introduce a new approach called\nEZIGen, which employs two main strategies: a carefully crafted subject image\nEncoder based on the pretrained UNet of the Stable Diffusion model to ensure\nhigh-quality identity transfer, following a process that decouples the guidance\nstages and iteratively refines the initial image layout. Through these\nstrategies, EZIGen achieves state-of-the-art results on multiple subject-driven\nbenchmarks with a unified model and 100 times less training data. The demo page\nis available at: https://zichengduan.github.io/pages/EZIGen/index.html.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-09-12T14:44:45Z",
    "updated": "2024-10-01T17:52:18Z",
    "doi": null
  },
  "1711.11586": {
    "id": "http://arxiv.org/abs/1711.11586v4",
    "title": "Toward Multimodal Image-to-Image Translation",
    "authors": [
      "Jun-Yan Zhu",
      "Richard Zhang",
      "Deepak Pathak",
      "Trevor Darrell",
      "Alexei A. Efros",
      "Oliver Wang",
      "Eli Shechtman"
    ],
    "abstract": "  Many image-to-image translation problems are ambiguous, as a single input\nimage may correspond to multiple possible outputs. In this work, we aim to\nmodel a \\emph{distribution} of possible outputs in a conditional generative\nmodeling setting. The ambiguity of the mapping is distilled in a\nlow-dimensional latent vector, which can be randomly sampled at test time. A\ngenerator learns to map the given input, combined with this latent code, to the\noutput. We explicitly encourage the connection between output and the latent\ncode to be invertible. This helps prevent a many-to-one mapping from the latent\ncode to the output during training, also known as the problem of mode collapse,\nand produces more diverse results. We explore several variants of this approach\nby employing different training objectives, network architectures, and methods\nof injecting the latent code. Our proposed method encourages bijective\nconsistency between the latent encoding and output modes. We present a\nsystematic comparison of our method and other variants on both perceptual\nrealism and diversity.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-11-30T18:59:01Z",
    "updated": "2018-10-24T00:29:43Z",
    "doi": null
  },
  "2311.15744": {
    "id": "http://arxiv.org/abs/2311.15744v1",
    "title": "One More Step: A Versatile Plug-and-Play Module for Rectifying Diffusion\n  Schedule Flaws and Enhancing Low-Frequency Controls",
    "authors": [
      "Minghui Hu",
      "Jianbin Zheng",
      "Chuanxia Zheng",
      "Chaoyue Wang",
      "Dacheng Tao",
      "Tat-Jen Cham"
    ],
    "abstract": "  It is well known that many open-released foundational diffusion models have\ndifficulty in generating images that substantially depart from average\nbrightness, despite such images being present in the training data. This is due\nto an inconsistency: while denoising starts from pure Gaussian noise during\ninference, the training noise schedule retains residual data even in the final\ntimestep distribution, due to difficulties in numerical conditioning in\nmainstream formulation, leading to unintended bias during inference. To\nmitigate this issue, certain $\\epsilon$-prediction models are combined with an\nad-hoc offset-noise methodology. In parallel, some contemporary models have\nadopted zero-terminal SNR noise schedules together with\n$\\mathbf{v}$-prediction, which necessitate major alterations to pre-trained\nmodels. However, such changes risk destabilizing a large multitude of\ncommunity-driven applications anchored on these pre-trained models. In light of\nthis, our investigation revisits the fundamental causes, leading to our\nproposal of an innovative and principled remedy, called One More Step (OMS). By\nintegrating a compact network and incorporating an additional simple yet\neffective step during inference, OMS elevates image fidelity and harmonizes the\ndichotomy between training and inference, while preserving original model\nparameters. Once trained, various pre-trained diffusion models with the same\nlatent domain can share the same OMS module.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-27T12:02:42Z",
    "updated": "2023-11-27T12:02:42Z",
    "doi": null
  },
  "2209.07046": {
    "id": "http://arxiv.org/abs/2209.07046v2",
    "title": "Exploring Visual Interpretability for Contrastive Language-Image\n  Pre-training",
    "authors": [
      "Yi Li",
      "Hualiang Wang",
      "Yiqun Duan",
      "Hang Xu",
      "Xiaomeng Li"
    ],
    "abstract": "  Contrastive Language-Image Pre-training (CLIP) learns rich representations\nvia readily available supervision of natural language. It improves the\nperformance of downstream vision tasks, including but not limited to the\nzero-shot, long tail, segmentation, retrieval, caption, and video. However, the\nvisual explainability of CLIP is rarely studied, especially for the raw feature\nmap. To provide visual explanations of its predictions, we propose the\nImage-Text Similarity Map (ITSM). Based on it, we surprisingly find that CLIP\nprefers the background regions than the foregrounds, and shows erroneous\nvisualization results against human understanding. This phenomenon is universal\nfor both vision transformers and convolutional networks, which suggests this\nproblem is unique and not owing to certain network. Experimentally, we find the\ndevil is in the pooling part, where inappropriate pooling methods lead to a\nphenomenon called semantic shift. For this problem, we propose the Explainable\nContrastive Language-Image Pre-training (ECLIP), which corrects the\nexplainability via the Masked Max Pooling. Specifically, to avoid the semantic\nshift, we replace the original attention pooling by max pooling to focus on the\nconfident foreground, with guidance from free attention during training.\nExperiments on three datasets suggest that ECLIP greatly improves the\nexplainability of CLIP, and beyond previous explainability methods at large\nmargins. The code will be released later.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "65D19 (Primary), 54H30 (Secondary)",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.4.0; I.4.10",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-15T05:01:03Z",
    "updated": "2022-11-27T01:07:52Z",
    "doi": null
  },
  "2306.08964": {
    "id": "http://arxiv.org/abs/2306.08964v2",
    "title": "Exploring Multi-Timestep Multi-Stage Diffusion Features for\n  Hyperspectral Image Classification",
    "authors": [
      "Jingyi Zhou",
      "Jiamu Sheng",
      "Jiayuan Fan",
      "Peng Ye",
      "Tong He",
      "Bin Wang",
      "Tao Chen"
    ],
    "abstract": "  The effectiveness of spectral-spatial feature learning is crucial for the\nhyperspectral image (HSI) classification task. Diffusion models, as a new class\nof groundbreaking generative models, have the ability to learn both contextual\nsemantics and textual details from the distinct timestep dimension, enabling\nthe modeling of complex spectral-spatial relations in HSIs. However, existing\ndiffusion-based HSI classification methods only utilize manually selected\nsingle-timestep single-stage features, limiting the full exploration and\nexploitation of rich contextual semantics and textual information hidden in the\ndiffusion model. To address this issue, we propose a novel diffusion-based\nfeature learning framework that explores Multi-Timestep Multi-Stage Diffusion\nfeatures for HSI classification for the first time, called MTMSD. Specifically,\nthe diffusion model is first pretrained with unlabeled HSI patches to mine the\nconnotation of unlabeled data, and then is used to extract the multi-timestep\nmulti-stage diffusion features. To effectively and efficiently leverage\nmulti-timestep multi-stage features,two strategies are further developed. One\nstrategy is class & timestep-oriented multi-stage feature purification module\nwith the inter-class and inter-timestep prior for reducing the redundancy of\nmulti-stage features and alleviating memory constraints. The other one is\nselective timestep feature fusion module with the guidance of global features\nto adaptively select different timestep features for integrating texture and\nsemantics. Both strategies facilitate the generality and adaptability of the\nMTMSD framework for diverse patterns of different HSI data. Extensive\nexperiments are conducted on four public HSI datasets, and the results\ndemonstrate that our method outperforms state-of-the-art methods for HSI\nclassification, especially on the challenging Houston 2018 dataset.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-15T08:56:58Z",
    "updated": "2024-06-03T04:50:18Z",
    "doi": null
  },
  "2208.14743": {
    "id": "http://arxiv.org/abs/2208.14743v1",
    "title": "SimpleRecon: 3D Reconstruction Without 3D Convolutions",
    "authors": [
      "Mohamed Sayed",
      "John Gibson",
      "Jamie Watson",
      "Victor Prisacariu",
      "Michael Firman",
      "Cl\u00e9ment Godard"
    ],
    "abstract": "  Traditionally, 3D indoor scene reconstruction from posed images happens in\ntwo phases: per-image depth estimation, followed by depth merging and surface\nreconstruction. Recently, a family of methods have emerged that perform\nreconstruction directly in final 3D volumetric feature space. While these\nmethods have shown impressive reconstruction results, they rely on expensive 3D\nconvolutional layers, limiting their application in resource-constrained\nenvironments. In this work, we instead go back to the traditional route, and\nshow how focusing on high quality multi-view depth prediction leads to highly\naccurate 3D reconstructions using simple off-the-shelf depth fusion. We propose\na simple state-of-the-art multi-view depth estimator with two main\ncontributions: 1) a carefully-designed 2D CNN which utilizes strong image\npriors alongside a plane-sweep feature volume and geometric losses, combined\nwith 2) the integration of keyframe and geometric metadata into the cost volume\nwhich allows informed depth plane scoring. Our method achieves a significant\nlead over the current state-of-the-art for depth estimation and close or better\nfor 3D reconstruction on ScanNet and 7-Scenes, yet still allows for online\nreal-time low-memory reconstruction. Code, models and results are available at\nhttps://nianticlabs.github.io/simplerecon\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-31T09:46:34Z",
    "updated": "2022-08-31T09:46:34Z",
    "doi": null
  },
  "2211.17084": {
    "id": "http://arxiv.org/abs/2211.17084v1",
    "title": "High-Fidelity Guided Image Synthesis with Latent Diffusion Models",
    "authors": [
      "Jaskirat Singh",
      "Stephen Gould",
      "Liang Zheng"
    ],
    "abstract": "  Controllable image synthesis with user scribbles has gained huge public\ninterest with the recent advent of text-conditioned latent diffusion models.\nThe user scribbles control the color composition while the text prompt provides\ncontrol over the overall image semantics. However, we note that prior works in\nthis direction suffer from an intrinsic domain shift problem, wherein the\ngenerated outputs often lack details and resemble simplistic representations of\nthe target domain. In this paper, we propose a novel guided image synthesis\nframework, which addresses this problem by modeling the output image as the\nsolution of a constrained optimization problem. We show that while computing an\nexact solution to the optimization is infeasible, an approximation of the same\ncan be achieved while just requiring a single pass of the reverse diffusion\nprocess. Additionally, we show that by simply defining a cross-attention based\ncorrespondence between the input text tokens and the user stroke-painting, the\nuser is also able to control the semantics of different painted regions without\nrequiring any conditional training or finetuning. Human user study results show\nthat the proposed approach outperforms the previous state-of-the-art by over\n85.32% on the overall user satisfaction scores. Project page for our paper is\navailable at https://1jsingh.github.io/gradop.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-30T15:43:20Z",
    "updated": "2022-11-30T15:43:20Z",
    "doi": null
  },
  "2206.01821": {
    "id": "http://arxiv.org/abs/2206.01821v1",
    "title": "EAANet: Efficient Attention Augmented Convolutional Networks",
    "authors": [
      "Runqing Zhang",
      "Tianshu Zhu"
    ],
    "abstract": "  Humans can effectively find salient regions in complex scenes. Self-attention\nmechanisms were introduced into Computer Vision (CV) to achieve this. Attention\nAugmented Convolutional Network (AANet) is a mixture of convolution and\nself-attention, which increases the accuracy of a typical ResNet. However, The\ncomplexity of self-attention is O(n2) in terms of computation and memory usage\nwith respect to the number of input tokens. In this project, we propose EAANet:\nEfficient Attention Augmented Convolutional Networks, which incorporates\nefficient self-attention mechanisms in a convolution and self-attention hybrid\narchitecture to reduce the model's memory footprint. Our best model show\nperformance improvement over AA-Net and ResNet18. We also explore different\nmethods to augment Convolutional Network with self-attention mechanisms and\nshow the difficulty of training those methods compared to ResNet. Finally, we\nshow that augmenting efficient self-attention mechanisms with ResNet scales\nbetter with input size than normal self-attention mechanisms. Therefore, our\nEAANet is more capable of working with high-resolution images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-03T21:22:12Z",
    "updated": "2022-06-03T21:22:12Z",
    "doi": null
  },
  "2110.04994": {
    "id": "http://arxiv.org/abs/2110.04994v1",
    "title": "Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision\n  Datasets from 3D Scans",
    "authors": [
      "Ainaz Eftekhar",
      "Alexander Sax",
      "Roman Bachmann",
      "Jitendra Malik",
      "Amir Zamir"
    ],
    "abstract": "  This paper introduces a pipeline to parametrically sample and render\nmulti-task vision datasets from comprehensive 3D scans from the real world.\nChanging the sampling parameters allows one to \"steer\" the generated datasets\nto emphasize specific information. In addition to enabling interesting lines of\nresearch, we show the tooling and generated data suffice to train robust vision\nmodels.\n  Common architectures trained on a generated starter dataset reached\nstate-of-the-art performance on multiple common vision tasks and benchmarks,\ndespite having seen no benchmark or non-pipeline data. The depth estimation\nnetwork outperforms MiDaS and the surface normal estimation network is the\nfirst to achieve human-level performance for in-the-wild surface normal\nestimation -- at least according to one metric on the OASIS benchmark.\n  The Dockerized pipeline with CLI, the (mostly python) code, PyTorch\ndataloaders for the generated data, the generated starter dataset, download\nscripts and other utilities are available through our project website,\nhttps://omnidata.vision.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-11T04:21:46Z",
    "updated": "2021-10-11T04:21:46Z",
    "doi": null
  },
  "2203.13954": {
    "id": "http://arxiv.org/abs/2203.13954v2",
    "title": "GEN-VLKT: Simplify Association and Enhance Interaction Understanding for\n  HOI Detection",
    "authors": [
      "Yue Liao",
      "Aixi Zhang",
      "Miao Lu",
      "Yongliang Wang",
      "Xiaobo Li",
      "Si Liu"
    ],
    "abstract": "  The task of Human-Object Interaction~(HOI) detection could be divided into\ntwo core problems, i.e., human-object association and interaction\nunderstanding. In this paper, we reveal and address the disadvantages of the\nconventional query-driven HOI detectors from the two aspects. For the\nassociation, previous two-branch methods suffer from complex and costly\npost-matching, while single-branch methods ignore the features distinction in\ndifferent tasks. We propose Guided-Embedding Network~(GEN) to attain a\ntwo-branch pipeline without post-matching. In GEN, we design an instance\ndecoder to detect humans and objects with two independent query sets and a\nposition Guided Embedding~(p-GE) to mark the human and object in the same\nposition as a pair. Besides, we design an interaction decoder to classify\ninteractions, where the interaction queries are made of instance Guided\nEmbeddings (i-GE) generated from the outputs of each instance decoder layer.\nFor the interaction understanding, previous methods suffer from long-tailed\ndistribution and zero-shot discovery. This paper proposes a Visual-Linguistic\nKnowledge Transfer (VLKT) training strategy to enhance interaction\nunderstanding by transferring knowledge from a visual-linguistic pre-trained\nmodel CLIP. In specific, we extract text embeddings for all labels with CLIP to\ninitialize the classifier and adopt a mimic loss to minimize the visual feature\ndistance between GEN and CLIP. As a result, GEN-VLKT outperforms the state of\nthe art by large margins on multiple datasets, e.g., +5.05 mAP on HICO-Det. The\nsource codes are available at https://github.com/YueLiao/gen-vlkt.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-26T01:04:13Z",
    "updated": "2022-04-14T13:07:54Z",
    "doi": null
  },
  "2111.00273": {
    "id": "http://arxiv.org/abs/2111.00273v4",
    "title": "Cross-Modality Fusion Transformer for Multispectral Object Detection",
    "authors": [
      "Fang Qingyun",
      "Han Dapeng",
      "Wang Zhaokui"
    ],
    "abstract": "  Multispectral image pairs can provide the combined information, making object\ndetection applications more reliable and robust in the open world. To fully\nexploit the different modalities, we present a simple yet effective\ncross-modality feature fusion approach, named Cross-Modality Fusion Transformer\n(CFT) in this paper. Unlike prior CNNs-based works, guided by the transformer\nscheme, our network learns long-range dependencies and integrates global\ncontextual information in the feature extraction stage. More importantly, by\nleveraging the self attention of the transformer, the network can naturally\ncarry out simultaneous intra-modality and inter-modality fusion, and robustly\ncapture the latent interactions between RGB and Thermal domains, thereby\nsignificantly improving the performance of multispectral object detection.\nExtensive experiments and ablation studies on multiple datasets demonstrate\nthat our approach is effective and achieves state-of-the-art detection\nperformance. Our code and models are available at\nhttps://github.com/DocF/multispectral-object-detection.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-30T15:34:12Z",
    "updated": "2022-10-04T09:52:39Z",
    "doi": null
  },
  "2007.14062": {
    "id": "http://arxiv.org/abs/2007.14062v2",
    "title": "Big Bird: Transformers for Longer Sequences",
    "authors": [
      "Manzil Zaheer",
      "Guru Guruganesh",
      "Avinava Dubey",
      "Joshua Ainslie",
      "Chris Alberti",
      "Santiago Ontanon",
      "Philip Pham",
      "Anirudh Ravula",
      "Qifan Wang",
      "Li Yang",
      "Amr Ahmed"
    ],
    "abstract": "  Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is\nthe quadratic dependency (mainly in terms of memory) on the sequence length due\nto their full attention mechanism. To remedy this, we propose, BigBird, a\nsparse attention mechanism that reduces this quadratic dependency to linear. We\nshow that BigBird is a universal approximator of sequence functions and is\nTuring complete, thereby preserving these properties of the quadratic, full\nattention model. Along the way, our theoretical analysis reveals some of the\nbenefits of having $O(1)$ global tokens (such as CLS), that attend to the\nentire sequence as part of the sparse attention mechanism. The proposed sparse\nattention can handle sequences of length up to 8x of what was previously\npossible using similar hardware. As a consequence of the capability to handle\nlonger context, BigBird drastically improves performance on various NLP tasks\nsuch as question answering and summarization. We also propose novel\napplications to genomics data.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-28T08:34:04Z",
    "updated": "2021-01-08T07:41:50Z",
    "doi": null
  },
  "2306.14898": {
    "id": "http://arxiv.org/abs/2306.14898v3",
    "title": "InterCode: Standardizing and Benchmarking Interactive Coding with\n  Execution Feedback",
    "authors": [
      "John Yang",
      "Akshara Prabhakar",
      "Karthik Narasimhan",
      "Shunyu Yao"
    ],
    "abstract": "  Humans write code in a fundamentally interactive manner and rely on constant\nexecution feedback to correct errors, resolve ambiguities, and decompose tasks.\nWhile LLMs have recently exhibited promising coding capabilities, current\ncoding benchmarks mostly consider a static instruction-to-code sequence\ntransduction process, which has the potential for error propagation and a\ndisconnect between the generated code and its final execution environment. To\naddress this gap, we introduce InterCode, a lightweight, flexible, and\neasy-to-use framework of interactive coding as a standard reinforcement\nlearning (RL) environment, with code as actions and execution feedback as\nobservations. Our framework is language and platform agnostic, uses\nself-contained Docker environments to provide safe and reproducible execution,\nand is compatible out-of-the-box with traditional seq2seq coding methods, while\nenabling the development of new methods for interactive code generation. We use\nInterCode to create three interactive code environments with Bash, SQL, and\nPython as action spaces, leveraging data from the static NL2Bash, Spider, and\nMBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating\nmultiple state-of-the-art LLMs configured with different prompting strategies\nsuch as ReAct and Plan & Solve. Our results showcase the benefits of\ninteractive code generation and demonstrate that InterCode can serve as a\nchallenging benchmark for advancing code understanding and generation\ncapabilities. InterCode is designed to be easily extensible and can even be\nused to create new tasks such as Capture the Flag, a popular coding puzzle that\nis inherently multi-step and involves multiple programming languages. Project\nsite with code and data: https://intercode-benchmark.github.io\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-26T17:59:50Z",
    "updated": "2023-10-30T17:52:18Z",
    "doi": null
  },
  "2211.16940": {
    "id": "http://arxiv.org/abs/2211.16940v3",
    "title": "DiffPose: Toward More Reliable 3D Pose Estimation",
    "authors": [
      "Jia Gong",
      "Lin Geng Foo",
      "Zhipeng Fan",
      "Qiuhong Ke",
      "Hossein Rahmani",
      "Jun Liu"
    ],
    "abstract": "  Monocular 3D human pose estimation is quite challenging due to the inherent\nambiguity and occlusion, which often lead to high uncertainty and\nindeterminacy. On the other hand, diffusion models have recently emerged as an\neffective tool for generating high-quality images from noise. Inspired by their\ncapability, we explore a novel pose estimation framework (DiffPose) that\nformulates 3D pose estimation as a reverse diffusion process. We incorporate\nnovel designs into our DiffPose to facilitate the diffusion process for 3D pose\nestimation: a pose-specific initialization of pose uncertainty distributions, a\nGaussian Mixture Model-based forward diffusion process, and a\ncontext-conditioned reverse diffusion process. Our proposed DiffPose\nsignificantly outperforms existing methods on the widely used pose estimation\nbenchmarks Human3.6M and MPI-INF-3DHP. Project page:\nhttps://gongjia0208.github.io/Diffpose/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-30T12:22:22Z",
    "updated": "2023-04-09T06:46:06Z",
    "doi": null
  },
  "2208.07339": {
    "id": "http://arxiv.org/abs/2208.07339v2",
    "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
    "authors": [
      "Tim Dettmers",
      "Mike Lewis",
      "Younes Belkada",
      "Luke Zettlemoyer"
    ],
    "abstract": "  Large language models have been widely adopted but require significant GPU\nmemory for inference. We develop a procedure for Int8 matrix multiplication for\nfeed-forward and attention projection layers in transformers, which cut the\nmemory needed for inference by half while retaining full precision performance.\nWith our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted\nto Int8, and used immediately without performance degradation. This is made\npossible by understanding and working around properties of highly systematic\nemergent features in transformer language models that dominate attention and\ntransformer predictive performance. To cope with these features, we develop a\ntwo-part quantization procedure, LLM.int8(). We first use vector-wise\nquantization with separate normalization constants for each inner product in\nthe matrix multiplication, to quantize most of the features. However, for the\nemergent outliers, we also include a new mixed-precision decomposition scheme,\nwhich isolates the outlier feature dimensions into a 16-bit matrix\nmultiplication while still more than 99.9% of values are multiplied in 8-bit.\nUsing LLM.int8(), we show empirically it is possible to perform inference in\nLLMs with up to 175B parameters without any performance degradation. This\nresult makes such models much more accessible, for example making it possible\nto use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our\nsoftware.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-15T17:08:50Z",
    "updated": "2022-11-10T18:14:31Z",
    "doi": null
  },
  "2203.10314": {
    "id": "http://arxiv.org/abs/2203.10314v1",
    "title": "Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from\n  Point Clouds",
    "authors": [
      "Chenhang He",
      "Ruihuang Li",
      "Shuai Li",
      "Lei Zhang"
    ],
    "abstract": "  Transformer has demonstrated promising performance in many 2D vision tasks.\nHowever, it is cumbersome to compute the self-attention on large-scale point\ncloud data because point cloud is a long sequence and unevenly distributed in\n3D space. To solve this issue, existing methods usually compute self-attention\nlocally by grouping the points into clusters of the same size, or perform\nconvolutional self-attention on a discretized representation. However, the\nformer results in stochastic point dropout, while the latter typically has\nnarrow attention fields. In this paper, we propose a novel voxel-based\narchitecture, namely Voxel Set Transformer (VoxSeT), to detect 3D objects from\npoint clouds by means of set-to-set translation. VoxSeT is built upon a\nvoxel-based set attention (VSA) module, which reduces the self-attention in\neach voxel by two cross-attentions and models features in a hidden space\ninduced by a group of latent codes. With the VSA module, VoxSeT can manage\nvoxelized point clusters with arbitrary size in a wide range, and process them\nin parallel with linear complexity. The proposed VoxSeT integrates the high\nperformance of transformer with the efficiency of voxel-based model, which can\nbe used as a good alternative to the convolutional and point-based backbones.\nVoxSeT reports competitive results on the KITTI and Waymo detection benchmarks.\nThe source codes can be found at \\url{https://github.com/skyhehe123/VoxSeT}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-19T12:31:46Z",
    "updated": "2022-03-19T12:31:46Z",
    "doi": null
  },
  "2407.11288": {
    "id": "http://arxiv.org/abs/2407.11288v1",
    "title": "Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion\n  Models in Inverse Problems",
    "authors": [
      "Ya\u015far Utku Al\u00e7alar",
      "Mehmet Ak\u00e7akaya"
    ],
    "abstract": "  Diffusion models have emerged as powerful generative techniques for solving\ninverse problems. Despite their success in a variety of inverse problems in\nimaging, these models require many steps to converge, leading to slow inference\ntime. Recently, there has been a trend in diffusion models for employing\nsophisticated noise schedules that involve more frequent iterations of\ntimesteps at lower noise levels, thereby improving image generation and\nconvergence speed. However, application of these ideas for solving inverse\nproblems with diffusion models remain challenging, as these noise schedules do\nnot perform well when using empirical tuning for the forward model\nlog-likelihood term weights. To tackle these challenges, we propose zero-shot\napproximate posterior sampling (ZAPS) that leverages connections to zero-shot\nphysics-driven deep learning. ZAPS fixes the number of sampling steps, and uses\nzero-shot training with a physics-guided loss function to learn log-likelihood\nweights at each irregular timestep. We apply ZAPS to the recently proposed\ndiffusion posterior sampling method as baseline, though ZAPS can also be used\nwith other posterior sampling diffusion models. We further approximate the\nHessian of the logarithm of the prior using a diagonalization approach with\nlearnable diagonal entries for computational efficiency. These parameters are\noptimized over a fixed number of epochs with a given computational budget. Our\nresults for various noisy inverse problems, including Gaussian and motion\ndeblurring, inpainting, and super-resolution show that ZAPS reduces inference\ntime, provides robustness to irregular noise schedules and improves\nreconstruction quality. Code is available at https://github.com/ualcalar17/ZAPS\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-16T00:09:37Z",
    "updated": "2024-07-16T00:09:37Z",
    "doi": null
  },
  "2312.12575": {
    "id": "http://arxiv.org/abs/2312.12575v3",
    "title": "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities\n  (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks",
    "authors": [
      "Saad Ullah",
      "Mingji Han",
      "Saurabh Pujar",
      "Hammond Pearce",
      "Ayse Coskun",
      "Gianluca Stringhini"
    ],
    "abstract": "  Large Language Models (LLMs) have been suggested for use in automated\nvulnerability repair, but benchmarks showing they can consistently identify\nsecurity-related bugs are lacking. We thus develop SecLLMHolmes, a fully\nautomated evaluation framework that performs the most detailed investigation to\ndate on whether LLMs can reliably identify and reason about security-related\nbugs. We construct a set of 228 code scenarios and analyze eight of the most\ncapable LLMs across eight different investigative dimensions using our\nframework. Our evaluation shows LLMs provide non-deterministic responses,\nincorrect and unfaithful reasoning, and perform poorly in real-world scenarios.\nMost importantly, our findings reveal significant non-robustness in even the\nmost advanced models like `PaLM2' and `GPT-4': by merely changing function or\nvariable names, or by the addition of library functions in the source code,\nthese models can yield incorrect answers in 26% and 17% of cases, respectively.\nThese findings demonstrate that further LLM advances are needed before LLMs can\nbe used as general purpose security assistants.\n",
    "categories": [
      {
        "@term": "cs.CR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-19T20:19:43Z",
    "updated": "2024-07-24T07:49:14Z",
    "doi": null
  },
  "2310.01602": {
    "id": "http://arxiv.org/abs/2310.01602v1",
    "title": "CAT-LM: Training Language Models on Aligned Code And Tests",
    "authors": [
      "Nikitha Rao",
      "Kush Jain",
      "Uri Alon",
      "Claire Le Goues",
      "Vincent J. Hellendoorn"
    ],
    "abstract": "  Testing is an integral part of the software development process. Yet, writing\ntests is time-consuming and therefore often neglected. Classical test\ngeneration tools such as EvoSuite generate behavioral test suites by optimizing\nfor coverage, but tend to produce tests that are hard to understand. Language\nmodels trained on code can generate code that is highly similar to that written\nby humans, but current models are trained to generate each file separately, as\nis standard practice in natural language processing, and thus fail to consider\nthe code-under-test context when producing a test file. In this work, we\npropose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style\nlanguage model with 2.7 Billion parameters, trained on a corpus of Python and\nJava projects. We utilize a novel pretraining signal that explicitly considers\nthe mapping between code and test files when available. We also drastically\nincrease the maximum sequence length of inputs to 8,192 tokens, 4x more than\ntypical code generation models, to ensure that the code context is available to\nthe model when generating test code. We analyze its usefulness for realistic\napplications, showing that sampling with filtering (e.g., by compilability,\ncoverage) allows it to efficiently produce tests that achieve coverage similar\nto ones written by developers while resembling their writing style. By\nutilizing the code context, CAT-LM generates more valid tests than even much\nlarger language models trained with more data (CodeGen 16B and StarCoder) and\nsubstantially outperforms a recent test-specific model (TeCo) at test\ncompletion. Overall, our work highlights the importance of incorporating\nsoftware-specific insights when training language models for code and paves the\nway to more powerful automated test generation.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-02T19:52:22Z",
    "updated": "2023-10-02T19:52:22Z",
    "doi": null
  },
  "2408.10161": {
    "id": "http://arxiv.org/abs/2408.10161v2",
    "title": "NeuFlow v2: High-Efficiency Optical Flow Estimation on Edge Devices",
    "authors": [
      "Zhiyong Zhang",
      "Aniket Gupta",
      "Huaizu Jiang",
      "Hanumant Singh"
    ],
    "abstract": "  Real-time high-accuracy optical flow estimation is crucial for various\nreal-world applications. While recent learning-based optical flow methods have\nachieved high accuracy, they often come with significant computational costs.\nIn this paper, we propose a highly efficient optical flow method that balances\nhigh accuracy with reduced computational demands. Building upon NeuFlow v1, we\nintroduce new components including a much more light-weight backbone and a fast\nrefinement module. Both these modules help in keeping the computational demands\nlight while providing close to state of the art accuracy. Compares to other\nstate of the art methods, our model achieves a 10x-70x speedup while\nmaintaining comparable performance on both synthetic and real-world data. It is\ncapable of running at over 20 FPS on 512x384 resolution images on a Jetson Orin\nNano. The full training and evaluation code is available at\nhttps://github.com/neufieldrobotics/NeuFlow_v2.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-19T17:13:34Z",
    "updated": "2024-08-21T23:23:10Z",
    "doi": null
  },
  "2211.06146": {
    "id": "http://arxiv.org/abs/2211.06146v2",
    "title": "An unobtrusive quality supervision approach for medical image annotation",
    "authors": [
      "Sonja Kunzmann",
      "Mathias \u00d6ttl",
      "Prathmesh Madhu",
      "Felix Denzinger",
      "Andreas Maier"
    ],
    "abstract": "  Image annotation is one essential prior step to enable data-driven\nalgorithms. In medical imaging, having large and reliably annotated data sets\nis crucial to recognize various diseases robustly. However, annotator\nperformance varies immensely, thus impacts model training. Therefore, often\nmultiple annotators should be employed, which is however expensive and\nresource-intensive. Hence, it is desirable that users should annotate unseen\ndata and have an automated system to unobtrusively rate their performance\nduring this process. We examine such a system based on whole slide images\n(WSIs) showing lung fluid cells. We evaluate two methods the generation of\nsynthetic individual cell images: conditional Generative Adversarial Networks\nand Diffusion Models (DM). For qualitative and quantitative evaluation, we\nconduct a user study to highlight the suitability of generated cells. Users\ncould not detect 52.12% of generated images by DM proofing the feasibility to\nreplace the original cells with synthetic cells without being noticed.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-11T11:57:26Z",
    "updated": "2022-11-22T15:15:36Z",
    "doi": null
  },
  "1307.5551": {
    "id": "http://arxiv.org/abs/1307.5551v1",
    "title": "Regularized Discrete Optimal Transport",
    "authors": [
      "Sira Ferradans",
      "Nicolas Papadakis",
      "Gabriel Peyr\u00e9",
      "Jean-Fran\u00e7ois Aujol"
    ],
    "abstract": "  This article introduces a generalization of the discrete optimal transport,\nwith applications to color image manipulations. This new formulation includes a\nrelaxation of the mass conservation constraint and a regularization term. These\ntwo features are crucial for image processing tasks, which necessitate to take\ninto account families of multimodal histograms, with large mass variation\nacross modes.\n  The corresponding relaxed and regularized transportation problem is the\nsolution of a convex optimization problem. Depending on the regularization\nused, this minimization can be solved using standard linear programming methods\nor first order proximal splitting schemes.\n  The resulting transportation plan can be used as a color transfer map, which\nis robust to mass variation across images color palettes. Furthermore, the\nregularization of the transport plan helps to remove colorization artifacts due\nto noise amplification.\n  We also extend this framework to the computation of barycenters of\ndistributions. The barycenter is the solution of an optimization problem, which\nis separately convex with respect to the barycenter and the transportation\nplans, but not jointly convex. A block coordinate descent scheme converges to a\nstationary point of the energy. We show that the resulting algorithm can be\nused for color normalization across several images. The relaxed and regularized\nbarycenter defines a common color palette for those images. Applying color\ntransfer toward this average palette performs a color normalization of the\ninput images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DM",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.OC",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2013-07-21T17:55:10Z",
    "updated": "2013-07-21T17:55:10Z",
    "doi": null
  },
  "2311.15510": {
    "id": "http://arxiv.org/abs/2311.15510v2",
    "title": "CaesarNeRF: Calibrated Semantic Representation for Few-shot\n  Generalizable Neural Rendering",
    "authors": [
      "Haidong Zhu",
      "Tianyu Ding",
      "Tianyi Chen",
      "Ilya Zharkov",
      "Ram Nevatia",
      "Luming Liang"
    ],
    "abstract": "  Generalizability and few-shot learning are key challenges in Neural Radiance\nFields (NeRF), often due to the lack of a holistic understanding in pixel-level\nrendering. We introduce CaesarNeRF, an end-to-end approach that leverages\nscene-level CAlibratEd SemAntic Representation along with pixel-level\nrepresentations to advance few-shot, generalizable neural rendering,\nfacilitating a holistic understanding without compromising high-quality\ndetails. CaesarNeRF explicitly models pose differences of reference views to\ncombine scene-level semantic representations, providing a calibrated holistic\nunderstanding. This calibration process aligns various viewpoints with precise\nlocation and is further enhanced by sequential refinement to capture varying\ndetails. Extensive experiments on public datasets, including LLFF, Shiny,\nmip-NeRF 360, and MVImgNet, show that CaesarNeRF delivers state-of-the-art\nperformance across varying numbers of reference views, proving effective even\nwith a single reference image.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-27T03:09:58Z",
    "updated": "2024-07-10T03:41:39Z",
    "doi": null
  },
  "2211.03264": {
    "id": "http://arxiv.org/abs/2211.03264v3",
    "title": "Few-shot Image Generation with Diffusion Models",
    "authors": [
      "Jingyuan Zhu",
      "Huimin Ma",
      "Jiansheng Chen",
      "Jian Yuan"
    ],
    "abstract": "  Denoising diffusion probabilistic models (DDPMs) have been proven capable of\nsynthesizing high-quality images with remarkable diversity when trained on\nlarge amounts of data. However, to our knowledge, few-shot image generation\ntasks have yet to be studied with DDPM-based approaches. Modern approaches are\nmainly built on Generative Adversarial Networks (GANs) and adapt models\npre-trained on large source domains to target domains using a few available\nsamples. In this paper, we make the first attempt to study when do DDPMs\noverfit and suffer severe diversity degradation as training data become scarce.\nThen we fine-tune DDPMs pre-trained on large source domains to solve the\noverfitting problem when training data is limited. Although the directly\nfine-tuned models accelerate convergence and improve generation quality and\ndiversity compared with training from scratch, they still fail to retain some\ndiverse features and can only produce coarse images. Therefore, we design a\nDDPM pairwise adaptation (DDPM-PA) approach to optimize few-shot DDPM domain\nadaptation. DDPM-PA efficiently preserves information learned from source\ndomains by keeping the relative pairwise distances between generated samples\nduring adaptation. Besides, DDPM-PA enhances the learning of high-frequency\ndetails from source models and limited training data. DDPM-PA further improves\ngeneration quality and diversity and achieves results better than current\nstate-of-the-art GAN-based approaches. We demonstrate the effectiveness of our\napproach on a series of few-shot image generation tasks qualitatively and\nquantitatively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-07T02:18:27Z",
    "updated": "2023-03-07T05:43:56Z",
    "doi": null
  },
  "2209.11133": {
    "id": "http://arxiv.org/abs/2209.11133v2",
    "title": "PACT: Perception-Action Causal Transformer for Autoregressive Robotics\n  Pre-Training",
    "authors": [
      "Rogerio Bonatti",
      "Sai Vemprala",
      "Shuang Ma",
      "Felipe Frujeri",
      "Shuhang Chen",
      "Ashish Kapoor"
    ],
    "abstract": "  Robotics has long been a field riddled with complex systems architectures\nwhose modules and connections, whether traditional or learning-based, require\nsignificant human expertise and prior knowledge. Inspired by large pre-trained\nlanguage models, this work introduces a paradigm for pre-training a general\npurpose representation that can serve as a starting point for multiple tasks on\na given robot. We present the Perception-Action Causal Transformer (PACT), a\ngenerative transformer-based architecture that aims to build representations\ndirectly from robot data in a self-supervised fashion. Through autoregressive\nprediction of states and actions over time, our model implicitly encodes\ndynamics and behaviors for a particular robot. Our experimental evaluation\nfocuses on the domain of mobile agents, where we show that this robot-specific\nrepresentation can function as a single starting point to achieve distinct\ntasks such as safe navigation, localization and mapping. We evaluate two form\nfactors: a wheeled robot that uses a LiDAR sensor as perception input (MuSHR),\nand a simulated agent that uses first-person RGB images (Habitat). We show that\nfinetuning small task-specific networks on top of the larger pretrained model\nresults in significantly better performance compared to training a single model\nfrom scratch for all tasks simultaneously, and comparable performance to\ntraining a separate large model for each task independently. By sharing a\ncommon good-quality representation across tasks we can lower overall model\ncapacity and speed up the real-time deployment of such systems.\n",
    "categories": [
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-22T16:20:17Z",
    "updated": "2022-09-23T18:14:39Z",
    "doi": null
  },
  "1603.01670": {
    "id": "http://arxiv.org/abs/1603.01670v2",
    "title": "Network Morphism",
    "authors": [
      "Tao Wei",
      "Changhu Wang",
      "Yong Rui",
      "Chang Wen Chen"
    ],
    "abstract": "  We present in this paper a systematic study on how to morph a well-trained\nneural network to a new one so that its network function can be completely\npreserved. We define this as \\emph{network morphism} in this research. After\nmorphing a parent network, the child network is expected to inherit the\nknowledge from its parent network and also has the potential to continue\ngrowing into a more powerful one with much shortened training time. The first\nrequirement for this network morphism is its ability to handle diverse morphing\ntypes of networks, including changes of depth, width, kernel size, and even\nsubnet. To meet this requirement, we first introduce the network morphism\nequations, and then develop novel morphing algorithms for all these morphing\ntypes for both classic and convolutional neural networks. The second\nrequirement for this network morphism is its ability to deal with non-linearity\nin a network. We propose a family of parametric-activation functions to\nfacilitate the morphing of any continuous non-linear activation neurons.\nExperimental results on benchmark datasets and typical neural networks\ndemonstrate the effectiveness of the proposed network morphism scheme.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.NE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-03-05T02:06:43Z",
    "updated": "2016-03-08T16:36:00Z",
    "doi": null
  },
  "2109.04398": {
    "id": "http://arxiv.org/abs/2109.04398v4",
    "title": "Neural-IMLS: Self-supervised Implicit Moving Least-Squares Network for\n  Surface Reconstruction",
    "authors": [
      "Zixiong Wang",
      "Pengfei Wang",
      "Pengshuai Wang",
      "Qiujie Dong",
      "Junjie Gao",
      "Shuangmin Chen",
      "Shiqing Xin",
      "Changhe Tu",
      "Wenping Wang"
    ],
    "abstract": "  Surface reconstruction is very challenging when the input point clouds,\nparticularly real scans, are noisy and lack normals. Observing that the\nMultilayer Perceptron (MLP) and the implicit moving least-square function\n(IMLS) provide a dual representation of the underlying surface, we introduce\nNeural-IMLS, a novel approach that directly learns the noise-resistant signed\ndistance function (SDF) from unoriented raw point clouds in a self-supervised\nfashion. We use the IMLS to regularize the distance values reported by the MLP\nwhile using the MLP to regularize the normals of the data points for running\nthe IMLS. We also prove that at the convergence, our neural network, benefiting\nfrom the mutual learning mechanism between the MLP and the IMLS, produces a\nfaithful SDF whose zero-level set approximates the underlying surface. We\nconducted extensive experiments on various benchmarks, including synthetic\nscans and real scans. The experimental results show that {\\em Neural-IMLS} can\nreconstruct faithful shapes on various benchmarks with noise and missing parts.\nThe source code can be found at~\\url{https://github.com/bearprin/Neural-IMLS}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-09T16:37:01Z",
    "updated": "2023-09-06T06:47:49Z",
    "doi": "10.1109/TVCG.2023.3284233"
  },
  "2202.00660": {
    "id": "http://arxiv.org/abs/2202.00660v3",
    "title": "Interactron: Embodied Adaptive Object Detection",
    "authors": [
      "Klemen Kotar",
      "Roozbeh Mottaghi"
    ],
    "abstract": "  Over the years various methods have been proposed for the problem of object\ndetection. Recently, we have witnessed great strides in this domain owing to\nthe emergence of powerful deep neural networks. However, there are typically\ntwo main assumptions common among these approaches. First, the model is trained\non a fixed training set and is evaluated on a pre-recorded test set. Second,\nthe model is kept frozen after the training phase, so no further updates are\nperformed after the training is finished. These two assumptions limit the\napplicability of these methods to real-world settings. In this paper, we\npropose Interactron, a method for adaptive object detection in an interactive\nsetting, where the goal is to perform object detection in images observed by an\nembodied agent navigating in different environments. Our idea is to continue\ntraining during inference and adapt the model at test time without any explicit\nsupervision via interacting with the environment. Our adaptive object detection\nmodel provides a 7.2 point improvement in AP (and 12.7 points in AP50) over\nDETR, a recent, high-performance object detector. Moreover, we show that our\nobject detection model adapts to environments with completely different\nappearance characteristics, and performs well in them. The code is available\nat: https://github.com/allenai/interactron .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-02-01T18:56:14Z",
    "updated": "2022-07-22T19:37:10Z",
    "doi": null
  },
  "1909.01387": {
    "id": "http://arxiv.org/abs/1909.01387v1",
    "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration\n  Problems",
    "authors": [
      "Tom Le Paine",
      "Caglar Gulcehre",
      "Bobak Shahriari",
      "Misha Denil",
      "Matt Hoffman",
      "Hubert Soyer",
      "Richard Tanburn",
      "Steven Kapturowski",
      "Neil Rabinowitz",
      "Duncan Williams",
      "Gabriel Barth-Maron",
      "Ziyu Wang",
      "Nando de Freitas",
      "Worlds Team"
    ],
    "abstract": "  This paper introduces R2D3, an agent that makes efficient use of\ndemonstrations to solve hard exploration problems in partially observable\nenvironments with highly variable initial conditions. We also introduce a suite\nof eight tasks that combine these three properties, and show that R2D3 can\nsolve several of the tasks where other state of the art methods (both with and\nwithout demonstrations) fail to see even a single successful trajectory after\ntens of billions of steps of exploration.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-09-03T18:20:48Z",
    "updated": "2019-09-03T18:20:48Z",
    "doi": null
  },
  "2309.07920": {
    "id": "http://arxiv.org/abs/2309.07920v2",
    "title": "Large-Vocabulary 3D Diffusion Model with Transformer",
    "authors": [
      "Ziang Cao",
      "Fangzhou Hong",
      "Tong Wu",
      "Liang Pan",
      "Ziwei Liu"
    ],
    "abstract": "  Creating diverse and high-quality 3D assets with an automatic generative\nmodel is highly desirable. Despite extensive efforts on 3D generation, most\nexisting works focus on the generation of a single category or a few\ncategories. In this paper, we introduce a diffusion-based feed-forward\nframework for synthesizing massive categories of real-world 3D objects with a\nsingle generative model. Notably, there are three major challenges for this\nlarge-vocabulary 3D generation: a) the need for expressive yet efficient 3D\nrepresentation; b) large diversity in geometry and texture across categories;\nc) complexity in the appearances of real-world objects. To this end, we propose\na novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, for\nhandling challenges via three aspects. 1) Considering efficiency and\nrobustness, we adopt a revised triplane representation and improve the fitting\nspeed and accuracy. 2) To handle the drastic variations in geometry and\ntexture, we regard the features of all 3D objects as a combination of\ngeneralized 3D knowledge and specialized 3D features. To extract generalized 3D\nknowledge from diverse categories, we propose a novel 3D-aware transformer with\nshared cross-plane attention. It learns the cross-plane relations across\ndifferent planes and aggregates the generalized 3D knowledge with specialized\n3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhance\nthe generalized 3D knowledge in the encoded triplanes for handling categories\nwith complex appearances. Extensive experiments on ShapeNet and OmniObject3D\n(over 200 diverse real-world categories) convincingly demonstrate that a single\nDiffTF model achieves state-of-the-art large-vocabulary 3D object generation\nperformance with large diversity, rich semantics, and high quality.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-14T17:59:53Z",
    "updated": "2023-09-15T07:56:34Z",
    "doi": null
  },
  "2210.05063": {
    "id": "http://arxiv.org/abs/2210.05063v2",
    "title": "Improving Dense Contrastive Learning with Dense Negative Pairs",
    "authors": [
      "Berk Iskender",
      "Zhenlin Xu",
      "Simon Kornblith",
      "En-Hung Chu",
      "Maryam Khademi"
    ],
    "abstract": "  Many contrastive representation learning methods learn a single global\nrepresentation of an entire image. However, dense contrastive representation\nlearning methods such as DenseCL (Wang et al., 2021) can learn better\nrepresentations for tasks requiring stronger spatial localization of features,\nsuch as multi-label classification, detection, and segmentation. In this work,\nwe study how to improve the quality of the representations learned by DenseCL\nby modifying the training scheme and objective function, and propose DenseCL++.\nWe also conduct several ablation studies to better understand the effects of:\n(i) various techniques to form dense negative pairs among augmentations of\ndifferent images, (ii) cross-view dense negative and positive pairs, and (iii)\nan auxiliary reconstruction task. Our results show 3.5% and 4% mAP improvement\nover SimCLR (Chen et al., 2020a) andDenseCL in COCO multi-label classification.\nIn COCO and VOC segmentation tasks, we achieve 1.8% and 0.7% mIoU improvements\nover SimCLR, respectively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-11T00:26:59Z",
    "updated": "2023-01-10T23:47:45Z",
    "doi": null
  },
  "1705.10413": {
    "id": "http://arxiv.org/abs/1705.10413v1",
    "title": "Learning to Generate Chairs with Generative Adversarial Nets",
    "authors": [
      "Evgeny Zamyatin",
      "Andrey Filchenkov"
    ],
    "abstract": "  Generative adversarial networks (GANs) has gained tremendous popularity\nlately due to an ability to reinforce quality of its predictive model with\ngenerated objects and the quality of the generative model with and supervised\nfeedback. GANs allow to synthesize images with a high degree of realism.\nHowever, the learning process of such models is a very complicated optimization\nproblem and certain limitation for such models were found. It affects the\nchoice of certain layers and nonlinearities when designing architectures. In\nparticular, it does not allow to train convolutional GAN models with\nfully-connected hidden layers. In our work, we propose a modification of the\npreviously described set of rules, as well as new approaches to designing\narchitectures that will allow us to train more powerful GAN models. We show the\neffectiveness of our methods on the problem of synthesizing projections of 3D\nobjects with the possibility of interpolation by class and view point.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-05-29T23:15:50Z",
    "updated": "2017-05-29T23:15:50Z",
    "doi": null
  },
  "2205.10793": {
    "id": "http://arxiv.org/abs/2205.10793v2",
    "title": "Knowledge Distillation via the Target-aware Transformer",
    "authors": [
      "Sihao Lin",
      "Hongwei Xie",
      "Bing Wang",
      "Kaicheng Yu",
      "Xiaojun Chang",
      "Xiaodan Liang",
      "Gang Wang"
    ],
    "abstract": "  Knowledge distillation becomes a de facto standard to improve the performance\nof small neural networks. Most of the previous works propose to regress the\nrepresentational features from the teacher to the student in a one-to-one\nspatial matching fashion. However, people tend to overlook the fact that, due\nto the architecture differences, the semantic information on the same spatial\nlocation usually vary. This greatly undermines the underlying assumption of the\none-to-one distillation approach. To this end, we propose a novel one-to-all\nspatial matching knowledge distillation approach. Specifically, we allow each\npixel of the teacher feature to be distilled to all spatial locations of the\nstudent features given its similarity, which is generated from a target-aware\ntransformer. Our approach surpasses the state-of-the-art methods by a\nsignificant margin on various computer vision benchmarks, such as ImageNet,\nPascal VOC and COCOStuff10k. Code is available at\nhttps://github.com/sihaoevery/TaT.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-22T10:26:54Z",
    "updated": "2024-04-08T16:59:24Z",
    "doi": null
  },
  "2304.11029": {
    "id": "http://arxiv.org/abs/2304.11029v4",
    "title": "CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic\n  Music Information Retrieval",
    "authors": [
      "Shangda Wu",
      "Dingyao Yu",
      "Xu Tan",
      "Maosong Sun"
    ],
    "abstract": "  We introduce CLaMP: Contrastive Language-Music Pre-training, which learns\ncross-modal representations between natural language and symbolic music using a\nmusic encoder and a text encoder trained jointly with a contrastive loss. To\npre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs.\nIt employed text dropout as a data augmentation technique and bar patching to\nefficiently represent music data which reduces sequence length to less than\n10\\%. In addition, we developed a masked music model pre-training objective to\nenhance the music encoder's comprehension of musical context and structure.\nCLaMP integrates textual information to enable semantic search and zero-shot\nclassification for symbolic music, surpassing the capabilities of previous\nmodels. To support the evaluation of semantic search and music classification,\nwe publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in\nABC notation, each accompanied by a title, artist, genre, and description. In\ncomparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP\ndemonstrated comparable or superior performance on score-oriented datasets. Our\nmodels and code are available at\nhttps://github.com/microsoft/muzic/tree/main/clamp.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-21T15:23:00Z",
    "updated": "2023-10-18T17:16:28Z",
    "doi": null
  },
  "1612.06778": {
    "id": "http://arxiv.org/abs/1612.06778v3",
    "title": "SCDV : Sparse Composite Document Vectors using soft clustering over\n  distributional representations",
    "authors": [
      "Dheeraj Mekala",
      "Vivek Gupta",
      "Bhargavi Paranjape",
      "Harish Karnick"
    ],
    "abstract": "  We present a feature vector formation technique for documents - Sparse\nComposite Document Vector (SCDV) - which overcomes several shortcomings of the\ncurrent distributional paragraph vector representations that are widely used\nfor text representation. In SCDV, word embedding's are clustered to capture\nmultiple semantic contexts in which words occur. They are then chained together\nto form document topic-vectors that can express complex, multi-topic documents.\nThrough extensive experiments on multi-class and multi-label classification\ntasks, we outperform the previous state-of-the-art method, NTSG (Liu et al.,\n2015a). We also show that SCDV embedding's perform well on heterogeneous tasks\nlike Topic Coherence, context-sensitive Learning and Information Retrieval.\nMoreover, we achieve significant reduction in training and prediction times\ncompared to other representation methods. SCDV achieves best of both worlds -\nbetter performance with lower time and space complexity.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-12-20T17:38:57Z",
    "updated": "2017-05-12T09:48:04Z",
    "doi": null
  },
  "2007.01758": {
    "id": "http://arxiv.org/abs/2007.01758v1",
    "title": "Collaborative Learning for Faster StyleGAN Embedding",
    "authors": [
      "Shanyan Guan",
      "Ying Tai",
      "Bingbing Ni",
      "Feida Zhu",
      "Feiyue Huang",
      "Xiaokang Yang"
    ],
    "abstract": "  The latent code of the recent popular model StyleGAN has learned disentangled\nrepresentations thanks to the multi-layer style-based generator. Embedding a\ngiven image back to the latent space of StyleGAN enables wide interesting\nsemantic image editing applications. Although previous works are able to yield\nimpressive inversion results based on an optimization framework, which however\nsuffers from the efficiency issue. In this work, we propose a novel\ncollaborative learning framework that consists of an efficient embedding\nnetwork and an optimization-based iterator. On one hand, with the progress of\ntraining, the embedding network gives a reasonable latent code initialization\nfor the iterator. On the other hand, the updated latent code from the iterator\nin turn supervises the embedding network. In the end, high-quality latent code\ncan be obtained efficiently with a single forward pass through our embedding\nnetwork. Extensive experiments demonstrate the effectiveness and efficiency of\nour work.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-03T15:27:37Z",
    "updated": "2020-07-03T15:27:37Z",
    "doi": null
  },
  "1604.06174": {
    "id": "http://arxiv.org/abs/1604.06174v2",
    "title": "Training Deep Nets with Sublinear Memory Cost",
    "authors": [
      "Tianqi Chen",
      "Bing Xu",
      "Chiyuan Zhang",
      "Carlos Guestrin"
    ],
    "abstract": "  We propose a systematic approach to reduce the memory consumption of deep\nneural network training. Specifically, we design an algorithm that costs\nO(sqrt(n)) memory to train a n layer network, with only the computational cost\nof an extra forward pass per mini-batch. As many of the state-of-the-art models\nhit the upper bound of the GPU memory, our algorithm allows deeper and more\ncomplex models to be explored, and helps advance the innovations in deep\nlearning research. We focus on reducing the memory cost to store the\nintermediate feature maps and gradients during training. Computation graph\nanalysis is used for automatic in-place operation and memory sharing\noptimizations. We show that it is possible to trade computation for memory -\ngiving a more memory efficient training algorithm with a little extra\ncomputation cost. In the extreme case, our analysis also shows that the memory\nconsumption can be reduced to O(log n) with as little as O(n log n) extra cost\nfor forward computation. Our experiments show that we can reduce the memory\ncost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent\nadditional running time cost on ImageNet problems. Similarly, significant\nmemory cost reduction is observed in training complex recurrent neural networks\non very long sequences.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-04-21T04:15:27Z",
    "updated": "2016-04-22T19:21:36Z",
    "doi": null
  },
  "1711.10337": {
    "id": "http://arxiv.org/abs/1711.10337v4",
    "title": "Are GANs Created Equal? A Large-Scale Study",
    "authors": [
      "Mario Lucic",
      "Karol Kurach",
      "Marcin Michalski",
      "Sylvain Gelly",
      "Olivier Bousquet"
    ],
    "abstract": "  Generative adversarial networks (GAN) are a powerful subclass of generative\nmodels. Despite a very rich research activity leading to numerous interesting\nGAN algorithms, it is still very hard to assess which algorithm(s) perform\nbetter than others. We conduct a neutral, multi-faceted large-scale empirical\nstudy on state-of-the art models and evaluation measures. We find that most\nmodels can reach similar scores with enough hyperparameter optimization and\nrandom restarts. This suggests that improvements can arise from a higher\ncomputational budget and tuning more than fundamental algorithmic changes. To\novercome some limitations of the current metrics, we also propose several data\nsets on which precision and recall can be computed. Our experimental results\nsuggest that future GAN research should be based on more systematic and\nobjective evaluation procedures. Finally, we did not find evidence that any of\nthe tested algorithms consistently outperforms the non-saturating GAN\nintroduced in \\cite{goodfellow2014generative}.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-11-28T15:19:53Z",
    "updated": "2018-10-29T15:34:15Z",
    "doi": null
  },
  "2207.02696": {
    "id": "http://arxiv.org/abs/2207.02696v1",
    "title": "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for\n  real-time object detectors",
    "authors": [
      "Chien-Yao Wang",
      "Alexey Bochkovskiy",
      "Hong-Yuan Mark Liao"
    ],
    "abstract": "  YOLOv7 surpasses all known object detectors in both speed and accuracy in the\nrange from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all\nknown real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6\nobject detector (56 FPS V100, 55.9% AP) outperforms both transformer-based\ndetector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed\nand 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask\nR-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as\nwell as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR,\nDeformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors\nin speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from\nscratch without using any other datasets or pre-trained weights. Source code is\nreleased in https://github.com/WongKinYiu/yolov7.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-06T14:01:58Z",
    "updated": "2022-07-06T14:01:58Z",
    "doi": null
  },
  "2312.00206": {
    "id": "http://arxiv.org/abs/2312.00206v2",
    "title": "SparseGS: Real-Time 360\u00b0 Sparse View Synthesis using Gaussian\n  Splatting",
    "authors": [
      "Haolin Xiong",
      "Sairisheek Muttukuru",
      "Rishi Upadhyay",
      "Pradyumna Chari",
      "Achuta Kadambi"
    ],
    "abstract": "  The problem of novel view synthesis has grown significantly in popularity\nrecently with the introduction of Neural Radiance Fields (NeRFs) and other\nimplicit scene representation methods. A recent advance, 3D Gaussian Splatting\n(3DGS), leverages an explicit representation to achieve real-time rendering\nwith high-quality results. However, 3DGS still requires an abundance of\ntraining views to generate a coherent scene representation. In few shot\nsettings, similar to NeRF, 3DGS tends to overfit to training views, causing\nbackground collapse and excessive floaters, especially as the number of\ntraining views are reduced. We propose a method to enable training coherent\n3DGS-based radiance fields of 360-degree scenes from sparse training views. We\nintegrate depth priors with generative and explicit constraints to reduce\nbackground collapse, remove floaters, and enhance consistency from unseen\nviewpoints. Experiments show that our method outperforms base 3DGS by 6.4% in\nLPIPS and by 12.2% in PSNR, and NeRF-based methods by at least 17.6% in LPIPS\non the MipNeRF-360 dataset with substantially less training and inference cost.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-30T21:38:22Z",
    "updated": "2024-05-13T05:11:37Z",
    "doi": null
  },
  "2011.06391": {
    "id": "http://arxiv.org/abs/2011.06391v2",
    "title": "FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph\n  Neural Networks",
    "authors": [
      "Md. Khaledur Rahman",
      "Majedul Haque Sujon",
      "Ariful Azad"
    ],
    "abstract": "  We develop a fused matrix multiplication kernel that unifies sampled\ndense-dense matrix multiplication and sparse-dense matrix multiplication under\na single operation called FusedMM. By using user-defined functions, FusedMM can\ncapture almost all computational patterns needed by popular graph embedding and\nGNN approaches. FusedMM is an order of magnitude faster than its equivalent\nkernels in Deep Graph Library. The superior performance of FusedMM comes from\nthe low-level vectorized kernels, a suitable load balancing scheme and an\nefficient utilization of the memory bandwidth. FusedMM can tune its performance\nusing a code generator and perform equally well on Intel, AMD and ARM\nprocessors. FusedMM speeds up an end-to-end graph embedding algorithm by up to\n28x on different processors.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-11-07T18:06:57Z",
    "updated": "2021-10-27T01:35:27Z",
    "doi": null
  },
  "2212.04098": {
    "id": "http://arxiv.org/abs/2212.04098v3",
    "title": "EPCL: Frozen CLIP Transformer is An Efficient Point Cloud Encoder",
    "authors": [
      "Xiaoshui Huang",
      "Zhou Huang",
      "Sheng Li",
      "Wentao Qu",
      "Tong He",
      "Yuenan Hou",
      "Yifan Zuo",
      "Wanli Ouyang"
    ],
    "abstract": "  The pretrain-finetune paradigm has achieved great success in NLP and 2D image\nfields because of the high-quality representation ability and transferability\nof their pretrained models. However, pretraining such a strong model is\ndifficult in the 3D point cloud field due to the limited amount of point cloud\nsequences. This paper introduces \\textbf{E}fficient \\textbf{P}oint\n\\textbf{C}loud \\textbf{L}earning (EPCL), an effective and efficient point cloud\nlearner for directly training high-quality point cloud models with a frozen\nCLIP transformer. Our EPCL connects the 2D and 3D modalities by semantically\naligning the image features and point cloud features without paired 2D-3D data.\nSpecifically, the input point cloud is divided into a series of local patches,\nwhich are converted to token embeddings by the designed point cloud tokenizer.\nThese token embeddings are concatenated with a task token and fed into the\nfrozen CLIP transformer to learn point cloud representation. The intuition is\nthat the proposed point cloud tokenizer projects the input point cloud into a\nunified token space that is similar to the 2D images. Comprehensive experiments\non 3D detection, semantic segmentation, classification and few-shot learning\ndemonstrate that the CLIP transformer can serve as an efficient point cloud\nencoder and our method achieves promising performance on both indoor and\noutdoor benchmarks. In particular, performance gains brought by our EPCL are\n$\\textbf{19.7}$ AP$_{50}$ on ScanNet V2 detection, $\\textbf{4.4}$ mIoU on S3DIS\nsegmentation and $\\textbf{1.2}$ mIoU on SemanticKITTI segmentation compared to\ncontemporary pretrained models. Code is available at\n\\url{https://github.com/XiaoshuiHuang/EPCL}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-08T06:27:11Z",
    "updated": "2023-12-10T16:47:58Z",
    "doi": null
  },
  "2003.11080": {
    "id": "http://arxiv.org/abs/2003.11080v5",
    "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating\n  Cross-lingual Generalization",
    "authors": [
      "Junjie Hu",
      "Sebastian Ruder",
      "Aditya Siddhant",
      "Graham Neubig",
      "Orhan Firat",
      "Melvin Johnson"
    ],
    "abstract": "  Much recent progress in applications of machine learning models to NLP has\nbeen driven by benchmarks that evaluate models across a wide variety of tasks.\nHowever, these broad-coverage benchmarks have been mostly limited to English,\nand despite an increasing interest in multilingual models, a benchmark that\nenables the comprehensive evaluation of such methods on a diverse range of\nlanguages and tasks is still missing. To this end, we introduce the\nCross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a\nmulti-task benchmark for evaluating the cross-lingual generalization\ncapabilities of multilingual representations across 40 languages and 9 tasks.\nWe demonstrate that while models tested on English reach human performance on\nmany tasks, there is still a sizable gap in the performance of cross-lingually\ntransferred models, particularly on syntactic and sentence retrieval tasks.\nThere is also a wide spread of results across languages. We release the\nbenchmark to encourage research on cross-lingual learning methods that transfer\nlinguistic knowledge across a diverse and representative set of languages and\ntasks.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-03-24T19:09:37Z",
    "updated": "2020-09-04T17:56:31Z",
    "doi": null
  },
  "1906.02940": {
    "id": "http://arxiv.org/abs/1906.02940v3",
    "title": "Selfie: Self-supervised Pretraining for Image Embedding",
    "authors": [
      "Trieu H. Trinh",
      "Minh-Thang Luong",
      "Quoc V. Le"
    ],
    "abstract": "  We introduce a pretraining technique called Selfie, which stands for SELFie\nsupervised Image Embedding. Selfie generalizes the concept of masked language\nmodeling of BERT (Devlin et al., 2019) to continuous data, such as images, by\nmaking use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given\nmasked-out patches in an input image, our method learns to select the correct\npatch, among other \"distractor\" patches sampled from the same image, to fill in\nthe masked location. This classification objective sidesteps the need for\npredicting exact pixel values of the target patches. The pretraining\narchitecture of Selfie includes a network of convolutional blocks to process\npatches followed by an attention pooling network to summarize the content of\nunmasked patches before predicting masked ones. During finetuning, we reuse the\nconvolutional weights found by pretraining. We evaluate Selfie on three\nbenchmarks (CIFAR-10, ImageNet 32 x 32, and ImageNet 224 x 224) with varying\namounts of labeled data, from 5% to 100% of the training sets. Our pretraining\nmethod provides consistent improvements to ResNet-50 across all settings\ncompared to the standard supervised training of the same network. Notably, on\nImageNet 224 x 224 with 60 examples per class (5%), our method improves the\nmean accuracy of ResNet-50 from 35.6% to 46.7%, an improvement of 11.1 points\nin absolute accuracy. Our pretraining method also improves ResNet-50 training\nstability, especially on low data regime, by significantly lowering the\nstandard deviation of test accuracies across different runs.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-06-07T07:47:24Z",
    "updated": "2019-07-27T08:03:46Z",
    "doi": null
  },
  "2108.07058": {
    "id": "http://arxiv.org/abs/2108.07058v2",
    "title": "FaPN: Feature-aligned Pyramid Network for Dense Image Prediction",
    "authors": [
      "Shihua Huang",
      "Zhichao Lu",
      "Ran Cheng",
      "Cheng He"
    ],
    "abstract": "  Recent advancements in deep neural networks have made remarkable\nleap-forwards in dense image prediction. However, the issue of feature\nalignment remains as neglected by most existing approaches for simplicity.\nDirect pixel addition between upsampled and local features leads to feature\nmaps with misaligned contexts that, in turn, translate to mis-classifications\nin prediction, especially on object boundaries. In this paper, we propose a\nfeature alignment module that learns transformation offsets of pixels to\ncontextually align upsampled higher-level features; and another feature\nselection module to emphasize the lower-level features with rich spatial\ndetails. We then integrate these two modules in a top-down pyramidal\narchitecture and present the Feature-aligned Pyramid Network (FaPN). Extensive\nexperimental evaluations on four dense prediction tasks and four datasets have\ndemonstrated the efficacy of FaPN, yielding an overall improvement of 1.2 - 2.6\npoints in AP / mIoU over FPN when paired with Faster / Mask R-CNN. In\nparticular, our FaPN achieves the state-of-the-art of 56.7% mIoU on ADE20K when\nintegrated within Mask-Former. The code is available from\nhttps://github.com/EMI-Group/FaPN.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-08-16T12:52:42Z",
    "updated": "2021-08-17T13:11:34Z",
    "doi": null
  },
  "2107.06263": {
    "id": "http://arxiv.org/abs/2107.06263v3",
    "title": "CMT: Convolutional Neural Networks Meet Vision Transformers",
    "authors": [
      "Jianyuan Guo",
      "Kai Han",
      "Han Wu",
      "Yehui Tang",
      "Xinghao Chen",
      "Yunhe Wang",
      "Chang Xu"
    ],
    "abstract": "  Vision transformers have been successfully applied to image recognition tasks\ndue to their ability to capture long-range dependencies within an image.\nHowever, there are still gaps in both performance and computational cost\nbetween transformers and existing convolutional neural networks (CNNs). In this\npaper, we aim to address this issue and develop a network that can outperform\nnot only the canonical transformers, but also the high-performance\nconvolutional models. We propose a new transformer based hybrid network by\ntaking advantage of transformers to capture long-range dependencies, and of\nCNNs to model local features. Furthermore, we scale it to obtain a family of\nmodels, called CMTs, obtaining much better accuracy and efficiency than\nprevious convolution and transformer based models. In particular, our CMT-S\nachieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on\nFLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S\nalso generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%),\nand other challenging vision datasets such as COCO (44.3% mAP), with\nconsiderably less computational cost.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-07-13T17:47:19Z",
    "updated": "2022-06-14T14:05:23Z",
    "doi": null
  },
  "2209.04881": {
    "id": "http://arxiv.org/abs/2209.04881v1",
    "title": "On The Computational Complexity of Self-Attention",
    "authors": [
      "Feyza Duman Keles",
      "Pruthuvi Mahesakya Wijewardena",
      "Chinmay Hegde"
    ],
    "abstract": "  Transformer architectures have led to remarkable progress in many\nstate-of-art applications. However, despite their successes, modern\ntransformers rely on the self-attention mechanism, whose time- and\nspace-complexity is quadratic in the length of the input. Several approaches\nhave been proposed to speed up self-attention mechanisms to achieve\nsub-quadratic running time; however, the large majority of these works are not\naccompanied by rigorous error guarantees. In this work, we establish lower\nbounds on the computational complexity of self-attention in a number of\nscenarios. We prove that the time complexity of self-attention is necessarily\nquadratic in the input length, unless the Strong Exponential Time Hypothesis\n(SETH) is false. This argument holds even if the attention computation is\nperformed only approximately, and for a variety of attention mechanisms. As a\ncomplement to our lower bounds, we show that it is indeed possible to\napproximate dot-product self-attention using finite Taylor series in\nlinear-time, at the cost of having an exponential dependence on the polynomial\norder.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CC",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-11T14:38:10Z",
    "updated": "2022-09-11T14:38:10Z",
    "doi": null
  },
  "2404.12642": {
    "id": "http://arxiv.org/abs/2404.12642v1",
    "title": "Cooperative Sentiment Agents for Multimodal Sentiment Analysis",
    "authors": [
      "Shanmin Wang",
      "Hui Shuai",
      "Qingshan Liu",
      "Fei Wang"
    ],
    "abstract": "  In this paper, we propose a new Multimodal Representation Learning (MRL)\nmethod for Multimodal Sentiment Analysis (MSA), which facilitates the adaptive\ninteraction between modalities through Cooperative Sentiment Agents, named\nCo-SA. Co-SA comprises two critical components: the Sentiment Agents\nEstablishment (SAE) phase and the Sentiment Agents Cooperation (SAC) phase.\nDuring the SAE phase, each sentiment agent deals with an unimodal signal and\nhighlights explicit dynamic sentiment variations within the modality via the\nModality-Sentiment Disentanglement (MSD) and Deep Phase Space Reconstruction\n(DPSR) modules. Subsequently, in the SAC phase, Co-SA meticulously designs\ntask-specific interaction mechanisms for sentiment agents so that coordinating\nmultimodal signals to learn the joint representation. Specifically, Co-SA\nequips an independent policy model for each sentiment agent that captures\nsignificant properties within the modality. These policies are optimized\nmutually through the unified reward adaptive to downstream tasks. Benefitting\nfrom the rewarding mechanism, Co-SA transcends the limitation of pre-defined\nfusion modes and adaptively captures unimodal properties for MRL in the\nmultimodal interaction setting. To demonstrate the effectiveness of Co-SA, we\napply it to address Multimodal Sentiment Analysis (MSA) and Multimodal Emotion\nRecognition (MER) tasks. Our comprehensive experimental results demonstrate\nthat Co-SA excels at discovering diverse cross-modal features, encompassing\nboth common and complementary aspects. The code can be available at\nhttps://github.com/smwanghhh/Co-SA.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-19T05:48:09Z",
    "updated": "2024-04-19T05:48:09Z",
    "doi": null
  },
  "2308.00692": {
    "id": "http://arxiv.org/abs/2308.00692v3",
    "title": "LISA: Reasoning Segmentation via Large Language Model",
    "authors": [
      "Xin Lai",
      "Zhuotao Tian",
      "Yukang Chen",
      "Yanwei Li",
      "Yuhui Yuan",
      "Shu Liu",
      "Jiaya Jia"
    ],
    "abstract": "  Although perception systems have made remarkable advancements in recent\nyears, they still rely on explicit human instruction or pre-defined categories\nto identify the target objects before executing visual recognition tasks. Such\nsystems cannot actively reason and comprehend implicit user intention. In this\nwork, we propose a new segmentation task -- reasoning segmentation. The task is\ndesigned to output a segmentation mask given a complex and implicit query text.\nFurthermore, we establish a benchmark comprising over one thousand\nimage-instruction-mask data samples, incorporating intricate reasoning and\nworld knowledge for evaluation purposes. Finally, we present LISA: large\nLanguage Instructed Segmentation Assistant, which inherits the language\ngeneration capabilities of multimodal Large Language Models (LLMs) while also\npossessing the ability to produce segmentation masks. We expand the original\nvocabulary with a <SEG> token and propose the embedding-as-mask paradigm to\nunlock the segmentation capability. Remarkably, LISA can handle cases involving\ncomplex reasoning and world knowledge. Also, it demonstrates robust zero-shot\ncapability when trained exclusively on reasoning-free datasets. In addition,\nfine-tuning the model with merely 239 reasoning segmentation data samples\nresults in further performance enhancement. Both quantitative and qualitative\nexperiments show our method effectively unlocks new reasoning segmentation\ncapabilities for multimodal LLMs. Code, models, and data are available at\nhttps://github.com/dvlab-research/LISA.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-01T17:50:17Z",
    "updated": "2024-05-01T05:10:13Z",
    "doi": null
  },
  "2404.03566": {
    "id": "http://arxiv.org/abs/2404.03566v1",
    "title": "PointInfinity: Resolution-Invariant Point Diffusion Models",
    "authors": [
      "Zixuan Huang",
      "Justin Johnson",
      "Shoubhik Debnath",
      "James M. Rehg",
      "Chao-Yuan Wu"
    ],
    "abstract": "  We present PointInfinity, an efficient family of point cloud diffusion\nmodels. Our core idea is to use a transformer-based architecture with a\nfixed-size, resolution-invariant latent representation. This enables efficient\ntraining with low-resolution point clouds, while allowing high-resolution point\nclouds to be generated during inference. More importantly, we show that scaling\nthe test-time resolution beyond the training resolution improves the fidelity\nof generated point clouds and surfaces. We analyze this phenomenon and draw a\nlink to classifier-free guidance commonly used in diffusion models,\ndemonstrating that both allow trading off fidelity and variability during\ninference. Experiments on CO3D show that PointInfinity can efficiently generate\nhigh-resolution point clouds (up to 131k points, 31 times more than Point-E)\nwith state-of-the-art quality.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-04T16:24:32Z",
    "updated": "2024-04-04T16:24:32Z",
    "doi": null
  },
  "2205.09712": {
    "id": "http://arxiv.org/abs/2205.09712v1",
    "title": "Selection-Inference: Exploiting Large Language Models for Interpretable\n  Logical Reasoning",
    "authors": [
      "Antonia Creswell",
      "Murray Shanahan",
      "Irina Higgins"
    ],
    "abstract": "  Large language models (LLMs) have been shown to be capable of impressive\nfew-shot generalisation to new tasks. However, they still tend to perform\npoorly on multi-step logical reasoning problems. Here we carry out a\ncomprehensive evaluation of LLMs on 50 tasks that probe different aspects of\nlogical reasoning. We show that language models tend to perform fairly well at\nsingle step inference or entailment tasks, but struggle to chain together\nmultiple reasoning steps to solve more complex problems. In light of this, we\npropose a Selection-Inference (SI) framework that exploits pre-trained LLMs as\ngeneral processing modules, and alternates between selection and inference to\ngenerate a series of interpretable, casual reasoning steps leading to the final\nanswer. We show that a 7B parameter LLM used within the SI framework in a\n5-shot generalisation setting, with no fine-tuning, yields a performance\nimprovement of over 100% compared to an equivalent vanilla baseline on a suite\nof 10 logical reasoning tasks. The same model in the same setting even\noutperforms a significantly larger 280B parameter baseline on the same suite of\ntasks. Moreover, answers produced by the SI framework are accompanied by a\ncausal natural-language-based reasoning trace, which has important implications\nfor the safety and trustworthiness of the system.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-19T17:25:28Z",
    "updated": "2022-05-19T17:25:28Z",
    "doi": null
  },
  "1908.04471": {
    "id": "http://arxiv.org/abs/1908.04471v2",
    "title": "Einconv: Exploring Unexplored Tensor Network Decompositions for\n  Convolutional Neural Networks",
    "authors": [
      "Kohei Hayashi",
      "Taiki Yamaguchi",
      "Yohei Sugawara",
      "Shin-ichi Maeda"
    ],
    "abstract": "  Tensor decomposition methods are widely used for model compression and fast\ninference in convolutional neural networks (CNNs). Although many decompositions\nare conceivable, only CP decomposition and a few others have been applied in\npractice, and no extensive comparisons have been made between available\nmethods. Previous studies have not determined how many decompositions are\navailable, nor which of them is optimal. In this study, we first characterize a\ndecomposition class specific to CNNs by adopting a flexible graphical notation.\nThe class includes such well-known CNN modules as depthwise separable\nconvolution layers and bottleneck layers, but also previously unknown modules\nwith nonlinear activations. We also experimentally compare the tradeoff between\nprediction accuracy and time/space complexity for modules found by enumerating\nall possible decompositions, or by using a neural architecture search. We find\nsome nonlinear decompositions outperform existing ones.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-08-13T03:11:46Z",
    "updated": "2019-11-27T09:08:40Z",
    "doi": null
  },
  "2005.00052": {
    "id": "http://arxiv.org/abs/2005.00052v3",
    "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
    "authors": [
      "Jonas Pfeiffer",
      "Ivan Vuli\u0107",
      "Iryna Gurevych",
      "Sebastian Ruder"
    ],
    "abstract": "  The main goal behind state-of-the-art pre-trained multilingual models such as\nmultilingual BERT and XLM-R is enabling and bootstrapping NLP applications in\nlow-resource languages through zero-shot or few-shot cross-lingual transfer.\nHowever, due to limited model capacity, their transfer performance is the\nweakest exactly on such low-resource languages and languages unseen during\npre-training. We propose MAD-X, an adapter-based framework that enables high\nportability and parameter-efficient transfer to arbitrary tasks and languages\nby learning modular language and task representations. In addition, we\nintroduce a novel invertible adapter architecture and a strong baseline method\nfor adapting a pre-trained multilingual model to a new language. MAD-X\noutperforms the state of the art in cross-lingual transfer across a\nrepresentative set of typologically diverse languages on named entity\nrecognition and causal commonsense reasoning, and achieves competitive results\non question answering. Our code and adapters are available at AdapterHub.ml\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-30T18:54:43Z",
    "updated": "2020-10-06T10:17:45Z",
    "doi": null
  },
  "2305.17858": {
    "id": "http://arxiv.org/abs/2305.17858v1",
    "title": "FastMESH: Fast Surface Reconstruction by Hexagonal Mesh-based Neural\n  Rendering",
    "authors": [
      "Yisu Zhang",
      "Jianke Zhu",
      "Lixiang Lin"
    ],
    "abstract": "  Despite the promising results of multi-view reconstruction, the recent neural\nrendering-based methods, such as implicit surface rendering (IDR) and volume\nrendering (NeuS), not only incur a heavy computational burden on training but\nalso have the difficulties in disentangling the geometric and appearance.\nAlthough having achieved faster training speed than implicit representation and\nhash coding, the explicit voxel-based method obtains the inferior results on\nrecovering surface. To address these challenges, we propose an effective\nmesh-based neural rendering approach, named FastMESH, which only samples at the\nintersection of ray and mesh. A coarse-to-fine scheme is introduced to\nefficiently extract the initial mesh by space carving. More importantly, we\nsuggest a hexagonal mesh model to preserve surface regularity by constraining\nthe second-order derivatives of vertices, where only low level of positional\nencoding is engaged for neural rendering. The experiments demonstrate that our\napproach achieves the state-of-the-art results on both reconstruction and novel\nview synthesis. Besides, we obtain 10-fold acceleration on training comparing\nto the implicit representation-based methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-29T02:43:14Z",
    "updated": "2023-05-29T02:43:14Z",
    "doi": null
  },
  "2203.16248": {
    "id": "http://arxiv.org/abs/2203.16248v1",
    "title": "InstaFormer: Instance-Aware Image-to-Image Translation with Transformer",
    "authors": [
      "Soohyun Kim",
      "Jongbeom Baek",
      "Jihye Park",
      "Gyeongnyeon Kim",
      "Seungryong Kim"
    ],
    "abstract": "  We present a novel Transformer-based network architecture for instance-aware\nimage-to-image translation, dubbed InstaFormer, to effectively integrate\nglobal- and instance-level information. By considering extracted content\nfeatures from an image as tokens, our networks discover global consensus of\ncontent features by considering context information through a self-attention\nmodule in Transformers. By augmenting such tokens with an instance-level\nfeature extracted from the content feature with respect to bounding box\ninformation, our framework is capable of learning an interaction between object\ninstances and the global image, thus boosting the instance-awareness. We\nreplace layer normalization (LayerNorm) in standard Transformers with adaptive\ninstance normalization (AdaIN) to enable a multi-modal translation with style\ncodes. In addition, to improve the instance-awareness and translation quality\nat object regions, we present an instance-level content contrastive loss\ndefined between input and translated image. We conduct experiments to\ndemonstrate the effectiveness of our InstaFormer over the latest methods and\nprovide extensive ablation studies.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-30T12:30:22Z",
    "updated": "2022-03-30T12:30:22Z",
    "doi": null
  },
  "2212.08158": {
    "id": "http://arxiv.org/abs/2212.08158v3",
    "title": "MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal\n  Contributions in Vision and Language Models & Tasks",
    "authors": [
      "Letitia Parcalabescu",
      "Anette Frank"
    ],
    "abstract": "  Vision and language models (VL) are known to exploit unrobust indicators in\nindividual modalities (e.g., introduced by distributional biases) instead of\nfocusing on relevant information in each modality. That a unimodal model\nachieves similar accuracy on a VL task to a multimodal one, indicates that\nso-called unimodal collapse occurred. However, accuracy-based tests fail to\ndetect e.g., when the model prediction is wrong, while the model used relevant\ninformation from a modality. Instead, we propose MM-SHAP, a\nperformance-agnostic multimodality score based on Shapley values that reliably\nquantifies in which proportions a multimodal model uses individual modalities.\nWe apply MM-SHAP in two ways: (1) to compare models for their average degree of\nmultimodality, and (2) to measure for individual models the contribution of\nindividual modalities for different tasks and datasets. Experiments with six VL\nmodels -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight\nthat unimodal collapse can occur to different degrees and in different\ndirections, contradicting the wide-spread assumption that unimodal collapse is\none-sided. Based on our results, we recommend MM-SHAP for analysing multimodal\ntasks, to diagnose and guide progress towards multimodal integration. Code\navailable at \\url{https://github.com/Heidelberg-NLP/MM-SHAP}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68Txx",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.2.7; I.2.10",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-15T21:41:06Z",
    "updated": "2024-09-18T19:00:22Z",
    "doi": "10.18653/v1/2023.acl-long.223"
  },
  "2112.06624": {
    "id": "http://arxiv.org/abs/2112.06624v1",
    "title": "Pedestrian Trajectory Prediction via Spatial Interaction Transformer\n  Network",
    "authors": [
      "Tong Su",
      "Yu Meng",
      "Yan Xu"
    ],
    "abstract": "  As a core technology of the autonomous driving system, pedestrian trajectory\nprediction can significantly enhance the function of active vehicle safety and\nreduce road traffic injuries. In traffic scenes, when encountering with\noncoming people, pedestrians may make sudden turns or stop immediately, which\noften leads to complicated trajectories. To predict such unpredictable\ntrajectories, we can gain insights into the interaction between pedestrians. In\nthis paper, we present a novel generative method named Spatial Interaction\nTransformer (SIT), which learns the spatio-temporal correlation of pedestrian\ntrajectories through attention mechanisms. Furthermore, we introduce the\nconditional variational autoencoder (CVAE) framework to model the future latent\nmotion states of pedestrians. In particular, the experiments based on\nlarge-scale trafc dataset nuScenes [2] show that SIT has an outstanding\nperformance than state-of-the-art (SOTA) methods. Experimental evaluation on\nthe challenging ETH and UCY datasets conrms the robustness of our proposed\nmodel\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-12-13T13:08:04Z",
    "updated": "2021-12-13T13:08:04Z",
    "doi": null
  },
  "2312.11459": {
    "id": "http://arxiv.org/abs/2312.11459v3",
    "title": "VolumeDiffusion: Flexible Text-to-3D Generation with Efficient\n  Volumetric Encoder",
    "authors": [
      "Zhicong Tang",
      "Shuyang Gu",
      "Chunyu Wang",
      "Ting Zhang",
      "Jianmin Bao",
      "Dong Chen",
      "Baining Guo"
    ],
    "abstract": "  This paper introduces a pioneering 3D volumetric encoder designed for\ntext-to-3D generation. To scale up the training data for the diffusion model, a\nlightweight network is developed to efficiently acquire feature volumes from\nmulti-view images. The 3D volumes are then trained on a diffusion model for\ntext-to-3D generation using a 3D U-Net. This research further addresses the\nchallenges of inaccurate object captions and high-dimensional feature volumes.\nThe proposed model, trained on the public Objaverse dataset, demonstrates\npromising outcomes in producing diverse and recognizable samples from text\nprompts. Notably, it empowers finer control over object part characteristics\nthrough textual cues, fostering model creativity by seamlessly combining\nmultiple concepts within a single object. This research significantly\ncontributes to the progress of 3D generation by introducing an efficient,\nflexible, and scalable representation methodology.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-18T18:59:05Z",
    "updated": "2024-08-13T04:35:02Z",
    "doi": null
  },
  "2404.16510": {
    "id": "http://arxiv.org/abs/2404.16510v1",
    "title": "Interactive3D: Create What You Want by Interactive 3D Generation",
    "authors": [
      "Shaocong Dong",
      "Lihe Ding",
      "Zhanpeng Huang",
      "Zibin Wang",
      "Tianfan Xue",
      "Dan Xu"
    ],
    "abstract": "  3D object generation has undergone significant advancements, yielding\nhigh-quality results. However, fall short of achieving precise user control,\noften yielding results that do not align with user expectations, thus limiting\ntheir applicability. User-envisioning 3D object generation faces significant\nchallenges in realizing its concepts using current generative models due to\nlimited interaction capabilities. Existing methods mainly offer two approaches:\n(i) interpreting textual instructions with constrained controllability, or (ii)\nreconstructing 3D objects from 2D images. Both of them limit customization to\nthe confines of the 2D reference and potentially introduce undesirable\nartifacts during the 3D lifting process, restricting the scope for direct and\nversatile 3D modifications. In this work, we introduce Interactive3D, an\ninnovative framework for interactive 3D generation that grants users precise\ncontrol over the generative process through extensive 3D interaction\ncapabilities. Interactive3D is constructed in two cascading stages, utilizing\ndistinct 3D representations. The first stage employs Gaussian Splatting for\ndirect user interaction, allowing modifications and guidance of the generative\ndirection at any intermediate step through (i) Adding and Removing components,\n(ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv)\nSemantic Editing. Subsequently, the Gaussian splats are transformed into\nInstantNGP. We introduce a novel (v) Interactive Hash Refinement module to\nfurther add details and extract the geometry in the second stage. Our\nexperiments demonstrate that Interactive3D markedly improves the\ncontrollability and quality of 3D generation. Our project webpage is available\nat \\url{https://interactive-3d.github.io/}.\n",
    "categories": [
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-25T11:06:57Z",
    "updated": "2024-04-25T11:06:57Z",
    "doi": null
  },
  "2208.02812": {
    "id": "http://arxiv.org/abs/2208.02812v2",
    "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with\n  Point-to-Pixel Prompting",
    "authors": [
      "Ziyi Wang",
      "Xumin Yu",
      "Yongming Rao",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "  Nowadays, pre-training big models on large-scale datasets has become a\ncrucial topic in deep learning. The pre-trained models with high representation\nability and transferability achieve a great success and dominate many\ndownstream tasks in natural language processing and 2D vision. However, it is\nnon-trivial to promote such a pretraining-tuning paradigm to the 3D vision,\ngiven the limited training data that are relatively inconvenient to collect. In\nthis paper, we provide a new perspective of leveraging pre-trained 2D knowledge\nin 3D domain to tackle this problem, tuning pre-trained image models with the\nnovel Point-to-Pixel prompting for point cloud analysis at a minor parameter\ncost. Following the principle of prompting engineering, we transform point\nclouds into colorful images with geometry-preserved projection and\ngeometry-aware coloring to adapt to pre-trained image models, whose weights are\nkept frozen during the end-to-end optimization of point cloud analysis tasks.\nWe conduct extensive experiments to demonstrate that cooperating with our\nproposed Point-to-Pixel Prompting, better pre-trained image model will lead to\nconsistently better performance in 3D vision. Enjoying prosperous development\nfrom image pre-training field, our method attains 89.3% accuracy on the hardest\nsetting of ScanObjectNN, surpassing conventional point cloud models with much\nfewer trainable parameters. Our framework also exhibits very competitive\nperformance on ModelNet classification and ShapeNet Part Segmentation. Code is\navailable at https://github.com/wangzy22/P2P.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-04T17:59:03Z",
    "updated": "2022-10-12T09:00:15Z",
    "doi": null
  },
  "2107.07589": {
    "id": "http://arxiv.org/abs/2107.07589v2",
    "title": "A Comparison of Modern General-Purpose Visual SLAM Approaches",
    "authors": [
      "Alexey Merzlyakov",
      "Steve Macenski"
    ],
    "abstract": "  Advancing maturity in mobile and legged robotics technologies is changing the\nlandscapes where robots are being deployed and found. This innovation calls for\na transformation in simultaneous localization and mapping (SLAM) systems to\nsupport this new generation of service and consumer robots. No longer can\ntraditionally robust 2D lidar systems dominate while robots are being deployed\nin multi-story indoor, outdoor unstructured, and urban domains with\nincreasingly inexpensive stereo and RGB-D cameras. Visual SLAM (VSLAM) systems\nhave been a topic of study for decades and a small number of openly available\nimplementations have stood out: ORB-SLAM3, OpenVSLAM and RTABMap.\n  This paper presents a comparison of these 3 modern, feature rich, and\nuniquely robust VSLAM techniques that have yet to be benchmarked against each\nother, using several different datasets spanning multiple domains negotiated by\nservice robots. ORB-SLAM3 and OpenVSLAM each were not compared against at least\none of these datasets previously in literature and we provide insight through\nthis lens. This analysis is motivated to find general purpose, feature\ncomplete, and multi-domain VSLAM options to support a broad class of robot\napplications for integration into the new and improved ROS 2 Nav2 System as\nsuitable alternatives to traditional 2D lidar solutions.\n",
    "categories": [
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-07-15T20:00:03Z",
    "updated": "2021-08-05T21:34:11Z",
    "doi": null
  },
  "1605.04624": {
    "id": "http://arxiv.org/abs/1605.04624v1",
    "title": "Learning to Rank Personalized Search Results in Professional Networks",
    "authors": [
      "Viet Ha-Thuc",
      "Shakti Sinha"
    ],
    "abstract": "  LinkedIn search is deeply personalized - for the same queries, different\nsearchers expect completely different results. This paper presents our approach\nto achieving this by mining various data sources available in LinkedIn to infer\nsearchers' intents (such as hiring, job seeking, etc.), as well as extending\nthe concept of homophily to capture the searcher-result similarities on many\naspects. Then, learning-to-rank (LTR) is applied to combine these signals with\nstandard search features.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-05-16T00:59:07Z",
    "updated": "2016-05-16T00:59:07Z",
    "doi": null
  },
  "2002.00569": {
    "id": "http://arxiv.org/abs/2002.00569v3",
    "title": "DiverseDepth: Affine-invariant Depth Prediction Using Diverse Data",
    "authors": [
      "Wei Yin",
      "Xinlong Wang",
      "Chunhua Shen",
      "Yifan Liu",
      "Zhi Tian",
      "Songcen Xu",
      "Changming Sun",
      "Dou Renyin"
    ],
    "abstract": "  We present a method for depth estimation with monocular images, which can\npredict high-quality depth on diverse scenes up to an affine transformation,\nthus preserving accurate shapes of a scene. Previous methods that predict\nmetric depth often work well only for a specific scene. In contrast, learning\nrelative depth (information of being closer or further) can enjoy better\ngeneralization, with the price of failing to recover the accurate geometric\nshape of the scene. In this work, we propose a dataset and methods to tackle\nthis dilemma, aiming to predict accurate depth up to an affine transformation\nwith good generalization to diverse scenes. First we construct a large-scale\nand diverse dataset, termed Diverse Scene Depth dataset (DiverseDepth), which\nhas a broad range of scenes and foreground contents. Compared with previous\nlearning objectives, i.e., learning metric depth or relative depth, we propose\nto learn the affine-invariant depth using our diverse dataset to ensure both\ngeneralization and high-quality geometric shapes of scenes. Furthermore, in\norder to train the model on the complex dataset effectively, we propose a\nmulti-curriculum learning method. Experiments show that our method outperforms\nprevious methods on 8 datasets by a large margin with the zero-shot test\nsetting, demonstrating the excellent generalization capacity of the learned\nmodel to diverse scenes. The reconstructed point clouds with the predicted\ndepth show that our method can recover high-quality 3D shapes. Code and dataset\nare available at: https://tinyurl.com/DiverseDepth\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-02-03T05:38:33Z",
    "updated": "2020-03-28T08:26:57Z",
    "doi": null
  },
  "2307.02421": {
    "id": "http://arxiv.org/abs/2307.02421v2",
    "title": "DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models",
    "authors": [
      "Chong Mou",
      "Xintao Wang",
      "Jiechong Song",
      "Ying Shan",
      "Jian Zhang"
    ],
    "abstract": "  Despite the ability of existing large-scale text-to-image (T2I) models to\ngenerate high-quality images from detailed textual descriptions, they often\nlack the ability to precisely edit the generated or real images. In this paper,\nwe propose a novel image editing method, DragonDiffusion, enabling Drag-style\nmanipulation on Diffusion models. Specifically, we construct classifier\nguidance based on the strong correspondence of intermediate features in the\ndiffusion model. It can transform the editing signals into gradients via\nfeature correspondence loss to modify the intermediate representation of the\ndiffusion model. Based on this guidance strategy, we also build a multi-scale\nguidance to consider both semantic and geometric alignment. Moreover, a\ncross-branch self-attention is added to maintain the consistency between the\noriginal image and the editing result. Our method, through an efficient design,\nachieves various editing modes for the generated or real images, such as object\nmoving, object resizing, object appearance replacement, and content dragging.\nIt is worth noting that all editing and content preservation signals come from\nthe image itself, and the model does not require fine-tuning or additional\nmodules. Our source code will be available at\nhttps://github.com/MC-E/DragonDiffusion.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-05T16:43:56Z",
    "updated": "2023-11-20T09:08:42Z",
    "doi": null
  },
  "2102.04664": {
    "id": "http://arxiv.org/abs/2102.04664v2",
    "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\n  and Generation",
    "authors": [
      "Shuai Lu",
      "Daya Guo",
      "Shuo Ren",
      "Junjie Huang",
      "Alexey Svyatkovskiy",
      "Ambrosio Blanco",
      "Colin Clement",
      "Dawn Drain",
      "Daxin Jiang",
      "Duyu Tang",
      "Ge Li",
      "Lidong Zhou",
      "Linjun Shou",
      "Long Zhou",
      "Michele Tufano",
      "Ming Gong",
      "Ming Zhou",
      "Nan Duan",
      "Neel Sundaresan",
      "Shao Kun Deng",
      "Shengyu Fu",
      "Shujie Liu"
    ],
    "abstract": "  Benchmark datasets have a significant impact on accelerating research in\nprogramming language tasks. In this paper, we introduce CodeXGLUE, a benchmark\ndataset to foster machine learning research for program understanding and\ngeneration. CodeXGLUE includes a collection of 10 tasks across 14 datasets and\na platform for model evaluation and comparison. CodeXGLUE also features three\nbaseline systems, including the BERT-style, GPT-style, and Encoder-Decoder\nmodels, to make it easy for researchers to use the platform. The availability\nof such data and baselines can help the development and validation of new\nmethods that can be applied to various program understanding and generation\nproblems.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-02-09T06:16:25Z",
    "updated": "2021-03-16T08:28:37Z",
    "doi": null
  },
  "2303.15413": {
    "id": "http://arxiv.org/abs/2303.15413v5",
    "title": "Debiasing Scores and Prompts of 2D Diffusion for View-consistent\n  Text-to-3D Generation",
    "authors": [
      "Susung Hong",
      "Donghoon Ahn",
      "Seungryong Kim"
    ],
    "abstract": "  Existing score-distilling text-to-3D generation techniques, despite their\nconsiderable promise, often encounter the view inconsistency problem. One of\nthe most notable issues is the Janus problem, where the most canonical view of\nan object (\\textit{e.g}., face or head) appears in other views. In this work,\nwe explore existing frameworks for score-distilling text-to-3D generation and\nidentify the main causes of the view inconsistency problem -- the embedded bias\nof 2D diffusion models. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for view-consistent text-to-3D\ngeneration. Our first approach, called score debiasing, involves cutting off\nthe score estimated by 2D diffusion models and gradually increasing the\ntruncation value throughout the optimization process. Our second approach,\ncalled prompt debiasing, identifies conflicting words between user prompts and\nview prompts using a language model, and adjusts the discrepancy between view\nprompts and the viewing direction of an object. Our experimental results show\nthat our methods improve the realism of the generated 3D objects by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead. Our project page is available\nat~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-27T17:31:13Z",
    "updated": "2023-12-19T22:03:12Z",
    "doi": null
  },
  "2104.07204": {
    "id": "http://arxiv.org/abs/2104.07204v2",
    "title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese\n  Pre-trained Language Models",
    "authors": [
      "Yuxuan Lai",
      "Yijia Liu",
      "Yansong Feng",
      "Songfang Huang",
      "Dongyan Zhao"
    ],
    "abstract": "  Chinese pre-trained language models usually process text as a sequence of\ncharacters, while ignoring more coarse granularity, e.g., words. In this work,\nwe propose a novel pre-training paradigm for Chinese -- Lattice-BERT, which\nexplicitly incorporates word representations along with characters, thus can\nmodel a sentence in a multi-granularity manner. Specifically, we construct a\nlattice graph from the characters and words in a sentence and feed all these\ntext units into transformers. We design a lattice position attention mechanism\nto exploit the lattice structures in self-attention layers. We further propose\na masked segment prediction task to push the model to learn from rich but\nredundant information inherent in lattices, while avoiding learning unexpected\ntricks. Experiments on 11 Chinese natural language understanding tasks show\nthat our model can bring an average increase of 1.5% under the 12-layer\nsetting, which achieves new state-of-the-art among base-size models on the CLUE\nbenchmarks. Further analysis shows that Lattice-BERT can harness the lattice\nstructures, and the improvement comes from the exploration of redundant\ninformation and multi-granularity representations. Our code will be available\nat https://github.com/alibaba/pretrained-language-models/LatticeBERT.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-15T02:36:49Z",
    "updated": "2021-05-28T11:41:35Z",
    "doi": null
  },
  "1911.02744": {
    "id": "http://arxiv.org/abs/1911.02744v1",
    "title": "PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud\n  Representation",
    "authors": [
      "Can Qin",
      "Haoxuan You",
      "Lichen Wang",
      "C. -C. Jay Kuo",
      "Yun Fu"
    ],
    "abstract": "  Domain Adaptation (DA) approaches achieved significant improvements in a wide\nrange of machine learning and computer vision tasks (i.e., classification,\ndetection, and segmentation). However, as far as we are aware, there are few\nmethods yet to achieve domain adaptation directly on 3D point cloud data. The\nunique challenge of point cloud data lies in its abundant spatial geometric\ninformation, and the semantics of the whole object is contributed by including\nregional geometric structures. Specifically, most general-purpose DA methods\nthat struggle for global feature alignment and ignore local geometric\ninformation are not suitable for 3D domain alignment. In this paper, we propose\na novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN\njointly aligns the global and local features in multi-level. For local\nalignment, we propose Self-Adaptive (SA) node module with an adjusted receptive\nfield to model the discriminative local structures for aligning domains. To\nrepresent hierarchically scaled features, node-attention module is further\nintroduced to weight the relationship of SA nodes across objects and domains.\nFor global alignment, an adversarial-training strategy is employed to learn and\nalign global features across domains. Since there is no common evaluation\nbenchmark for 3D point cloud DA scenario, we build a general benchmark (i.e.,\nPointDA-10) extracted from three popular 3D object/scene datasets (i.e.,\nModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification\nfashion. Extensive experiments on PointDA-10 illustrate the superiority of our\nmodel over the state-of-the-art general-purpose DA methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-11-07T04:03:07Z",
    "updated": "2019-11-07T04:03:07Z",
    "doi": null
  },
  "1709.07857": {
    "id": "http://arxiv.org/abs/1709.07857v2",
    "title": "Using Simulation and Domain Adaptation to Improve Efficiency of Deep\n  Robotic Grasping",
    "authors": [
      "Konstantinos Bousmalis",
      "Alex Irpan",
      "Paul Wohlhart",
      "Yunfei Bai",
      "Matthew Kelcey",
      "Mrinal Kalakrishnan",
      "Laura Downs",
      "Julian Ibarz",
      "Peter Pastor",
      "Kurt Konolige",
      "Sergey Levine",
      "Vincent Vanhoucke"
    ],
    "abstract": "  Instrumenting and collecting annotated visual grasping datasets to train\nmodern machine learning algorithms can be extremely time-consuming and\nexpensive. An appealing alternative is to use off-the-shelf simulators to\nrender synthetic data for which ground-truth annotations are generated\nautomatically. Unfortunately, models trained purely on simulated data often\nfail to generalize to the real world. We study how randomized simulated\nenvironments and domain adaptation methods can be extended to train a grasping\nsystem to grasp novel objects from raw monocular RGB images. We extensively\nevaluate our approaches with a total of more than 25,000 physical test grasps,\nstudying a range of simulation conditions and domain adaptation methods,\nincluding a novel extension of pixel-level domain adaptation that we term the\nGraspGAN. We show that, by using synthetic data and domain adaptation, we are\nable to reduce the number of real-world samples needed to achieve a given level\nof performance by up to 50 times, using only randomly generated simulated\nobjects. We also show that by using only unlabeled real-world data and our\nGraspGAN methodology, we obtain real-world grasping performance without any\nreal-world labels that is similar to that achieved with 939,777 labeled\nreal-world samples.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-09-22T17:23:12Z",
    "updated": "2017-09-25T21:35:45Z",
    "doi": null
  },
  "2109.08478": {
    "id": "http://arxiv.org/abs/2109.08478v1",
    "title": "Multimodal Incremental Transformer with Visual Grounding for Visual\n  Dialogue Generation",
    "authors": [
      "Feilong Chen",
      "Fandong Meng",
      "Xiuyi Chen",
      "Peng Li",
      "Jie Zhou"
    ],
    "abstract": "  Visual dialogue is a challenging task since it needs to answer a series of\ncoherent questions on the basis of understanding the visual environment.\nPrevious studies focus on the implicit exploration of multimodal co-reference\nby implicitly attending to spatial image features or object-level image\nfeatures but neglect the importance of locating the objects explicitly in the\nvisual content, which is associated with entities in the textual content.\nTherefore, in this paper we propose a {\\bf M}ultimodal {\\bf I}ncremental {\\bf\nT}ransformer with {\\bf V}isual {\\bf G}rounding, named MITVG, which consists of\ntwo key parts: visual grounding and multimodal incremental transformer. Visual\ngrounding aims to explicitly locate related objects in the image guided by\ntextual entities, which helps the model exclude the visual content that does\nnot need attention. On the basis of visual grounding, the multimodal\nincremental transformer encodes the multi-turn dialogue history combined with\nvisual scene step by step according to the order of the dialogue and then\ngenerates a contextually and visually coherent response. Experimental results\non the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the\nproposed model, which achieves comparable performance.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-17T11:39:29Z",
    "updated": "2021-09-17T11:39:29Z",
    "doi": null
  },
  "1707.01613": {
    "id": "http://arxiv.org/abs/1707.01613v4",
    "title": "SSGAN: Secure Steganography Based on Generative Adversarial Networks",
    "authors": [
      "Haichao Shi",
      "Jing Dong",
      "Wei Wang",
      "Yinlong Qian",
      "Xiaoyu Zhang"
    ],
    "abstract": "  In this paper, a novel strategy of Secure Steganograpy based on Generative\nAdversarial Networks is proposed to generate suitable and secure covers for\nsteganography. The proposed architecture has one generative network, and two\ndiscriminative networks. The generative network mainly evaluates the visual\nquality of the generated images for steganography, and the discriminative\nnetworks are utilized to assess their suitableness for information hiding.\nDifferent from the existing work which adopts Deep Convolutional Generative\nAdversarial Networks, we utilize another form of generative adversarial\nnetworks. By using this new form of generative adversarial networks,\nsignificant improvements are made on the convergence speed, the training\nstability and the image quality. Furthermore, a sophisticated steganalysis\nnetwork is reconstructed for the discriminative network, and the network can\nbetter evaluate the performance of the generated images. Numerous experiments\nare conducted on the publicly available datasets to demonstrate the\neffectiveness and robustness of the proposed method.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-07-06T02:05:51Z",
    "updated": "2018-11-24T02:32:03Z",
    "doi": null
  },
  "1812.06834": {
    "id": "http://arxiv.org/abs/1812.06834v3",
    "title": "A Tutorial on Deep Latent Variable Models of Natural Language",
    "authors": [
      "Yoon Kim",
      "Sam Wiseman",
      "Alexander M. Rush"
    ],
    "abstract": "  There has been much recent, exciting work on combining the complementary\nstrengths of latent variable models and deep learning. Latent variable modeling\nmakes it easy to explicitly specify model constraints through conditional\nindependence properties, while deep learning makes it possible to parameterize\nthese conditional likelihoods with powerful function approximators. While these\n\"deep latent variable\" models provide a rich, flexible framework for modeling\nmany real-world phenomena, difficulties exist: deep parameterizations of\nconditional likelihoods usually make posterior inference intractable, and\nlatent variable objectives often complicate backpropagation by introducing\npoints of non-differentiability. This tutorial explores these issues in depth\nthrough the lens of variational inference.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-12-17T15:26:29Z",
    "updated": "2019-08-05T01:14:32Z",
    "doi": null
  },
  "2009.14068": {
    "id": "http://arxiv.org/abs/2009.14068v1",
    "title": "Graph convolutional regression of cardiac depolarization from sparse\n  endocardial maps",
    "authors": [
      "Felix Meister",
      "Tiziano Passerini",
      "Chlo\u00e9 Audigier",
      "\u00c8ric Lluch",
      "Viorel Mihalef",
      "Hiroshi Ashikaga",
      "Andreas Maier",
      "Henry Halperin",
      "Tommaso Mansi"
    ],
    "abstract": "  Electroanatomic mapping as routinely acquired in ablation therapy of\nventricular tachycardia is the gold standard method to identify the\narrhythmogenic substrate. To reduce the acquisition time and still provide maps\nwith high spatial resolution, we propose a novel deep learning method based on\ngraph convolutional neural networks to estimate the depolarization time in the\nmyocardium, given sparse catheter data on the left ventricular endocardium,\nECG, and magnetic resonance images. The training set consists of data produced\nby a computational model of cardiac electrophysiology on a large cohort of\nsynthetically generated geometries of ischemic hearts. The predicted\ndepolarization pattern has good agreement with activation times computed by the\ncardiac electrophysiology model in a validation set of five swine heart\ngeometries with complex scar and border zone morphologies. The mean absolute\nerror hereby measures 8 ms on the entire myocardium when providing 50\\% of the\nendocardial ground truth in over 500 computed depolarization patterns.\nFurthermore, when considering a complete animal data set with high density\nelectroanatomic mapping data as reference, the neural network can accurately\nreproduce the endocardial depolarization pattern, even when a small percentage\nof measurements are provided as input features (mean absolute error of 7 ms\nwith 50\\% of input samples). The results show that the proposed method, trained\non synthetically generated data, may generalize to real data.\n",
    "categories": [
      {
        "@term": "physics.med-ph",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "q-bio.QM",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "q-bio.TO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-09-28T09:21:14Z",
    "updated": "2020-09-28T09:21:14Z",
    "doi": null
  },
  "2303.14124": {
    "id": "http://arxiv.org/abs/2303.14124v1",
    "title": "Towards Scalable Neural Representation for Diverse Videos",
    "authors": [
      "Bo He",
      "Xitong Yang",
      "Hanyu Wang",
      "Zuxuan Wu",
      "Hao Chen",
      "Shuaiyi Huang",
      "Yixuan Ren",
      "Ser-Nam Lim",
      "Abhinav Shrivastava"
    ],
    "abstract": "  Implicit neural representations (INR) have gained increasing attention in\nrepresenting 3D scenes and images, and have been recently applied to encode\nvideos (e.g., NeRV, E-NeRV). While achieving promising results, existing\nINR-based methods are limited to encoding a handful of short videos (e.g.,\nseven 5-second videos in the UVG dataset) with redundant visual content,\nleading to a model design that fits individual video frames independently and\nis not efficiently scalable to a large number of diverse videos. This paper\nfocuses on developing neural representations for a more practical setup --\nencoding long and/or a large number of videos with diverse visual content. We\nfirst show that instead of dividing videos into small subsets and encoding them\nwith separate models, encoding long and diverse videos jointly with a unified\nmodel achieves better compression results. Based on this observation, we\npropose D-NeRV, a novel neural representation framework designed to encode\ndiverse videos by (i) decoupling clip-specific visual content from motion\ninformation, (ii) introducing temporal reasoning into the implicit neural\nnetwork, and (iii) employing the task-oriented flow as intermediate output to\nreduce spatial redundancies. Our new model largely surpasses NeRV and\ntraditional video compression techniques on UCF101 and UVG datasets on the\nvideo compression task. Moreover, when used as an efficient data-loader, D-NeRV\nachieves 3%-10% higher accuracy than NeRV on action recognition tasks on the\nUCF101 dataset under the same compression ratios.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-24T16:32:19Z",
    "updated": "2023-03-24T16:32:19Z",
    "doi": null
  },
  "1708.04538": {
    "id": "http://arxiv.org/abs/1708.04538v3",
    "title": "Artistic style transfer for videos and spherical images",
    "authors": [
      "Manuel Ruder",
      "Alexey Dosovitskiy",
      "Thomas Brox"
    ],
    "abstract": "  Manually re-drawing an image in a certain artistic style takes a professional\nartist a long time. Doing this for a video sequence single-handedly is beyond\nimagination. We present two computational approaches that transfer the style\nfrom one image (for example, a painting) to a whole video sequence. In our\nfirst approach, we adapt to videos the original image style transfer technique\nby Gatys et al. based on energy minimization. We introduce new ways of\ninitialization and new loss functions to generate consistent and stable\nstylized video sequences even in cases with large motion and strong occlusion.\nOur second approach formulates video stylization as a learning problem. We\npropose a deep network architecture and training procedures that allow us to\nstylize arbitrary-length videos in a consistent and stable way, and nearly in\nreal time. We show that the proposed methods clearly outperform simpler\nbaselines both qualitatively and quantitatively. Finally, we propose a way to\nadapt these approaches also to 360 degree images and videos as they emerge with\nrecent virtual reality hardware.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-08-13T21:17:59Z",
    "updated": "2018-08-05T09:07:11Z",
    "doi": "10.1007/s11263-018-1089-z"
  },
  "2203.02986": {
    "id": "http://arxiv.org/abs/2203.02986v1",
    "title": "Modeling Coreference Relations in Visual Dialog",
    "authors": [
      "Mingxiao Li",
      "Marie-Francine Moens"
    ],
    "abstract": "  Visual dialog is a vision-language task where an agent needs to answer a\nseries of questions grounded in an image based on the understanding of the\ndialog history and the image. The occurrences of coreference relations in the\ndialog makes it a more challenging task than visual question-answering. Most\nprevious works have focused on learning better multi-modal representations or\non exploring different ways of fusing visual and language features, while the\ncoreferences in the dialog are mainly ignored. In this paper, based on\nlinguistic knowledge and discourse features of human dialog we propose two soft\nconstraints that can improve the model's ability of resolving coreferences in\ndialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset\nshows that our model, which integrates two novel and linguistically inspired\nsoft constraints in a deep transformer neural architecture, obtains new\nstate-of-the-art performance in terms of recall at 1 and other evaluation\nmetrics compared to current existing models and this without pretraining on\nother vision-language datasets. Our qualitative results also demonstrate the\neffectiveness of the method that we propose.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-06T15:22:24Z",
    "updated": "2022-03-06T15:22:24Z",
    "doi": null
  },
  "1802.10026": {
    "id": "http://arxiv.org/abs/1802.10026v4",
    "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs",
    "authors": [
      "Timur Garipov",
      "Pavel Izmailov",
      "Dmitrii Podoprikhin",
      "Dmitry Vetrov",
      "Andrew Gordon Wilson"
    ],
    "abstract": "  The loss functions of deep neural networks are complex and their geometric\nproperties are not well understood. We show that the optima of these complex\nloss functions are in fact connected by simple curves over which training and\ntest accuracy are nearly constant. We introduce a training procedure to\ndiscover these high-accuracy pathways between modes. Inspired by this new\ngeometric insight, we also propose a new ensembling method entitled Fast\nGeometric Ensembling (FGE). Using FGE we can train high-performing ensembles in\nthe time required to train a single model. We achieve improved performance\ncompared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10,\nCIFAR-100, and ImageNet.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-02-27T17:13:28Z",
    "updated": "2018-10-30T11:39:49Z",
    "doi": null
  },
  "2407.15017": {
    "id": "http://arxiv.org/abs/2407.15017v3",
    "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
    "authors": [
      "Mengru Wang",
      "Yunzhi Yao",
      "Ziwen Xu",
      "Shuofei Qiao",
      "Shumin Deng",
      "Peng Wang",
      "Xiang Chen",
      "Jia-Chen Gu",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "  Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.HC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-22T06:15:59Z",
    "updated": "2024-10-06T15:42:55Z",
    "doi": null
  },
  "1412.6550": {
    "id": "http://arxiv.org/abs/1412.6550v4",
    "title": "FitNets: Hints for Thin Deep Nets",
    "authors": [
      "Adriana Romero",
      "Nicolas Ballas",
      "Samira Ebrahimi Kahou",
      "Antoine Chassang",
      "Carlo Gatta",
      "Yoshua Bengio"
    ],
    "abstract": "  While depth tends to improve network performances, it also makes\ngradient-based training more difficult since deeper networks tend to be more\nnon-linear. The recently proposed knowledge distillation approach is aimed at\nobtaining small and fast-to-execute models, and it has shown that a student\nnetwork could imitate the soft output of a larger teacher network or ensemble\nof networks. In this paper, we extend this idea to allow the training of a\nstudent that is deeper and thinner than the teacher, using not only the outputs\nbut also the intermediate representations learned by the teacher as hints to\nimprove the training process and final performance of the student. Because the\nstudent intermediate hidden layer will generally be smaller than the teacher's\nintermediate hidden layer, additional parameters are introduced to map the\nstudent hidden layer to the prediction of the teacher hidden layer. This allows\none to train deeper students that can generalize better or run faster, a\ntrade-off that is controlled by the chosen student capacity. For example, on\nCIFAR-10, a deep student network with almost 10.4 times less parameters\noutperforms a larger, state-of-the-art teacher network.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.NE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2014-12-19T22:40:51Z",
    "updated": "2015-03-27T11:52:28Z",
    "doi": null
  },
  "1804.01947": {
    "id": "http://arxiv.org/abs/1804.01947v3",
    "title": "Sliced-Wasserstein Autoencoder: An Embarrassingly Simple Generative\n  Model",
    "authors": [
      "Soheil Kolouri",
      "Phillip E. Pope",
      "Charles E. Martin",
      "Gustavo K. Rohde"
    ],
    "abstract": "  In this paper we study generative modeling via autoencoders while using the\nelegant geometric properties of the optimal transport (OT) problem and the\nWasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE),\nwhich are generative models that enable one to shape the distribution of the\nlatent space into any samplable probability distribution without the need for\ntraining an adversarial network or defining a closed-form for the distribution.\nIn short, we regularize the autoencoder loss with the sliced-Wasserstein\ndistance between the distribution of the encoded training samples and a\npredefined samplable distribution. We show that the proposed formulation has an\nefficient numerical solution that provides similar capabilities to Wasserstein\nAutoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an\nembarrassingly simple implementation.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-05T16:45:06Z",
    "updated": "2018-06-27T00:05:29Z",
    "doi": null
  },
  "1808.01454": {
    "id": "http://arxiv.org/abs/1808.01454v1",
    "title": "T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth\n  Estimation Tasks",
    "authors": [
      "Chuanxia Zheng",
      "Tat-Jen Cham",
      "Jianfei Cai"
    ],
    "abstract": "  Current methods for single-image depth estimation use training datasets with\nreal image-depth pairs or stereo pairs, which are not easy to acquire. We\npropose a framework, trained on synthetic image-depth pairs and unpaired real\nimages, that comprises an image translation network for enhancing realism of\ninput images, followed by a depth prediction network. A key idea is having the\nfirst network act as a wide-spectrum input translator, taking in either\nsynthetic or real images, and ideally producing minimally modified realistic\nimages. This is done via a reconstruction loss when the training input is real,\nand GAN loss when synthetic, removing the need for heuristic\nself-regularization. The second network is trained on a task loss for synthetic\nimage-depth pairs, with extra GAN loss to unify real and synthetic feature\ndistributions. Importantly, the framework can be trained end-to-end, leading to\ngood results, even surpassing early deep-learning methods that use real paired\ndata.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-08-04T09:10:14Z",
    "updated": "2018-08-04T09:10:14Z",
    "doi": null
  },
  "1804.05313": {
    "id": "http://arxiv.org/abs/1804.05313v2",
    "title": "FSCNMF: Fusing Structure and Content via Non-negative Matrix\n  Factorization for Embedding Information Networks",
    "authors": [
      "Sambaran Bandyopadhyay",
      "Harsh Kara",
      "Aswin Kannan",
      "M N Murty"
    ],
    "abstract": "  Analysis and visualization of an information network can be facilitated\nbetter using an appropriate embedding of the network. Network embedding learns\na compact low-dimensional vector representation for each node of the network,\nand uses this lower dimensional representation for different network analysis\ntasks. Only the structure of the network is considered by a majority of the\ncurrent embedding algorithms. However, some content is associated with each\nnode, in most of the practical applications, which can help to understand the\nunderlying semantics of the network. It is not straightforward to integrate the\ncontent of each node in the current state-of-the-art network embedding methods.\n  In this paper, we propose a nonnegative matrix factorization based\noptimization framework, namely FSCNMF which considers both the network\nstructure and the content of the nodes while learning a lower dimensional\nrepresentation of each node in the network. Our approach systematically\nregularizes structure based on content and vice versa to exploit the\nconsistency between the structure and content to the best possible extent. We\nfurther extend the basic FSCNMF to an advanced method, namely FSCNMF++ to\ncapture the higher order proximities in the network. We conduct experiments on\nreal world information networks for different types of machine learning\napplications such as node clustering, visualization, and multi-class\nclassification. The results show that our method can represent the network\nsignificantly better than the state-of-the-art algorithms and improve the\nperformance across all the applications that we consider.\n",
    "categories": [
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-15T07:09:34Z",
    "updated": "2018-07-04T14:11:35Z",
    "doi": null
  },
  "2403.10255": {
    "id": "http://arxiv.org/abs/2403.10255v1",
    "title": "Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion\n  Model and Implicit Neural Decoder",
    "authors": [
      "Jinseok Kim",
      "Tae-Kyun Kim"
    ],
    "abstract": "  Super-resolution (SR) and image generation are important tasks in computer\nvision and are widely adopted in real-world applications. Most existing\nmethods, however, generate images only at fixed-scale magnification and suffer\nfrom over-smoothing and artifacts. Additionally, they do not offer enough\ndiversity of output images nor image consistency at different scales. Most\nrelevant work applied Implicit Neural Representation (INR) to the denoising\ndiffusion model to obtain continuous-resolution yet diverse and high-quality SR\nresults. Since this model operates in the image space, the larger the\nresolution of image is produced, the more memory and inference time is\nrequired, and it also does not maintain scale-specific consistency. We propose\na novel pipeline that can super-resolve an input image or generate from a\nrandom noise a novel image at arbitrary scales. The method consists of a\npretrained auto-encoder, a latent diffusion model, and an implicit neural\ndecoder, and their learning strategies. The proposed method adopts diffusion\nprocesses in a latent space, thus efficient, yet aligned with output image\nspace decoded by MLPs at arbitrary scales. More specifically, our\narbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling\nfrom the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in\nseries. The latent diffusion process is learnt by the denoising and the\nalignment losses jointly. Errors in output images are backpropagated via the\nfixed decoder, improving the quality of output images. In the extensive\nexperiments using multiple public benchmarks on the two tasks i.e. image\nsuper-resolution and novel image generation at arbitrary scales, the proposed\nmethod outperforms relevant methods in metrics of image quality, diversity and\nscale consistency. It is significantly better than the relevant prior-art in\nthe inference speed and memory usage.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-15T12:45:40Z",
    "updated": "2024-03-15T12:45:40Z",
    "doi": null
  },
  "1811.01287": {
    "id": "http://arxiv.org/abs/1811.01287v1",
    "title": "Towards Sparse Hierarchical Graph Classifiers",
    "authors": [
      "C\u0103t\u0103lina Cangea",
      "Petar Veli\u010dkovi\u0107",
      "Nikola Jovanovi\u0107",
      "Thomas Kipf",
      "Pietro Li\u00f2"
    ],
    "abstract": "  Recent advances in representation learning on graphs, mainly leveraging graph\nconvolutional networks, have brought a substantial improvement on many\ngraph-based benchmark tasks. While novel approaches to learning node embeddings\nare highly suitable for node classification and link prediction, their\napplication to graph classification (predicting a single label for the entire\ngraph) remains mostly rudimentary, typically using a single global pooling step\nto aggregate node features or a hand-designed, fixed heuristic for hierarchical\ncoarsening of the graph structure. An important step towards ameliorating this\nis differentiable graph coarsening---the ability to reduce the size of the\ngraph in an adaptive, data-dependent manner within a graph neural network\npipeline, analogous to image downsampling within CNNs. However, the previous\nprominent approach to pooling has quadratic memory requirements during training\nand is therefore not scalable to large graphs. Here we combine several recent\nadvances in graph neural network design to demonstrate that competitive\nhierarchical graph classification results are possible without sacrificing\nsparsity. Our results are verified on several established graph classification\nbenchmarks, and highlight an important direction for future research in\ngraph-based neural networks.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-11-03T21:39:43Z",
    "updated": "2018-11-03T21:39:43Z",
    "doi": null
  },
  "1806.03968": {
    "id": "http://arxiv.org/abs/1806.03968v1",
    "title": "CapsGAN: Using Dynamic Routing for Generative Adversarial Networks",
    "authors": [
      "Raeid Saqur",
      "Sal Vivona"
    ],
    "abstract": "  In this paper, we propose a novel technique for generating images in the 3D\ndomain from images with high degree of geometrical transformations. By\ncoalescing two popular concurrent methods that have seen rapid ascension to the\nmachine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and\nCapsule networks (Sabour, Hinton et. al.) - we present: \\textbf{CapsGAN}. We\nshow that CapsGAN performs better than or equal to traditional CNN based GANs\nin generating images with high geometric transformations using rotated MNIST.\nIn the process, we also show the efficacy of using capsules architecture in the\nGANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the\nperformance control and training stability by experimenting with using\nWasserstein distance (gradient clipping, penalty) and Spectral Normalization.\nThe experimental findings of this paper should propel the application of\ncapsules and GANs in the still exciting and nascent domain of 3D image\ngeneration, and plausibly video (frame) generation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-06-07T21:33:46Z",
    "updated": "2018-06-07T21:33:46Z",
    "doi": null
  },
  "2205.08787": {
    "id": "http://arxiv.org/abs/2205.08787v1",
    "title": "Cross-subject Action Unit Detection with Meta Learning and\n  Transformer-based Relation Modeling",
    "authors": [
      "Jiyuan Cao",
      "Zhilei Liu",
      "Yong Zhang"
    ],
    "abstract": "  Facial Action Unit (AU) detection is a crucial task for emotion analysis from\nfacial movements. The apparent differences of different subjects sometimes\nmislead changes brought by AUs, resulting in inaccurate results. However, most\nof the existing AU detection methods based on deep learning didn't consider the\nidentity information of different subjects. The paper proposes a\nmeta-learning-based cross-subject AU detection model to eliminate the\nidentity-caused differences. Besides, a transformer-based relation learning\nmodule is introduced to learn the latent relations of multiple AUs. To be\nspecific, our proposed work is composed of two sub-tasks. The first sub-task is\nmeta-learning-based AU local region representation learning, called MARL, which\nlearns discriminative representation of local AU regions that incorporates the\nshared information of multiple subjects and eliminates identity-caused\ndifferences. The second sub-task uses the local region representation of AU of\nthe first sub-task as input, then adds relationship learning based on the\ntransformer encoder architecture to capture AU relationships. The entire\ntraining process is cascaded. Ablation study and visualization show that our\nMARL can eliminate identity-caused differences, thus obtaining a robust and\ngeneralized AU discriminative embedding representation. Our results prove that\non the two public datasets BP4D and DISFA, our method is superior to the\nstate-of-the-art technology, and the F1 score is improved by 1.3% and 1.4%,\nrespectively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-18T08:17:59Z",
    "updated": "2022-05-18T08:17:59Z",
    "doi": "10.1109/IJCNN55064.2022.9891984"
  },
  "1901.10124": {
    "id": "http://arxiv.org/abs/1901.10124v1",
    "title": "Adversarial Adaptation of Scene Graph Models for Understanding Civic\n  Issues",
    "authors": [
      "Shanu Kumar",
      "Shubham Atreja",
      "Anjali Singh",
      "Mohit Jain"
    ],
    "abstract": "  Citizen engagement and technology usage are two emerging trends driven by\nsmart city initiatives. Governments around the world are adopting technology\nfor faster resolution of civic issues. Typically, citizens report issues, such\nas broken roads, garbage dumps, etc. through web portals and mobile apps, in\norder for the government authorities to take appropriate actions. Several\nmediums -- text, image, audio, video -- are used to report these issues.\nThrough a user study with 13 citizens and 3 authorities, we found that image is\nthe most preferred medium to report civic issues. However, analyzing civic\nissue related images is challenging for the authorities as it requires manual\neffort. Moreover, previous works have been limited to identifying a specific\nset of issues from images. In this work, given an image, we propose to generate\na Civic Issue Graph consisting of a set of objects and the semantic relations\nbetween them, which are representative of the underlying civic issue. We also\nrelease two multi-modal (text and images) datasets, that can help in further\nanalysis of civic issues from images. We present a novel approach for\nadversarial training of existing scene graph models that enables the use of\nscene graphs for new applications in the absence of any labelled training data.\nWe conduct several experiments to analyze the efficacy of our approach, and\nusing human evaluation, we establish the appropriateness of our model at\nrepresenting different civic issues.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CY",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-01-29T06:02:45Z",
    "updated": "2019-01-29T06:02:45Z",
    "doi": null
  },
  "2401.01060": {
    "id": "http://arxiv.org/abs/2401.01060v1",
    "title": "Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively\n  Tuning Pre-trained Code Models",
    "authors": [
      "Shuzheng Gao",
      "Wenxin Mao",
      "Cuiyun Gao",
      "Li Li",
      "Xing Hu",
      "Xin Xia",
      "Michael R. Lyu"
    ],
    "abstract": "  Pre-trained code models have recently achieved substantial improvements in\nmany code intelligence tasks. These models are first pre-trained on large-scale\nunlabeled datasets in a task-agnostic manner using self-supervised learning,\nand then fine-tuned on labeled datasets in downstream tasks. However, the\nlabeled datasets are usually limited in size (i.e., human intensive efforts),\nwhich may hinder the performance of pre-trained code models in specific tasks.\nTo mitigate this, one possible solution is to leverage the large-scale\nunlabeled data in the tuning stage by pseudo-labeling. However, directly\nemploying the pseudo-labeled data can bring a large amount of noise, i.e.,\nincorrect labels, leading to suboptimal performance. How to effectively\nleverage the noisy pseudo-labeled data is a challenging yet under-explored\nproblem.In this paper, we propose a novel approach named HINT to improve\npre-trained code models with large-scale unlabeled datasets by better utilizing\nthe pseudo-labeled data. HINT includes two main modules: HybrId pseudo-labeled\ndata selection and Noise-tolerant Training. In the hybrid pseudo-data selection\nmodule, considering the robustness issue, apart from directly measuring the\nquality of pseudo labels through training loss, we further propose to employ a\nretrieval-based method to filter low-quality pseudo-labeled data. The\nnoise-tolerant training module aims to further mitigate the influence of errors\nin pseudo labels by training the model with a noise-tolerant loss function and\nby regularizing the consistency of model predictions.The experimental results\nshow that HINT can better leverage those unlabeled data in a task-specific way\nand provide complementary benefits for pre-trained models, e.g., improving the\nbest baseline model by 15.33%, 16.50%, and 8.98% on code summarization, defect\ndetection, and assertion generation, respectively.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-02T06:39:00Z",
    "updated": "2024-01-02T06:39:00Z",
    "doi": "10.1145/3597503.3639216"
  },
  "2302.06646": {
    "id": "http://arxiv.org/abs/2302.06646v1",
    "title": "Simple Hardware-Efficient Long Convolutions for Sequence Modeling",
    "authors": [
      "Daniel Y. Fu",
      "Elliot L. Epstein",
      "Eric Nguyen",
      "Armin W. Thomas",
      "Michael Zhang",
      "Tri Dao",
      "Atri Rudra",
      "Christopher R\u00e9"
    ],
    "abstract": "  State space models (SSMs) have high performance on long sequence modeling but\nrequire sophisticated initialization techniques and specialized implementations\nfor high quality and runtime performance. We study whether a simple alternative\ncan match SSMs in performance and efficiency: directly learning long\nconvolutions over the sequence. We find that a key requirement to achieving\nhigh performance is keeping the convolution kernels smooth. We find that simple\ninterventions--such as squashing the kernel weights--result in smooth kernels\nand recover SSM performance on a range of tasks including the long range arena,\nimage classification, language modeling, and brain data modeling. Next, we\ndevelop FlashButterfly, an IO-aware algorithm to improve the runtime\nperformance of long convolutions. FlashButterfly appeals to classic Butterfly\ndecompositions of the convolution to reduce GPU memory IO and increase FLOP\nutilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows\nus to train on Path256, a challenging task with sequence length 64K, where we\nset state-of-the-art by 29.1 points while training 7.2$\\times$ faster than\nprior work. Lastly, we introduce an extension to FlashButterfly that learns the\ncoefficients of the Butterfly decomposition, increasing expressivity without\nincreasing runtime. Using this extension, we outperform a Transformer on\nWikiText103 by 0.2 PPL with 30% fewer parameters.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-13T19:19:23Z",
    "updated": "2023-02-13T19:19:23Z",
    "doi": null
  },
  "2312.06713": {
    "id": "http://arxiv.org/abs/2312.06713v1",
    "title": "TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint\n  Video",
    "authors": [
      "Minye Wu",
      "Zehao Wang",
      "Georgios Kouros",
      "Tinne Tuytelaars"
    ],
    "abstract": "  Neural Radiance Fields (NeRF) revolutionize the realm of visual media by\nproviding photorealistic Free-Viewpoint Video (FVV) experiences, offering\nviewers unparalleled immersion and interactivity. However, the technology's\nsignificant storage requirements and the computational complexity involved in\ngeneration and rendering currently limit its broader application. To close this\ngap, this paper presents Temporal Tri-Plane Radiance Fields (TeTriRF), a novel\ntechnology that significantly reduces the storage size for Free-Viewpoint Video\n(FVV) while maintaining low-cost generation and rendering. TeTriRF introduces a\nhybrid representation with tri-planes and voxel grids to support scaling up to\nlong-duration sequences and scenes with complex motions or rapid changes. We\npropose a group training scheme tailored to achieving high training efficiency\nand yielding temporally consistent, low-entropy scene representations.\nLeveraging these properties of the representations, we introduce a compression\npipeline with off-the-shelf video codecs, achieving an order of magnitude less\nstorage size compared to the state-of-the-art. Our experiments demonstrate that\nTeTriRF can achieve competitive quality with a higher compression rate.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-10T23:00:24Z",
    "updated": "2023-12-10T23:00:24Z",
    "doi": null
  },
  "2307.08597": {
    "id": "http://arxiv.org/abs/2307.08597v1",
    "title": "Multimodal Diffusion Segmentation Model for Object Segmentation from\n  Manipulation Instructions",
    "authors": [
      "Yui Iioka",
      "Yu Yoshida",
      "Yuiga Wada",
      "Shumpei Hatanaka",
      "Komei Sugiura"
    ],
    "abstract": "  In this study, we aim to develop a model that comprehends a natural language\ninstruction (e.g., \"Go to the living room and get the nearest pillow to the\nradio art on the wall\") and generates a segmentation mask for the target\neveryday object. The task is challenging because it requires (1) the\nunderstanding of the referring expressions for multiple objects in the\ninstruction, (2) the prediction of the target phrase of the sentence among the\nmultiple phrases, and (3) the generation of pixel-wise segmentation masks\nrather than bounding boxes. Studies have been conducted on languagebased\nsegmentation methods; however, they sometimes mask irrelevant regions for\ncomplex sentences. In this paper, we propose the Multimodal Diffusion\nSegmentation Model (MDSM), which generates a mask in the first stage and\nrefines it in the second stage. We introduce a crossmodal parallel feature\nextraction mechanism and extend diffusion probabilistic models to handle\ncrossmodal features. To validate our model, we built a new dataset based on the\nwell-known Matterport3D and REVERIE datasets. This dataset consists of\ninstructions with complex referring expressions accompanied by real indoor\nenvironmental images that feature various target objects, in addition to\npixel-wise segmentation masks. The performance of MDSM surpassed that of the\nbaseline method by a large margin of +10.13 mean IoU.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-17T16:07:07Z",
    "updated": "2023-07-17T16:07:07Z",
    "doi": null
  },
  "2211.08428": {
    "id": "http://arxiv.org/abs/2211.08428v2",
    "title": "CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming",
    "authors": [
      "Qihua Zhou",
      "Ruibin Li",
      "Song Guo",
      "Peiran Dong",
      "Yi Liu",
      "Jingcai Guo",
      "Zhenda Xu"
    ],
    "abstract": "  Recent years have witnessed the dramatic growth of Internet video traffic,\nwhere the video bitstreams are often compressed and delivered in low quality to\nfit the streamer's uplink bandwidth. To alleviate the quality degradation, it\ncomes the rise of Neural-enhanced Video Streaming (NVS), which shows great\nprospects for recovering low-quality videos by mostly deploying neural\nsuper-resolution (SR) on the media server. Despite its benefit, we reveal that\ncurrent mainstream works with SR enhancement have not achieved the desired\nrate-distortion trade-off between bitrate saving and quality restoration, due\nto: (1) overemphasizing the enhancement on the decoder side while omitting the\nco-design of encoder, (2) limited generative capacity to recover high-fidelity\nperceptual details, and (3) optimizing the compression-and-restoration pipeline\nfrom the resolution perspective solely, without considering color bit-depth.\nAiming at overcoming these limitations, we are the first to conduct an\nencoder-decoder (i.e., codec) synergy by leveraging the inherent\nvisual-generative property of diffusion models. Specifically, we present the\nCodec-aware Diffusion Modeling (CaDM), a novel NVS paradigm to significantly\nreduce streaming delivery bitrates while holding pretty higher restoration\ncapacity over existing methods. First, CaDM improves the encoder's compression\nefficiency by simultaneously reducing resolution and color bit-depth of video\nframes. Second, CaDM empowers the decoder with high-quality enhancement by\nmaking the denoising diffusion restoration aware of encoder's resolution-color\nconditions. Evaluation on public cloud services with OpenMMLab benchmarks shows\nthat CaDM effectively saves up to 5.12 - 21.44 times bitrates based on common\nvideo standards and achieves much better recovery quality (e.g., FID of 0.61)\nover state-of-the-art neural-enhancing methods.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-15T05:14:48Z",
    "updated": "2023-03-08T14:25:10Z",
    "doi": null
  },
  "2309.16585": {
    "id": "http://arxiv.org/abs/2309.16585v4",
    "title": "Text-to-3D using Gaussian Splatting",
    "authors": [
      "Zilong Chen",
      "Feng Wang",
      "Yikai Wang",
      "Huaping Liu"
    ],
    "abstract": "  Automatic text-to-3D generation that combines Score Distillation Sampling\n(SDS) with the optimization of volume rendering has achieved remarkable\nprogress in synthesizing realistic 3D objects. Yet most existing text-to-3D\nmethods by SDS and volume rendering suffer from inaccurate geometry, e.g., the\nJanus issue, since it is hard to explicitly integrate 3D priors into implicit\n3D representations. Besides, it is usually time-consuming for them to generate\nelaborate 3D models with rich colors. In response, this paper proposes GSGEN, a\nnovel method that adopts Gaussian Splatting, a recent state-of-the-art\nrepresentation, to text-to-3D generation. GSGEN aims at generating high-quality\n3D objects and addressing existing shortcomings by exploiting the explicit\nnature of Gaussian Splatting that enables the incorporation of 3D prior.\nSpecifically, our method adopts a progressive optimization strategy, which\nincludes a geometry optimization stage and an appearance refinement stage. In\ngeometry optimization, a coarse representation is established under 3D point\ncloud diffusion prior along with the ordinary 2D SDS optimization, ensuring a\nsensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians\nundergo an iterative appearance refinement to enrich texture details. In this\nstage, we increase the number of Gaussians by compactness-based densification\nto enhance continuity and improve fidelity. With these designs, our approach\ncan generate 3D assets with delicate details and accurate geometry. Extensive\nevaluations demonstrate the effectiveness of our method, especially for\ncapturing high-frequency components. Our code is available at\nhttps://github.com/gsgen3d/gsgen\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-28T16:44:31Z",
    "updated": "2024-04-02T05:10:02Z",
    "doi": null
  },
  "2310.08992": {
    "id": "http://arxiv.org/abs/2310.08992v3",
    "title": "CodeChain: Towards Modular Code Generation Through Chain of\n  Self-revisions with Representative Sub-modules",
    "authors": [
      "Hung Le",
      "Hailin Chen",
      "Amrita Saha",
      "Akash Gokul",
      "Doyen Sahoo",
      "Shafiq Joty"
    ],
    "abstract": "  Large Language Models (LLMs) have already become quite proficient at solving\nsimpler programming tasks like those in HumanEval or MBPP benchmarks. However,\nsolving more complex and competitive programming tasks is still quite\nchallenging for these models - possibly due to their tendency to generate\nsolutions as monolithic code blocks instead of decomposing them into logical\nsub-tasks and sub-modules. On the other hand, experienced programmers\ninstinctively write modularized code with abstraction for solving complex\ntasks, often reusing previously developed modules. To address this gap, we\npropose CodeChain, a novel framework for inference that elicits modularized\ncode generation through a chain of self-revisions, each being guided by some\nrepresentative sub-modules generated in previous iterations. Concretely,\nCodeChain first instructs the LLM to generate modularized codes through\nchain-of-thought prompting. Then it applies a chain of self-revisions by\niterating the two steps: 1) extracting and clustering the generated sub-modules\nand selecting the cluster representatives as the more generic and re-usable\nimplementations, and 2) augmenting the original chain-of-thought prompt with\nthese selected module-implementations and instructing the LLM to re-generate\nnew modularized solutions. We find that by naturally encouraging the LLM to\nreuse the previously developed and verified sub-modules, CodeChain can\nsignificantly boost both modularity as well as correctness of the generated\nsolutions, achieving relative pass@1 improvements of 35% on APPS and 76% on\nCodeContests. It is shown to be effective on both OpenAI LLMs as well as\nopen-sourced LLMs like WizardCoder. We also conduct comprehensive ablation\nstudies with different methods of prompting, number of clusters, model sizes,\nprogram qualities, etc., to provide useful insights that underpin CodeChain's\nsuccess.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-13T10:17:48Z",
    "updated": "2024-03-14T03:29:09Z",
    "doi": null
  },
  "1911.04252": {
    "id": "http://arxiv.org/abs/1911.04252v4",
    "title": "Self-training with Noisy Student improves ImageNet classification",
    "authors": [
      "Qizhe Xie",
      "Minh-Thang Luong",
      "Eduard Hovy",
      "Quoc V. Le"
    ],
    "abstract": "  We present Noisy Student Training, a semi-supervised learning approach that\nworks well even when labeled data is abundant. Noisy Student Training achieves\n88.4% top-1 accuracy on ImageNet, which is 2.0% better than the\nstate-of-the-art model that requires 3.5B weakly labeled Instagram images. On\nrobustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to\n83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces\nImageNet-P mean flip rate from 27.8 to 12.2.\n  Noisy Student Training extends the idea of self-training and distillation\nwith the use of equal-or-larger student models and noise added to the student\nduring learning. On ImageNet, we first train an EfficientNet model on labeled\nimages and use it as a teacher to generate pseudo labels for 300M unlabeled\nimages. We then train a larger EfficientNet as a student model on the\ncombination of labeled and pseudo labeled images. We iterate this process by\nputting back the student as the teacher. During the learning of the student, we\ninject noise such as dropout, stochastic depth, and data augmentation via\nRandAugment to the student so that the student generalizes better than the\nteacher. Models are available at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\nCode is available at https://github.com/google-research/noisystudent.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-11-11T18:59:27Z",
    "updated": "2020-06-19T17:36:57Z",
    "doi": null
  },
  "2404.03577": {
    "id": "http://arxiv.org/abs/2404.03577v1",
    "title": "Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning\n  Skills in Large Language Models",
    "authors": [
      "Yantao Liu",
      "Zijun Yao",
      "Xin Lv",
      "Yuchen Fan",
      "Shulin Cao",
      "Jifan Yu",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "  Providing knowledge documents for large language models (LLMs) has emerged as\na promising solution to update the static knowledge inherent in their\nparameters. However, knowledge in the document may conflict with the memory of\nLLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leads\nto the necessity of examining the capability of LLMs to assimilate supplemental\nexternal knowledge that conflicts with their memory. While previous studies\nhave explained to what extent LLMs extract conflicting knowledge from the\nprovided text, they neglect the necessity to reason with conflicting knowledge.\nFurthermore, there lack a detailed analysis on strategies to enable LLMs to\nresolve conflicting knowledge via prompting, decoding strategy, and supervised\nfine-tuning. To address these limitations, we construct a new dataset, dubbed\nKNOT, for knowledge conflict resolution examination in the form of question\nanswering. KNOT facilitates in-depth analysis by dividing reasoning with\nconflicting knowledge into three levels: (1) Direct Extraction, which directly\nextracts conflicting knowledge to answer questions. (2) Explicit Reasoning,\nwhich reasons with conflicting knowledge when the reasoning path is explicitly\nprovided in the question. (3) Implicit Reasoning, where reasoning with\nconflicting knowledge requires LLMs to infer the reasoning path independently\nto answer questions. We also conduct extensive experiments on KNOT to establish\nempirical guidelines for LLMs to utilize conflicting knowledge in complex\ncircumstances. Dataset and associated codes can be accessed at\nhttps://github.com/THU-KEG/KNOT .\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-04T16:40:11Z",
    "updated": "2024-04-04T16:40:11Z",
    "doi": null
  },
  "2301.13195": {
    "id": "http://arxiv.org/abs/2301.13195v2",
    "title": "Adaptive Computation with Elastic Input Sequence",
    "authors": [
      "Fuzhao Xue",
      "Valerii Likhosherstov",
      "Anurag Arnab",
      "Neil Houlsby",
      "Mostafa Dehghani",
      "Yang You"
    ],
    "abstract": "  Humans have the ability to adapt the type of information they use, the\nprocedure they employ, and the amount of time they spend when solving problems.\nHowever, most standard neural networks have a fixed function type and\ncomputation budget regardless of the sample's nature or difficulty. Adaptivity\nis a powerful paradigm as it not only imbues practitioners with flexibility\npertaining to the downstream usage of these models but can also serve as a\npowerful inductive bias for solving certain challenging classes of problems. In\nthis work, we introduce a new approach called AdaTape, which allows for dynamic\ncomputation in neural networks through adaptive tape tokens. AdaTape utilizes\nan elastic input sequence by equipping an architecture with a dynamic\nread-and-write tape. Specifically, we adaptively generate input sequences using\ntape tokens obtained from a tape bank which can be either trainable or derived\nfrom input data. We examine the challenges and requirements to obtain dynamic\nsequence content and length, and propose the Adaptive Tape Reading (ATR)\nalgorithm to achieve both goals. Through extensive experiments on image\nrecognition tasks, we show that AdaTape can achieve better performance while\nmaintaining the computational cost. To facilitate further research, we have\nreleased code at https://github.com/google-research/scenic.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-30T18:57:24Z",
    "updated": "2023-06-03T06:19:23Z",
    "doi": null
  },
  "1912.00953": {
    "id": "http://arxiv.org/abs/1912.00953v2",
    "title": "LOGAN: Latent Optimisation for Generative Adversarial Networks",
    "authors": [
      "Yan Wu",
      "Jeff Donahue",
      "David Balduzzi",
      "Karen Simonyan",
      "Timothy Lillicrap"
    ],
    "abstract": "  Training generative adversarial networks requires balancing of delicate\nadversarial dynamics. Even with careful tuning, training may diverge or end up\nin a bad equilibrium with dropped modes. In this work, we improve CS-GAN with\nnatural gradient-based latent optimisation and show that it improves\nadversarial dynamics by enhancing interactions between the discriminator and\nthe generator. Our experiments demonstrate that latent optimisation can\nsignificantly improve GAN training, obtaining state-of-the-art performance for\nthe ImageNet ($128 \\times 128$) dataset. Our model achieves an Inception Score\n(IS) of $148$ and an Fr\\'echet Inception Distance (FID) of $3.4$, an\nimprovement of $17\\%$ and $32\\%$ in IS and FID respectively, compared with the\nbaseline BigGAN-deep model with the same architecture and number of parameters.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-12-02T17:30:05Z",
    "updated": "2020-07-01T16:53:32Z",
    "doi": null
  },
  "2010.05646": {
    "id": "http://arxiv.org/abs/2010.05646v2",
    "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High\n  Fidelity Speech Synthesis",
    "authors": [
      "Jungil Kong",
      "Jaehyeon Kim",
      "Jaekyoung Bae"
    ],
    "abstract": "  Several recent work on speech synthesis have employed generative adversarial\nnetworks (GANs) to produce raw waveforms. Although such methods improve the\nsampling efficiency and memory usage, their sample quality has not yet reached\nthat of autoregressive and flow-based generative models. In this work, we\npropose HiFi-GAN, which achieves both efficient and high-fidelity speech\nsynthesis. As speech audio consists of sinusoidal signals with various periods,\nwe demonstrate that modeling periodic patterns of an audio is crucial for\nenhancing sample quality. A subjective human evaluation (mean opinion score,\nMOS) of a single speaker dataset indicates that our proposed method\ndemonstrates similarity to human quality while generating 22.05 kHz\nhigh-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We\nfurther show the generality of HiFi-GAN to the mel-spectrogram inversion of\nunseen speakers and end-to-end speech synthesis. Finally, a small footprint\nversion of HiFi-GAN generates samples 13.4 times faster than real-time on CPU\nwith comparable quality to an autoregressive counterpart.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-10-12T12:33:43Z",
    "updated": "2020-10-23T09:12:04Z",
    "doi": null
  },
  "2403.07807": {
    "id": "http://arxiv.org/abs/2403.07807v1",
    "title": "StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting",
    "authors": [
      "Kunhao Liu",
      "Fangneng Zhan",
      "Muyu Xu",
      "Christian Theobalt",
      "Ling Shao",
      "Shijian Lu"
    ],
    "abstract": "  We introduce StyleGaussian, a novel 3D style transfer technique that allows\ninstant transfer of any image's style to a 3D scene at 10 frames per second\n(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style\ntransfer without compromising its real-time rendering ability and multi-view\nconsistency. It achieves instant style transfer with three steps: embedding,\ntransfer, and decoding. Initially, 2D VGG scene features are embedded into\nreconstructed 3D Gaussians. Next, the embedded features are transformed\naccording to a reference style image. Finally, the transformed features are\ndecoded into the stylized RGB. StyleGaussian has two novel designs. The first\nis an efficient feature rendering strategy that first renders low-dimensional\nfeatures and then maps them into high-dimensional features while embedding VGG\nfeatures. It cuts the memory consumption significantly and enables 3DGS to\nrender the high-dimensional memory-intensive features. The second is a\nK-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized\nfeatures, it eliminates the 2D CNN operations that compromise strict multi-view\nconsistency. Extensive experiments show that StyleGaussian achieves instant 3D\nstylization with superior stylization quality while preserving real-time\nrendering and strict multi-view consistency. Project page:\nhttps://kunhao-liu.github.io/StyleGaussian/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-12T16:44:52Z",
    "updated": "2024-03-12T16:44:52Z",
    "doi": null
  },
  "2304.13157": {
    "id": "http://arxiv.org/abs/2304.13157v1",
    "title": "Generative Relevance Feedback with Large Language Models",
    "authors": [
      "Iain Mackie",
      "Shubham Chatterjee",
      "Jeffrey Dalton"
    ],
    "abstract": "  Current query expansion models use pseudo-relevance feedback to improve\nfirst-pass retrieval effectiveness; however, this fails when the initial\nresults are not relevant. Instead of building a language model from retrieved\nresults, we propose Generative Relevance Feedback (GRF) that builds\nprobabilistic feedback models from long-form text generated from Large Language\nModels. We study the effective methods for generating text by varying the\nzero-shot generation subtasks: queries, entities, facts, news articles,\ndocuments, and essays. We evaluate GRF on document retrieval benchmarks\ncovering a diverse set of queries and document collections, and the results\nshow that GRF methods significantly outperform previous PRF methods.\nSpecifically, we improve MAP between 5-19% and NDCG@10 17-24% compared to RM3\nexpansion, and achieve the best R@1k effectiveness on all datasets compared to\nstate-of-the-art sparse, dense, and expansion models.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "H.3.3",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-25T21:35:29Z",
    "updated": "2023-04-25T21:35:29Z",
    "doi": "10.1145/3539618.3591992"
  },
  "1703.04782": {
    "id": "http://arxiv.org/abs/1703.04782v3",
    "title": "Online Learning Rate Adaptation with Hypergradient Descent",
    "authors": [
      "Atilim Gunes Baydin",
      "Robert Cornish",
      "David Martinez Rubio",
      "Mark Schmidt",
      "Frank Wood"
    ],
    "abstract": "  We introduce a general method for improving the convergence rate of\ngradient-based optimizers that is easy to implement and works well in practice.\nWe demonstrate the effectiveness of the method in a range of optimization\nproblems by applying it to stochastic gradient descent, stochastic gradient\ndescent with Nesterov momentum, and Adam, showing that it significantly reduces\nthe need for the manual tuning of the initial learning rate for these commonly\nused algorithms. Our method works by dynamically updating the learning rate\nduring optimization using the gradient with respect to the learning rate of the\nupdate rule itself. Computing this \"hypergradient\" needs little additional\ncomputation, requires only one extra copy of the original gradient to be stored\nin memory, and relies upon nothing more than what is provided by reverse-mode\nautomatic differentiation.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68T05",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "G.1.6; I.2.6",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-03-14T22:28:27Z",
    "updated": "2018-02-26T01:36:49Z",
    "doi": null
  },
  "2409.04847": {
    "id": "http://arxiv.org/abs/2409.04847v1",
    "title": "Rethinking The Training And Evaluation of Rich-Context Layout-to-Image\n  Generation",
    "authors": [
      "Jiaxin Cheng",
      "Zixu Zhao",
      "Tong He",
      "Tianjun Xiao",
      "Yicong Zhou",
      "Zheng Zhang"
    ],
    "abstract": "  Recent advancements in generative models have significantly enhanced their\ncapacity for image generation, enabling a wide range of applications such as\nimage editing, completion and video editing. A specialized area within\ngenerative modeling is layout-to-image (L2I) generation, where predefined\nlayouts of objects guide the generative process. In this study, we introduce a\nnovel regional cross-attention module tailored to enrich layout-to-image\ngeneration. This module notably improves the representation of layout regions,\nparticularly in scenarios where existing methods struggle with highly complex\nand detailed textual descriptions. Moreover, while current open-vocabulary L2I\nmethods are trained in an open-set setting, their evaluations often occur in\nclosed-set environments. To bridge this gap, we propose two metrics to assess\nL2I performance in open-vocabulary scenarios. Additionally, we conduct a\ncomprehensive user study to validate the consistency of these metrics with\nhuman preferences.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-09-07T14:57:03Z",
    "updated": "2024-09-07T14:57:03Z",
    "doi": null
  },
  "2212.05922": {
    "id": "http://arxiv.org/abs/2212.05922v3",
    "title": "Audiovisual Masked Autoencoders",
    "authors": [
      "Mariana-Iuliana Georgescu",
      "Eduardo Fonseca",
      "Radu Tudor Ionescu",
      "Mario Lucic",
      "Cordelia Schmid",
      "Anurag Arnab"
    ],
    "abstract": "  Can we leverage the audiovisual information already present in video to\nimprove self-supervised representation learning? To answer this question, we\nstudy various pretraining architectures and objectives within the masked\nautoencoding framework, motivated by the success of similar methods in natural\nlanguage and image understanding. We show that we can achieve significant\nimprovements on audiovisual downstream classification tasks, surpassing the\nstate-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our\naudiovisual pretraining scheme for multiple unimodal downstream tasks using a\nsingle audiovisual pretrained model. We additionally demonstrate the\ntransferability of our representations, achieving state-of-the-art audiovisual\nresults on Epic Kitchens without pretraining specifically for this dataset.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-09T17:34:53Z",
    "updated": "2024-01-04T16:52:34Z",
    "doi": null
  },
  "2004.04398": {
    "id": "http://arxiv.org/abs/2004.04398v2",
    "title": "Online Meta-Learning for Multi-Source and Semi-Supervised Domain\n  Adaptation",
    "authors": [
      "Da Li",
      "Timothy Hospedales"
    ],
    "abstract": "  Domain adaptation (DA) is the topical problem of adapting models from\nlabelled source datasets so that they perform well on target datasets where\nonly unlabelled or partially labelled data is available. Many methods have been\nproposed to address this problem through different ways to minimise the domain\nshift between source and target datasets. In this paper we take an orthogonal\nperspective and propose a framework to further enhance performance by\nmeta-learning the initial conditions of existing DA algorithms. This is\nchallenging compared to the more widely considered setting of few-shot\nmeta-learning, due to the length of the computation graph involved. Therefore\nwe propose an online shortest-path meta-learning framework that is both\ncomputationally tractable and practically effective for improving DA\nperformance. We present variants for both multi-source unsupervised domain\nadaptation (MSDA), and semi-supervised domain adaptation (SSDA). Importantly,\nour approach is agnostic to the base adaptation algorithm, and can be applied\nto improve many techniques. Experimentally, we demonstrate improvements on\nclassic (DANN) and recent (MCD and MME) techniques for MSDA and SSDA, and\nultimately achieve state of the art results on several DA benchmarks including\nthe largest scale DomainNet.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-09T07:48:22Z",
    "updated": "2020-07-27T12:55:37Z",
    "doi": null
  },
  "1911.02685": {
    "id": "http://arxiv.org/abs/1911.02685v3",
    "title": "A Comprehensive Survey on Transfer Learning",
    "authors": [
      "Fuzhen Zhuang",
      "Zhiyuan Qi",
      "Keyu Duan",
      "Dongbo Xi",
      "Yongchun Zhu",
      "Hengshu Zhu",
      "Hui Xiong",
      "Qing He"
    ],
    "abstract": "  Transfer learning aims at improving the performance of target learners on\ntarget domains by transferring the knowledge contained in different but related\nsource domains. In this way, the dependence on a large number of target domain\ndata can be reduced for constructing target learners. Due to the wide\napplication prospects, transfer learning has become a popular and promising\narea in machine learning. Although there are already some valuable and\nimpressive surveys on transfer learning, these surveys introduce approaches in\na relatively isolated way and lack the recent advances in transfer learning.\nDue to the rapid expansion of the transfer learning area, it is both necessary\nand challenging to comprehensively review the relevant studies. This survey\nattempts to connect and systematize the existing transfer learning researches,\nas well as to summarize and interpret the mechanisms and the strategies of\ntransfer learning in a comprehensive way, which may help readers have a better\nunderstanding of the current research status and ideas. Unlike previous\nsurveys, this survey paper reviews more than forty representative transfer\nlearning approaches, especially homogeneous transfer learning approaches, from\nthe perspectives of data and model. The applications of transfer learning are\nalso briefly introduced. In order to show the performance of different transfer\nlearning models, over twenty representative transfer learning models are used\nfor experiments. The models are performed on three different datasets, i.e.,\nAmazon Reviews, Reuters-21578, and Office-31. And the experimental results\ndemonstrate the importance of selecting appropriate transfer learning models\nfor different applications in practice.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-11-07T00:15:02Z",
    "updated": "2020-06-23T15:52:46Z",
    "doi": null
  },
  "1802.05365": {
    "id": "http://arxiv.org/abs/1802.05365v2",
    "title": "Deep contextualized word representations",
    "authors": [
      "Matthew E. Peters",
      "Mark Neumann",
      "Mohit Iyyer",
      "Matt Gardner",
      "Christopher Clark",
      "Kenton Lee",
      "Luke Zettlemoyer"
    ],
    "abstract": "  We introduce a new type of deep contextualized word representation that\nmodels both (1) complex characteristics of word use (e.g., syntax and\nsemantics), and (2) how these uses vary across linguistic contexts (i.e., to\nmodel polysemy). Our word vectors are learned functions of the internal states\nof a deep bidirectional language model (biLM), which is pre-trained on a large\ntext corpus. We show that these representations can be easily added to existing\nmodels and significantly improve the state of the art across six challenging\nNLP problems, including question answering, textual entailment and sentiment\nanalysis. We also present an analysis showing that exposing the deep internals\nof the pre-trained network is crucial, allowing downstream models to mix\ndifferent types of semi-supervision signals.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-02-15T00:05:11Z",
    "updated": "2018-03-22T21:59:40Z",
    "doi": null
  },
  "2006.00719": {
    "id": "http://arxiv.org/abs/2006.00719v3",
    "title": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning",
    "authors": [
      "Zhewei Yao",
      "Amir Gholami",
      "Sheng Shen",
      "Mustafa Mustafa",
      "Kurt Keutzer",
      "Michael W. Mahoney"
    ],
    "abstract": "  We introduce ADAHESSIAN, a second order stochastic optimization algorithm\nwhich dynamically incorporates the curvature of the loss function via ADAptive\nestimates of the HESSIAN. Second order algorithms are among the most powerful\noptimization algorithms with superior convergence properties as compared to\nfirst order methods such as SGD and Adam. The main disadvantage of traditional\nsecond order methods is their heavier per iteration computation and poor\naccuracy as compared to first order methods. To address these, we incorporate\nseveral novel approaches in ADAHESSIAN, including: (i) a fast Hutchinson based\nmethod to approximate the curvature matrix with low computational overhead;\n(ii) a root-mean-square exponential moving average to smooth out variations of\nthe Hessian diagonal across different iterations; and (iii) a block diagonal\naveraging to reduce the variance of Hessian diagonal elements. We show that\nADAHESSIAN achieves new state-of-the-art results by a large margin as compared\nto other adaptive optimization methods, including variants of Adam. In\nparticular, we perform extensive tests on CV, NLP, and recommendation system\ntasks and find that ADAHESSIAN: (i) achieves 1.80%/1.45% higher accuracy on\nResNets20/32 on Cifar10, and 5.55% higher accuracy on ImageNet as compared to\nAdam; (ii) outperforms AdamW for transformers by 0.13/0.33 BLEU score on\nIWSLT14/WMT14 and 2.7/1.0 PPL on PTB/Wikitext-103; (iii) outperforms AdamW for\nSqueezeBert by 0.41 points on GLUE; and (iv) achieves 0.032% better score than\nAdagrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the\ncost per iteration of ADAHESSIAN is comparable to first order methods, and that\nit exhibits robustness towards its hyperparameters.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.NA",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.NA",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-06-01T05:00:51Z",
    "updated": "2021-04-29T00:52:09Z",
    "doi": null
  },
  "2109.03867": {
    "id": "http://arxiv.org/abs/2109.03867v4",
    "title": "LSB: Local Self-Balancing MCMC in Discrete Spaces",
    "authors": [
      "Emanuele Sansone"
    ],
    "abstract": "  We present the Local Self-Balancing sampler (LSB), a local Markov Chain Monte\nCarlo (MCMC) method for sampling in purely discrete domains, which is able to\nautonomously adapt to the target distribution and to reduce the number of\ntarget evaluations required to converge. LSB is based on (i) a parametrization\nof locally balanced proposals, (ii) a newly proposed objective function based\non mutual information and (iii) a self-balancing learning procedure, which\nminimises the proposed objective to update the proposal parameters. Experiments\non energy-based models and Markov networks show that LSB converges using a\nsmaller number of queries to the oracle distribution compared to recent local\nMCMC samplers.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-08T18:31:26Z",
    "updated": "2022-07-04T19:17:34Z",
    "doi": null
  },
  "2312.11666": {
    "id": "http://arxiv.org/abs/2312.11666v1",
    "title": "HAAR: Text-Conditioned Generative Model of 3D Strand-based Human\n  Hairstyles",
    "authors": [
      "Vanessa Sklyarova",
      "Egor Zakharov",
      "Otmar Hilliges",
      "Michael J. Black",
      "Justus Thies"
    ],
    "abstract": "  We present HAAR, a new strand-based generative model for 3D human hairstyles.\nSpecifically, based on textual inputs, HAAR produces 3D hairstyles that could\nbe used as production-level assets in modern computer graphics engines. Current\nAI-based generative models take advantage of powerful 2D priors to reconstruct\n3D content in the form of point clouds, meshes, or volumetric functions.\nHowever, by using the 2D priors, they are intrinsically limited to only\nrecovering the visual parts. Highly occluded hair structures can not be\nreconstructed with those methods, and they only model the ''outer shell'',\nwhich is not ready to be used in physics-based rendering or simulation\npipelines. In contrast, we propose a first text-guided generative method that\nuses 3D hair strands as an underlying representation. Leveraging 2D visual\nquestion-answering (VQA) systems, we automatically annotate synthetic hair\nmodels that are generated from a small set of artist-created hairstyles. This\nallows us to train a latent diffusion model that operates in a common hairstyle\nUV space. In qualitative and quantitative studies, we demonstrate the\ncapabilities of the proposed model and compare it to existing hairstyle\ngeneration approaches.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-18T19:19:32Z",
    "updated": "2023-12-18T19:19:32Z",
    "doi": null
  },
  "1801.03244": {
    "id": "http://arxiv.org/abs/1801.03244v1",
    "title": "eCommerceGAN : A Generative Adversarial Network for E-commerce",
    "authors": [
      "Ashutosh Kumar",
      "Arijit Biswas",
      "Subhajit Sanyal"
    ],
    "abstract": "  E-commerce companies such as Amazon, Alibaba and Flipkart process billions of\norders every year. However, these orders represent only a small fraction of all\nplausible orders. Exploring the space of all plausible orders could help us\nbetter understand the relationships between the various entities in an\ne-commerce ecosystem, namely the customers and the products they purchase. In\nthis paper, we propose a Generative Adversarial Network (GAN) for orders made\nin e-commerce websites. Once trained, the generator in the GAN could generate\nany number of plausible orders. Our contributions include: (a) creating a dense\nand low-dimensional representation of e-commerce orders, (b) train an\necommerceGAN (ecGAN) with real orders to show the feasibility of the proposed\nparadigm, and (c) train an ecommerce-conditional-GAN (ec^2GAN) to generate the\nplausible orders involving a particular product. We propose several qualitative\nmethods to evaluate ecGAN and demonstrate its effectiveness. The ec^2GAN is\nused for various kinds of characterization of possible orders involving a\nproduct that has just been introduced into the e-commerce system. The proposed\napproach ec^2GAN performs significantly better than the baseline in most of the\nscenarios.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-01-10T05:58:09Z",
    "updated": "2018-01-10T05:58:09Z",
    "doi": null
  },
  "2406.11280": {
    "id": "http://arxiv.org/abs/2406.11280v1",
    "title": "i-SRT: Aligning Large Multimodal Models for Videos by Iterative\n  Self-Retrospective Judgment",
    "authors": [
      "Daechul Ahn",
      "Yura Choi",
      "San Kim",
      "Youngjae Yu",
      "Dongyeop Kang",
      "Jonghyun Choi"
    ],
    "abstract": "  Aligning Video Large Multimodal Models (VLMMs) face challenges such as\nmodality misalignment and verbose responses. Although iterative approaches such\nas self-rewarding or iterative direct preference optimization (DPO) recently\nshowed a significant improvement in language model alignment, particularly on\nreasoning tasks, self-aligned models applied to large video-language models\noften result in lengthy and irrelevant responses. To address these challenges,\nwe propose a novel method that employs self-retrospection to enhance both\nresponse generation and preference modeling, and call iterative\nself-retrospective judgment (i-SRT). By revisiting and evaluating already\ngenerated content and preference in loop, i-SRT improves the alignment between\ntextual and visual modalities, reduce verbosity, and enhances content\nrelevance. Our empirical evaluations across diverse video question answering\nbenchmarks demonstrate that i-SRT significantly outperforms prior arts. We are\ncommitted to opensourcing our code, models, and datasets to encourage further\ninvestigation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-17T07:33:30Z",
    "updated": "2024-06-17T07:33:30Z",
    "doi": null
  },
  "2306.00783": {
    "id": "http://arxiv.org/abs/2306.00783v2",
    "title": "FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and\n  Relighting with Diffusion Models",
    "authors": [
      "Hao Zhang",
      "Yanbo Xu",
      "Tianyuan Dai",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ],
    "abstract": "  The ability to create high-quality 3D faces from a single image has become\nincreasingly important with wide applications in video conferencing, AR/VR, and\nadvanced video editing in movie industries. In this paper, we propose Face\nDiffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality\nFace NeRFs from single images, complete with semantic editing and relighting\ncapabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly\ntrained 2D latent-diffusion model, allowing users to manipulate and construct\nFace NeRFs in zero-shot learning without the need for explicit 3D data. With\ncarefully designed illumination and identity preserving loss, as well as\nmulti-modal pre-training, FaceDNeRF offers users unparalleled control over the\nediting process enabling them to create and edit face NeRFs using just\nsingle-view images, text prompts, and explicit target lighting. The advanced\nfeatures of FaceDNeRF have been designed to produce more impressive results\nthan existing 2D editing approaches that rely on 2D segmentation maps for\neditable attributes. Experiments show that our FaceDNeRF achieves exceptionally\nrealistic results and unprecedented flexibility in editing compared with\nstate-of-the-art 3D face reconstruction and editing methods. Our code will be\navailable at https://github.com/BillyXYB/FaceDNeRF.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-01T15:14:39Z",
    "updated": "2023-12-04T16:25:49Z",
    "doi": null
  },
  "2306.12991": {
    "id": "http://arxiv.org/abs/2306.12991v2",
    "title": "Speech Emotion Diarization: Which Emotion Appears When?",
    "authors": [
      "Yingzhi Wang",
      "Mirco Ravanelli",
      "Alya Yacoubi"
    ],
    "abstract": "  Speech Emotion Recognition (SER) typically relies on utterance-level\nsolutions. However, emotions conveyed through speech should be considered as\ndiscrete speech events with definite temporal boundaries, rather than\nattributes of the entire utterance. To reflect the fine-grained nature of\nspeech emotions, we propose a new task: Speech Emotion Diarization (SED). Just\nas Speaker Diarization answers the question of \"Who speaks when?\", Speech\nEmotion Diarization answers the question of \"Which emotion appears when?\". To\nfacilitate the evaluation of the performance and establish a common benchmark\nfor researchers, we introduce the Zaion Emotion Dataset (ZED), an openly\naccessible speech emotion dataset that includes non-acted emotions recorded in\nreal-life conditions, along with manually-annotated boundaries of emotion\nsegments within the utterance. We provide competitive baselines and open-source\nthe code and the pre-trained models.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-22T15:47:36Z",
    "updated": "2023-10-20T11:31:26Z",
    "doi": null
  },
  "2203.13817": {
    "id": "http://arxiv.org/abs/2203.13817v1",
    "title": "AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling",
    "authors": [
      "Ziqian Bai",
      "Timur Bagautdinov",
      "Javier Romero",
      "Michael Zollh\u00f6fer",
      "Ping Tan",
      "Shunsuke Saito"
    ],
    "abstract": "  Neural fields such as implicit surfaces have recently enabled avatar modeling\nfrom raw scans without explicit temporal correspondences. In this work, we\nexploit autoregressive modeling to further extend this notion to capture\ndynamic effects, such as soft-tissue deformations. Although autoregressive\nmodels are naturally capable of handling dynamics, it is non-trivial to apply\nthem to implicit representations, as explicit state decoding is infeasible due\nto prohibitive memory requirements. In this work, for the first time, we enable\nautoregressive modeling of implicit avatars. To reduce the memory bottleneck\nand efficiently model dynamic implicit surfaces, we introduce the notion of\narticulated observer points, which relate implicit states to the explicit\nsurface of a parametric human body model. We demonstrate that encoding implicit\nsurfaces as a set of height fields defined on articulated observer points leads\nto significantly better generalization compared to a latent representation. The\nexperiments show that our approach outperforms the state of the art, achieving\nplausible dynamic deformations even for unseen motions.\nhttps://zqbai-jeremy.github.io/autoavatar\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-25T17:59:59Z",
    "updated": "2022-03-25T17:59:59Z",
    "doi": null
  },
  "1901.11333": {
    "id": "http://arxiv.org/abs/1901.11333v4",
    "title": "IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and\n  Translation",
    "authors": [
      "Zhijing Jin",
      "Di Jin",
      "Jonas Mueller",
      "Nicholas Matthews",
      "Enrico Santus"
    ],
    "abstract": "  Text attribute transfer aims to automatically rewrite sentences such that\nthey possess certain linguistic attributes, while simultaneously preserving\ntheir semantic content. This task remains challenging due to a lack of\nsupervised parallel data. Existing approaches try to explicitly disentangle\ncontent and attribute information, but this is difficult and often results in\npoor content-preservation and ungrammaticality. In contrast, we propose a\nsimpler approach, Iterative Matching and Translation (IMaT), which: (1)\nconstructs a pseudo-parallel corpus by aligning a subset of semantically\nsimilar sentences from the source and the target corpora; (2) applies a\nstandard sequence-to-sequence model to learn the attribute transfer; (3)\niteratively improves the learned transfer function by refining imperfections in\nthe alignment. In sentiment modification and formality transfer tasks, our\nmethod outperforms complex state-of-the-art systems by a large margin. As an\nauxiliary contribution, we produce a publicly-available test set with\nhuman-generated transfer references.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-01-31T12:41:57Z",
    "updated": "2020-01-24T05:12:45Z",
    "doi": null
  },
  "2306.01732": {
    "id": "http://arxiv.org/abs/2306.01732v1",
    "title": "Video Colorization with Pre-trained Text-to-Image Diffusion Models",
    "authors": [
      "Hanyuan Liu",
      "Minshan Xie",
      "Jinbo Xing",
      "Chengze Li",
      "Tien-Tsin Wong"
    ],
    "abstract": "  Video colorization is a challenging task that involves inferring plausible\nand temporally consistent colors for grayscale frames. In this paper, we\npresent ColorDiffuser, an adaptation of a pre-trained text-to-image latent\ndiffusion model for video colorization. With the proposed adapter-based\napproach, we repropose the pre-trained text-to-image model to accept input\ngrayscale video frames, with the optional text description, for video\ncolorization. To enhance the temporal coherence and maintain the vividness of\ncolorization across frames, we propose two novel techniques: the Color\nPropagation Attention and Alternated Sampling Strategy. Color Propagation\nAttention enables the model to refine its colorization decision based on a\nreference latent frame, while Alternated Sampling Strategy captures\nspatiotemporal dependencies by using the next and previous adjacent latent\nframes alternatively as reference during the generative diffusion sampling\nsteps. This encourages bidirectional color information propagation between\nadjacent video frames, leading to improved color consistency across frames. We\nconduct extensive experiments on benchmark datasets, and the results\ndemonstrate the effectiveness of our proposed framework. The evaluations show\nthat ColorDiffuser achieves state-of-the-art performance in video colorization,\nsurpassing existing methods in terms of color fidelity, temporal consistency,\nand visual quality.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-02T17:58:00Z",
    "updated": "2023-06-02T17:58:00Z",
    "doi": null
  },
  "2311.10089": {
    "id": "http://arxiv.org/abs/2311.10089v1",
    "title": "Emu Edit: Precise Image Editing via Recognition and Generation Tasks",
    "authors": [
      "Shelly Sheynin",
      "Adam Polyak",
      "Uriel Singer",
      "Yuval Kirstain",
      "Amit Zohar",
      "Oron Ashual",
      "Devi Parikh",
      "Yaniv Taigman"
    ],
    "abstract": "  Instruction-based image editing holds immense potential for a variety of\napplications, as it enables users to perform any editing operation using a\nnatural language instruction. However, current models in this domain often\nstruggle with accurately executing user instructions. We present Emu Edit, a\nmulti-task image editing model which sets state-of-the-art results in\ninstruction-based image editing. To develop Emu Edit we train it to multi-task\nacross an unprecedented range of tasks, such as region-based editing, free-form\nediting, and Computer Vision tasks, all of which are formulated as generative\ntasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we\nprovide it with learned task embeddings which guide the generation process\ntowards the correct edit type. Both these elements are essential for Emu Edit's\noutstanding performance. Furthermore, we show that Emu Edit can generalize to\nnew tasks, such as image inpainting, super-resolution, and compositions of\nediting tasks, with just a few labeled examples. This capability offers a\nsignificant advantage in scenarios where high-quality samples are scarce.\nLastly, to facilitate a more rigorous and informed assessment of instructable\nimage editing models, we release a new challenging and versatile benchmark that\nincludes seven different image editing tasks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-16T18:55:58Z",
    "updated": "2023-11-16T18:55:58Z",
    "doi": null
  },
  "2312.16457": {
    "id": "http://arxiv.org/abs/2312.16457v2",
    "title": "City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web",
    "authors": [
      "Kaiwen Song",
      "Xiaoyi Zeng",
      "Chenqu Ren",
      "Juyong Zhang"
    ],
    "abstract": "  Existing neural radiance field-based methods can achieve real-time rendering\nof small scenes on the web platform. However, extending these methods to\nlarge-scale scenes still poses significant challenges due to limited resources\nin computation, memory, and bandwidth. In this paper, we propose City-on-Web,\nthe first method for real-time rendering of large-scale scenes on the web. We\npropose a block-based volume rendering method to guarantee 3D consistency and\ncorrect occlusion between blocks, and introduce a Level-of-Detail strategy\ncombined with dynamic loading/unloading of resources to significantly reduce\nmemory demands. Our system achieves real-time rendering of large-scale scenes\nat approximately 32FPS with RTX 3060 GPU on the web and maintains rendering\nquality comparable to the current state-of-the-art novel view synthesis\nmethods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-27T08:00:47Z",
    "updated": "2024-04-01T03:10:53Z",
    "doi": null
  },
  "2310.02242": {
    "id": "http://arxiv.org/abs/2310.02242v1",
    "title": "Hierarchical Generation of Human-Object Interactions with Diffusion\n  Probabilistic Models",
    "authors": [
      "Huaijin Pi",
      "Sida Peng",
      "Minghui Yang",
      "Xiaowei Zhou",
      "Hujun Bao"
    ],
    "abstract": "  This paper presents a novel approach to generating the 3D motion of a human\ninteracting with a target object, with a focus on solving the challenge of\nsynthesizing long-range and diverse motions, which could not be fulfilled by\nexisting auto-regressive models or path planning-based methods. We propose a\nhierarchical generation framework to solve this challenge. Specifically, our\nframework first generates a set of milestones and then synthesizes the motion\nalong them. Therefore, the long-range motion generation could be reduced to\nsynthesizing several short motion sequences guided by milestones. The\nexperiments on the NSM, COUCH, and SAMP datasets show that our approach\noutperforms previous methods by a large margin in both quality and diversity.\nThe source code is available on our project page\nhttps://zju3dv.github.io/hghoi.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-03T17:50:23Z",
    "updated": "2023-10-03T17:50:23Z",
    "doi": null
  },
  "2302.04869": {
    "id": "http://arxiv.org/abs/2302.04869v1",
    "title": "Reversible Vision Transformers",
    "authors": [
      "Karttikeya Mangalam",
      "Haoqi Fan",
      "Yanghao Li",
      "Chao-Yuan Wu",
      "Bo Xiong",
      "Christoph Feichtenhofer",
      "Jitendra Malik"
    ],
    "abstract": "  We present Reversible Vision Transformers, a memory efficient architecture\ndesign for visual recognition. By decoupling the GPU memory requirement from\nthe depth of the model, Reversible Vision Transformers enable scaling up\narchitectures with efficient memory usage. We adapt two popular models, namely\nVision Transformer and Multiscale Vision Transformers, to reversible variants\nand benchmark extensively across both model sizes and tasks of image\nclassification, object detection and video classification. Reversible Vision\nTransformers achieve a reduced memory footprint of up to 15.5x at roughly\nidentical model complexity, parameters and accuracy, demonstrating the promise\nof reversible vision transformers as an efficient backbone for hardware\nresource limited training regimes. Finally, we find that the additional\ncomputational burden of recomputing activations is more than overcome for\ndeeper models, where throughput can increase up to 2.3x over their\nnon-reversible counterparts. Full code and trained models are available at\nhttps://github.com/facebookresearch/slowfast. A simpler, easy to understand and\nmodify version is also available at https://github.com/karttikeya/minREV\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-09T18:59:54Z",
    "updated": "2023-02-09T18:59:54Z",
    "doi": null
  },
  "2207.11718": {
    "id": "http://arxiv.org/abs/2207.11718v1",
    "title": "TIPS: Text-Induced Pose Synthesis",
    "authors": [
      "Prasun Roy",
      "Subhankar Ghosh",
      "Saumik Bhattacharya",
      "Umapada Pal",
      "Michael Blumenstein"
    ],
    "abstract": "  In computer vision, human pose synthesis and transfer deal with probabilistic\nimage generation of a person in a previously unseen pose from an already\navailable observation of that person. Though researchers have recently proposed\nseveral methods to achieve this task, most of these techniques derive the\ntarget pose directly from the desired target image on a specific dataset,\nmaking the underlying process challenging to apply in real-world scenarios as\nthe generation of the target image is the actual aim. In this paper, we first\npresent the shortcomings of current pose transfer algorithms and then propose a\nnovel text-based pose transfer technique to address those issues. We divide the\nproblem into three independent stages: (a) text to pose representation, (b)\npose refinement, and (c) pose rendering. To the best of our knowledge, this is\none of the first attempts to develop a text-based pose transfer framework where\nwe also introduce a new dataset DF-PASS, by adding descriptive pose annotations\nfor the images of the DeepFashion dataset. The proposed method generates\npromising results with significant qualitative and quantitative scores in our\nexperiments.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-24T11:14:46Z",
    "updated": "2022-07-24T11:14:46Z",
    "doi": null
  },
  "2206.00927": {
    "id": "http://arxiv.org/abs/2206.00927v3",
    "title": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling\n  in Around 10 Steps",
    "authors": [
      "Cheng Lu",
      "Yuhao Zhou",
      "Fan Bao",
      "Jianfei Chen",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "abstract": "  Diffusion probabilistic models (DPMs) are emerging powerful generative\nmodels. Despite their high-quality generation performance, DPMs still suffer\nfrom their slow sampling as they generally need hundreds or thousands of\nsequential function evaluations (steps) of large neural networks to draw a\nsample. Sampling from DPMs can be viewed alternatively as solving the\ncorresponding diffusion ordinary differential equations (ODEs). In this work,\nwe propose an exact formulation of the solution of diffusion ODEs. The\nformulation analytically computes the linear part of the solution, rather than\nleaving all terms to black-box ODE solvers as adopted in previous works. By\napplying change-of-variable, the solution can be equivalently simplified to an\nexponentially weighted integral of the neural network. Based on our\nformulation, we propose DPM-Solver, a fast dedicated high-order solver for\ndiffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for\nboth discrete-time and continuous-time DPMs without any further training.\nExperimental results show that DPM-Solver can generate high-quality samples in\nonly 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in\n10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10\ndataset, and a $4\\sim 16\\times$ speedup compared with previous state-of-the-art\ntraining-free samplers on various datasets.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-02T08:43:16Z",
    "updated": "2022-10-13T12:04:36Z",
    "doi": null
  },
  "2111.13677": {
    "id": "http://arxiv.org/abs/2111.13677v3",
    "title": "SWAT: Spatial Structure Within and Among Tokens",
    "authors": [
      "Kumara Kahatapitiya",
      "Michael S. Ryoo"
    ],
    "abstract": "  Modeling visual data as tokens (i.e., image patches) using attention\nmechanisms, feed-forward networks or convolutions has been highly effective in\nrecent years. Such methods usually have a common pipeline: a tokenization\nmethod, followed by a set of layers/blocks for information mixing, both within\nand among tokens. When image patches are converted into tokens, they are often\nflattened, discarding the spatial structure within each patch. As a result, any\nprocessing that follows (eg: multi-head self-attention) may fail to recover\nand/or benefit from such information. In this paper, we argue that models can\nhave significant gains when spatial structure is preserved during tokenization,\nand is explicitly used during the mixing stage. We propose two key\ncontributions: (1) Structure-aware Tokenization and, (2) Structure-aware\nMixing, both of which can be combined with existing models with minimal effort.\nWe introduce a family of models (SWAT), showing improvements over the likes of\nDeiT, MLP-Mixer and Swin Transformer, across multiple benchmarks including\nImageNet classification and ADE20K segmentation. Our code is available at\nhttps://github.com/kkahatapitiya/SWAT.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-26T18:59:38Z",
    "updated": "2023-11-20T16:37:05Z",
    "doi": null
  },
  "2110.02488": {
    "id": "http://arxiv.org/abs/2110.02488v2",
    "title": "ABC: Attention with Bounded-memory Control",
    "authors": [
      "Hao Peng",
      "Jungo Kasai",
      "Nikolaos Pappas",
      "Dani Yogatama",
      "Zhaofeng Wu",
      "Lingpeng Kong",
      "Roy Schwartz",
      "Noah A. Smith"
    ],
    "abstract": "  Transformer architectures have achieved state-of-the-art results on a variety\nof sequence modeling tasks. However, their attention mechanism comes with a\nquadratic complexity in sequence lengths, making the computational overhead\nprohibitive, especially for long sequences. Attention context can be seen as a\nrandom-access memory with each token taking a slot. Under this perspective, the\nmemory size grows linearly with the sequence length, and so does the overhead\nof reading from it. One way to improve the efficiency is to bound the memory\nsize. We show that disparate approaches can be subsumed into one abstraction,\nattention with bounded-memory control (ABC), and they vary in their\norganization of the memory. ABC reveals new, unexplored possibilities. First,\nit connects several efficient attention variants that would otherwise seem\napart. Second, this abstraction gives new insights--an established approach\n(Wang et al., 2020b) previously thought to be not applicable in causal\nattention, actually is. Last, we present a new instance of ABC, which draws\ninspiration from existing ABC approaches, but replaces their heuristic\nmemory-organizing functions with a learned, contextualized one. Our experiments\non language modeling, machine translation, and masked language model finetuning\nshow that our approach outperforms previous efficient attention models;\ncompared to the strong transformer baselines, it significantly improves the\ninference time and space efficiency with no or negligible accuracy loss.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-06T03:53:25Z",
    "updated": "2022-06-01T23:58:25Z",
    "doi": null
  },
  "1812.06162": {
    "id": "http://arxiv.org/abs/1812.06162v1",
    "title": "An Empirical Model of Large-Batch Training",
    "authors": [
      "Sam McCandlish",
      "Jared Kaplan",
      "Dario Amodei",
      "OpenAI Dota Team"
    ],
    "abstract": "  In an increasing number of domains it has been demonstrated that deep\nlearning models can be trained using relatively large batch sizes without\nsacrificing data efficiency. However the limits of this massive data\nparallelism seem to differ from domain to domain, ranging from batches of tens\nof thousands in ImageNet to batches of millions in RL agents that play the game\nDota 2. To our knowledge there is limited conceptual understanding of why these\nlimits to batch size differ or how we might choose the correct batch size in a\nnew domain. In this paper, we demonstrate that a simple and easy-to-measure\nstatistic called the gradient noise scale predicts the largest useful batch\nsize across many domains and applications, including a number of supervised\nlearning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word),\nreinforcement learning domains (Atari and Dota), and even generative model\ntraining (autoencoders on SVHN). We find that the noise scale increases as the\nloss decreases over a training run and depends on the model size primarily\nthrough improved model performance. Our empirically-motivated theory also\ndescribes the tradeoff between compute-efficiency and time-efficiency, and\nprovides a rough model of the benefits of adaptive batch-size training.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-12-14T20:49:09Z",
    "updated": "2018-12-14T20:49:09Z",
    "doi": null
  },
  "2207.11192": {
    "id": "http://arxiv.org/abs/2207.11192v2",
    "title": "Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image\n  Synthesis",
    "authors": [
      "Sangyun Lee",
      "Hyungjin Chung",
      "Jaehyeon Kim",
      "Jong Chul Ye"
    ],
    "abstract": "  Recently, diffusion models have shown remarkable results in image synthesis\nby gradually removing noise and amplifying signals. Although the simple\ngenerative process surprisingly works well, is this the best way to generate\nimage data? For instance, despite the fact that human perception is more\nsensitive to the low frequencies of an image, diffusion models themselves do\nnot consider any relative importance of each frequency component. Therefore, to\nincorporate the inductive bias for image data, we propose a novel generative\nprocess that synthesizes images in a coarse-to-fine manner. First, we\ngeneralize the standard diffusion models by enabling diffusion in a rotated\ncoordinate system with different velocities for each component of the vector.\nWe further propose a blur diffusion as a special case, where each frequency\ncomponent of an image is diffused at different speeds. Specifically, the\nproposed blur diffusion consists of a forward process that blurs an image and\nadds noise gradually, after which a corresponding reverse process deblurs an\nimage and removes noise progressively. Experiments show that the proposed model\noutperforms the previous method in FID on LSUN bedroom and church datasets.\nCode is available at https://github.com/sangyun884/blur-diffusion.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-16T15:00:21Z",
    "updated": "2022-11-21T12:44:45Z",
    "doi": null
  },
  "2207.01056": {
    "id": "http://arxiv.org/abs/2207.01056v2",
    "title": "Counterfactually Measuring and Eliminating Social Bias in\n  Vision-Language Pre-training Models",
    "authors": [
      "Yi Zhang",
      "Junyang Wang",
      "Jitao Sang"
    ],
    "abstract": "  Vision-Language Pre-training (VLP) models have achieved state-of-the-art\nperformance in numerous cross-modal tasks. Since they are optimized to capture\nthe statistical properties of intra- and inter-modality, there remains risk to\nlearn social biases presented in the data as well. In this work, we (1)\nintroduce a counterfactual-based bias measurement \\emph{CounterBias} to\nquantify the social bias in VLP models by comparing the [MASK]ed prediction\nprobabilities of factual and counterfactual samples; (2) construct a novel\nVL-Bias dataset including 24K image-text pairs for measuring gender bias in VLP\nmodels, from which we observed that significant gender bias is prevalent in VLP\nmodels; and (3) propose a VLP debiasing method \\emph{FairVLP} to minimize the\ndifference in the [MASK]ed prediction probabilities between factual and\ncounterfactual image-text pairs for VLP debiasing. Although CounterBias and\nFairVLP focus on social bias, they are generalizable to serve as tools and\nprovide new insights to probe and regularize more knowledge in VLP models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-03T14:39:32Z",
    "updated": "2022-07-12T02:31:50Z",
    "doi": null
  },
  "2305.07440": {
    "id": "http://arxiv.org/abs/2305.07440v2",
    "title": "Optimizing Memory Mapping Using Deep Reinforcement Learning",
    "authors": [
      "Pengming Wang",
      "Mikita Sazanovich",
      "Berkin Ilbeyi",
      "Phitchaya Mangpo Phothilimthana",
      "Manish Purohit",
      "Han Yang Tay",
      "Ng\u00e2n V\u0169",
      "Miaosen Wang",
      "Cosmin Paduraru",
      "Edouard Leurent",
      "Anton Zhernov",
      "Po-Sen Huang",
      "Julian Schrittwieser",
      "Thomas Hubert",
      "Robert Tung",
      "Paula Kurylowicz",
      "Kieran Milan",
      "Oriol Vinyals",
      "Daniel J. Mankowitz"
    ],
    "abstract": "  Resource scheduling and allocation is a critical component of many high\nimpact systems ranging from congestion control to cloud computing. Finding more\noptimal solutions to these problems often has significant impact on resource\nand time savings, reducing device wear-and-tear, and even potentially improving\ncarbon emissions. In this paper, we focus on a specific instance of a\nscheduling problem, namely the memory mapping problem that occurs during\ncompilation of machine learning programs: That is, mapping tensors to different\nmemory layers to optimize execution time.\n  We introduce an approach for solving the memory mapping problem using\nReinforcement Learning. RL is a solution paradigm well-suited for sequential\ndecision making problems that are amenable to planning, and combinatorial\nsearch spaces with high-dimensional data inputs. We formulate the problem as a\nsingle-player game, which we call the mallocGame, such that high-reward\ntrajectories of the game correspond to efficient memory mappings on the target\nhardware. We also introduce a Reinforcement Learning agent, mallocMuZero, and\nshow that it is capable of playing this game to discover new and improved\nmemory mapping solutions that lead to faster execution times on real ML\nworkloads on ML accelerators. We compare the performance of mallocMuZero to the\ndefault solver used by the Accelerated Linear Algebra (XLA) compiler on a\nbenchmark of realistic ML workloads. In addition, we show that mallocMuZero is\ncapable of improving the execution time of the recently published AlphaTensor\nmatrix multiplication model.\n",
    "categories": [
      {
        "@term": "cs.PF",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-11T11:55:16Z",
    "updated": "2023-10-17T09:53:45Z",
    "doi": null
  },
  "2010.04259": {
    "id": "http://arxiv.org/abs/2010.04259v1",
    "title": "Unsupervised Joint $k$-node Graph Representations with Compositional\n  Energy-Based Models",
    "authors": [
      "Leonardo Cotta",
      "Carlos H. C. Teixeira",
      "Ananthram Swami",
      "Bruno Ribeiro"
    ],
    "abstract": "  Existing Graph Neural Network (GNN) methods that learn inductive unsupervised\ngraph representations focus on learning node and edge representations by\npredicting observed edges in the graph. Although such approaches have shown\nadvances in downstream node classification tasks, they are ineffective in\njointly representing larger $k$-node sets, $k{>}2$. We propose MHM-GNN, an\ninductive unsupervised graph representation approach that combines joint\n$k$-node representations with energy-based models (hypergraph Markov networks)\nand GNNs. To address the intractability of the loss that arises from this\ncombination, we endow our optimization with a loss upper bound using a\nfinite-sample unbiased Markov Chain Monte Carlo estimator. Our experiments show\nthat the unsupervised MHM-GNN representations of MHM-GNN produce better\nunsupervised representations than existing approaches from the literature.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-10-08T21:13:37Z",
    "updated": "2020-10-08T21:13:37Z",
    "doi": null
  },
  "2310.14108": {
    "id": "http://arxiv.org/abs/2310.14108v1",
    "title": "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
    "authors": [
      "Mohammadreza Salehi",
      "Mehrdad Farajtabar",
      "Maxwell Horton",
      "Fartash Faghri",
      "Hadi Pouransari",
      "Raviteja Vemulapalli",
      "Oncel Tuzel",
      "Ali Farhadi",
      "Mohammad Rastegari",
      "Sachin Mehta"
    ],
    "abstract": "  Contrastive language image pretraining (CLIP) is a standard method for\ntraining vision-language models. While CLIP is scalable, promptable, and robust\nto distribution shifts on image classification tasks, it lacks object\nlocalization capabilities. This paper studies the following question: Can we\naugment CLIP training with task-specific vision models from model zoos to\nimprove its visual representations? Towards this end, we leverage open-source\ntask-specific vision models to generate pseudo-labels for an uncurated and\nnoisy image-text dataset. Subsequently, we train CLIP models on these\npseudo-labels in addition to the contrastive training on image and text pairs.\nThis simple setup shows substantial improvements of up to 16.3% across\ndifferent vision tasks, including segmentation, detection, depth estimation,\nand surface normal estimation. Importantly, these enhancements are achieved\nwithout compromising CLIP's existing capabilities, including its proficiency in\npromptable zero-shot classification.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-21T20:20:13Z",
    "updated": "2023-10-21T20:20:13Z",
    "doi": null
  },
  "1904.09092": {
    "id": "http://arxiv.org/abs/1904.09092v1",
    "title": "Weakly Supervised Adversarial Domain Adaptation for Semantic\n  Segmentation in Urban Scenes",
    "authors": [
      "Qi Wang",
      "Junyu Gao",
      "Xuelong Li"
    ],
    "abstract": "  Semantic segmentation, a pixel-level vision task, is developed rapidly by\nusing convolutional neural networks (CNNs). Training CNNs requires a large\namount of labeled data, but manually annotating data is difficult. For\nemancipating manpower, in recent years, some synthetic datasets are released.\nHowever, they are still different from real scenes, which causes that training\na model on the synthetic data (source domain) cannot achieve a good performance\non real urban scenes (target domain). In this paper, we propose a weakly\nsupervised adversarial domain adaptation to improve the segmentation\nperformance from synthetic data to real scenes, which consists of three deep\nneural networks. To be specific, a detection and segmentation (\"DS\" for short)\nmodel focuses on detecting objects and predicting segmentation map; a\npixel-level domain classifier (\"PDC\" for short) tries to distinguish the image\nfeatures from which domains; an object-level domain classifier (\"ODC\" for\nshort) discriminates the objects from which domains and predicts the objects\nclasses. PDC and ODC are treated as the discriminators, and DS is considered as\nthe generator. By adversarial learning, DS is supposed to learn\ndomain-invariant features. In experiments, our proposed method yields the new\nrecord of mIoU metric in the same problem.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-04-19T06:30:36Z",
    "updated": "2019-04-19T06:30:36Z",
    "doi": "10.1109/TIP.2019.2910667"
  },
  "2202.07603": {
    "id": "http://arxiv.org/abs/2202.07603v1",
    "title": "Fairness Indicators for Systematic Assessments of Visual Feature\n  Extractors",
    "authors": [
      "Priya Goyal",
      "Adriana Romero Soriano",
      "Caner Hazirbas",
      "Levent Sagun",
      "Nicolas Usunier"
    ],
    "abstract": "  Does everyone equally benefit from computer vision systems? Answers to this\nquestion become more and more important as computer vision systems are deployed\nat large scale, and can spark major concerns when they exhibit vast performance\ndiscrepancies between people from various demographic and social backgrounds.\nSystematic diagnosis of fairness, harms, and biases of computer vision systems\nis an important step towards building socially responsible systems. To initiate\nan effort towards standardized fairness audits, we propose three fairness\nindicators, which aim at quantifying harms and biases of visual systems. Our\nindicators use existing publicly available datasets collected for fairness\nevaluations, and focus on three main types of harms and bias identified in the\nliterature, namely harmful label associations, disparity in learned\nrepresentations of social and demographic traits, and biased performance on\ngeographically diverse images from across the world.We define precise\nexperimental protocols applicable to a wide range of computer vision models.\nThese indicators are part of an ever-evolving suite of fairness probes and are\nnot intended to be a substitute for a thorough analysis of the broader impact\nof the new computer vision technologies. Yet, we believe it is a necessary\nfirst step towards (1) facilitating the widespread adoption and mandate of the\nfairness assessments in computer vision research, and (2) tracking progress\ntowards building socially responsible models. To study the practical\neffectiveness and broad applicability of our proposed indicators to any visual\nsystem, we apply them to off-the-shelf models built using widely adopted model\ntraining paradigms which vary in their ability to whether they can predict\nlabels on a given image or only produce the embeddings. We also systematically\nstudy the effect of data domain and model size.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CY",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-02-15T17:45:33Z",
    "updated": "2022-02-15T17:45:33Z",
    "doi": null
  },
  "2207.04132": {
    "id": "http://arxiv.org/abs/2207.04132v2",
    "title": "Cross-Attention Transformer for Video Interpolation",
    "authors": [
      "Hannah Halin Kim",
      "Shuzhi Yu",
      "Shuai Yuan",
      "Carlo Tomasi"
    ],
    "abstract": "  We propose TAIN (Transformers and Attention for video INterpolation), a\nresidual neural network for video interpolation, which aims to interpolate an\nintermediate frame given two consecutive image frames around it. We first\npresent a novel vision transformer module, named Cross Similarity (CS), to\nglobally aggregate input image features with similar appearance as those of the\npredicted interpolated frame. These CS features are then used to refine the\ninterpolated prediction. To account for occlusions in the CS features, we\npropose an Image Attention (IA) module to allow the network to focus on CS\nfeatures from one frame over those of the other. TAIN outperforms existing\nmethods that do not require flow estimation and performs comparably to\nflow-based methods while being computationally efficient in terms of inference\ntime on Vimeo90k, UCF101, and SNU-FILM benchmarks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-08T21:38:54Z",
    "updated": "2022-12-02T02:48:37Z",
    "doi": null
  },
  "2106.00592": {
    "id": "http://arxiv.org/abs/2106.00592v2",
    "title": "Semi-Supervised Domain Generalization with Stochastic StyleMatch",
    "authors": [
      "Kaiyang Zhou",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "abstract": "  Ideally, visual learning algorithms should be generalizable, for dealing with\nany unseen domain shift when deployed in a new target environment; and\ndata-efficient, for reducing development costs by using as little labels as\npossible. To this end, we study semi-supervised domain generalization (SSDG),\nwhich aims to learn a domain-generalizable model using multi-source,\npartially-labeled training data. We design two benchmarks that cover\nstate-of-the-art methods developed in two related fields, i.e., domain\ngeneralization (DG) and semi-supervised learning (SSL). We find that the DG\nmethods, which by design are unable to handle unlabeled data, perform poorly\nwith limited labels in SSDG; the SSL methods, especially FixMatch, obtain much\nbetter results but are still far away from the basic vanilla model trained\nusing full labels. We propose StyleMatch, a simple approach that extends\nFixMatch with a couple of new ingredients tailored for SSDG: 1) stochastic\nmodeling for reducing overfitting in scarce labels, and 2) multi-view\nconsistency learning for enhancing domain generalization. Despite the concise\ndesigns, StyleMatch achieves significant improvements in SSDG. We hope our\napproach and the comprehensive benchmarks can pave the way for future research\non generalizable and data-efficient learning systems. The source code is\nreleased at \\url{https://github.com/KaiyangZhou/ssdg-benchmark}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-01T16:00:08Z",
    "updated": "2021-12-16T01:05:48Z",
    "doi": null
  },
  "2006.15437": {
    "id": "http://arxiv.org/abs/2006.15437v1",
    "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
    "authors": [
      "Ziniu Hu",
      "Yuxiao Dong",
      "Kuansan Wang",
      "Kai-Wei Chang",
      "Yizhou Sun"
    ],
    "abstract": "  Graph neural networks (GNNs) have been demonstrated to be powerful in\nmodeling graph-structured data. However, training GNNs usually requires\nabundant task-specific labeled data, which is often arduously expensive to\nobtain. One effective way to reduce the labeling effort is to pre-train an\nexpressive GNN model on unlabeled data with self-supervision and then transfer\nthe learned model to downstream tasks with only a few labels. In this paper, we\npresent the GPT-GNN framework to initialize GNNs by generative pre-training.\nGPT-GNN introduces a self-supervised attributed graph generation task to\npre-train a GNN so that it can capture the structural and semantic properties\nof the graph. We factorize the likelihood of the graph generation into two\ncomponents: 1) Attribute Generation and 2) Edge Generation. By modeling both\ncomponents, GPT-GNN captures the inherent dependency between node attributes\nand graph structure during the generative process. Comprehensive experiments on\nthe billion-scale Open Academic Graph and Amazon recommendation data\ndemonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models\nwithout pre-training by up to 9.1% across various downstream tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-06-27T20:12:33Z",
    "updated": "2020-06-27T20:12:33Z",
    "doi": null
  },
  "2305.11337": {
    "id": "http://arxiv.org/abs/2305.11337v1",
    "title": "RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent\n  Geometry and Texture",
    "authors": [
      "Liangchen Song",
      "Liangliang Cao",
      "Hongyu Xu",
      "Kai Kang",
      "Feng Tang",
      "Junsong Yuan",
      "Yang Zhao"
    ],
    "abstract": "  The techniques for 3D indoor scene capturing are widely used, but the meshes\nproduced leave much to be desired. In this paper, we propose \"RoomDreamer\",\nwhich leverages powerful natural language to synthesize a new room with a\ndifferent style. Unlike existing image synthesis methods, our work addresses\nthe challenge of synthesizing both geometry and texture aligned to the input\nscene structure and prompt simultaneously. The key insight is that a scene\nshould be treated as a whole, taking into account both scene texture and\ngeometry. The proposed framework consists of two significant components:\nGeometry Guided Diffusion and Mesh Optimization. Geometry Guided Diffusion for\n3D Scene guarantees the consistency of the scene style by applying the 2D prior\nto the entire scene simultaneously. Mesh Optimization improves the geometry and\ntexture jointly and eliminates the artifacts in the scanned scene. To validate\nthe proposed method, real indoor scenes scanned with smartphones are used for\nextensive experiments, through which the effectiveness of our method is\ndemonstrated.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-18T22:57:57Z",
    "updated": "2023-05-18T22:57:57Z",
    "doi": null
  },
  "2209.05773": {
    "id": "http://arxiv.org/abs/2209.05773v1",
    "title": "CAIBC: Capturing All-round Information Beyond Color for Text-based\n  Person Retrieval",
    "authors": [
      "Zijie Wang",
      "Aichun Zhu",
      "Jingyi Xue",
      "Xili Wan",
      "Chao Liu",
      "Tian Wang",
      "Yifeng Li"
    ],
    "abstract": "  Given a natural language description, text-based person retrieval aims to\nidentify images of a target person from a large-scale person image database.\nExisting methods generally face a \\textbf{color over-reliance problem}, which\nmeans that the models rely heavily on color information when matching\ncross-modal data. Indeed, color information is an important decision-making\naccordance for retrieval, but the over-reliance on color would distract the\nmodel from other key clues (e.g. texture information, structural information,\netc.), and thereby lead to a sub-optimal retrieval performance. To solve this\nproblem, in this paper, we propose to \\textbf{C}apture \\textbf{A}ll-round\n\\textbf{I}nformation \\textbf{B}eyond \\textbf{C}olor (\\textbf{CAIBC}) via a\njointly optimized multi-branch architecture for text-based person retrieval.\nCAIBC contains three branches including an RGB branch, a grayscale (GRS) branch\nand a color (CLR) branch. Besides, with the aim of making full use of all-round\ninformation in a balanced and effective way, a mutual learning mechanism is\nemployed to enable the three branches which attend to varied aspects of\ninformation to communicate with and learn from each other. Extensive\nexperimental analysis is carried out to evaluate our proposed CAIBC method on\nthe CUHK-PEDES and RSTPReid datasets in both \\textbf{supervised} and\n\\textbf{weakly supervised} text-based person retrieval settings, which\ndemonstrates that CAIBC significantly outperforms existing methods and achieves\nthe state-of-the-art performance on all the three tasks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-13T07:10:58Z",
    "updated": "2022-09-13T07:10:58Z",
    "doi": "10.1145/3503161.3548057"
  },
  "2305.18670": {
    "id": "http://arxiv.org/abs/2305.18670v2",
    "title": "SAVE: Spectral-Shift-Aware Adaptation of Image Diffusion Models for\n  Text-driven Video Editing",
    "authors": [
      "Nazmul Karim",
      "Umar Khalid",
      "Mohsen Joneidi",
      "Chen Chen",
      "Nazanin Rahnavard"
    ],
    "abstract": "  Text-to-Image (T2I) diffusion models have achieved remarkable success in\nsynthesizing high-quality images conditioned on text prompts. Recent methods\nhave tried to replicate the success by either training text-to-video (T2V)\nmodels on a very large number of text-video pairs or adapting T2I models on\ntext-video pairs independently. Although the latter is computationally less\nexpensive, it still takes a significant amount of time for per-video adaption.\nTo address this issue, we propose SAVE, a novel spectral-shift-aware adaptation\nframework, in which we fine-tune the spectral shift of the parameter space\ninstead of the parameters themselves. Specifically, we take the spectral\ndecomposition of the pre-trained T2I weights and only update the singular\nvalues while freezing the corresponding singular vectors. In addition, we\nintroduce a spectral shift regularizer aimed at placing tighter constraints on\nlarger singular values compared to smaller ones. This form of regularization\nenables the model to grasp finer details within the video that align with the\nprovided textual descriptions. We also offer theoretical justification for our\nproposed regularization technique. Since we are only dealing with spectral\nshifts, the proposed method reduces the adaptation time significantly (approx.\n10 times) and has fewer resource constraints for training. Such attributes\nposit SAVE to be more suitable for real-world applications, e.g. editing\nundesirable content during video streaming. We validate the effectiveness of\nSAVE with an extensive experimental evaluation under different settings, e.g.\nstyle transfer, object replacement, privacy preservation, etc.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-30T01:00:31Z",
    "updated": "2023-12-01T05:37:12Z",
    "doi": null
  },
  "2210.09729": {
    "id": "http://arxiv.org/abs/2210.09729v1",
    "title": "HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes",
    "authors": [
      "Zan Wang",
      "Yixin Chen",
      "Tengyu Liu",
      "Yixin Zhu",
      "Wei Liang",
      "Siyuan Huang"
    ],
    "abstract": "  Learning to generate diverse scene-aware and goal-oriented human motions in\n3D scenes remains challenging due to the mediocre characteristics of the\nexisting datasets on Human-Scene Interaction (HSI); they only have limited\nscale/quality and lack semantics. To fill in the gap, we propose a large-scale\nand semantic-rich synthetic HSI dataset, denoted as HUMANISE, by aligning the\ncaptured human motion sequences with various 3D indoor scenes. We automatically\nannotate the aligned motions with language descriptions that depict the action\nand the unique interacting objects in the scene; e.g., sit on the armchair near\nthe desk. HUMANISE thus enables a new generation task, language-conditioned\nhuman motion generation in 3D scenes. The proposed task is challenging as it\nrequires joint modeling of the 3D scene, human motion, and natural language. To\ntackle this task, we present a novel scene-and-language conditioned generative\nmodel that can produce 3D human motions of the desirable action interacting\nwith the specified objects. Our experiments demonstrate that our model\ngenerates diverse and semantically consistent human motions in 3D scenes.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-18T10:14:11Z",
    "updated": "2022-10-18T10:14:11Z",
    "doi": null
  },
  "2205.05982": {
    "id": "http://arxiv.org/abs/2205.05982v1",
    "title": "Vectorized and performance-portable Quicksort",
    "authors": [
      "Mark Blacher",
      "Joachim Giesen",
      "Peter Sanders",
      "Jan Wassenberg"
    ],
    "abstract": "  Recent works showed that implementations of Quicksort using vector CPU\ninstructions can outperform the non-vectorized algorithms in widespread use.\nHowever, these implementations are typically single-threaded, implemented for a\nparticular instruction set, and restricted to a small set of key types. We lift\nthese three restrictions: our proposed 'vqsort' algorithm integrates into the\nstate-of-the-art parallel sorter 'ips4o', with a geometric mean speedup of\n1.59. The same implementation works on seven instruction sets (including SVE\nand RISC-V V) across four platforms. It also supports floating-point and 16-128\nbit integer keys. To the best of our knowledge, this is the fastest sort for\nnon-tuple keys on CPUs, up to 20 times as fast as the sorting algorithms\nimplemented in standard libraries. This paper focuses on the practical\nengineering aspects enabling the speed and portability, which we have not yet\nseen demonstrated for a Quicksort implementation. Furthermore, we introduce\ncompact and transpose-free sorting networks for in-register sorting of small\narrays, and a vector-friendly pivot sampling strategy that is robust against\nadversarial input.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "H.3; C.4; D.1.3",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-12T09:41:31Z",
    "updated": "2022-05-12T09:41:31Z",
    "doi": null
  },
  "2102.06171": {
    "id": "http://arxiv.org/abs/2102.06171v1",
    "title": "High-Performance Large-Scale Image Recognition Without Normalization",
    "authors": [
      "Andrew Brock",
      "Soham De",
      "Samuel L. Smith",
      "Karen Simonyan"
    ],
    "abstract": "  Batch normalization is a key component of most image classification models,\nbut it has many undesirable properties stemming from its dependence on the\nbatch size and interactions between examples. Although recent work has\nsucceeded in training deep ResNets without normalization layers, these models\ndo not match the test accuracies of the best batch-normalized networks, and are\noften unstable for large learning rates or strong data augmentations. In this\nwork, we develop an adaptive gradient clipping technique which overcomes these\ninstabilities, and design a significantly improved class of Normalizer-Free\nResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on\nImageNet while being up to 8.7x faster to train, and our largest models attain\na new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free\nmodels attain significantly better performance than their batch-normalized\ncounterparts when finetuning on ImageNet after large-scale pre-training on a\ndataset of 300 million labeled images, with our best models obtaining an\naccuracy of 89.2%. Our code is available at https://github.com/deepmind/\ndeepmind-research/tree/master/nfnets\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-02-11T18:23:20Z",
    "updated": "2021-02-11T18:23:20Z",
    "doi": null
  },
  "2007.08668": {
    "id": "http://arxiv.org/abs/2007.08668v4",
    "title": "BRP-NAS: Prediction-based NAS using GCNs",
    "authors": [
      "\u0141ukasz Dudziak",
      "Thomas Chau",
      "Mohamed S. Abdelfattah",
      "Royson Lee",
      "Hyeji Kim",
      "Nicholas D. Lane"
    ],
    "abstract": "  Neural architecture search (NAS) enables researchers to automatically explore\nbroad design spaces in order to improve efficiency of neural networks. This\nefficiency is especially important in the case of on-device deployment, where\nimprovements in accuracy should be balanced out with computational demands of a\nmodel. In practice, performance metrics of model are computationally expensive\nto obtain. Previous work uses a proxy (e.g., number of operations) or a\nlayer-wise measurement of neural network layers to estimate end-to-end hardware\nperformance but the imprecise prediction diminishes the quality of NAS. To\naddress this problem, we propose BRP-NAS, an efficient hardware-aware NAS\nenabled by an accurate performance predictor-based on graph convolutional\nnetwork (GCN). What is more, we investigate prediction quality on different\nmetrics and show that sample efficiency of the predictor-based NAS can be\nimproved by considering binary relations of models and an iterative data\nselection strategy. We show that our proposed method outperforms all prior\nmethods on NAS-Bench-101 and NAS-Bench-201, and that our predictor can\nconsistently learn to extract useful features from the DARTS search space,\nimproving upon the second-order baseline. Finally, to raise awareness of the\nfact that accurate latency estimation is not a trivial task, we release\nLatBench -- a latency dataset of NAS-Bench-201 models running on a broad range\nof devices.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.SP",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-16T21:58:43Z",
    "updated": "2021-01-19T17:29:16Z",
    "doi": null
  },
  "1903.07291": {
    "id": "http://arxiv.org/abs/1903.07291v2",
    "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
    "authors": [
      "Taesung Park",
      "Ming-Yu Liu",
      "Ting-Chun Wang",
      "Jun-Yan Zhu"
    ],
    "abstract": "  We propose spatially-adaptive normalization, a simple but effective layer for\nsynthesizing photorealistic images given an input semantic layout. Previous\nmethods directly feed the semantic layout as input to the deep network, which\nis then processed through stacks of convolution, normalization, and\nnonlinearity layers. We show that this is suboptimal as the normalization\nlayers tend to ``wash away'' semantic information. To address the issue, we\npropose using the input layout for modulating the activations in normalization\nlayers through a spatially-adaptive, learned transformation. Experiments on\nseveral challenging datasets demonstrate the advantage of the proposed method\nover existing approaches, regarding both visual fidelity and alignment with\ninput layouts. Finally, our model allows user control over both semantic and\nstyle. Code is available at https://github.com/NVlabs/SPADE .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.5; I.5.4; I.3.3",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-03-18T08:12:23Z",
    "updated": "2019-11-05T15:41:27Z",
    "doi": null
  },
  "2206.02780": {
    "id": "http://arxiv.org/abs/2206.02780v2",
    "title": "GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions",
    "authors": [
      "Gene Chou",
      "Ilya Chugunov",
      "Felix Heide"
    ],
    "abstract": "  We investigate the generalization capabilities of neural signed distance\nfunctions (SDFs) for learning 3D object representations for unseen and\nunlabeled point clouds. Existing methods can fit SDFs to a handful of object\nclasses and boast fine detail or fast inference speeds, but do not generalize\nwell to unseen shapes. We introduce a two-stage semi-supervised meta-learning\napproach that transfers shape priors from labeled to unlabeled data to\nreconstruct unseen object categories. The first stage uses an episodic training\nscheme to simulate training on unlabeled data and meta-learns initial shape\npriors. The second stage then introduces unlabeled data with disjoint classes\nin a semi-supervised scheme to diversify these priors and achieve\ngeneralization. We assess our method on both synthetic data and real collected\npoint clouds. Experimental results and analysis validate that our approach\noutperforms existing neural SDF methods and is capable of robust zero-shot\ninference on 100+ unseen classes. Code can be found at\nhttps://github.com/princeton-computational-imaging/gensdf.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-06T17:58:29Z",
    "updated": "2022-10-05T02:09:09Z",
    "doi": null
  },
  "2210.02390": {
    "id": "http://arxiv.org/abs/2210.02390v3",
    "title": "Bayesian Prompt Learning for Image-Language Model Generalization",
    "authors": [
      "Mohammad Mahdi Derakhshani",
      "Enrique Sanchez",
      "Adrian Bulat",
      "Victor Guilherme Turrisi da Costa",
      "Cees G. M. Snoek",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ],
    "abstract": "  Foundational image-language models have generated considerable interest due\nto their efficient adaptation to downstream tasks by prompt learning. Prompt\nlearning treats part of the language model input as trainable while freezing\nthe rest, and optimizes an Empirical Risk Minimization objective. However,\nEmpirical Risk Minimization is known to suffer from distributional shifts which\nhurt generalizability to prompts unseen during training. By leveraging the\nregularization ability of Bayesian methods, we frame prompt learning from the\nBayesian perspective and formulate it as a variational inference problem. Our\napproach regularizes the prompt space, reduces overfitting to the seen prompts\nand improves the prompt generalization on unseen prompts. Our framework is\nimplemented by modeling the input prompt space in a probabilistic manner, as an\na priori distribution which makes our proposal compatible with prompt learning\napproaches that are unconditional or conditional on the image. We demonstrate\nempirically on 15 benchmarks that Bayesian prompt learning provides an\nappropriate coverage of the prompt space, prevents learning spurious features,\nand exploits transferable invariant features. This results in better\ngeneralization of unseen prompts, even across different datasets and domains.\nCode available at: https://github.com/saic-fi/Bayesian-Prompt-Learning\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-05T17:05:56Z",
    "updated": "2023-08-20T13:08:34Z",
    "doi": null
  },
  "1912.04216": {
    "id": "http://arxiv.org/abs/1912.04216v2",
    "title": "cGANs with Multi-Hinge Loss",
    "authors": [
      "Ilya Kavalerov",
      "Wojciech Czaja",
      "Rama Chellappa"
    ],
    "abstract": "  We propose a new algorithm to incorporate class conditional information into\nthe critic of GANs via a multi-class generalization of the commonly used Hinge\nloss that is compatible with both supervised and semi-supervised settings. We\nstudy the compromise between training a state of the art generator and an\naccurate classifier simultaneously, and propose a way to use our algorithm to\nmeasure the degree to which a generator and critic are class conditional. We\nshow the trade-off between a generator-critic pair respecting class\nconditioning inputs and generating the highest quality images. With our\nmulti-hinge loss modification we are able to improve Inception Scores and\nFrechet Inception Distance on the Imagenet dataset. We make our tensorflow code\navailable at https://github.com/ilyakava/gan.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-12-09T17:51:50Z",
    "updated": "2020-11-21T21:01:27Z",
    "doi": null
  },
  "2207.09763": {
    "id": "http://arxiv.org/abs/2207.09763v1",
    "title": "GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D\n  LiDAR Segmentation",
    "authors": [
      "Cristiano Saltori",
      "Evgeny Krivosheev",
      "St\u00e9phane Lathuili\u00e8re",
      "Nicu Sebe",
      "Fabio Galasso",
      "Giuseppe Fiameni",
      "Elisa Ricci",
      "Fabio Poiesi"
    ],
    "abstract": "  3D point cloud semantic segmentation is fundamental for autonomous driving.\nMost approaches in the literature neglect an important aspect, i.e., how to\ndeal with domain shift when handling dynamic scenes. This can significantly\nhinder the navigation capabilities of self-driving vehicles. This paper\nadvances the state of the art in this research field. Our first contribution\nconsists in analysing a new unexplored scenario in point cloud segmentation,\nnamely Source-Free Online Unsupervised Domain Adaptation (SF-OUDA). We\nexperimentally show that state-of-the-art methods have a rather limited ability\nto adapt pre-trained deep network models to unseen domains in an online manner.\nOur second contribution is an approach that relies on adaptive self-training\nand geometric-feature propagation to adapt a pre-trained source model online\nwithout requiring either source data or target labels. Our third contribution\nis to study SF-OUDA in a challenging setup where source data is synthetic and\ntarget data is point clouds captured in the real world. We use the recent\nSynLiDAR dataset as a synthetic source and introduce two new synthetic (source)\ndatasets, which can stimulate future synthetic-to-real autonomous driving\nresearch. Our experiments show the effectiveness of our segmentation approach\non thousands of real-world point clouds. Code and synthetic datasets are\navailable at https://github.com/saltoricristiano/gipso-sfouda.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-20T09:06:07Z",
    "updated": "2022-07-20T09:06:07Z",
    "doi": null
  },
  "2312.02120": {
    "id": "http://arxiv.org/abs/2312.02120v2",
    "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
    "authors": [
      "Yuxiang Wei",
      "Zhe Wang",
      "Jiawei Liu",
      "Yifeng Ding",
      "Lingming Zhang"
    ],
    "abstract": "  We introduce Magicoder, a series of fully open-source (code, weights, and\ndata) Large Language Models (LLMs) for code that significantly closes the gap\nwith top code models while having no more than 7B parameters. Magicoder models\nare trained on 75K synthetic instruction data using OSS-Instruct, a novel\napproach to enlightening LLMs with open-source code snippets to generate\ndiverse instruction data for code. Our main motivation is to mitigate the\ninherent bias of the synthetic data generated by LLMs through the wealth of\nopen-source references for the production of more realistic and controllable\ndata. The orthogonality of OSS-Instruct and other data generation methods like\nEvol-Instruct further enables us to build an enhanced MagicoderS. Both\nMagicoder and MagicoderS substantially outperform state-of-the-art code models\nwith similar or even larger sizes on a wide range of coding benchmarks.\nNotably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent\nChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a\nnew direction for crafting diverse synthetic instruction data for code using\nabundant open-source references.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-04T18:50:35Z",
    "updated": "2024-06-07T02:50:56Z",
    "doi": null
  },
  "2408.00998": {
    "id": "http://arxiv.org/abs/2408.00998v2",
    "title": "FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation",
    "authors": [
      "Xiang Gao",
      "Jiaying Liu"
    ],
    "abstract": "  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-02T04:13:38Z",
    "updated": "2024-08-06T12:01:17Z",
    "doi": null
  },
  "2303.17968": {
    "id": "http://arxiv.org/abs/2303.17968v1",
    "title": "VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence\n  Normalization",
    "authors": [
      "Bingfan Zhu",
      "Yanchao Yang",
      "Xulong Wang",
      "Youyi Zheng",
      "Leonidas Guibas"
    ],
    "abstract": "  We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) for\nbetter geometry under non-Lambertian surface and dynamic lighting conditions\nthat cause significant variation in the radiance of a point when viewed from\ndifferent angles. Instead of explicitly modeling the underlying factors that\nresult in the view-dependent phenomenon, which could be complex yet not\ninclusive, we develop a simple and effective technique that normalizes the\nview-dependence by distilling invariant information already encoded in the\nlearned NeRFs. We then jointly train NeRFs for view synthesis with\nview-dependence normalization to attain quality geometry. Our experiments show\nthat even though shape-radiance ambiguity is inevitable, the proposed\nnormalization can minimize its effect on geometry, which essentially aligns the\noptimal capacity needed for explaining view-dependent variations. Our method\napplies to various baselines and significantly improves geometry without\nchanging the volume rendering pipeline, even if the data is captured under a\nmoving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-31T11:13:17Z",
    "updated": "2023-03-31T11:13:17Z",
    "doi": null
  },
  "2210.01069": {
    "id": "http://arxiv.org/abs/2210.01069v1",
    "title": "Dual-former: Hybrid Self-attention Transformer for Efficient Image\n  Restoration",
    "authors": [
      "Sixiang Chen",
      "Tian Ye",
      "Yun Liu",
      "Erkang Chen"
    ],
    "abstract": "  Recently, image restoration transformers have achieved comparable performance\nwith previous state-of-the-art CNNs. However, how to efficiently leverage such\narchitectures remains an open problem. In this work, we present Dual-former\nwhose critical insight is to combine the powerful global modeling ability of\nself-attention modules and the local modeling ability of convolutions in an\noverall architecture. With convolution-based Local Feature Extraction modules\nequipped in the encoder and the decoder, we only adopt a novel Hybrid\nTransformer Block in the latent layer to model the long-distance dependence in\nspatial dimensions and handle the uneven distribution between channels. Such a\ndesign eliminates the substantial computational complexity in previous image\nrestoration transformers and achieves superior performance on multiple image\nrestoration tasks. Experiments demonstrate that Dual-former achieves a 1.91dB\ngain over the state-of-the-art MAXIM method on the Indoor dataset for single\nimage dehazing while consuming only 4.2% GFLOPs as MAXIM. For single image\nderaining, it exceeds the SOTA method by 0.1dB PSNR on the average results of\nfive datasets with only 21.5% GFLOPs. Dual-former also substantially surpasses\nthe latest desnowing method on various datasets, with fewer parameters.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-03T16:39:21Z",
    "updated": "2022-10-03T16:39:21Z",
    "doi": null
  },
  "2203.02358": {
    "id": "http://arxiv.org/abs/2203.02358v1",
    "title": "ViT-P: Rethinking Data-efficient Vision Transformers from Locality",
    "authors": [
      "Bin Chen",
      "Ran Wang",
      "Di Ming",
      "Xin Feng"
    ],
    "abstract": "  Recent advances of Transformers have brought new trust to computer vision\ntasks. However, on small dataset, Transformers is hard to train and has lower\nperformance than convolutional neural networks. We make vision transformers as\ndata-efficient as convolutional neural networks by introducing multi-focal\nattention bias. Inspired by the attention distance in a well-trained ViT, we\nconstrain the self-attention of ViT to have multi-scale localized receptive\nfield. The size of receptive field is adaptable during training so that optimal\nconfiguration can be learned. We provide empirical evidence that proper\nconstrain of receptive field can reduce the amount of training data for vision\ntransformers. On Cifar100, our ViT-P Base model achieves the state-of-the-art\naccuracy (83.16%) trained from scratch. We also perform analysis on ImageNet to\nshow our method does not lose accuracy on large data sets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-04T14:49:48Z",
    "updated": "2022-03-04T14:49:48Z",
    "doi": null
  },
  "2205.03891": {
    "id": "http://arxiv.org/abs/2205.03891v1",
    "title": "Cross-lingual Adaptation for Recipe Retrieval with Mixup",
    "authors": [
      "Bin Zhu",
      "Chong-Wah Ngo",
      "Jingjing Chen",
      "Wing-Kwong Chan"
    ],
    "abstract": "  Cross-modal recipe retrieval has attracted research attention in recent\nyears, thanks to the availability of large-scale paired data for training.\nNevertheless, obtaining adequate recipe-image pairs covering the majority of\ncuisines for supervised learning is difficult if not impossible. By\ntransferring knowledge learnt from a data-rich cuisine to a data-scarce\ncuisine, domain adaptation sheds light on this practical problem. Nevertheless,\nexisting works assume recipes in source and target domains are mostly\noriginated from the same cuisine and written in the same language. This paper\nstudies unsupervised domain adaptation for image-to-recipe retrieval, where\nrecipes in source and target domains are in different languages. Moreover, only\nrecipes are available for training in the target domain. A novel recipe mixup\nmethod is proposed to learn transferable embedding features between the two\ndomains. Specifically, recipe mixup produces mixed recipes to form an\nintermediate domain by discretely exchanging the section(s) between source and\ntarget recipes. To bridge the domain gap, recipe mixup loss is proposed to\nenforce the intermediate domain to locate in the shortest geodesic path between\nsource and target domains in the recipe embedding space. By using Recipe 1M\ndataset as source domain (English) and Vireo-FoodTransfer dataset as target\ndomain (Chinese), empirical experiments verify the effectiveness of recipe\nmixup for cross-lingual adaptation in the context of image-to-recipe retrieval.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-08T15:04:39Z",
    "updated": "2022-05-08T15:04:39Z",
    "doi": null
  },
  "2004.05498": {
    "id": "http://arxiv.org/abs/2004.05498v1",
    "title": "FDA: Fourier Domain Adaptation for Semantic Segmentation",
    "authors": [
      "Yanchao Yang",
      "Stefano Soatto"
    ],
    "abstract": "  We describe a simple method for unsupervised domain adaptation, whereby the\ndiscrepancy between the source and target distributions is reduced by swapping\nthe low-frequency spectrum of one with the other. We illustrate the method in\nsemantic segmentation, where densely annotated images are aplenty in one domain\n(synthetic data), but difficult to obtain in another (real images). Current\nstate-of-the-art methods are complex, some requiring adversarial optimization\nto render the backbone of a neural network invariant to the discrete domain\nselection variable. Our method does not require any training to perform the\ndomain alignment, just a simple Fourier Transform and its inverse. Despite its\nsimplicity, it achieves state-of-the-art performance in the current benchmarks,\nwhen integrated into a relatively standard semantic segmentation model. Our\nresults indicate that even simple procedures can discount nuisance variability\nin the data that more sophisticated methods struggle to learn away.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-11T22:20:48Z",
    "updated": "2020-04-11T22:20:48Z",
    "doi": null
  },
  "1606.01540": {
    "id": "http://arxiv.org/abs/1606.01540v1",
    "title": "OpenAI Gym",
    "authors": [
      "Greg Brockman",
      "Vicki Cheung",
      "Ludwig Pettersson",
      "Jonas Schneider",
      "John Schulman",
      "Jie Tang",
      "Wojciech Zaremba"
    ],
    "abstract": "  OpenAI Gym is a toolkit for reinforcement learning research. It includes a\ngrowing collection of benchmark problems that expose a common interface, and a\nwebsite where people can share their results and compare the performance of\nalgorithms. This whitepaper discusses the components of OpenAI Gym and the\ndesign decisions that went into the software.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-06-05T17:54:48Z",
    "updated": "2016-06-05T17:54:48Z",
    "doi": null
  },
  "2308.09932": {
    "id": "http://arxiv.org/abs/2308.09932v2",
    "title": "Unveiling Memorization in Code Models",
    "authors": [
      "Zhou Yang",
      "Zhipeng Zhao",
      "Chenyu Wang",
      "Jieke Shi",
      "Dongsun Kim",
      "DongGyun Han",
      "David Lo"
    ],
    "abstract": "  The availability of large-scale datasets, advanced architectures, and\npowerful computational resources have led to effective code models that\nautomate diverse software engineering activities. The datasets usually consist\nof billions of lines of code from both open-source and private repositories. A\ncode model memorizes and produces source code verbatim, which potentially\ncontains vulnerabilities, sensitive information, or code with strict licenses,\nleading to potential security and privacy issues. This paper investigates an\nimportant problem: to what extent do code models memorize their training data?\nWe conduct an empirical study to explore memorization in large pre-trained code\nmodels. Our study highlights that simply extracting 20,000 outputs (each having\n512 tokens) from a code model can produce over 40,125 code snippets that are\nmemorized from the training data. To provide a better understanding, we build a\ntaxonomy of memorized contents with 3 categories and 14 subcategories. The\nresults show that the prompts sent to the code models affect the distribution\nof memorized contents. We identify several key factors of memorization.\nSpecifically, given the same architecture, larger models suffer more from\nmemorization problems. A code model produces more memorization when it is\nallowed to generate longer outputs. We also find a strong positive correlation\nbetween the number of an output's occurrences in the training data and that in\nthe generated outputs, which indicates that a potential way to reduce\nmemorization is to remove duplicates in the training data. We then identify\neffective metrics that infer whether an output contains memorization\naccurately. We also make suggestions to deal with memorization.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-19T07:25:39Z",
    "updated": "2024-01-12T02:39:25Z",
    "doi": "10.1145/3597503.3639074"
  },
  "2303.11224": {
    "id": "http://arxiv.org/abs/2303.11224v1",
    "title": "Cascaded Latent Diffusion Models for High-Resolution Chest X-ray\n  Synthesis",
    "authors": [
      "Tobias Weber",
      "Michael Ingrisch",
      "Bernd Bischl",
      "David R\u00fcgamer"
    ],
    "abstract": "  While recent advances in large-scale foundational models show promising\nresults, their application to the medical domain has not yet been explored in\ndetail. In this paper, we progress into the realms of large-scale modeling in\nmedical synthesis by proposing Cheff - a foundational cascaded latent diffusion\nmodel, which generates highly-realistic chest radiographs providing\nstate-of-the-art quality on a 1-megapixel scale. We further propose MaCheX,\nwhich is a unified interface for public chest datasets and forms the largest\nopen collection of chest X-rays up to date. With Cheff conditioned on\nradiological reports, we further guide the synthesis process over text prompts\nand unveil the research area of report-to-chest-X-ray generation.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-20T16:00:20Z",
    "updated": "2023-03-20T16:00:20Z",
    "doi": null
  },
  "2209.10074": {
    "id": "http://arxiv.org/abs/2209.10074v1",
    "title": "PicT: A Slim Weakly Supervised Vision Transformer for Pavement Distress\n  Classification",
    "authors": [
      "Wenhao Tang",
      "Sheng Huang",
      "Xiaoxian Zhang",
      "Luwen Huangfu"
    ],
    "abstract": "  Automatic pavement distress classification facilitates improving the\nefficiency of pavement maintenance and reducing the cost of labor and\nresources. A recently influential branch of this task divides the pavement\nimage into patches and addresses these issues from the perspective of\nmulti-instance learning. However, these methods neglect the correlation between\npatches and suffer from a low efficiency in the model optimization and\ninference. Meanwhile, Swin Transformer is able to address both of these issues\nwith its unique strengths. Built upon Swin Transformer, we present a vision\nTransformer named \\textbf{P}avement \\textbf{I}mage \\textbf{C}lassification\n\\textbf{T}ransformer (\\textbf{PicT}) for pavement distress classification. In\norder to better exploit the discriminative information of pavement images at\nthe patch level, the \\textit{Patch Labeling Teacher} is proposed to leverage a\nteacher model to dynamically generate pseudo labels of patches from image\nlabels during each iteration, and guides the model to learn the discriminative\nfeatures of patches. The broad classification head of Swin Transformer may\ndilute the discriminative features of distressed patches in the feature\naggregation step due to the small distressed area ratio of the pavement image.\nTo overcome this drawback, we present a \\textit{Patch Refiner} to cluster\npatches into different groups and only select the highest distress-risk group\nto yield a slim head for the final image classification. We evaluate our method\non CQU-BPDD. Extensive results show that \\textbf{PicT} outperforms the\nsecond-best performed model by a large margin of $+2.4\\%$ in P@R on detection\ntask, $+3.9\\%$ in $F1$ on recognition task, and 1.8x throughput, while enjoying\n7x faster training speed using the same computing resources. Our codes and\nmodels have been released on\n\\href{https://github.com/DearCaat/PicT}{https://github.com/DearCaat/PicT}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-21T02:33:49Z",
    "updated": "2022-09-21T02:33:49Z",
    "doi": null
  },
  "2106.03798": {
    "id": "http://arxiv.org/abs/2106.03798v4",
    "title": "DoubleField: Bridging the Neural Surface and Radiance Fields for\n  High-fidelity Human Reconstruction and Rendering",
    "authors": [
      "Ruizhi Shao",
      "Hongwen Zhang",
      "He Zhang",
      "Mingjia Chen",
      "Yanpei Cao",
      "Tao Yu",
      "Yebin Liu"
    ],
    "abstract": "  We introduce DoubleField, a novel framework combining the merits of both\nsurface field and radiance field for high-fidelity human reconstruction and\nrendering. Within DoubleField, the surface field and radiance field are\nassociated together by a shared feature embedding and a surface-guided sampling\nstrategy. Moreover, a view-to-view transformer is introduced to fuse multi-view\nfeatures and learn view-dependent features directly from high-resolution\ninputs. With the modeling power of DoubleField and the view-to-view\ntransformer, our method significantly improves the reconstruction quality of\nboth geometry and appearance, while supporting direct inference, scene-specific\nhigh-resolution finetuning, and fast rendering. The efficacy of DoubleField is\nvalidated by the quantitative evaluations on several datasets and the\nqualitative results in a real-world sparse multi-view system, showing its\nsuperior capability for high-quality human model reconstruction and\nphoto-realistic free-viewpoint human rendering. Data and source code will be\nmade public for the research purpose. Please refer to our project page:\nhttp://www.liuyebin.com/dbfield/dbfield.html.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-07T17:08:17Z",
    "updated": "2022-03-27T18:14:48Z",
    "doi": null
  },
  "2108.10840": {
    "id": "http://arxiv.org/abs/2108.10840v1",
    "title": "Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark",
    "authors": [
      "Shuhao Qiu",
      "Chuang Zhu",
      "Wenli Zhou"
    ],
    "abstract": "  In recent years, deep learning-based methods have shown promising results in\ncomputer vision area. However, a common deep learning model requires a large\namount of labeled data, which is labor-intensive to collect and label. What's\nmore, the model can be ruined due to the domain shift between training data and\ntesting data. Text recognition is a broadly studied field in computer vision\nand suffers from the same problems noted above due to the diversity of fonts\nand complicated backgrounds. In this paper, we focus on the text recognition\nproblem and mainly make three contributions toward these problems. First, we\ncollect a multi-source domain adaptation dataset for text recognition,\nincluding five different domains with over five million images, which is the\nfirst multi-domain text recognition dataset to our best knowledge. Secondly, we\npropose a new method called Meta Self-Learning, which combines the\nself-learning method with the meta-learning paradigm and achieves a better\nrecognition result under the scene of multi-domain adaptation. Thirdly,\nextensive experiments are conducted on the dataset to provide a benchmark and\nalso show the effectiveness of our method. The code of our work and dataset are\navailable soon at https://bupt-ai-cz.github.io/Meta-SelfLearning/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-08-24T17:07:34Z",
    "updated": "2021-08-24T17:07:34Z",
    "doi": null
  },
  "2103.14659v1": {
    "id": "http://arxiv.org/abs/2103.14659v1",
    "title": "Alignment of Language Agents",
    "authors": [
      "Zachary Kenton",
      "Tom Everitt",
      "Laura Weidinger",
      "Iason Gabriel",
      "Vladimir Mikulik",
      "Geoffrey Irving"
    ],
    "abstract": "For artificial intelligence to be beneficial to humans the behaviour of AI\nagents needs to be aligned with what humans want. In this paper we discuss some\nbehavioural issues for language agents, arising from accidental\nmisspecification by the system designer. We highlight some ways that\nmisspecification can occur and discuss some behavioural issues that could arise\nfrom misspecification, including deceptive or manipulative language, and review\nsome approaches for avoiding these issues.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2021-03-26T18:01:48+00:00",
    "updated": "2021-03-26T18:01:48+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "1609.03499v2": {
    "id": "http://arxiv.org/abs/1609.03499v2",
    "title": "WaveNet: A Generative Model for Raw Audio",
    "authors": [
      "Aaron van den Oord",
      "Sander Dieleman",
      "Heiga Zen",
      "Karen Simonyan",
      "Oriol Vinyals",
      "Alex Graves",
      "Nal Kalchbrenner",
      "Andrew Senior",
      "Koray Kavukcuoglu"
    ],
    "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.",
    "categories": [
      "cs.SD",
      "cs.LG"
    ],
    "published": "2016-09-12T17:29:40+00:00",
    "updated": "2016-09-19T18:04:35+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "1805.01972v1": {
    "id": "http://arxiv.org/abs/1805.01972v1",
    "title": "Fast-converging Conditional Generative Adversarial Networks for Image Synthesis",
    "authors": [
      "Chengcheng Li",
      "Zi Wang",
      "Hairong Qi"
    ],
    "abstract": "Building on top of the success of generative adversarial networks (GANs),\nconditional GANs attempt to better direct the data generation process by\nconditioning with certain additional information. Inspired by the most recent\nAC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In\naddition to the real/fake classifier used in vanilla GANs, our discriminator\nhas an advanced auxiliary classifier which distinguishes each real class from\nan extra `fake' class. The `fake' class avoids mixing generated data with real\ndata, which can potentially confuse the classification of real data as AC-GAN\ndoes, and makes the advanced auxiliary classifier behave as another real/fake\nclassifier. As a result, FC-GAN can accelerate the process of differentiation\nof all classes, thus boost the convergence speed. Experimental results on image\nsynthesis demonstrate our model is competitive in the quality of images\ngenerated while achieving a faster convergence rate.",
    "categories": [
      "cs.CV"
    ],
    "published": "2018-05-05T00:18:19+00:00",
    "updated": "2018-05-05T00:18:19+00:00",
    "doi": null,
    "comment": "Accepted by ICIP 2018",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.01918v1": {
    "id": "http://arxiv.org/abs/2309.01918v1",
    "title": "RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking",
    "authors": [
      "Homanga Bharadhwaj",
      "Jay Vakil",
      "Mohit Sharma",
      "Abhinav Gupta",
      "Shubham Tulsiani",
      "Vikash Kumar"
    ],
    "abstract": "The grand aim of having a single robot that can manipulate arbitrary objects\nin diverse settings is at odds with the paucity of robotics datasets. Acquiring\nand growing such datasets is strenuous due to manual efforts, operational\ncosts, and safety challenges. A path toward such an universal agent would\nrequire a structured framework capable of wide generalization but trained\nwithin a reasonable data budget. In this paper, we develop an efficient system\n(RoboAgent) for training universal agents capable of multi-task manipulation\nskills using (a) semantic augmentations that can rapidly multiply existing\ndatasets and (b) action representations that can extract performant policies\nwith small yet diverse multi-modal datasets without overfitting. In addition,\nreliable task conditioning and an expressive policy architecture enable our\nagent to exhibit a diverse repertoire of skills in novel situations specified\nusing language commands. Using merely 7500 demonstrations, we are able to train\na single agent capable of 12 unique skills, and demonstrate its generalization\nover 38 tasks spread across common daily activities in diverse kitchen scenes.\nOn average, RoboAgent outperforms prior methods by over 40% in unseen\nsituations while being more sample efficient and being amenable to capability\nimprovements and extensions through fine-tuning. Videos at\nhttps://robopen.github.io/",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2023-09-05T03:14:39+00:00",
    "updated": "2023-09-05T03:14:39+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.RO"
  },
  "2406.05688v1": {
    "id": "http://arxiv.org/abs/2406.05688v1",
    "title": "Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions",
    "authors": [
      "Cheng Tan",
      "Dongxin Lyu",
      "Siyuan Li",
      "Zhangyang Gao",
      "Jingxuan Wei",
      "Siqi Ma",
      "Zicheng Liu",
      "Stan Z. Li"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated wide-ranging applications\nacross various fields and have shown significant potential in the academic\npeer-review process. However, existing applications are primarily limited to\nstatic review generation based on submitted papers, which fail to capture the\ndynamic and iterative nature of real-world peer reviews. In this paper, we\nreformulate the peer-review process as a multi-turn, long-context dialogue,\nincorporating distinct roles for authors, reviewers, and decision makers. We\nconstruct a comprehensive dataset containing over 26,841 papers with 92,017\nreviews collected from multiple sources, including the top-tier conference and\nprestigious journal. This dataset is meticulously designed to facilitate the\napplications of LLMs for multi-turn dialogues, effectively simulating the\ncomplete peer-review process. Furthermore, we propose a series of metrics to\nevaluate the performance of LLMs for each role under this reformulated\npeer-review setting, ensuring fair and comprehensive evaluations. We believe\nthis work provides a promising perspective on enhancing the LLM-driven\npeer-review process by incorporating dynamic, role-based interactions. It\naligns closely with the iterative and interactive nature of real-world academic\npeer review, offering a robust foundation for future research and development\nin this area. We open-source the dataset at\nhttps://github.com/chengtan9907/ReviewMT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-09T08:24:17+00:00",
    "updated": "2024-06-09T08:24:17+00:00",
    "doi": null,
    "comment": "Under review",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2204.13101v2": {
    "id": "http://arxiv.org/abs/2204.13101v2",
    "title": "Self-Supervised Learning of Object Parts for Semantic Segmentation",
    "authors": [
      "Adrian Ziegler",
      "Yuki M. Asano"
    ],
    "abstract": "Progress in self-supervised learning has brought strong general image\nrepresentation learning methods. Yet so far, it has mostly focused on\nimage-level learning. In turn, tasks such as unsupervised image segmentation\nhave not benefited from this trend as they require spatially-diverse\nrepresentations. However, learning dense representations is challenging, as in\nthe unsupervised context it is not clear how to guide the model to learn\nrepresentations that correspond to various potential object categories. In this\npaper, we argue that self-supervised learning of object parts is a solution to\nthis issue. Object parts are generalizable: they are a priori independent of an\nobject definition, but can be grouped to form objects a posteriori. To this\nend, we leverage the recently proposed Vision Transformer's capability of\nattending to objects and combine it with a spatially dense clustering task for\nfine-tuning the spatial tokens. Our method surpasses the state-of-the-art on\nthree semantic segmentation benchmarks by 17%-3%, showing that our\nrepresentations are versatile under various object definitions. Finally, we\nextend this to fully unsupervised segmentation - which refrains completely from\nusing label information even at test-time - and demonstrate that a simple\nmethod for automatically merging discovered object parts based on community\ndetection yields substantial gains.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-04-27T17:55:17+00:00",
    "updated": "2022-06-21T01:05:28+00:00",
    "doi": null,
    "comment": "Accepted at CVPR 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2011.10650v2": {
    "id": "http://arxiv.org/abs/2011.10650v2",
    "title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images",
    "authors": [
      "Rewon Child"
    ],
    "abstract": "We present a hierarchical VAE that, for the first time, generates samples\nquickly while outperforming the PixelCNN in log-likelihood on all natural image\nbenchmarks. We begin by observing that, in theory, VAEs can actually represent\nautoregressive models, as well as faster, better models if they exist, when\nmade sufficiently deep. Despite this, autoregressive models have historically\noutperformed VAEs in log-likelihood. We test if insufficient depth explains why\nby scaling a VAE to greater stochastic depth than previously explored and\nevaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN,\nthese very deep VAEs achieve higher likelihoods, use fewer parameters, generate\nsamples thousands of times faster, and are more easily applied to\nhigh-resolution images. Qualitative studies suggest this is because the VAE\nlearns efficient hierarchical visual representations. We release our source\ncode and models at https://github.com/openai/vdvae.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2020-11-20T21:35:31+00:00",
    "updated": "2021-03-16T18:33:19+00:00",
    "doi": null,
    "comment": "17 pages, 14 figures",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2407.06150v1": {
    "id": "http://arxiv.org/abs/2407.06150v1",
    "title": "PanDORA: Casual HDR Radiance Acquisition for Indoor Scenes",
    "authors": [
      "Mohammad Reza Karimi Dastjerdi",
      "Fr\u00e9d\u00e9ric Fortier-Chouinard",
      "Yannick Hold-Geoffroy",
      "Marc H\u00e9bert",
      "Claude Demers",
      "Nima Kalantari",
      "Jean-Fran\u00e7ois Lalonde"
    ],
    "abstract": "Most novel view synthesis methods such as NeRF are unable to capture the true\nhigh dynamic range (HDR) radiance of scenes since they are typically trained on\nphotos captured with standard low dynamic range (LDR) cameras. While the\ntraditional exposure bracketing approach which captures several images at\ndifferent exposures has recently been adapted to the multi-view case, we find\nsuch methods to fall short of capturing the full dynamic range of indoor\nscenes, which includes very bright light sources. In this paper, we present\nPanDORA: a PANoramic Dual-Observer Radiance Acquisition system for the casual\ncapture of indoor scenes in high dynamic range. Our proposed system comprises\ntwo 360{\\deg} cameras rigidly attached to a portable tripod. The cameras\nsimultaneously acquire two 360{\\deg} videos: one at a regular exposure and the\nother at a very fast exposure, allowing a user to simply wave the apparatus\ncasually around the scene in a matter of minutes. The resulting images are fed\nto a NeRF-based algorithm that reconstructs the scene's full high dynamic\nrange. Compared to HDR baselines from previous work, our approach reconstructs\nthe full HDR radiance of indoor scenes without sacrificing the visual quality\nwhile retaining the ease of capture from recent NeRF-like approaches.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-07-08T17:22:27+00:00",
    "updated": "2024-07-08T17:22:27+00:00",
    "doi": null,
    "comment": "10 pages, 8 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2406.05127v3": {
    "id": "http://arxiv.org/abs/2406.05127v3",
    "title": "Towards Semantic Equivalence of Tokenization in Multimodal LLM",
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Xiangtai Li",
      "Jiayi Ji",
      "Hanwang Zhang",
      "Tat-Seng Chua",
      "Shuicheng Yan"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-06-07T17:55:43+00:00",
    "updated": "2024-10-09T12:01:24+00:00",
    "doi": null,
    "comment": "Technical Report. The project page:\n  https://chocowu.github.io/SeTok-web/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1511.06297v2": {
    "id": "http://arxiv.org/abs/1511.06297v2",
    "title": "Conditional Computation in Neural Networks for faster models",
    "authors": [
      "Emmanuel Bengio",
      "Pierre-Luc Bacon",
      "Joelle Pineau",
      "Doina Precup"
    ],
    "abstract": "Deep learning has become the state-of-art tool in many applications, but the\nevaluation and training of deep models can be time-consuming and\ncomputationally expensive. The conditional computation approach has been\nproposed to tackle this problem (Bengio et al., 2013; Davis & Arel, 2013). It\noperates by selectively activating only parts of the network at a time. In this\npaper, we use reinforcement learning as a tool to optimize conditional\ncomputation policies. More specifically, we cast the problem of learning\nactivation-dependent policies for dropping out blocks of units as a\nreinforcement learning problem. We propose a learning scheme motivated by\ncomputation speed, capturing the idea of wanting to have parsimonious\nactivations while maintaining prediction accuracy. We apply a policy gradient\nalgorithm for learning policies that optimize this loss function and propose a\nregularization mechanism that encourages diversification of the dropout policy.\nWe present encouraging empirical results showing that this approach improves\nthe speed of computation without impacting the quality of the approximation.",
    "categories": [
      "cs.LG"
    ],
    "published": "2015-11-19T18:40:22+00:00",
    "updated": "2016-01-07T22:41:10+00:00",
    "doi": null,
    "comment": "ICLR 2016 submission, revised",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1809.00981v1": {
    "id": "http://arxiv.org/abs/1809.00981v1",
    "title": "DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification",
    "authors": [
      "Xiaofeng Zhang",
      "Zhangyang Wang",
      "Dong Liu",
      "Qing Ling"
    ],
    "abstract": "Deep learning has revolutionized the performance of classification, but\nmeanwhile demands sufficient labeled data for training. Given insufficient\ndata, while many techniques have been developed to help combat overfitting, the\nchallenge remains if one tries to train deep networks, especially in the\nill-posed extremely low data regimes: only a small set of labeled data are\navailable, and nothing -- including unlabeled data -- else. Such regimes arise\nfrom practical situations where not only data labeling but also data collection\nitself is expensive. We propose a deep adversarial data augmentation (DADA)\ntechnique to address the problem, in which we elaborately formulate data\naugmentation as a problem of training a class-conditional and supervised\ngenerative adversarial network (GAN). Specifically, a new discriminator loss is\nproposed to fit the goal of data augmentation, through which both real and\naugmented samples are enforced to contribute to and be consistent in finding\nthe decision boundaries. Tailored training techniques are developed\naccordingly. To quantitatively validate its effectiveness, we first perform\nextensive simulations to show that DADA substantially outperforms both\ntraditional data augmentation and a few GAN-based options. We then extend\nexperiments to three real-world small labeled datasets where existing data\naugmentation and/or transfer learning strategies are either less effective or\ninfeasible. All results endorse the superior capability of DADA in enhancing\nthe generalization ability of deep networks trained in practical extremely low\ndata regimes. Source code is available at\nhttps://github.com/SchafferZhang/DADA.",
    "categories": [
      "cs.CV"
    ],
    "published": "2018-08-29T09:01:31+00:00",
    "updated": "2018-08-29T09:01:31+00:00",
    "doi": null,
    "comment": "15 pages, 5 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2310.02446v2": {
    "id": "http://arxiv.org/abs/2310.02446v2",
    "title": "Low-Resource Languages Jailbreak GPT-4",
    "authors": [
      "Zheng-Xin Yong",
      "Cristina Menghini",
      "Stephen H. Bach"
    ],
    "abstract": "AI safety training and red-teaming of large language models (LLMs) are\nmeasures to mitigate the generation of unsafe content. Our work exposes the\ninherent cross-lingual vulnerability of these safety mechanisms, resulting from\nthe linguistic inequality of safety training data, by successfully\ncircumventing GPT-4's safeguard through translating unsafe English inputs into\nlow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe\ntranslated inputs and provides actionable items that can get the users towards\ntheir harmful goals 79% of the time, which is on par with or even surpassing\nstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages have\nsignificantly lower attack success rate, which suggests that the cross-lingual\nvulnerability mainly applies to low-resource languages. Previously, limited\ntraining on low-resource languages primarily affects speakers of those\nlanguages, causing technological disparities. However, our work highlights a\ncrucial shift: this deficiency now poses a risk to all LLMs users. Publicly\navailable translation APIs enable anyone to exploit LLMs' safety\nvulnerabilities. Therefore, our work calls for a more holistic red-teaming\nefforts to develop robust multilingual safeguards with wide language coverage.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2023-10-03T21:30:56+00:00",
    "updated": "2024-01-27T22:54:52+00:00",
    "doi": null,
    "comment": "NeurIPS Workshop on Socially Responsible Language Modelling Research\n  (SoLaR) 2023. Best Paper Award",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2311.13601v1": {
    "id": "http://arxiv.org/abs/2311.13601v1",
    "title": "Visual In-Context Prompting",
    "authors": [
      "Feng Li",
      "Qing Jiang",
      "Hao Zhang",
      "Tianhe Ren",
      "Shilong Liu",
      "Xueyan Zou",
      "Huaizhe Xu",
      "Hongyang Li",
      "Chunyuan Li",
      "Jianwei Yang",
      "Lei Zhang",
      "Jianfeng Gao"
    ],
    "abstract": "In-context prompting in large language models (LLMs) has become a prevalent\napproach to improve zero-shot capabilities, but this idea is less explored in\nthe vision domain. Existing visual prompting methods focus on referring\nsegmentation to segment the most relevant object, falling short of addressing\nmany generic vision tasks like open-set segmentation and detection. In this\npaper, we introduce a universal visual in-context prompting framework for both\ntasks. In particular, we build on top of an encoder-decoder architecture, and\ndevelop a versatile prompt encoder to support a variety of prompts like\nstrokes, boxes, and points. We further enhance it to take an arbitrary number\nof reference image segments as the context. Our extensive explorations show\nthat the proposed visual in-context prompting elicits extraordinary referring\nand generic segmentation capabilities to refer and detect, yielding competitive\nperformance to close-set in-domain datasets and showing promising results on\nmany open-set segmentation datasets. By joint training on COCO and SA-1B, our\nmodel achieves $57.7$ PQ on COCO and $23.2$ PQ on ADE20K. Code will be\navailable at https://github.com/UX-Decoder/DINOv.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-11-22T18:59:48+00:00",
    "updated": "2023-11-22T18:59:48+00:00",
    "doi": null,
    "comment": "technical report",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2310.03337v5": {
    "id": "http://arxiv.org/abs/2310.03337v5",
    "title": "Denoising Diffusion Step-aware Models",
    "authors": [
      "Shuai Yang",
      "Yukang Chen",
      "Luozhou Wang",
      "Shu Liu",
      "Yingcong Chen"
    ],
    "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for\ndata generation across various domains. However, a significant bottleneck is\nthe necessity for whole-network computation during every step of the generative\nprocess, leading to high computational overheads. This paper presents a novel\nframework, Denoising Diffusion Step-aware Models (DDSM), to address this\nchallenge. Unlike conventional approaches, DDSM employs a spectrum of neural\nnetworks whose sizes are adapted according to the importance of each generative\nstep, as determined through evolutionary search. This step-wise network\nvariation effectively circumvents redundant computational efforts, particularly\nin less critical steps, thereby enhancing the efficiency of the diffusion\nmodel. Furthermore, the step-aware design can be seamlessly integrated with\nother efficiency-geared diffusion models such as DDIMs and latent diffusion,\nthus broadening the scope of computational savings. Empirical evaluations\ndemonstrate that DDSM achieves computational savings of 49% for CIFAR-10, 61%\nfor CelebA-HQ, 59% for LSUN-bedroom, 71% for AFHQ, and 76% for ImageNet, all\nwithout compromising the generation quality.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-10-05T06:44:13+00:00",
    "updated": "2024-05-24T09:17:29+00:00",
    "doi": null,
    "comment": "ICLR2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1709.02371v3": {
    "id": "http://arxiv.org/abs/1709.02371v3",
    "title": "PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume",
    "authors": [
      "Deqing Sun",
      "Xiaodong Yang",
      "Ming-Yu Liu",
      "Jan Kautz"
    ],
    "abstract": "We present a compact but effective CNN model for optical flow, called\nPWC-Net. PWC-Net has been designed according to simple and well-established\nprinciples: pyramidal processing, warping, and the use of a cost volume. Cast\nin a learnable feature pyramid, PWC-Net uses the cur- rent optical flow\nestimate to warp the CNN features of the second image. It then uses the warped\nfeatures and features of the first image to construct a cost volume, which is\nprocessed by a CNN to estimate the optical flow. PWC-Net is 17 times smaller in\nsize and easier to train than the recent FlowNet2 model. Moreover, it\noutperforms all published optical flow methods on the MPI Sintel final pass and\nKITTI 2015 benchmarks, running at about 35 fps on Sintel resolution (1024x436)\nimages. Our models are available on https://github.com/NVlabs/PWC-Net.",
    "categories": [
      "cs.CV"
    ],
    "published": "2017-09-07T17:47:59+00:00",
    "updated": "2018-06-25T20:34:58+00:00",
    "doi": null,
    "comment": "CVPR 2018 camera ready version (with github link to Caffe and PyTorch\n  code)",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2205.10337v3": {
    "id": "http://arxiv.org/abs/2205.10337v3",
    "title": "UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes",
    "authors": [
      "Alexander Kolesnikov",
      "Andr\u00e9 Susano Pinto",
      "Lucas Beyer",
      "Xiaohua Zhai",
      "Jeremiah Harmsen",
      "Neil Houlsby"
    ],
    "abstract": "We introduce UViM, a unified approach capable of modeling a wide range of\ncomputer vision tasks. In contrast to previous models, UViM has the same\nfunctional form for all tasks; it requires no task-specific modifications which\nrequire extensive human expertise. The approach involves two components: (I) a\nbase model (feed-forward) which is trained to directly predict raw vision\noutputs, guided by a learned discrete code and (II) a language model\n(autoregressive) that is trained to generate the guiding code. These components\ncomplement each other: the language model is well-suited to modeling structured\ninterdependent data, while the base model is efficient at dealing with\nhigh-dimensional outputs. We demonstrate the effectiveness of UViM on three\ndiverse and challenging vision tasks: panoptic segmentation, depth prediction\nand image colorization, where we achieve competitive and near state-of-the-art\nresults. Our experimental results suggest that UViM is a promising candidate\nfor a unified modeling approach in computer vision.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-05-20T17:47:59+00:00",
    "updated": "2022-10-14T11:36:32+00:00",
    "doi": null,
    "comment": "22 pages. Accepted at NeurIPS 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2209.00860v1": {
    "id": "http://arxiv.org/abs/2209.00860v1",
    "title": "Real-time 3D Single Object Tracking with Transformer",
    "authors": [
      "Jiayao Shan",
      "Sifan Zhou",
      "Yubo Cui",
      "Zheng Fang"
    ],
    "abstract": "LiDAR-based 3D single object tracking is a challenging issue in robotics and\nautonomous driving. Currently, existing approaches usually suffer from the\nproblem that objects at long distance often have very sparse or\npartially-occluded point clouds, which makes the features extracted by the\nmodel ambiguous. Ambiguous features will make it hard to locate the target\nobject and finally lead to bad tracking results. To solve this problem, we\nutilize the powerful Transformer architecture and propose a\nPoint-Track-Transformer (PTT) module for point cloud-based 3D single object\ntracking task. Specifically, PTT module generates fine-tuned attention features\nby computing attention weights, which guides the tracker focusing on the\nimportant features of the target and improves the tracking ability in complex\nscenarios. To evaluate our PTT module, we embed PTT into the dominant method\nand construct a novel 3D SOT tracker named PTT-Net. In PTT-Net, we embed PTT\ninto the voting stage and proposal generation stage, respectively. PTT module\nin the voting stage could model the interactions among point patches, which\nlearns context-dependent features. Meanwhile, PTT module in the proposal\ngeneration stage could capture the contextual information between object and\nbackground. We evaluate our PTT-Net on KITTI and NuScenes datasets.\nExperimental results demonstrate the effectiveness of PTT module and the\nsuperiority of PTT-Net, which surpasses the baseline by a noticeable margin,\n~10% in the Car category. Meanwhile, our method also has a significant\nperformance improvement in sparse scenarios. In general, the combination of\ntransformer and tracking pipeline enables our PTT-Net to achieve\nstate-of-the-art performance on both two datasets. Additionally, PTT-Net could\nrun in real-time at 40FPS on NVIDIA 1080Ti GPU. Our code is open-sourced for\nthe research community at https://github.com/shanjiayao/PTT.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-09-02T07:36:20+00:00",
    "updated": "2022-09-02T07:36:20+00:00",
    "doi": "10.1109/TMM.2022.3146714",
    "comment": "IEEE Transactions on Multimedia. arXiv admin note: text overlap with\n  arXiv:2108.06455",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2105.09720v2": {
    "id": "http://arxiv.org/abs/2105.09720v2",
    "title": "Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks",
    "authors": [
      "Thosini Bamunu Mudiyanselage",
      "Nipuna Senanayake",
      "Chunyan Ji",
      "Yi Pan",
      "Yanqing Zhang"
    ],
    "abstract": "The novel corona virus (Covid-19) has introduced significant challenges due\nto its rapid spreading nature through respiratory transmission. As a result,\nthere is a huge demand for Artificial Intelligence (AI) based quick disease\ndiagnosis methods as an alternative to high demand tests such as Polymerase\nChain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective\nradiography technique due to resource availability and quick screening. But, a\nsufficient and systematic data collection that is required by complex deep\nleaning (DL) models is more difficult and hence there are recent efforts that\nutilize transfer learning to address this issue. Still these transfer learnt\nmodels suffer from lack of generalization and increased bias to the training\ndataset resulting poor performance for unseen data. Limited correlation of the\ntransferred features from the pre-trained model to a specific medical imaging\ndomain like X-ray and overfitting on fewer data can be reasons for this\ncircumstance. In this work, we propose a novel Graph Convolution Neural Network\n(GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR\nimages and meta information about patients. The proposed method exploits\nimportant relational knowledge between data instances and their features using\ngraph representation and applies convolution to learn the graph data which is\nnot possible with conventional convolution on Euclidean domain. The results of\nextensive experiments of proposed model on binary (Covid vs normal) and three\nclass (Covid, normal, other pneumonia) classification problems outperform\ndifferent benchmark transfer learnt models, hence overcoming the aforementioned\ndrawbacks.",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2021-05-20T13:13:29+00:00",
    "updated": "2021-05-21T12:38:45+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "eess.IV"
  },
  "1709.06636v1": {
    "id": "http://arxiv.org/abs/1709.06636v1",
    "title": "An Attention-based Collaboration Framework for Multi-View Network Representation Learning",
    "authors": [
      "Meng Qu",
      "Jian Tang",
      "Jingbo Shang",
      "Xiang Ren",
      "Ming Zhang",
      "Jiawei Han"
    ],
    "abstract": "Learning distributed node representations in networks has been attracting\nincreasing attention recently due to its effectiveness in a variety of\napplications. Existing approaches usually study networks with a single type of\nproximity between nodes, which defines a single view of a network. However, in\nreality there usually exists multiple types of proximities between nodes,\nyielding networks with multiple views. This paper studies learning node\nrepresentations for networks with multiple views, which aims to infer robust\nnode representations across different views. We propose a multi-view\nrepresentation learning approach, which promotes the collaboration of different\nviews and lets them vote for the robust representations. During the voting\nprocess, an attention mechanism is introduced, which enables each node to focus\non the most informative views. Experimental results on real-world networks show\nthat the proposed approach outperforms existing state-of-the-art approaches for\nnetwork representation learning with a single view and other competitive\napproaches with multiple views.",
    "categories": [
      "cs.SI",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2017-09-19T20:30:30+00:00",
    "updated": "2017-09-19T20:30:30+00:00",
    "doi": null,
    "comment": "CIKM 2017",
    "journal_ref": null,
    "primary_category": "cs.SI"
  },
  "2203.03821v5": {
    "id": "http://arxiv.org/abs/2203.03821v5",
    "title": "CF-ViT: A General Coarse-to-Fine Method for Vision Transformer",
    "authors": [
      "Mengzhao Chen",
      "Mingbao Lin",
      "Ke Li",
      "Yunhang Shen",
      "Yongjian Wu",
      "Fei Chao",
      "Rongrong Ji"
    ],
    "abstract": "Vision Transformers (ViT) have made many breakthroughs in computer vision\ntasks. However, considerable redundancy arises in the spatial dimension of an\ninput image, leading to massive computational costs. Therefore, We propose a\ncoarse-to-fine vision transformer (CF-ViT) to relieve computational burden\nwhile retaining performance in this paper. Our proposed CF-ViT is motivated by\ntwo important observations in modern ViT models: (1) The coarse-grained patch\nsplitting can locate informative regions of an input image. (2) Most images can\nbe well recognized by a ViT model in a small-length token sequence. Therefore,\nour CF-ViT implements network inference in a two-stage manner. At coarse\ninference stage, an input image is split into a small-length patch sequence for\na computationally economical classification. If not well recognized, the\ninformative patches are identified and further re-split in a fine-grained\ngranularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For\nexample, without any compromise on performance, CF-ViT reduces 53% FLOPs of\nLV-ViT, and also achieves 2.01x throughput.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-03-08T02:57:49+00:00",
    "updated": "2022-11-21T09:47:20+00:00",
    "doi": null,
    "comment": "Accepted by AAAI 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2301.11376v2": {
    "id": "http://arxiv.org/abs/2301.11376v2",
    "title": "Screen Space Indirect Lighting with Visibility Bitmask",
    "authors": [
      "Olivier Therrien",
      "Yannick Levesque",
      "Guillaume Gilet"
    ],
    "abstract": "Horizon-based indirect illumination efficiently estimates a diffuse light\nbounce in screen space by analytically integrating the horizon angle difference\nbetween samples along a given direction. Like other horizon-based methods, this\ntechnique cannot properly simulate light passing behind thin surfaces. We\npropose the concept of a visibility bitmask that replaces the two horizon\nangles by a bit field representing the binary state (occluded / un-occluded) of\nN sectors uniformly distributed around the hemisphere slice. It allows light to\npass behind surfaces of constant thickness while keeping the efficiency of\nhorizon-based methods. It can also do more accurate ambient lighting than bent\nnormal by sampling more than one visibility cone. This technique improves the\nvisual quality of ambient occlusion, indirect diffuse, and ambient light\ncompared to previous screen space methods while minimizing noise and keeping a\nlow performance overhead.",
    "categories": [
      "cs.GR",
      "I.3.7"
    ],
    "published": "2023-01-26T19:44:03+00:00",
    "updated": "2023-01-31T20:16:35+00:00",
    "doi": "10.1007/s00371-022-02703-y",
    "comment": "Vis Comput (2022)",
    "journal_ref": null,
    "primary_category": "cs.GR"
  },
  "2310.10195v3": {
    "id": "http://arxiv.org/abs/2310.10195v3",
    "title": "AdaLomo: Low-memory Optimization with Adaptive Learning Rate",
    "authors": [
      "Kai Lv",
      "Hang Yan",
      "Qipeng Guo",
      "Haijun Lv",
      "Xipeng Qiu"
    ],
    "abstract": "Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2023-10-16T09:04:28+00:00",
    "updated": "2024-06-06T13:22:25+00:00",
    "doi": null,
    "comment": "ACL 2024 camera ready version",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2112.08654v2": {
    "id": "http://arxiv.org/abs/2112.08654v2",
    "title": "Learning to Prompt for Continual Learning",
    "authors": [
      "Zifeng Wang",
      "Zizhao Zhang",
      "Chen-Yu Lee",
      "Han Zhang",
      "Ruoxi Sun",
      "Xiaoqi Ren",
      "Guolong Su",
      "Vincent Perot",
      "Jennifer Dy",
      "Tomas Pfister"
    ],
    "abstract": "The mainstream paradigm behind continual learning has been to adapt the model\nparameters to non-stationary data distributions, where catastrophic forgetting\nis the central challenge. Typical methods rely on a rehearsal buffer or known\ntask identity at test time to retrieve learned knowledge and address\nforgetting, while this work presents a new paradigm for continual learning that\naims to train a more succinct memory system without accessing task identity at\ntest time. Our method learns to dynamically prompt (L2P) a pre-trained model to\nlearn tasks sequentially under different task transitions. In our proposed\nframework, prompts are small learnable parameters, which are maintained in a\nmemory space. The objective is to optimize prompts to instruct the model\nprediction and explicitly manage task-invariant and task-specific knowledge\nwhile maintaining model plasticity. We conduct comprehensive experiments under\npopular image classification benchmarks with different challenging continual\nlearning settings, where L2P consistently outperforms prior state-of-the-art\nmethods. Surprisingly, L2P achieves competitive results against rehearsal-based\nmethods even without a rehearsal buffer and is directly applicable to\nchallenging task-agnostic continual learning. Source code is available at\nhttps://github.com/google-research/l2p.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2021-12-16T06:17:07+00:00",
    "updated": "2022-03-21T19:26:32+00:00",
    "doi": null,
    "comment": "Published at CVPR 2022 as a conference paper",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2211.13095v1": {
    "id": "http://arxiv.org/abs/2211.13095v1",
    "title": "Schr\u00f6dinger's Bat: Diffusion Models Sometimes Generate Polysemous Words in Superposition",
    "authors": [
      "Jennifer C. White",
      "Ryan Cotterell"
    ],
    "abstract": "Recent work has shown that despite their impressive capabilities,\ntext-to-image diffusion models such as DALL-E 2 (Ramesh et al., 2022) can\ndisplay strange behaviours when a prompt contains a word with multiple possible\nmeanings, often generating images containing both senses of the word (Rassin et\nal., 2022). In this work we seek to put forward a possible explanation of this\nphenomenon. Using the similar Stable Diffusion model (Rombach et al., 2022), we\nfirst show that when given an input that is the sum of encodings of two\ndistinct words, the model can produce an image containing both concepts\nrepresented in the sum. We then demonstrate that the CLIP encoder used to\nencode prompts (Radford et al., 2021) encodes polysemous words as a\nsuperposition of meanings, and that using linear algebraic techniques we can\nedit these representations to influence the senses represented in the generated\nimages. Combining these two findings, we suggest that the homonym duplication\nphenomenon described by Rassin et al. (2022) is caused by diffusion models\nproducing images representing both of the meanings that are present in\nsuperposition in the encoding of a polysemous word.",
    "categories": [
      "cs.CL"
    ],
    "published": "2022-11-23T16:26:49+00:00",
    "updated": "2022-11-23T16:26:49+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2106.02019v2": {
    "id": "http://arxiv.org/abs/2106.02019v2",
    "title": "Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control",
    "authors": [
      "Lingjie Liu",
      "Marc Habermann",
      "Viktor Rudnev",
      "Kripasindhu Sarkar",
      "Jiatao Gu",
      "Christian Theobalt"
    ],
    "abstract": "We propose Neural Actor (NA), a new method for high-quality synthesis of\nhumans from arbitrary viewpoints and under arbitrary controllable poses. Our\nmethod is built upon recent neural scene representation and rendering works\nwhich learn representations of geometry and appearance from only 2D images.\nWhile existing works demonstrated compelling rendering of static scenes and\nplayback of dynamic scenes, photo-realistic reconstruction and rendering of\nhumans with neural implicit methods, in particular under user-controlled novel\nposes, is still difficult. To address this problem, we utilize a coarse body\nmodel as the proxy to unwarp the surrounding 3D space into a canonical pose. A\nneural radiance field learns pose-dependent geometric deformations and pose-\nand view-dependent appearance effects in the canonical space from multi-view\nvideo input. To synthesize novel views of high fidelity dynamic geometry and\nappearance, we leverage 2D texture maps defined on the body model as latent\nvariables for predicting residual deformations and the dynamic appearance.\nExperiments demonstrate that our method achieves better quality than the\nstate-of-the-arts on playback as well as novel pose synthesis, and can even\ngeneralize well to new poses that starkly differ from the training poses.\nFurthermore, our method also supports body shape control of the synthesized\nresults.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2021-06-03T17:40:48+00:00",
    "updated": "2022-01-04T17:13:00+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2211.09206v1": {
    "id": "http://arxiv.org/abs/2211.09206v1",
    "title": "Learning to Kindle the Starlight",
    "authors": [
      "Yu Yuan",
      "Jiaqi Wu",
      "Lindong Wang",
      "Zhongliang Jing",
      "Henry Leung",
      "Shuyuan Zhu",
      "Han Pan"
    ],
    "abstract": "Capturing highly appreciated star field images is extremely challenging due\nto light pollution, the requirements of specialized hardware, and the high\nlevel of photographic skills needed. Deep learning-based techniques have\nachieved remarkable results in low-light image enhancement (LLIE) but have not\nbeen widely applied to star field image enhancement due to the lack of training\ndata. To address this problem, we construct the first Star Field Image\nEnhancement Benchmark (SFIEB) that contains 355 real-shot and 854\nsemi-synthetic star field images, all having the corresponding reference\nimages. Using the presented dataset, we propose the first star field image\nenhancement approach, namely StarDiffusion, based on conditional denoising\ndiffusion probabilistic models (DDPM). We introduce dynamic stochastic\ncorruptions to the inputs of conditional DDPM to improve the performance and\ngeneralization of the network on our small-scale dataset. Experiments show\npromising results of our method, which outperforms state-of-the-art low-light\nimage enhancement algorithms. The dataset and codes will be open-sourced.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "published": "2022-11-16T20:48:46+00:00",
    "updated": "2022-11-16T20:48:46+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2212.00794v2": {
    "id": "http://arxiv.org/abs/2212.00794v2",
    "title": "Scaling Language-Image Pre-training via Masking",
    "authors": [
      "Yanghao Li",
      "Haoqi Fan",
      "Ronghang Hu",
      "Christoph Feichtenhofer",
      "Kaiming He"
    ],
    "abstract": "We present Fast Language-Image Pre-training (FLIP), a simple and more\nefficient method for training CLIP. Our method randomly masks out and removes a\nlarge portion of image patches during training. Masking allows us to learn from\nmore image-text pairs given the same wall-clock time and contrast more samples\nper iteration with similar memory footprint. It leads to a favorable trade-off\nbetween accuracy and training time. In our experiments on 400 million\nimage-text pairs, FLIP improves both accuracy and speed over the no-masking\nbaseline. On a large diversity of downstream tasks, FLIP dominantly outperforms\nthe CLIP counterparts trained on the same data. Facilitated by the speedup, we\nexplore the scaling behavior of increasing the model size, data size, or\ntraining length, and report encouraging results and comparisons. We hope that\nour work will foster future research on scaling vision-language learning.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-12-01T18:59:57+00:00",
    "updated": "2023-03-30T05:04:28+00:00",
    "doi": null,
    "comment": "Tech report; arXiv v2: update scaling results and add code repo",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.03199v2": {
    "id": "http://arxiv.org/abs/2309.03199v2",
    "title": "Matcha-TTS: A fast TTS architecture with conditional flow matching",
    "authors": [
      "Shivam Mehta",
      "Ruibo Tu",
      "Jonas Beskow",
      "\u00c9va Sz\u00e9kely",
      "Gustav Eje Henter"
    ],
    "abstract": "We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS\nacoustic modelling, trained using optimal-transport conditional flow matching\n(OT-CFM). This yields an ODE-based decoder capable of high output quality in\nfewer synthesis steps than models trained using score matching. Careful design\nchoices additionally ensure each synthesis step is fast to run. The method is\nprobabilistic, non-autoregressive, and learns to speak from scratch without\nexternal alignments. Compared to strong pre-trained baseline models, the\nMatcha-TTS system has the smallest memory footprint, rivals the speed of the\nfastest models on long utterances, and attains the highest mean opinion score\nin a listening test. Please see https://shivammehta25.github.io/Matcha-TTS/ for\naudio examples, code, and pre-trained models.",
    "categories": [
      "eess.AS",
      "cs.HC",
      "cs.LG",
      "cs.SD",
      "68T07",
      "I.2.7; I.2.6; H.5.5"
    ],
    "published": "2023-09-06T17:59:57+00:00",
    "updated": "2024-01-09T21:02:34+00:00",
    "doi": null,
    "comment": "5 pages, 3 figures. Final version, accepted to IEEE ICASSP 2024",
    "journal_ref": null,
    "primary_category": "eess.AS"
  },
  "2309.09328v1": {
    "id": "http://arxiv.org/abs/2309.09328v1",
    "title": "Enhancing Knee Osteoarthritis severity level classification using diffusion augmented images",
    "authors": [
      "Paleti Nikhil Chowdary",
      "Gorantla V N S L Vishnu Vardhan",
      "Menta Sai Akshay",
      "Menta Sai Aashish",
      "Vadlapudi Sai Aravind",
      "Garapati Venkata Krishna Rayalu",
      "Aswathy P"
    ],
    "abstract": "This research paper explores the classification of knee osteoarthritis (OA)\nseverity levels using advanced computer vision models and augmentation\ntechniques. The study investigates the effectiveness of data preprocessing,\nincluding Contrast-Limited Adaptive Histogram Equalization (CLAHE), and data\naugmentation using diffusion models. Three experiments were conducted: training\nmodels on the original dataset, training models on the preprocessed dataset,\nand training models on the augmented dataset. The results show that data\npreprocessing and augmentation significantly improve the accuracy of the\nmodels. The EfficientNetB3 model achieved the highest accuracy of 84\\% on the\naugmented dataset. Additionally, attention visualization techniques, such as\nGrad-CAM, are utilized to provide detailed attention maps, enhancing the\nunderstanding and trustworthiness of the models. These findings highlight the\npotential of combining advanced models with augmented data and attention\nvisualization for accurate knee OA severity classification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-17T17:22:29+00:00",
    "updated": "2023-09-17T17:22:29+00:00",
    "doi": "10.2991/978-94-6463-314-6_27",
    "comment": "Paper has been accepted to be presented at ICACECS 2023 and the final\n  version will be published by Atlantis Highlights in Computer Science (AHCS) ,\n  Atlantis Press(part of Springer Nature)",
    "journal_ref": "Proceedings of the International e-Conference on Advances in\n  Computer Engineering and Communication Systems (ICACECS 2023) 266-274 (2023)",
    "primary_category": "cs.CV"
  },
  "2010.04838v1": {
    "id": "http://arxiv.org/abs/2010.04838v1",
    "title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator",
    "authors": [
      "Max B. Paulus",
      "Chris J. Maddison",
      "Andreas Krause"
    ],
    "abstract": "Gradient estimation in models with discrete latent variables is a challenging\nproblem, because the simplest unbiased estimators tend to have high variance.\nTo counteract this, modern estimators either introduce bias, rely on multiple\nfunction evaluations, or use learned, input-dependent baselines. Thus, there is\na need for estimators that require minimal tuning, are computationally cheap,\nand have low mean squared error. In this paper, we show that the variance of\nthe straight-through variant of the popular Gumbel-Softmax estimator can be\nreduced through Rao-Blackwellization without increasing the number of function\nevaluations. This provably reduces the mean squared error. We empirically\ndemonstrate that this leads to variance reduction, faster convergence, and\ngenerally improved performance in two unsupervised latent variable models.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2020-10-09T22:54:38+00:00",
    "updated": "2020-10-09T22:54:38+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2010.12771v2": {
    "id": "http://arxiv.org/abs/2010.12771v2",
    "title": "On Learning Text Style Transfer with Direct Rewards",
    "authors": [
      "Yixin Liu",
      "Graham Neubig",
      "John Wieting"
    ],
    "abstract": "In most cases, the lack of parallel corpora makes it impossible to directly\ntrain supervised models for the text style transfer task. In this paper, we\nexplore training algorithms that instead optimize reward functions that\nexplicitly consider different aspects of the style-transferred outputs. In\nparticular, we leverage semantic similarity metrics originally used for\nfine-tuning neural machine translation models to explicitly assess the\npreservation of content between system outputs and input texts. We also\ninvestigate the potential weaknesses of the existing automatic metrics and\npropose efficient strategies of using these metrics for training. The\nexperimental results show that our model provides significant gains in both\nautomatic and human evaluation over strong baselines, indicating the\neffectiveness of our proposed methods and training strategies.",
    "categories": [
      "cs.CL"
    ],
    "published": "2020-10-24T04:30:02+00:00",
    "updated": "2021-05-13T15:00:38+00:00",
    "doi": null,
    "comment": "Published as a long paper at NAACL 2021",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2403.08733v4": {
    "id": "http://arxiv.org/abs/2403.08733v4",
    "title": "GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing",
    "authors": [
      "Jing Wu",
      "Jia-Wang Bian",
      "Xinghui Li",
      "Guangrun Wang",
      "Ian Reid",
      "Philip Torr",
      "Victor Adrian Prisacariu"
    ],
    "abstract": "We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed\nby the 3D Gaussian Splatting (3DGS).\n  Our method first renders a collection of images by using the 3DGS and edits\nthem by using a pre-trained 2D diffusion model (ControlNet) based on the input\nprompt, which is then used to optimise the 3D model.\n  Our key contribution is multi-view consistent editing, which enables editing\nall images together instead of iteratively editing one image while updating the\n3D model as in previous works.\n  It leads to faster editing as well as higher visual quality.\n  This is achieved by the two terms:\n  (a) depth-conditioned editing that enforces geometric consistency across\nmulti-view images by leveraging naturally consistent depth maps.\n  (b) attention-based latent code alignment that unifies the appearance of\nedited images by conditioning their editing to several reference views through\nself and cross-view attention between images' latent representations.\n  Experiments demonstrate that our method achieves faster editing and better\nvisual results than previous state-of-the-art methods.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-13T17:35:28+00:00",
    "updated": "2024-07-14T10:31:58+00:00",
    "doi": null,
    "comment": "ECCV2024, Project Website: https://gaussctrl.active.vision/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2003.03703v2": {
    "id": "http://arxiv.org/abs/2003.03703v2",
    "title": "Transferring Cross-domain Knowledge for Video Sign Language Recognition",
    "authors": [
      "Dongxu Li",
      "Xin Yu",
      "Chenchen Xu",
      "Lars Petersson",
      "Hongdong Li"
    ],
    "abstract": "Word-level sign language recognition (WSLR) is a fundamental task in sign\nlanguage interpretation. It requires models to recognize isolated sign words\nfrom videos. However, annotating WSLR data needs expert knowledge, thus\nlimiting WSLR dataset acquisition. On the contrary, there are abundant\nsubtitled sign news videos on the internet. Since these videos have no\nword-level annotation and exhibit a large domain gap from isolated signs, they\ncannot be directly used for training WSLR models. We observe that despite the\nexistence of a large domain gap, isolated and news signs share the same visual\nconcepts, such as hand gestures and body movements. Motivated by this\nobservation, we propose a novel method that learns domain-invariant visual\nconcepts and fertilizes WSLR models by transferring knowledge of subtitled news\nsign to them. To this end, we extract news signs using a base WSLR model, and\nthen design a classifier jointly trained on news and isolated signs to coarsely\nalign these two domain features. In order to learn domain-invariant features\nwithin each class and suppress domain-specific features, our method further\nresorts to an external memory to store the class centroids of the aligned news\nsigns. We then design a temporal attention based on the learnt descriptor to\nimprove recognition performance. Experimental results on standard WSLR datasets\nshow that our method outperforms previous state-of-the-art methods\nsignificantly. We also demonstrate the effectiveness of our method on\nautomatically localizing signs from sign news, achieving 28.1 for AP@0.5.",
    "categories": [
      "cs.CV",
      "cs.HC",
      "cs.MM"
    ],
    "published": "2020-03-08T03:05:21+00:00",
    "updated": "2020-03-17T14:53:06+00:00",
    "doi": null,
    "comment": "CVPR2020 (oral) preprint",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.03011v2": {
    "id": "http://arxiv.org/abs/2312.03011v2",
    "title": "InstructBooth: Instruction-following Personalized Text-to-Image Generation",
    "authors": [
      "Daewon Chae",
      "Nokyung Park",
      "Jinkyu Kim",
      "Kimin Lee"
    ],
    "abstract": "Personalizing text-to-image models using a limited set of images for a\nspecific object has been explored in subject-specific image generation.\nHowever, existing methods often face challenges in aligning with text prompts\ndue to overfitting to the limited training images. In this work, we introduce\nInstructBooth, a novel method designed to enhance image-text alignment in\npersonalized text-to-image models without sacrificing the personalization\nability. Our approach first personalizes text-to-image models with a small\nnumber of subject-specific images using a unique identifier. After\npersonalization, we fine-tune personalized text-to-image models using\nreinforcement learning to maximize a reward that quantifies image-text\nalignment. Additionally, we propose complementary techniques to increase the\nsynergy between these two processes. Our method demonstrates superior\nimage-text alignment compared to existing baselines, while maintaining high\npersonalization ability. In human evaluations, InstructBooth outperforms them\nwhen considering all comprehensive factors. Our project page is at\nhttps://sites.google.com/view/instructbooth.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-12-04T20:34:46+00:00",
    "updated": "2024-02-15T16:38:46+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.07940v1": {
    "id": "http://arxiv.org/abs/2210.07940v1",
    "title": "AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments",
    "authors": [
      "Sudipta Paul",
      "Amit K. Roy-Chowdhury",
      "Anoop Cherian"
    ],
    "abstract": "Recent years have seen embodied visual navigation advance in two distinct\ndirections: (i) in equipping the AI agent to follow natural language\ninstructions, and (ii) in making the navigable world multimodal, e.g.,\naudio-visual navigation. However, the real world is not only multimodal, but\nalso often complex, and thus in spite of these advances, agents still need to\nunderstand the uncertainty in their actions and seek instructions to navigate.\nTo this end, we present AVLEN~ -- an interactive agent for\nAudio-Visual-Language Embodied Navigation. Similar to audio-visual navigation\ntasks, the goal of our embodied agent is to localize an audio event via\nnavigating the 3D visual world; however, the agent may also seek help from a\nhuman (oracle), where the assistance is provided in free-form natural language.\nTo realize these abilities, AVLEN uses a multimodal hierarchical reinforcement\nlearning backbone that learns: (a) high-level policies to choose either\naudio-cues for navigation or to query the oracle, and (b) lower-level policies\nto select navigation actions based on its audio-visual and language inputs. The\npolicies are trained via rewarding for the success on the navigation task while\nminimizing the number of queries to the oracle. To empirically evaluate AVLEN,\nwe present experiments on the SoundSpaces framework for semantic audio-visual\nnavigation tasks. Our results show that equipping the agent to ask for help\nleads to a clear improvement in performance, especially in challenging cases,\ne.g., when the sound is unheard during training or in the presence of\ndistractor sounds.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2022-10-14T16:35:06+00:00",
    "updated": "2022-10-14T16:35:06+00:00",
    "doi": null,
    "comment": "Accepted at NeurIPS 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.05956v1": {
    "id": "http://arxiv.org/abs/2309.05956v1",
    "title": "Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation",
    "authors": [
      "Yunhao Ge",
      "Jiashu Xu",
      "Brian Nlong Zhao",
      "Neel Joshi",
      "Laurent Itti",
      "Vibhav Vineet"
    ],
    "abstract": "We propose a new paradigm to automatically generate training data with\naccurate labels at scale using the text-to-image synthesis frameworks (e.g.,\nDALL-E, Stable Diffusion, etc.). The proposed approach1 decouples training data\ngeneration into foreground object generation, and contextually coherent\nbackground generation. To generate foreground objects, we employ a\nstraightforward textual template, incorporating the object class name as input\nprompts. This is fed into a text-to-image synthesis framework, producing\nvarious foreground images set against isolated backgrounds. A\nforeground-background segmentation algorithm is then used to generate\nforeground object masks. To generate context images, we begin by creating\nlanguage descriptions of the context. This is achieved by applying an image\ncaptioning method to a small set of images representing the desired context.\nThese textual descriptions are then transformed into a diverse array of context\nimages via a text-to-image synthesis framework. Subsequently, we composite\nthese with the foreground object masks produced in the initial step, utilizing\na cut-and-paste method, to formulate the training data. We demonstrate the\nadvantages of our approach on five object detection and segmentation datasets,\nincluding Pascal VOC and COCO. We found that detectors trained solely on\nsynthetic data produced by our method achieve performance comparable to those\ntrained on real data (Fig. 1). Moreover, a combination of real and synthetic\ndata yields even much better results. Further analysis indicates that the\nsynthetic data distribution complements the real data distribution effectively.\nAdditionally, we emphasize the compositional nature of our data generation\napproach in out-of-distribution and zero-shot data generation scenarios. We\nopen-source our code at https://github.com/gyhandy/Text2Image-for-Detection",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-09-12T04:41:45+00:00",
    "updated": "2023-09-12T04:41:45+00:00",
    "doi": null,
    "comment": "Code in https://github.com/gyhandy/Text2Image-for-Detection",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2305.15690v2": {
    "id": "http://arxiv.org/abs/2305.15690v2",
    "title": "Beryllium: Neural Search for Algorithm Implementations",
    "authors": [
      "Adithya Kulkarni",
      "Mohna Chakraborty",
      "Yonas Sium",
      "Sai Charishma Valluri",
      "Wei Le",
      "Qi Li"
    ],
    "abstract": "In this paper, we explore the feasibility of finding algorithm\nimplementations from code. Successfully matching code and algorithms can help\nunderstand unknown code, provide reference implementations, and automatically\ncollect data for learning-based program synthesis. To achieve the goal, we\ndesigned a new language named p-language to specify the algorithms and a static\nanalyzer for the p-language to automatically extract control flow, math, and\nnatural language information from the algorithm descriptions. We embedded the\noutput of p-language (p-code) and source code in a common vector space using\nself-supervised machine learning methods to match algorithm with code without\nany manual annotation. We developed a tool named Beryllium. It takes pseudo\ncode as a query and returns a list of ranked code snippets that likely match\nthe algorithm query. Our evaluation on Stony Brook Algorithm Repository and\npopular GitHub projects show that Beryllium significantly outperformed the\nstate-of-the-art code search tools in both C and Java. Specifically, for 98.5%,\n93.8%, and 66.2% queries, we found the algorithm implementations in the top 25,\n10, and 1 ranked list, respectively. Given 87 algorithm queries, we found\nimplementations for 74 algorithms in the GitHub projects where we did not know\nthe algorithms before.",
    "categories": [
      "cs.SE"
    ],
    "published": "2023-05-25T03:49:36+00:00",
    "updated": "2023-07-01T22:33:04+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "2303.11324v2": {
    "id": "http://arxiv.org/abs/2303.11324v2",
    "title": "Open-vocabulary Panoptic Segmentation with Embedding Modulation",
    "authors": [
      "Xi Chen",
      "Shuang Li",
      "Ser-Nam Lim",
      "Antonio Torralba",
      "Hengshuang Zhao"
    ],
    "abstract": "Open-vocabulary image segmentation is attracting increasing attention due to\nits critical applications in the real world. Traditional closed-vocabulary\nsegmentation methods are not able to characterize novel objects, whereas\nseveral recent open-vocabulary attempts obtain unsatisfactory results, i.e.,\nnotable performance reduction on the closed vocabulary and massive demand for\nextra data. To this end, we propose OPSNet, an omnipotent and data-efficient\nframework for Open-vocabulary Panoptic Segmentation. Specifically, the\nexquisitely designed Embedding Modulation module, together with several\nmeticulous components, enables adequate embedding enhancement and information\nexchange between the segmentation model and the visual-linguistic well-aligned\nCLIP encoder, resulting in superior segmentation performance under both open-\nand closed-vocabulary settings with much fewer need of additional data.\nExtensive experimental evaluations are conducted across multiple datasets\n(e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various\ncircumstances, where the proposed OPSNet achieves state-of-the-art results,\nwhich demonstrates the effectiveness and generality of the proposed approach.\nThe code and trained models will be made publicly available.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-03-20T17:58:48+00:00",
    "updated": "2023-07-15T11:04:26+00:00",
    "doi": null,
    "comment": "ICCV2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2203.07732v2": {
    "id": "http://arxiv.org/abs/2203.07732v2",
    "title": "S2F2: Self-Supervised High Fidelity Face Reconstruction from Monocular Image",
    "authors": [
      "Abdallah Dib",
      "Junghyun Ahn",
      "Cedric Thebault",
      "Philippe-Henri Gosselin",
      "Louis Chevallier"
    ],
    "abstract": "We present a novel face reconstruction method capable of reconstructing\ndetailed face geometry, spatially varying face reflectance from a single\nmonocular image. We build our work upon the recent advances of DNN-based\nauto-encoders with differentiable ray tracing image formation, trained in\nself-supervised manner. While providing the advantage of learning-based\napproaches and real-time reconstruction, the latter methods lacked fidelity. In\nthis work, we achieve, for the first time, high fidelity face reconstruction\nusing self-supervised learning only. Our novel coarse-to-fine deep architecture\nallows us to solve the challenging problem of decoupling face reflectance from\ngeometry using a single image, at high computational speed. Compared to\nstate-of-the-art methods, our method achieves more visually appealing\nreconstruction.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG",
      "68T45, 68T07, 68U10, 68U05",
      "I.4.5; I.4.8; I.4.9; I.3.3; I.2.10"
    ],
    "published": "2022-03-15T08:55:45+00:00",
    "updated": "2022-04-05T11:52:51+00:00",
    "doi": null,
    "comment": "24 Pages, 22 Figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2409.15146v2": {
    "id": "http://arxiv.org/abs/2409.15146v2",
    "title": "COHERENT: Collaboration of Heterogeneous Multi-Robot System with Large Language Models",
    "authors": [
      "Kehui Liu",
      "Zixin Tang",
      "Dong Wang",
      "Zhigang Wang",
      "Bin Zhao",
      "Xuelong Li"
    ],
    "abstract": "Leveraging the powerful reasoning capabilities of large language models\n(LLMs), recent LLM-based robot task planning methods yield promising results.\nHowever, they mainly focus on single or multiple homogeneous robots on simple\ntasks. Practically, complex long-horizon tasks always require collaborations\namong multiple heterogeneous robots especially with more complex action spaces,\nwhich makes these tasks more challenging. To this end, we propose COHERENT, a\nnovel LLM-based task planning framework for collaboration of heterogeneous\nmulti-robot systems including quadrotors, robotic dogs, and robotic arms.\nSpecifically, a Proposal-Execution-Feedback-Adjustment (PEFA) mechanism is\ndesigned to decompose and assign actions for individual robots, where a\ncentralized task assigner makes a task planning proposal to decompose the\ncomplex task into subtasks, and then assigns subtasks to robot executors. Each\nrobot executor selects a feasible action to implement the assigned subtask and\nreports self-reflection feedback to the task assigner for plan adjustment. The\nPEFA loops until the task is completed. Moreover, we create a challenging\nheterogeneous multi-robot task planning benchmark encompassing 100 complex\nlong-horizon tasks. The experimental results show that our work surpasses the\nprevious methods by a large margin in terms of success rate and execution\nefficiency. The experimental videos, code, and benchmark are released at\nhttps://github.com/MrKeee/COHERENT.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2024-09-23T15:53:41+00:00",
    "updated": "2024-09-25T05:59:08+00:00",
    "doi": null,
    "comment": "7 pages, 5 figures. Submitted to IEEE International Conference on\n  Robotics and Automation (ICRA), 2025",
    "journal_ref": null,
    "primary_category": "cs.RO"
  },
  "2206.05282v3": {
    "id": "http://arxiv.org/abs/2206.05282v3",
    "title": "Learning to Estimate Shapley Values with Vision Transformers",
    "authors": [
      "Ian Covert",
      "Chanwoo Kim",
      "Su-In Lee"
    ],
    "abstract": "Transformers have become a default architecture in computer vision, but\nunderstanding what drives their predictions remains a challenging problem.\nCurrent explanation approaches rely on attention values or input gradients, but\nthese provide a limited view of a model's dependencies. Shapley values offer a\ntheoretically sound alternative, but their computational cost makes them\nimpractical for large, high-dimensional models. In this work, we aim to make\nShapley values practical for vision transformers (ViTs). To do so, we first\nleverage an attention masking approach to evaluate ViTs with partial\ninformation, and we then develop a procedure to generate Shapley value\nexplanations via a separate, learned explainer model. Our experiments compare\nShapley values to many baseline methods (e.g., attention rollout, GradCAM,\nLRP), and we find that our approach provides more accurate explanations than\nexisting methods for ViTs.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2022-06-10T07:09:28+00:00",
    "updated": "2023-03-01T20:24:58+00:00",
    "doi": null,
    "comment": "ICLR 2023 camera-ready",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2201.12133v2": {
    "id": "http://arxiv.org/abs/2201.12133v2",
    "title": "O-ViT: Orthogonal Vision Transformer",
    "authors": [
      "Yanhong Fei",
      "Yingjie Liu",
      "Xian Wei",
      "Mingsong Chen"
    ],
    "abstract": "Inspired by the tremendous success of the self-attention mechanism in natural\nlanguage processing, the Vision Transformer (ViT) creatively applies it to\nimage patch sequences and achieves incredible performance. However, the scaled\ndot-product self-attention of ViT brings about scale ambiguity to the structure\nof the original feature space. To address this problem, we propose a novel\nmethod named Orthogonal Vision Transformer (O-ViT), to optimize ViT from the\ngeometric perspective. O-ViT limits parameters of self-attention blocks to be\non the norm-keeping orthogonal manifold, which can keep the geometry of the\nfeature space. Moreover, O-ViT achieves both orthogonal constraints and cheap\noptimization overhead by adopting a surjective mapping between the orthogonal\ngroup and its Lie algebra.We have conducted comparative experiments on image\nrecognition tasks to demonstrate O-ViT's validity and experiments show that\nO-ViT can boost the performance of ViT by up to 3.6%.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2022-01-28T14:18:52+00:00",
    "updated": "2022-02-16T13:49:43+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2406.15982v1": {
    "id": "http://arxiv.org/abs/2406.15982v1",
    "title": "Learning with Noisy Ground Truth: From 2D Classification to 3D Reconstruction",
    "authors": [
      "Yangdi Lu",
      "Wenbo He"
    ],
    "abstract": "Deep neural networks has been highly successful in data-intense computer\nvision applications, while such success relies heavily on the massive and clean\ndata. In real-world scenarios, clean data sometimes is difficult to obtain. For\nexample, in image classification and segmentation tasks, precise annotations of\nmillions samples are generally very expensive and time-consuming. In 3D static\nscene reconstruction task, most NeRF related methods require the foundational\nassumption of the static scene (e.g. consistent lighting condition and\npersistent object positions), which is often violated in real-world scenarios.\nTo address these problem, learning with noisy ground truth (LNGT) has emerged\nas an effective learning method and shows great potential. In this short\nsurvey, we propose a formal definition unify the analysis of LNGT LNGT in the\ncontext of different machine learning tasks (classification and regression).\nBased on this definition, we propose a novel taxonomy to classify the existing\nwork according to the error decomposition with the fundamental definition of\nmachine learning. Further, we provide in-depth analysis on memorization effect\nand insightful discussion about potential future research opportunities from 2D\nclassification to 3D reconstruction, in the hope of providing guidance to\nfollow-up research.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-23T02:21:48+00:00",
    "updated": "2024-06-23T02:21:48+00:00",
    "doi": null,
    "comment": "Computer vision, Noisy Labels, 3D reconstruction, 3D Gaussian Splats,\n  (Work still in progress)",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2302.12173v2": {
    "id": "http://arxiv.org/abs/2302.12173v2",
    "title": "Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
    "authors": [
      "Kai Greshake",
      "Sahar Abdelnabi",
      "Shailesh Mishra",
      "Christoph Endres",
      "Thorsten Holz",
      "Mario Fritz"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks' practical viability against\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication's functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2023-02-23T17:14:38+00:00",
    "updated": "2023-05-05T14:26:17+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CR"
  },
  "2112.04632v2": {
    "id": "http://arxiv.org/abs/2112.04632v2",
    "title": "Recurrent Glimpse-based Decoder for Detection with Transformer",
    "authors": [
      "Zhe Chen",
      "Jing Zhang",
      "Dacheng Tao"
    ],
    "abstract": "Although detection with Transformer (DETR) is increasingly popular, its\nglobal attention modeling requires an extremely long training period to\noptimize and achieve promising detection performance. Alternative to existing\nstudies that mainly develop advanced feature or embedding designs to tackle the\ntraining issue, we point out that the Region-of-Interest (RoI) based detection\nrefinement can easily help mitigate the difficulty of training for DETR\nmethods. Based on this, we introduce a novel REcurrent Glimpse-based decOder\n(REGO) in this paper. In particular, the REGO employs a multi-stage recurrent\nprocessing structure to help the attention of DETR gradually focus on\nforeground objects more accurately. In each processing stage, visual features\nare extracted as glimpse features from RoIs with enlarged bounding box areas of\ndetection results from the previous stage. Then, a glimpse-based decoder is\nintroduced to provide refined detection results based on both the glimpse\nfeatures and the attention modeling outputs of the previous stage. In practice,\nREGO can be easily embedded in representative DETR variants while maintaining\ntheir fully end-to-end training and inference pipelines. In particular, REGO\nhelps Deformable DETR achieve 44.8 AP on the MSCOCO dataset with only 36\ntraining epochs, compared with the first DETR and the Deformable DETR that\nrequire 500 and 50 epochs to achieve comparable performance, respectively.\nExperiments also show that REGO consistently boosts the performance of\ndifferent DETR detectors by up to 7% relative gain at the same setting of 50\ntraining epochs. Code is available via\nhttps://github.com/zhechen/Deformable-DETR-REGO.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-12-09T00:29:19+00:00",
    "updated": "2022-04-12T07:20:05+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2304.12410v2": {
    "id": "http://arxiv.org/abs/2304.12410v2",
    "title": "PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques",
    "authors": [
      "Mohammed Sabry",
      "Anya Belz"
    ],
    "abstract": "Recent parameter-efficient finetuning (PEFT) techniques aim to improve over\nthe considerable cost of fully finetuning large pretrained language models\n(PLM). As different PEFT techniques proliferate, it is becoming difficult to\ncompare them, in particular in terms of (i) the structure and functionality\nthey add to the PLM, (ii) the different types and degrees of efficiency\nimprovements achieved, (iii) performance at different downstream tasks, and\n(iv) how differences in structure and functionality relate to efficiency and\ntask performance. To facilitate such comparisons, this paper presents a\nreference architecture which standardises aspects shared by different PEFT\ntechniques, while isolating differences to specific locations and interactions\nwith the standard components. Through this process of standardising and\nisolating differences, a modular view of PEFT techniques emerges, supporting\nnot only direct comparison of different techniques and their efficiency and\ntask performance, but also systematic exploration of reusability and\ncomposability of the different types of finetuned modules. We demonstrate how\nthe reference architecture can be applied to understand properties and relative\nadvantages of PEFT techniques, hence to inform selection of techniques for\nspecific tasks, and design choices for new PEFT techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-04-24T19:43:11+00:00",
    "updated": "2023-10-19T15:08:05+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1811.00002v1": {
    "id": "http://arxiv.org/abs/1811.00002v1",
    "title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis",
    "authors": [
      "Ryan Prenger",
      "Rafael Valle",
      "Bryan Catanzaro"
    ],
    "abstract": "In this paper we propose WaveGlow: a flow-based network capable of generating\nhigh quality speech from mel-spectrograms. WaveGlow combines insights from Glow\nand WaveNet in order to provide fast, efficient and high-quality audio\nsynthesis, without the need for auto-regression. WaveGlow is implemented using\nonly a single network, trained using only a single cost function: maximizing\nthe likelihood of the training data, which makes the training procedure simple\nand stable. Our PyTorch implementation produces audio samples at a rate of more\nthan 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers\naudio quality as good as the best publicly available WaveNet implementation.\nAll code will be made publicly available online.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS",
      "stat.ML"
    ],
    "published": "2018-10-31T03:22:25+00:00",
    "updated": "2018-10-31T03:22:25+00:00",
    "doi": null,
    "comment": "5 pages, 1 figure, 1 table, 13 equations",
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "2405.10300v2": {
    "id": "http://arxiv.org/abs/2405.10300v2",
    "title": "Grounding DINO 1.5: Advance the \"Edge\" of Open-Set Object Detection",
    "authors": [
      "Tianhe Ren",
      "Qing Jiang",
      "Shilong Liu",
      "Zhaoyang Zeng",
      "Wenlong Liu",
      "Han Gao",
      "Hongjie Huang",
      "Zhengyu Ma",
      "Xiaoke Jiang",
      "Yihao Chen",
      "Yuda Xiong",
      "Hao Zhang",
      "Feng Li",
      "Peijun Tang",
      "Kent Yu",
      "Lei Zhang"
    ],
    "abstract": "This paper introduces Grounding DINO 1.5, a suite of advanced open-set object\ndetection models developed by IDEA Research, which aims to advance the \"Edge\"\nof open-set object detection. The suite encompasses two models: Grounding DINO\n1.5 Pro, a high-performance model designed for stronger generalization\ncapability across a wide range of scenarios, and Grounding DINO 1.5 Edge, an\nefficient model optimized for faster speed demanded in many applications\nrequiring edge deployment. The Grounding DINO 1.5 Pro model advances its\npredecessor by scaling up the model architecture, integrating an enhanced\nvision backbone, and expanding the training dataset to over 20 million images\nwith grounding annotations, thereby achieving a richer semantic understanding.\nThe Grounding DINO 1.5 Edge model, while designed for efficiency with reduced\nfeature scales, maintains robust detection capabilities by being trained on the\nsame comprehensive dataset. Empirical results demonstrate the effectiveness of\nGrounding DINO 1.5, with the Grounding DINO 1.5 Pro model attaining a 54.3 AP\non the COCO detection benchmark and a 55.7 AP on the LVIS-minival zero-shot\ntransfer benchmark, setting new records for open-set object detection.\nFurthermore, the Grounding DINO 1.5 Edge model, when optimized with TensorRT,\nachieves a speed of 75.2 FPS while attaining a zero-shot performance of 36.2 AP\non the LVIS-minival benchmark, making it more suitable for edge computing\nscenarios. Model examples and demos with API will be released at\nhttps://github.com/IDEA-Research/Grounding-DINO-1.5-API",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-05-16T17:54:15+00:00",
    "updated": "2024-06-01T03:35:22+00:00",
    "doi": null,
    "comment": "homepage: https://deepdataspace.com/home",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2307.00184v3": {
    "id": "http://arxiv.org/abs/2307.00184v3",
    "title": "Personality Traits in Large Language Models",
    "authors": [
      "Greg Serapio-Garc\u00eda",
      "Mustafa Safdari",
      "Cl\u00e9ment Crepy",
      "Luning Sun",
      "Stephen Fitz",
      "Peter Romero",
      "Marwa Abdulhai",
      "Aleksandra Faust",
      "Maja Matari\u0107"
    ],
    "abstract": "The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling the generation of coherent and contextually\nrelevant human-like text. As LLMs increasingly power conversational agents used\nby the general public world-wide, the synthetic personality embedded in these\nmodels, by virtue of training on large amounts of human data, is becoming\nincreasingly important. Since personality is a key factor determining the\neffectiveness of communication, we present a comprehensive method for\nadministering and validating personality tests on widely-used LLMs, as well as\nfor shaping personality in the generated text of such LLMs. Applying this\nmethod, we found: 1) personality measurements in the outputs of some LLMs under\nspecific prompting configurations are reliable and valid; 2) evidence of\nreliability and validity of synthetic LLM personality is stronger for larger\nand instruction fine-tuned models; and 3) personality in LLM outputs can be\nshaped along desired dimensions to mimic specific human personality profiles.\nWe discuss application and ethical implications of the measurement and shaping\nmethod, in particular regarding responsible AI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "68T35",
      "I.2.7"
    ],
    "published": "2023-07-01T00:58:51+00:00",
    "updated": "2023-09-21T21:10:56+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2310.00673v2": {
    "id": "http://arxiv.org/abs/2310.00673v2",
    "title": "Learning Type Inference for Enhanced Dataflow Analysis",
    "authors": [
      "Lukas Seidel",
      "Sedick David Baker Effendi",
      "Xavier Pinho",
      "Konrad Rieck",
      "Brink van der Merwe",
      "Fabian Yamaguchi"
    ],
    "abstract": "Statically analyzing dynamically-typed code is a challenging endeavor, as\neven seemingly trivial tasks such as determining the targets of procedure calls\nare non-trivial without knowing the types of objects at compile time.\nAddressing this challenge, gradual typing is increasingly added to\ndynamically-typed languages, a prominent example being TypeScript that\nintroduces static typing to JavaScript. Gradual typing improves the developer's\nability to verify program behavior, contributing to robust, secure and\ndebuggable programs. In practice, however, users only sparsely annotate types\ndirectly. At the same time, conventional type inference faces\nperformance-related challenges as program size grows. Statistical techniques\nbased on machine learning offer faster inference, but although recent\napproaches demonstrate overall improved accuracy, they still perform\nsignificantly worse on user-defined types than on the most common built-in\ntypes. Limiting their real-world usefulness even more, they rarely integrate\nwith user-facing applications. We propose CodeTIDAL5, a Transformer-based model\ntrained to reliably predict type annotations. For effective result retrieval\nand re-integration, we extract usage slices from a program's code property\ngraph. Comparing our approach against recent neural type inference systems, our\nmodel outperforms the current state-of-the-art by 7.85% on the\nManyTypes4TypeScript benchmark, achieving 71.27% accuracy overall. Furthermore,\nwe present JoernTI, an integration of our approach into Joern, an open source\nstatic analysis tool, and demonstrate that the analysis benefits from the\nadditional type information. As our model allows for fast inference times even\non commodity CPUs, making our system available through Joern leads to high\naccessibility and facilitates security research.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2023-10-01T13:52:28+00:00",
    "updated": "2023-10-04T15:15:00+00:00",
    "doi": null,
    "comment": "- fixed last author's name - fixed header",
    "journal_ref": "28th European Symposium on Research in Computer Security (ESORICS)\n  2023",
    "primary_category": "cs.LG"
  },
  "2203.03187v1": {
    "id": "http://arxiv.org/abs/2203.03187v1",
    "title": "Knowledge Amalgamation for Object Detection with Transformers",
    "authors": [
      "Haofei Zhang",
      "Feng Mao",
      "Mengqi Xue",
      "Gongfan Fang",
      "Zunlei Feng",
      "Jie Song",
      "Mingli Song"
    ],
    "abstract": "Knowledge amalgamation (KA) is a novel deep model reusing task aiming to\ntransfer knowledge from several well-trained teachers to a multi-talented and\ncompact student. Currently, most of these approaches are tailored for\nconvolutional neural networks (CNNs). However, there is a tendency that\ntransformers, with a completely different architecture, are starting to\nchallenge the domination of CNNs in many computer vision tasks. Nevertheless,\ndirectly applying the previous KA methods to transformers leads to severe\nperformance degradation. In this work, we explore a more effective KA scheme\nfor transformer-based object detection models. Specifically, considering the\narchitecture characteristics of transformers, we propose to dissolve the KA\ninto two aspects: sequence-level amalgamation (SA) and task-level amalgamation\n(TA). In particular, a hint is generated within the sequence-level amalgamation\nby concatenating teacher sequences instead of redundantly aggregating them to a\nfixed-size one as previous KA works. Besides, the student learns heterogeneous\ndetection tasks through soft targets with efficiency in the task-level\namalgamation. Extensive experiments on PASCAL VOC and COCO have unfolded that\nthe sequence-level amalgamation significantly boosts the performance of\nstudents, while the previous methods impair the students. Moreover, the\ntransformer-based students excel in learning amalgamated knowledge, as they\nhave mastered heterogeneous detection tasks rapidly and achieved superior or at\nleast comparable performance to those of the teachers in their specializations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2022-03-07T07:45:22+00:00",
    "updated": "2022-03-07T07:45:22+00:00",
    "doi": "10.1109/TIP.2023.3263105",
    "comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.17934v1": {
    "id": "http://arxiv.org/abs/2403.17934v1",
    "title": "AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation",
    "authors": [
      "Qingping Sun",
      "Yanjun Wang",
      "Ailing Zeng",
      "Wanqi Yin",
      "Chen Wei",
      "Wenjia Wang",
      "Haiyi Mei",
      "Chi Sing Leung",
      "Ziwei Liu",
      "Lei Yang",
      "Zhongang Cai"
    ],
    "abstract": "Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh\nrecovery) involves the human body, hand, and expression estimation. Most\nexisting methods have tackled this task in a two-stage manner, first detecting\nthe human body part with an off-the-shelf detection model and inferring the\ndifferent human body parts individually. Despite the impressive results\nachieved, these methods suffer from 1) loss of valuable contextual information\nvia cropping, 2) introducing distractions, and 3) lacking inter-association\namong different persons and body parts, inevitably causing performance\ndegradation, especially for crowded scenes. To address these issues, we\nintroduce a novel all-in-one-stage framework, AiOS, for multiple expressive\nhuman pose and shape recovery without an additional human detection step.\nSpecifically, our method is built upon DETR, which treats multi-person\nwhole-body mesh recovery task as a progressive set prediction problem with\nvarious sequential detection. We devise the decoder tokens and extend them to\nour task. Specifically, we first employ a human token to probe a human location\nin the image and encode global features for each instance, which provides a\ncoarse location for the later transformer block. Then, we introduce a\njoint-related token to probe the human joint in the image and encoder a\nfine-grained local feature, which collaborates with the global feature to\nregress the whole-body mesh. This straightforward but effective model\noutperforms previous state-of-the-art methods by a 9% reduction in NMVE on\nAGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a\n3% reduction in PVE on EgoBody.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-26T17:59:23+00:00",
    "updated": "2024-03-26T17:59:23+00:00",
    "doi": null,
    "comment": "Homepage: https://ttxskk.github.io/AiOS/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2206.01612v8": {
    "id": "http://arxiv.org/abs/2206.01612v8",
    "title": "OmniXAI: A Library for Explainable AI",
    "authors": [
      "Wenzhuo Yang",
      "Hung Le",
      "Tanmay Laud",
      "Silvio Savarese",
      "Steven C. H. Hoi"
    ],
    "abstract": "We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python\nlibrary of eXplainable AI (XAI), which offers omni-way explainable AI\ncapabilities and various interpretable machine learning techniques to address\nthe pain points of understanding and interpreting the decisions made by machine\nlearning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library\nthat makes explainable AI easy for data scientists, ML researchers and\npractitioners who need explanation for various types of data, models and\nexplanation methods at different stages of ML process (data exploration,\nfeature engineering, model development, evaluation, and decision-making, etc).\nIn particular, our library includes a rich family of explanation methods\nintegrated in a unified interface, which supports multiple data types (tabular\ndata, images, texts, time-series), multiple types of ML models (traditional ML\nin Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of\ndiverse explanation methods including \"model-specific\" and \"model-agnostic\"\nones (such as feature-attribution explanation, counterfactual explanation,\ngradient-based explanation, etc). For practitioners, the library provides an\neasy-to-use unified interface to generate the explanations for their\napplications by only writing a few lines of codes, and also a GUI dashboard for\nvisualization of different explanations for more insights about decisions. In\nthis technical report, we present OmniXAI's design principles, system\narchitectures, and major functionalities, and also demonstrate several example\nuse cases across different types of data, tasks, and models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "68T09, 68T20, 68T01",
      "I.2.6; I.2.5"
    ],
    "published": "2022-06-01T11:35:37+00:00",
    "updated": "2022-12-12T09:26:32+00:00",
    "doi": null,
    "comment": "Github repo: https://github.com/salesforce/OmniXAI",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2204.00231v1": {
    "id": "http://arxiv.org/abs/2204.00231v1",
    "title": "Online panoptic 3D reconstruction as a Linear Assignment Problem",
    "authors": [
      "Leevi Raivio",
      "Esa Rahtu"
    ],
    "abstract": "Real-time holistic scene understanding would allow machines to interpret\ntheir surrounding in a much more detailed manner than is currently possible.\nWhile panoptic image segmentation methods have brought image segmentation\ncloser to this goal, this information has to be described relative to the 3D\nenvironment for the machine to be able to utilise it effectively. In this\npaper, we investigate methods for sequentially reconstructing static\nenvironments from panoptic image segmentations in 3D. We specifically target\nreal-time operation: the algorithm must process data strictly online and be\nable to run at relatively fast frame rates. Additionally, the method should be\nscalable for environments large enough for practical applications. By applying\na simple but powerful data-association algorithm, we outperform earlier similar\nworks when operating purely online. Our method is also capable of reaching\nframe-rates high enough for real-time applications and is scalable to larger\nenvironments as well. Source code and further demonstrations are released to\nthe public at: \\url{https://tutvision.github.io/Online-Panoptic-3D/}",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-04-01T06:43:34+00:00",
    "updated": "2022-04-01T06:43:34+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.11774v1": {
    "id": "http://arxiv.org/abs/2312.11774v1",
    "title": "Text-Image Conditioned Diffusion for Consistent Text-to-3D Generation",
    "authors": [
      "Yuze He",
      "Yushi Bai",
      "Matthieu Lin",
      "Jenny Sheng",
      "Yubin Hu",
      "Qi Wang",
      "Yu-Hui Wen",
      "Yong-Jin Liu"
    ],
    "abstract": "By lifting the pre-trained 2D diffusion models into Neural Radiance Fields\n(NeRFs), text-to-3D generation methods have made great progress. Many\nstate-of-the-art approaches usually apply score distillation sampling (SDS) to\noptimize the NeRF representations, which supervises the NeRF optimization with\npre-trained text-conditioned 2D diffusion models such as Imagen. However, the\nsupervision signal provided by such pre-trained diffusion models only depends\non text prompts and does not constrain the multi-view consistency. To inject\nthe cross-view consistency into diffusion priors, some recent works finetune\nthe 2D diffusion model with multi-view data, but still lack fine-grained view\ncoherence. To tackle this challenge, we incorporate multi-view image conditions\ninto the supervision signal of NeRF optimization, which explicitly enforces\nfine-grained view consistency. With such stronger supervision, our proposed\ntext-to-3D method effectively mitigates the generation of floaters (due to\nexcessive densities) and completely empty spaces (due to insufficient\ndensities). Our quantitative evaluations on the T$^3$Bench dataset demonstrate\nthat our method achieves state-of-the-art performance over existing text-to-3D\nmethods. We will make the code publicly available.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-19T01:09:49+00:00",
    "updated": "2023-12-19T01:09:49+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.04109v2": {
    "id": "http://arxiv.org/abs/2309.04109v2",
    "title": "From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models",
    "authors": [
      "Changming Xiao",
      "Qi Yang",
      "Feng Zhou",
      "Changshui Zhang"
    ],
    "abstract": "Diffusion models have revolted the field of text-to-image generation\nrecently. The unique way of fusing text and image information contributes to\ntheir remarkable capability of generating highly text-related images. From\nanother perspective, these generative models imply clues about the precise\ncorrelation between words and pixels. In this work, a simple but effective\nmethod is proposed to utilize the attention mechanism in the denoising network\nof text-to-image diffusion models. Without re-training nor inference-time\noptimization, the semantic grounding of phrases can be attained directly. We\nevaluate our method on Pascal VOC 2012 and Microsoft COCO 2014 under\nweakly-supervised semantic segmentation setting and our method achieves\nsuperior performance to prior methods. In addition, the acquired word-pixel\ncorrelation is found to be generalizable for the learned text embedding of\ncustomized generation methods, requiring only a few modifications. To validate\nour discovery, we introduce a new practical task called \"personalized referring\nimage segmentation\" with a new dataset. Experiments in various situations\ndemonstrate the advantages of our method compared to strong baselines on this\ntask. In summary, our work reveals a novel way to extract the rich multi-modal\nknowledge hidden in diffusion models for segmentation.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-09-08T04:10:01+00:00",
    "updated": "2024-10-01T10:30:07+00:00",
    "doi": "10.1016/j.neucom.2024.128437",
    "comment": "A revised version of this paper will be published in Neurocomputing,\n  see https://doi.org/10.1016/j.neucom.2024.128437",
    "journal_ref": "Neurocomputing, Volume 610, 2024, 128437",
    "primary_category": "cs.CV"
  },
  "1807.11143v2": {
    "id": "http://arxiv.org/abs/1807.11143v2",
    "title": "ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks",
    "authors": [
      "Mingzhang Yin",
      "Mingyuan Zhou"
    ],
    "abstract": "To backpropagate the gradients through stochastic binary layers, we propose\nthe augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low\nvariance, and has low computational complexity. Exploiting variable\naugmentation, REINFORCE, and reparameterization, the ARM estimator achieves\nadaptive variance reduction for Monte Carlo integration by merging two\nexpectations via common random numbers. The variance-reduction mechanism of the\nARM estimator can also be attributed to either antithetic sampling in an\naugmented space, or the use of an optimal anti-symmetric \"self-control\"\nbaseline function together with the REINFORCE estimator in that augmented\nspace. Experimental results show the ARM estimator provides state-of-the-art\nperformance in auto-encoding variational inference and maximum likelihood\nestimation, for discrete latent variable models with one or multiple stochastic\nbinary layers. Python code for reproducible research is publicly available.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.CO",
      "stat.ME"
    ],
    "published": "2018-07-30T02:21:07+00:00",
    "updated": "2019-09-09T22:34:47+00:00",
    "doi": null,
    "comment": "ICLR 2019",
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2405.07857v3": {
    "id": "http://arxiv.org/abs/2405.07857v3",
    "title": "Synergistic Integration of Coordinate Network and Tensorial Feature for Improving Neural Radiance Fields from Sparse Inputs",
    "authors": [
      "Mingyu Kim",
      "Jun-Seong Kim",
      "Se-Young Yun",
      "Jin-Hwa Kim"
    ],
    "abstract": "The multi-plane representation has been highlighted for its fast training and\ninference across static and dynamic neural radiance fields. This approach\nconstructs relevant features via projection onto learnable grids and\ninterpolating adjacent vertices. However, it has limitations in capturing\nlow-frequency details and tends to overuse parameters for low-frequency\nfeatures due to its bias toward fine details, despite its multi-resolution\nconcept. This phenomenon leads to instability and inefficiency when training\nposes are sparse. In this work, we propose a method that synergistically\nintegrates multi-plane representation with a coordinate-based MLP network known\nfor strong bias toward low-frequency signals. The coordinate-based network is\nresponsible for capturing low-frequency details, while the multi-plane\nrepresentation focuses on capturing fine-grained details. We demonstrate that\nusing residual connections between them seamlessly preserves their own inherent\nproperties. Additionally, the proposed progressive training scheme accelerates\nthe disentanglement of these two features. We demonstrate empirically that our\nproposed method not only outperforms baseline models for both static and\ndynamic NeRFs with sparse inputs, but also achieves comparable results with\nfewer parameters.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-05-13T15:42:46+00:00",
    "updated": "2024-06-05T09:32:36+00:00",
    "doi": null,
    "comment": "ICML2024 ; Project page is accessible at\n  https://mingyukim87.github.io/SynergyNeRF ; Code is available at\n  https://github.com/MingyuKim87/SynergyNeRF",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2310.16146v1": {
    "id": "http://arxiv.org/abs/2310.16146v1",
    "title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
    "authors": [
      "Alejandro Lozano",
      "Scott L Fleming",
      "Chia-Chun Chiang",
      "Nigam Shah"
    ],
    "abstract": "The quickly-expanding nature of published medical literature makes it\nchallenging for clinicians and researchers to keep up with and summarize\nrecent, relevant findings in a timely manner. While several closed-source\nsummarization tools based on large language models (LLMs) now exist, rigorous\nand systematic evaluations of their outputs are lacking. Furthermore, there is\na paucity of high-quality datasets and appropriate benchmark tasks with which\nto evaluate these tools. We address these issues with four contributions: we\nrelease Clinfo.ai, an open-source WebApp that answers clinical questions based\non dynamically retrieved scientific literature; we specify an information\nretrieval and abstractive summarization task to evaluate the performance of\nsuch retrieval-augmented LLM systems; we release a dataset of 200 questions and\ncorresponding answers derived from published systematic reviews, which we name\nPubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for\nClinfo.ai and other publicly available OpenQA systems on PubMedRS-200.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-10-24T19:43:39+00:00",
    "updated": "2023-10-24T19:43:39+00:00",
    "doi": null,
    "comment": "Preprint of an article published in Pacific Symposium on Biocomputing\n  copyright 2024 World Scientific Publishing Co., Singapore,\n  http://psb.stanford.edu/",
    "journal_ref": null,
    "primary_category": "cs.IR"
  },
  "2406.11277v1": {
    "id": "http://arxiv.org/abs/2406.11277v1",
    "title": "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector",
    "authors": [
      "Xiaoxue Cheng",
      "Junyi Li",
      "Wayne Xin Zhao",
      "Hongzhi Zhang",
      "Fuzheng Zhang",
      "Di Zhang",
      "Kun Gai",
      "Ji-Rong Wen"
    ],
    "abstract": "Hallucination detection is a challenging task for large language models\n(LLMs), and existing studies heavily rely on powerful closed-source LLMs such\nas GPT-4. In this paper, we propose an autonomous LLM-based agent framework,\ncalled HaluAgent, which enables relatively smaller LLMs (e.g. Baichuan2-Chat\n7B) to actively select suitable tools for detecting multiple hallucination\ntypes such as text, code, and mathematical expression. In HaluAgent, we\nintegrate the LLM, multi-functional toolbox, and design a fine-grained\nthree-stage detection framework along with memory mechanism. To facilitate the\neffectiveness of HaluAgent, we leverage existing Chinese and English datasets\nto synthesize detection trajectories for fine-tuning, which endows HaluAgent\nwith the capability for bilingual hallucination detection. Extensive\nexperiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent\ncan perform hallucination detection on various types of tasks and datasets,\nachieving performance comparable to or even higher than GPT-4 without tool\nenhancements on both in-domain and out-of-domain datasets. We release our\ndataset and code at https://github.com/RUCAIBox/HaluAgent.",
    "categories": [
      "cs.CL"
    ],
    "published": "2024-06-17T07:30:05+00:00",
    "updated": "2024-06-17T07:30:05+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1608.00859v1": {
    "id": "http://arxiv.org/abs/1608.00859v1",
    "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
    "authors": [
      "Limin Wang",
      "Yuanjun Xiong",
      "Zhe Wang",
      "Yu Qiao",
      "Dahua Lin",
      "Xiaoou Tang",
      "Luc Van Gool"
    ],
    "abstract": "Deep convolutional networks have achieved great success for visual\nrecognition in still images. However, for action recognition in videos, the\nadvantage over traditional methods is not so evident. This paper aims to\ndiscover the principles to design effective ConvNet architectures for action\nrecognition in videos and learn these models given limited training samples.\nOur first contribution is temporal segment network (TSN), a novel framework for\nvideo-based action recognition. which is based on the idea of long-range\ntemporal structure modeling. It combines a sparse temporal sampling strategy\nand video-level supervision to enable efficient and effective learning using\nthe whole action video. The other contribution is our study on a series of good\npractices in learning ConvNets on video data with the help of temporal segment\nnetwork. Our approach obtains the state-the-of-art performance on the datasets\nof HMDB51 ( $ 69.4\\% $) and UCF101 ($ 94.2\\% $). We also visualize the learned\nConvNet models, which qualitatively demonstrates the effectiveness of temporal\nsegment network and the proposed good practices.",
    "categories": [
      "cs.CV"
    ],
    "published": "2016-08-02T15:06:50+00:00",
    "updated": "2016-08-02T15:06:50+00:00",
    "doi": null,
    "comment": "Accepted by ECCV 2016. Based on this method, we won the ActivityNet\n  challenge 2016 in untrimmed video classification",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1804.01552v1": {
    "id": "http://arxiv.org/abs/1804.01552v1",
    "title": "Self-supervised Learning of Geometrically Stable Features Through Probabilistic Introspection",
    "authors": [
      "David Novotny",
      "Samuel Albanie",
      "Diane Larlus",
      "Andrea Vedaldi"
    ],
    "abstract": "Self-supervision can dramatically cut back the amount of manually-labelled\ndata required to train deep neural networks. While self-supervision has usually\nbeen considered for tasks such as image classification, in this paper we aim at\nextending it to geometry-oriented tasks such as semantic matching and part\ndetection. We do so by building on several recent ideas in unsupervised\nlandmark detection. Our approach learns dense distinctive visual descriptors\nfrom an unlabelled dataset of images using synthetic image transformations. It\ndoes so by means of a robust probabilistic formulation that can introspectively\ndetermine which image regions are likely to result in stable image matching. We\nshow empirically that a network pre-trained in this manner requires\nsignificantly less supervision to learn semantic object parts compared to\nnumerous pre-training alternatives. We also show that the pre-trained\nrepresentation is excellent for semantic object matching.",
    "categories": [
      "cs.CV"
    ],
    "published": "2018-04-04T18:15:17+00:00",
    "updated": "2018-04-04T18:15:17+00:00",
    "doi": null,
    "comment": "In 2018 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR 2018)",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2203.05132v1": {
    "id": "http://arxiv.org/abs/2203.05132v1",
    "title": "Compilable Neural Code Generation with Compiler Feedback",
    "authors": [
      "Xin Wang",
      "Yasheng Wang",
      "Yao Wan",
      "Fei Mi",
      "Yitong Li",
      "Pingyi Zhou",
      "Jin Liu",
      "Hao Wu",
      "Xin Jiang",
      "Qun Liu"
    ],
    "abstract": "Automatically generating compilable programs with (or without) natural\nlanguage descriptions has always been a touchstone problem for computational\nlinguistics and automated software engineering. Existing deep-learning\napproaches model code generation as text generation, either constrained by\ngrammar structures in decoder, or driven by pre-trained language models on\nlarge-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of\nthem account for compilability of the generated programs. To improve\ncompilability of the generated programs, this paper proposes COMPCODER, a\nthree-stage pipeline utilizing compiler feedback for compilable code\ngeneration, including language model fine-tuning, compilability reinforcement,\nand compilability discrimination. Comprehensive experiments on two code\ngeneration tasks demonstrate the effectiveness of our proposed approach,\nimproving the success rate of compilation from 44.18 to 89.18 in code\ncompletion on average and from 70.3 to 96.2 in text-to-code generation,\nrespectively, when comparing with the state-of-the-art CodeGPT.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PL"
    ],
    "published": "2022-03-10T03:15:17+00:00",
    "updated": "2022-03-10T03:15:17+00:00",
    "doi": null,
    "comment": "Accepted by ACL 2022",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1801.04883v1": {
    "id": "http://arxiv.org/abs/1801.04883v1",
    "title": "Unsupervised Cipher Cracking Using Discrete GANs",
    "authors": [
      "Aidan N. Gomez",
      "Sicong Huang",
      "Ivan Zhang",
      "Bryan M. Li",
      "Muhammad Osama",
      "Lukasz Kaiser"
    ],
    "abstract": "This work details CipherGAN, an architecture inspired by CycleGAN used for\ninferring the underlying cipher mapping given banks of unpaired ciphertext and\nplaintext. We demonstrate that CipherGAN is capable of cracking language data\nenciphered using shift and Vigenere ciphers to a high degree of fidelity and\nfor vocabularies much larger than previously achieved. We present how CycleGAN\ncan be made compatible with discrete data and train in a stable way. We then\nprove that the technique used in CipherGAN avoids the common problem of\nuninformative discrimination associated with GANs applied to discrete data.",
    "categories": [
      "cs.LG"
    ],
    "published": "2018-01-15T17:32:04+00:00",
    "updated": "2018-01-15T17:32:04+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2310.01352v4": {
    "id": "http://arxiv.org/abs/2310.01352v4",
    "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning",
    "authors": [
      "Xi Victoria Lin",
      "Xilun Chen",
      "Mingda Chen",
      "Weijia Shi",
      "Maria Lomeli",
      "Rich James",
      "Pedro Rodriguez",
      "Jacob Kahn",
      "Gergely Szilvasy",
      "Mike Lewis",
      "Luke Zettlemoyer",
      "Scott Yih"
    ],
    "abstract": "Retrieval-augmented language models (RALMs) improve performance by accessing\nlong-tail and up-to-date knowledge from external data stores, but are\nchallenging to build. Existing approaches require either expensive\nretrieval-specific modifications to LM pre-training or use post-hoc integration\nof the data store that leads to suboptimal performance. We introduce\nRetrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning\nmethodology that provides a third option by retrofitting any LLM with retrieval\ncapabilities. Our approach operates in two distinct fine-tuning steps: (1) one\nupdates a pre-trained LM to better use retrieved information, while (2) the\nother updates the retriever to return more relevant results, as preferred by\nthe LM. By fine-tuning over tasks that require both knowledge utilization and\ncontextual awareness, we demonstrate that each stage yields significant\nperformance improvements, and using both leads to additional gains. Our best\nmodel, RA-DIT 65B, achieves state-of-the-art performance across a range of\nknowledge-intensive zero- and few-shot learning benchmarks, significantly\noutperforming existing in-context RALM approaches by up to +8.9% in 0-shot\nsetting and +1.4% in 5-shot setting on average.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-02T17:16:26+00:00",
    "updated": "2024-05-06T07:50:35+00:00",
    "doi": null,
    "comment": "v4: ICLR 2024 camera-ready version",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2305.17398v1": {
    "id": "http://arxiv.org/abs/2305.17398v1",
    "title": "NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects from Multiview Images",
    "authors": [
      "Yuan Liu",
      "Peng Wang",
      "Cheng Lin",
      "Xiaoxiao Long",
      "Jiepeng Wang",
      "Lingjie Liu",
      "Taku Komura",
      "Wenping Wang"
    ],
    "abstract": "We present a neural rendering-based method called NeRO for reconstructing the\ngeometry and the BRDF of reflective objects from multiview images captured in\nan unknown environment. Multiview reconstruction of reflective objects is\nextremely challenging because specular reflections are view-dependent and thus\nviolate the multiview consistency, which is the cornerstone for most multiview\nreconstruction methods. Recent neural rendering techniques can model the\ninteraction between environment lights and the object surfaces to fit the\nview-dependent reflections, thus making it possible to reconstruct reflective\nobjects from multiview images. However, accurately modeling environment lights\nin the neural rendering is intractable, especially when the geometry is\nunknown. Most existing neural rendering methods, which can model environment\nlights, only consider direct lights and rely on object masks to reconstruct\nobjects with weak specular reflections. Therefore, these methods fail to\nreconstruct reflective objects, especially when the object mask is not\navailable and the object is illuminated by indirect lights. We propose a\ntwo-step approach to tackle this problem. First, by applying the split-sum\napproximation and the integrated directional encoding to approximate the\nshading effects of both direct and indirect lights, we are able to accurately\nreconstruct the geometry of reflective objects without any object masks. Then,\nwith the object geometry fixed, we use more accurate sampling to recover the\nenvironment lights and the BRDF of the object. Extensive experiments\ndemonstrate that our method is capable of accurately reconstructing the\ngeometry and the BRDF of reflective objects from only posed RGB images without\nknowing the environment lights and the object masks. Codes and datasets are\navailable at https://github.com/liuyuan-pal/NeRO.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2023-05-27T07:40:07+00:00",
    "updated": "2023-05-27T07:40:07+00:00",
    "doi": null,
    "comment": "Accepted to SIGGRAPH 2023. Project page:\n  https://liuyuan-pal.github.io/NeRO/ Codes:\n  https://github.com/liuyuan-pal/NeRO",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2201.05047v4": {
    "id": "http://arxiv.org/abs/2201.05047v4",
    "title": "TransVOD: End-to-End Video Object Detection with Spatial-Temporal Transformers",
    "authors": [
      "Qianyu Zhou",
      "Xiangtai Li",
      "Lu He",
      "Yibo Yang",
      "Guangliang Cheng",
      "Yunhai Tong",
      "Lizhuang Ma",
      "Dacheng Tao"
    ],
    "abstract": "Detection Transformer (DETR) and Deformable DETR have been proposed to\neliminate the need for many hand-designed components in object detection while\ndemonstrating good performance as previous complex hand-crafted detectors.\nHowever, their performance on Video Object Detection (VOD) has not been well\nexplored. In this paper, we present TransVOD, the first end-to-end video object\ndetection system based on spatial-temporal Transformer architectures. The first\ngoal of this paper is to streamline the pipeline of VOD, effectively removing\nthe need for many hand-crafted components for feature aggregation, e.g.,\noptical flow model, relation networks. Besides, benefited from the object query\ndesign in DETR, our method does not need complicated post-processing methods\nsuch as Seq-NMS. In particular, we present a temporal Transformer to aggregate\nboth the spatial object queries and the feature memories of each frame. Our\ntemporal transformer consists of two components: Temporal Query Encoder (TQE)\nto fuse object queries, and Temporal Deformable Transformer Decoder (TDTD) to\nobtain current frame detection results. These designs boost the strong baseline\ndeformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID\ndataset. Then, we present two improved versions of TransVOD including\nTransVOD++ and TransVOD Lite. The former fuses object-level information into\nobject query via dynamic convolution while the latter models the entire video\nclips as the output to speed up the inference time. We give detailed analysis\nof all three models in the experiment part. In particular, our proposed\nTransVOD++ sets a new state-of-the-art record in terms of accuracy on ImageNet\nVID with 90.0% mAP. Our proposed TransVOD Lite also achieves the best speed and\naccuracy trade-off with 83.7% mAP while running at around 30 FPS on a single\nV100 GPU device.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-01-13T16:17:34+00:00",
    "updated": "2022-11-22T06:07:22+00:00",
    "doi": "10.1109/TPAMI.2022.3223955",
    "comment": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (IEEE TPAMI), extended version of arXiv:2105.10920",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2111.07868v1": {
    "id": "http://arxiv.org/abs/2111.07868v1",
    "title": "Tracking People with 3D Representations",
    "authors": [
      "Jathushan Rajasegaran",
      "Georgios Pavlakos",
      "Angjoo Kanazawa",
      "Jitendra Malik"
    ],
    "abstract": "We present a novel approach for tracking multiple people in video. Unlike\npast approaches which employ 2D representations, we focus on using 3D\nrepresentations of people, located in three-dimensional space. To this end, we\ndevelop a method, Human Mesh and Appearance Recovery (HMAR) which in addition\nto extracting the 3D geometry of the person as a SMPL mesh, also extracts\nappearance as a texture map on the triangles of the mesh. This serves as a 3D\nrepresentation for appearance that is robust to viewpoint and pose changes.\nGiven a video clip, we first detect bounding boxes corresponding to people, and\nfor each one, we extract 3D appearance, pose, and location information using\nHMAR. These embedding vectors are then sent to a transformer, which performs\nspatio-temporal aggregation of the representations over the duration of the\nsequence. The similarity of the resulting representations is used to solve for\nassociations that assigns each person to a tracklet. We evaluate our approach\non the Posetrack, MuPoTs and AVA datasets. We find that 3D representations are\nmore effective than 2D representations for tracking in these settings, and we\nobtain state-of-the-art performance. Code and results are available at:\nhttps://brjathu.github.io/T3DP.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-11-15T16:15:21+00:00",
    "updated": "2021-11-15T16:15:21+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1809.04556v6": {
    "id": "http://arxiv.org/abs/1809.04556v6",
    "title": "Unsupervised Controllable Text Formalization",
    "authors": [
      "Parag Jain",
      "Abhijit Mishra",
      "Amar Prakash Azad",
      "Karthik Sankaranarayanan"
    ],
    "abstract": "We propose a novel framework for controllable natural language\ntransformation. Realizing that the requirement of parallel corpus is\npractically unsustainable for controllable generation tasks, an unsupervised\ntraining scheme is introduced. The crux of the framework is a deep neural\nencoder-decoder that is reinforced with text-transformation knowledge through\nauxiliary modules (called scorers). The scorers, based on off-the-shelf\nlanguage processing tools, decide the learning scheme of the encoder-decoder\nbased on its actions. We apply this framework for the text-transformation task\nof formalizing an input text by improving its readability grade; the degree of\nrequired formalization can be controlled by the user at run-time. Experiments\non public datasets demonstrate the efficacy of our model towards: (a)\ntransforming a given text to a more formal style, and (b) introducing\nappropriate amount of formalness in the output text pertaining to the input\ncontrol. Our code and datasets are released for academic use.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2018-09-10T17:25:46+00:00",
    "updated": "2019-02-20T15:33:03+00:00",
    "doi": null,
    "comment": "AAAI",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2405.07883v1": {
    "id": "http://arxiv.org/abs/2405.07883v1",
    "title": "Zero-Shot Tokenizer Transfer",
    "authors": [
      "Benjamin Minixhofer",
      "Edoardo Maria Ponti",
      "Ivan Vuli\u0107"
    ],
    "abstract": "Language models (LMs) are bound to their tokenizer, which maps raw text to a\nsequence of vocabulary items (tokens). This restricts their flexibility: for\nexample, LMs trained primarily on English may still perform well in other\nnatural and programming languages, but have vastly decreased efficiency due to\ntheir English-centric tokenizer. To mitigate this, we should be able to swap\nthe original LM tokenizer with an arbitrary one, on the fly, without degrading\nperformance. Hence, in this work we define a new problem: Zero-Shot Tokenizer\nTransfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for\nthe tokens in the vocabulary of the new tokenizer. Since prior heuristics for\ninitializing embeddings often perform at chance level in a ZeTT setting, we\npropose a new solution: we train a hypernetwork taking a tokenizer as input and\npredicting the corresponding embeddings. We empirically demonstrate that the\nhypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and\ndecoder LLMs (e.g., Mistral-7B). Our method comes close to the original models'\nperformance in cross-lingual and coding tasks while markedly reducing the\nlength of the tokenized sequence. We also find that the remaining gap can be\nquickly closed by continued training on less than 1B tokens. Finally, we show\nthat a ZeTT hypernetwork trained for a base (L)LM can also be applied to\nfine-tuned variants without extra training. Overall, our results make\nsubstantial strides toward detaching LMs from their tokenizer.",
    "categories": [
      "cs.CL"
    ],
    "published": "2024-05-13T16:17:10+00:00",
    "updated": "2024-05-13T16:17:10+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2402.05699v3": {
    "id": "http://arxiv.org/abs/2402.05699v3",
    "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation",
    "authors": [
      "Xianghe Pang",
      "Shuo Tang",
      "Rui Ye",
      "Yuxin Xiong",
      "Bolun Zhang",
      "Yanfeng Wang",
      "Siheng Chen"
    ],
    "abstract": "Aligning large language models (LLMs) with human values is imperative to\nmitigate potential adverse effects resulting from their misuse. Drawing from\nthe sociological insight that acknowledging all parties' concerns is a key\nfactor in shaping human values, this paper proposes a novel direction to align\nLLMs by themselves: social scene simulation. To achieve this, we present\nMATRIX, a novel social scene simulator that emulates realistic scenes around a\nuser's input query, enabling the LLM to take social consequences into account\nbefore responding. MATRIX serves as a virtual rehearsal space, akin to a\nMonopolylogue, where the LLM performs diverse roles related to the query and\npractice by itself. To inject this alignment, we fine-tune the LLM with\nMATRIX-simulated data, ensuring adherence to human values without compromising\ninference speed. We theoretically show that the LLM with MATRIX outperforms\nConstitutional AI under mild assumptions. Finally, extensive experiments\nvalidate that our method outperforms over 10 baselines across 4 benchmarks. As\nevidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning\nwith human values. See our project page at\nhttps://shuotang123.github.io/MATRIX.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-02-08T14:21:03+00:00",
    "updated": "2024-06-08T06:13:55+00:00",
    "doi": null,
    "comment": "32 pages, 9 figures",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2407.16260v1": {
    "id": "http://arxiv.org/abs/2407.16260v1",
    "title": "DreamDissector: Learning Disentangled Text-to-3D Generation from 2D Diffusion Priors",
    "authors": [
      "Zizheng Yan",
      "Jiapeng Zhou",
      "Fanpeng Meng",
      "Yushuang Wu",
      "Lingteng Qiu",
      "Zisheng Ye",
      "Shuguang Cui",
      "Guanying Chen",
      "Xiaoguang Han"
    ],
    "abstract": "Text-to-3D generation has recently seen significant progress. To enhance its\npracticality in real-world applications, it is crucial to generate multiple\nindependent objects with interactions, similar to layer-compositing in 2D image\nediting. However, existing text-to-3D methods struggle with this task, as they\nare designed to generate either non-independent objects or independent objects\nlacking spatially plausible interactions. Addressing this, we propose\nDreamDissector, a text-to-3D method capable of generating multiple independent\nobjects with interactions. DreamDissector accepts a multi-object text-to-3D\nNeRF as input and produces independent textured meshes. To achieve this, we\nintroduce the Neural Category Field (NeCF) for disentangling the input NeRF.\nAdditionally, we present the Category Score Distillation Sampling (CSDS),\nfacilitated by a Deep Concept Mining (DCM) module, to tackle the concept gap\nissue in diffusion models. By leveraging NeCF and CSDS, we can effectively\nderive sub-NeRFs from the original scene. Further refinement enhances geometry\nand texture. Our experimental results validate the effectiveness of\nDreamDissector, providing users with novel means to control 3D synthesis at the\nobject level and potentially opening avenues for various creative applications\nin the future.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-07-23T07:59:57+00:00",
    "updated": "2024-07-23T07:59:57+00:00",
    "doi": null,
    "comment": "ECCV 2024. Project page: https://chester256.github.io/dreamdissector",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1704.02227v1": {
    "id": "http://arxiv.org/abs/1704.02227v1",
    "title": "Training Triplet Networks with GAN",
    "authors": [
      "Maciej Zieba",
      "Lei Wang"
    ],
    "abstract": "Triplet networks are widely used models that are characterized by good\nperformance in classification and retrieval tasks. In this work we propose to\ntrain a triplet network by putting it as the discriminator in Generative\nAdversarial Nets (GANs). We make use of the good capability of representation\nlearning of the discriminator to increase the predictive quality of the model.\nWe evaluated our approach on Cifar10 and MNIST datasets and observed\nsignificant improvement on the classification performance using the simple k-nn\nmethod.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2017-04-06T17:09:20+00:00",
    "updated": "2017-04-06T17:09:20+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2401.01173v1": {
    "id": "http://arxiv.org/abs/2401.01173v1",
    "title": "En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data",
    "authors": [
      "Yifang Men",
      "Biwen Lei",
      "Yuan Yao",
      "Miaomiao Cui",
      "Zhouhui Lian",
      "Xuansong Xie"
    ],
    "abstract": "We present En3D, an enhanced generative scheme for sculpting high-quality 3D\nhuman avatars. Unlike previous works that rely on scarce 3D datasets or limited\n2D collections with imbalanced viewing angles and imprecise pose priors, our\napproach aims to develop a zero-shot 3D generative scheme capable of producing\nvisually realistic, geometrically accurate and content-wise diverse 3D humans\nwithout relying on pre-existing 3D or 2D assets. To address this challenge, we\nintroduce a meticulously crafted workflow that implements accurate physical\nmodeling to learn the enhanced 3D generative model from synthetic 2D data.\nDuring inference, we integrate optimization modules to bridge the gap between\nrealistic appearances and coarse 3D shapes. Specifically, En3D comprises three\nmodules: a 3D generator that accurately models generalizable 3D humans with\nrealistic appearance from synthesized balanced, diverse, and structured human\nimages; a geometry sculptor that enhances shape quality using multi-view normal\nconstraints for intricate human anatomy; and a texturing module that\ndisentangles explicit texture maps with fidelity and editability, leveraging\nsemantical UV partitioning and a differentiable rasterizer. Experimental\nresults show that our approach significantly outperforms prior works in terms\nof image quality, geometry accuracy and content diversity. We also showcase the\napplicability of our generated avatars for animation and editing, as well as\nthe scalability of our approach for content-style free adaptation.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-01-02T12:06:31+00:00",
    "updated": "2024-01-02T12:06:31+00:00",
    "doi": null,
    "comment": "Project Page: https://menyifang.github.io/projects/En3D/index.html",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.06031v2": {
    "id": "http://arxiv.org/abs/2210.06031v2",
    "title": "Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning",
    "authors": [
      "Yuchong Sun",
      "Hongwei Xue",
      "Ruihua Song",
      "Bei Liu",
      "Huan Yang",
      "Jianlong Fu"
    ],
    "abstract": "Large-scale video-language pre-training has shown significant improvement in\nvideo-language understanding tasks. Previous studies of video-language\npretraining mainly focus on short-form videos (i.e., within 30 seconds) and\nsentences, leaving long-form video-language pre-training rarely explored.\nDirectly learning representation from long-form videos and language may benefit\nmany long-form video-language understanding tasks. However, it is challenging\ndue to the difficulty of modeling long-range relationships and the heavy\ncomputational burden caused by more frames. In this paper, we introduce a\nLong-Form VIdeo-LAnguage pre-training model (LF-VILA) and train it on a\nlarge-scale long-form video and paragraph dataset constructed from an existing\npublic dataset. To effectively capture the rich temporal dynamics and to better\nalign video and language in an efficient end-to-end manner, we introduce two\nnovel designs in our LF-VILA model. We first propose a Multimodal Temporal\nContrastive (MTC) loss to learn the temporal relation across different\nmodalities by encouraging fine-grained alignment between long-form videos and\nparagraphs. Second, we propose a Hierarchical Temporal Window Attention (HTWA)\nmechanism to effectively capture long-range dependency while reducing\ncomputational cost in Transformer. We fine-tune the pre-trained LF-VILA model\non seven downstream long-form video-language understanding tasks of\nparagraph-to-video retrieval and long-form video question-answering, and\nachieve new state-of-the-art performances. Specifically, our model achieves\n16.1% relative improvement on ActivityNet paragraph-to-video retrieval task and\n2.4% on How2QA task, respectively. We release our code, dataset, and\npre-trained models at https://github.com/microsoft/XPretrain.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-10-12T09:08:27+00:00",
    "updated": "2023-03-02T09:05:43+00:00",
    "doi": null,
    "comment": "Accepted by NeurIPS 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.15430v1": {
    "id": "http://arxiv.org/abs/2312.15430v1",
    "title": "Make-A-Character: High Quality Text-to-3D Character Generation within Minutes",
    "authors": [
      "Jianqiang Ren",
      "Chao He",
      "Lin Liu",
      "Jiahao Chen",
      "Yutong Wang",
      "Yafei Song",
      "Jianfang Li",
      "Tangli Xue",
      "Siqi Hu",
      "Tao Chen",
      "Kunkun Zheng",
      "Jianjing Xiang",
      "Liefeng Bo"
    ],
    "abstract": "There is a growing demand for customized and expressive 3D characters with\nthe emergence of AI agents and Metaverse, but creating 3D characters using\ntraditional computer graphics tools is a complex and time-consuming task. To\naddress these challenges, we propose a user-friendly framework named\nMake-A-Character (Mach) to create lifelike 3D avatars from text descriptions.\nThe framework leverages the power of large language and vision models for\ntextual intention understanding and intermediate image generation, followed by\na series of human-oriented visual perception and 3D generation modules. Our\nsystem offers an intuitive approach for users to craft controllable, realistic,\nfully-realized 3D characters that meet their expectations within 2 minutes,\nwhile also enabling easy integration with existing CG pipeline for dynamic\nexpressiveness. For more information, please visit the project page at\nhttps://human3daigc.github.io/MACH/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-24T08:11:39+00:00",
    "updated": "2023-12-24T08:11:39+00:00",
    "doi": null,
    "comment": "Technical Report",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2303.08340v3": {
    "id": "http://arxiv.org/abs/2303.08340v3",
    "title": "VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation",
    "authors": [
      "Xiaoyu Shi",
      "Zhaoyang Huang",
      "Weikang Bian",
      "Dasong Li",
      "Manyuan Zhang",
      "Ka Chun Cheung",
      "Simon See",
      "Hongwei Qin",
      "Jifeng Dai",
      "Hongsheng Li"
    ],
    "abstract": "We introduce VideoFlow, a novel optical flow estimation framework for videos.\nIn contrast to previous methods that learn to estimate optical flow from two\nframes, VideoFlow concurrently estimates bi-directional optical flows for\nmultiple frames that are available in videos by sufficiently exploiting\ntemporal cues. We first propose a TRi-frame Optical Flow (TROF) module that\nestimates bi-directional optical flows for the center frame in a three-frame\nmanner. The information of the frame triplet is iteratively fused onto the\ncenter frame. To extend TROF for handling more frames, we further propose a\nMOtion Propagation (MOP) module that bridges multiple TROFs and propagates\nmotion features between adjacent TROFs. With the iterative flow estimation\nrefinement, the information fused in individual TROFs can be propagated into\nthe whole sequence via MOP. By effectively exploiting video information,\nVideoFlow presents extraordinary performance, ranking 1st on all public\nbenchmarks. On the Sintel benchmark, VideoFlow achieves 1.649 and 0.991 average\nend-point-error (AEPE) on the final and clean passes, a 15.1% and 7.6% error\nreduction from the best-published results (1.943 and 1.073 from FlowFormer++).\nOn the KITTI-2015 benchmark, VideoFlow achieves an F1-all error of 3.65%, a\n19.2% error reduction from the best-published result (4.52% from FlowFormer++).\nCode is released at \\url{https://github.com/XiaoyuShi97/VideoFlow}.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-03-15T03:14:30+00:00",
    "updated": "2023-08-20T15:14:34+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2302.06144v4": {
    "id": "http://arxiv.org/abs/2302.06144v4",
    "title": "SkCoder: A Sketch-based Approach for Automatic Code Generation",
    "authors": [
      "Jia Li",
      "Yongmin Li",
      "Ge Li",
      "Zhi Jin",
      "Yiyang Hao",
      "Xing Hu"
    ],
    "abstract": "Recently, deep learning techniques have shown great success in automatic code\ngeneration. Inspired by the code reuse, some researchers propose copy-based\napproaches that can copy the content from similar code snippets to obtain\nbetter performance. Practically, human developers recognize the content in the\nsimilar code that is relevant to their needs, which can be viewed as a code\nsketch. The sketch is further edited to the desired code. However, existing\ncopy-based approaches ignore the code sketches and tend to repeat the similar\ncode without necessary modifications, which leads to generating wrong results.\n  In this paper, we propose a sketch-based code generation approach named\nSkCoder to mimic developers' code reuse behavior. Given a natural language\nrequirement, SkCoder retrieves a similar code snippet, extracts relevant parts\nas a code sketch, and edits the sketch into the desired code. Our motivations\nare that the extracted sketch provides a well-formed pattern for telling models\n\"how to write\". The post-editing further adds requirement-specific details to\nthe sketch and outputs the complete code. We conduct experiments on two public\ndatasets and a new dataset collected by this work. We compare our approach to\n20 baselines using 5 widely used metrics. Experimental results show that (1)\nSkCoder can generate more correct programs, and outperforms the\nstate-of-the-art - CodeT5-base by 30.30%, 35.39%, and 29.62% on three datasets.\n(2) Our approach is effective to multiple code generation models and improves\nthem by up to 120.1% in Pass@1. (3) We investigate three plausible code\nsketches and discuss the importance of sketches. (4) We manually evaluate the\ngenerated code and prove the superiority of our SkCoder in three aspects.",
    "categories": [
      "cs.SE"
    ],
    "published": "2023-02-13T07:05:39+00:00",
    "updated": "2023-09-07T11:26:46+00:00",
    "doi": null,
    "comment": "Accepted by the 45th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2023)",
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "2310.06320v1": {
    "id": "http://arxiv.org/abs/2310.06320v1",
    "title": "Automatic Generation of Test Cases based on Bug Reports: a Feasibility Study with Large Language Models",
    "authors": [
      "Laura Plein",
      "Wendk\u00fbuni C. Ou\u00e9draogo",
      "Jacques Klein",
      "Tegawend\u00e9 F. Bissyand\u00e9"
    ],
    "abstract": "Software testing is a core discipline in software engineering where a large\narray of research results has been produced, notably in the area of automatic\ntest generation. Because existing approaches produce test cases that either can\nbe qualified as simple (e.g. unit tests) or that require precise\nspecifications, most testing procedures still rely on test cases written by\nhumans to form test suites. Such test suites, however, are incomplete: they\nonly cover parts of the project or they are produced after the bug is fixed.\nYet, several research challenges, such as automatic program repair, and\npractitioner processes, build on the assumption that available test suites are\nsufficient. There is thus a need to break existing barriers in automatic test\ncase generation. While prior work largely focused on random unit testing\ninputs, we propose to consider generating test cases that realistically\nrepresent complex user execution scenarios, which reveal buggy behaviour. Such\nscenarios are informally described in bug reports, which should therefore be\nconsidered as natural inputs for specifying bug-triggering test cases. In this\nwork, we investigate the feasibility of performing this generation by\nleveraging large language models (LLMs) and using bug reports as inputs. Our\nexperiments include the use of ChatGPT, as an online service, as well as\nCodeGPT, a code-related pre-trained LLM that was fine-tuned for our task.\nOverall, we experimentally show that bug reports associated to up to 50% of\nDefects4J bugs can prompt ChatGPT to generate an executable test case. We show\nthat even new bug reports can indeed be used as input for generating executable\ntest cases. Finally, we report experimental results which confirm that\nLLM-generated test cases are immediately useful in software engineering tasks\nsuch as fault localization as well as patch validation in automated program\nrepair.",
    "categories": [
      "cs.SE"
    ],
    "published": "2023-10-10T05:30:12+00:00",
    "updated": "2023-10-10T05:30:12+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "1906.04733v2": {
    "id": "http://arxiv.org/abs/1906.04733v2",
    "title": "DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections",
    "authors": [
      "Ofir Nachum",
      "Yinlam Chow",
      "Bo Dai",
      "Lihong Li"
    ],
    "abstract": "In many real-world reinforcement learning applications, access to the\nenvironment is limited to a fixed dataset, instead of direct (online)\ninteraction with the environment. When using this data for either evaluation or\ntraining of a new policy, accurate estimates of discounted stationary\ndistribution ratios -- correction terms which quantify the likelihood that the\nnew policy will experience a certain state-action pair normalized by the\nprobability with which the state-action pair appears in the dataset -- can\nimprove accuracy and performance. In this work, we propose an algorithm,\nDualDICE, for estimating these quantities. In contrast to previous approaches,\nour algorithm is agnostic to knowledge of the behavior policy (or policies)\nused to generate the dataset. Furthermore, it eschews any direct use of\nimportance weights, thus avoiding potential optimization instabilities endemic\nof previous methods. In addition to providing theoretical guarantees, we\npresent an empirical study of our algorithm applied to off-policy policy\nevaluation and find that our algorithm significantly improves accuracy compared\nto existing techniques.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2019-06-10T19:33:00+00:00",
    "updated": "2019-11-04T22:01:49+00:00",
    "doi": null,
    "comment": "Appearing in NeurIPS 2019 Vancouver, Canada",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2208.08932v1": {
    "id": "http://arxiv.org/abs/2208.08932v1",
    "title": "ManiFlow: Implicitly Representing Manifolds with Normalizing Flows",
    "authors": [
      "Janis Postels",
      "Martin Danelljan",
      "Luc Van Gool",
      "Federico Tombari"
    ],
    "abstract": "Normalizing Flows (NFs) are flexible explicit generative models that have\nbeen shown to accurately model complex real-world data distributions. However,\ntheir invertibility constraint imposes limitations on data distributions that\nreside on lower dimensional manifolds embedded in higher dimensional space.\nPractically, this shortcoming is often bypassed by adding noise to the data\nwhich impacts the quality of the generated samples. In contrast to prior work,\nwe approach this problem by generating samples from the original data\ndistribution given full knowledge about the perturbed distribution and the\nnoise model. To this end, we establish that NFs trained on perturbed data\nimplicitly represent the manifold in regions of maximum likelihood. Then, we\npropose an optimization objective that recovers the most likely point on the\nmanifold given a sample from the perturbed distribution. Finally, we focus on\n3D point clouds for which we utilize the explicit nature of NFs, i.e. surface\nnormals extracted from the gradient of the log-likelihood and the\nlog-likelihood itself, to apply Poisson surface reconstruction to refine\ngenerated point sets.",
    "categories": [
      "cs.CV",
      "stat.ML"
    ],
    "published": "2022-08-18T16:07:59+00:00",
    "updated": "2022-08-18T16:07:59+00:00",
    "doi": null,
    "comment": "International Conference on 3D Vision 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2111.08413v1": {
    "id": "http://arxiv.org/abs/2111.08413v1",
    "title": "Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding",
    "authors": [
      "Bum Jun Kim",
      "Hyeyeon Choi",
      "Hyeonah Jang",
      "Dong Gu Lee",
      "Wonseok Jeong",
      "Sang Woo Kim"
    ],
    "abstract": "Vision transformers (ViTs) have recently demonstrated state-of-the-art\nperformance in a variety of vision tasks, replacing convolutional neural\nnetworks (CNNs). Meanwhile, since ViT has a different architecture than CNN, it\nmay behave differently. To investigate the reliability of ViT, this paper\nstudies the behavior and robustness of ViT. We compared the robustness of CNN\nand ViT by assuming various image corruptions that may appear in practical\nvision tasks. We confirmed that for most image transformations, ViT showed\nrobustness comparable to CNN or more improved. However, for contrast\nenhancement, severe performance degradations were consistently observed in ViT.\nFrom a detailed analysis, we identified a potential problem: positional\nembedding in ViT's patch embedding could work improperly when the color scale\nchanges. Here we claim the use of PreLayerNorm, a modified patch embedding\nstructure to ensure scale-invariant behavior of ViT. ViT with PreLayerNorm\nshowed improved robustness in various corruptions including contrast-varying\nenvironments.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-11-16T12:32:03+00:00",
    "updated": "2021-11-16T12:32:03+00:00",
    "doi": null,
    "comment": "7 pages, 8 figures. Work in Progress",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2212.01558v2": {
    "id": "http://arxiv.org/abs/2212.01558v2",
    "title": "PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models",
    "authors": [
      "Minghua Liu",
      "Yinhao Zhu",
      "Hong Cai",
      "Shizhong Han",
      "Zhan Ling",
      "Fatih Porikli",
      "Hao Su"
    ],
    "abstract": "Generalizable 3D part segmentation is important but challenging in vision and\nrobotics. Training deep models via conventional supervised methods requires\nlarge-scale 3D datasets with fine-grained part annotations, which are costly to\ncollect. This paper explores an alternative way for low-shot part segmentation\nof 3D point clouds by leveraging a pretrained image-language model, GLIP, which\nachieves superior performance on open-vocabulary 2D detection. We transfer the\nrich knowledge from 2D to 3D through GLIP-based part detection on point cloud\nrendering and a novel 2D-to-3D label lifting algorithm. We also utilize\nmulti-view 3D priors and few-shot prompt tuning to boost performance\nsignificantly. Extensive evaluation on PartNet and PartNet-Mobility datasets\nshows that our method enables excellent zero-shot 3D part segmentation. Our\nfew-shot version not only outperforms existing few-shot approaches by a large\nmargin but also achieves highly competitive results compared to the fully\nsupervised counterpart. Furthermore, we demonstrate that our method can be\ndirectly applied to iPhone-scanned point clouds without significant domain\ngaps.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2022-12-03T06:59:01+00:00",
    "updated": "2023-06-19T07:27:14+00:00",
    "doi": null,
    "comment": "CVPR 2023, project page: https://colin97.github.io/PartSLIP_page/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.17528v1": {
    "id": "http://arxiv.org/abs/2404.17528v1",
    "title": "Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields",
    "authors": [
      "Tianqi Liu",
      "Xinyi Ye",
      "Min Shi",
      "Zihao Huang",
      "Zhiyu Pan",
      "Zhan Peng",
      "Zhiguo Cao"
    ],
    "abstract": "Generalizable NeRF aims to synthesize novel views for unseen scenes. Common\npractices involve constructing variance-based cost volumes for geometry\nreconstruction and encoding 3D descriptors for decoding novel views. However,\nexisting methods show limited generalization ability in challenging conditions\ndue to inaccurate geometry, sub-optimal descriptors, and decoding strategies.\nWe address these issues point by point. First, we find the variance-based cost\nvolume exhibits failure patterns as the features of pixels corresponding to the\nsame point can be inconsistent across different views due to occlusions or\nreflections. We introduce an Adaptive Cost Aggregation (ACA) approach to\namplify the contribution of consistent pixel pairs and suppress inconsistent\nones. Unlike previous methods that solely fuse 2D features into descriptors,\nour approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D\ncontext into descriptors through spatial and inter-view interaction. When\ndecoding the descriptors, we observe the two existing decoding strategies excel\nin different areas, which are complementary. A Consistency-Aware Fusion (CAF)\nstrategy is proposed to leverage the advantages of both. We incorporate the\nabove ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware\nReconstruction and Fusion-refined Rendering (GeFu). GeFu attains\nstate-of-the-art performance across multiple datasets. Code is available at\nhttps://github.com/TQTQliu/GeFu .",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-26T16:46:28+00:00",
    "updated": "2024-04-26T16:46:28+00:00",
    "doi": null,
    "comment": "Accepted by CVPR 2024. Project page: https://gefucvpr24.github.io",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.11258v1": {
    "id": "http://arxiv.org/abs/2309.11258v1",
    "title": "TwinTex: Geometry-aware Texture Generation for Abstracted 3D Architectural Models",
    "authors": [
      "Weidan Xiong",
      "Hongqian Zhang",
      "Botao Peng",
      "Ziyu Hu",
      "Yongli Wu",
      "Jianwei Guo",
      "Hui Huang"
    ],
    "abstract": "Coarse architectural models are often generated at scales ranging from\nindividual buildings to scenes for downstream applications such as Digital Twin\nCity, Metaverse, LODs, etc. Such piece-wise planar models can be abstracted as\ntwins from 3D dense reconstructions. However, these models typically lack\nrealistic texture relative to the real building or scene, making them\nunsuitable for vivid display or direct reference. In this paper, we present\nTwinTex, the first automatic texture mapping framework to generate a\nphoto-realistic texture for a piece-wise planar proxy. Our method addresses\nmost challenges occurring in such twin texture generation. Specifically, for\neach primitive plane, we first select a small set of photos with greedy\nheuristics considering photometric quality, perspective quality and facade\ntexture completeness. Then, different levels of line features (LoLs) are\nextracted from the set of selected photos to generate guidance for later steps.\nWith LoLs, we employ optimization algorithms to align texture with geometry\nfrom local to global. Finally, we fine-tune a diffusion model with a multi-mask\ninitialization component and a new dataset to inpaint the missing region.\nExperimental results on many buildings, indoor scenes and man-made objects of\nvarying complexity demonstrate the generalization ability of our algorithm. Our\napproach surpasses state-of-the-art texture mapping methods in terms of\nhigh-fidelity quality and reaches a human-expert production level with much\nless effort. Project page: https://vcc.tech/research/2023/TwinTex.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "published": "2023-09-20T12:33:53+00:00",
    "updated": "2023-09-20T12:33:53+00:00",
    "doi": "10.1145/3618328",
    "comment": "Accepted to SIGGRAPH ASIA 2023",
    "journal_ref": null,
    "primary_category": "cs.GR"
  },
  "1910.02720v2": {
    "id": "http://arxiv.org/abs/1910.02720v2",
    "title": "Meta-Learning Deep Energy-Based Memory Models",
    "authors": [
      "Sergey Bartunov",
      "Jack W Rae",
      "Simon Osindero",
      "Timothy P Lillicrap"
    ],
    "abstract": "We study the problem of learning associative memory -- a system which is able\nto retrieve a remembered pattern based on its distorted or incomplete version.\nAttractor networks provide a sound model of associative memory: patterns are\nstored as attractors of the network dynamics and associative retrieval is\nperformed by running the dynamics starting from a query pattern until it\nconverges to an attractor. In such models the dynamics are often implemented as\nan optimization procedure that minimizes an energy function, such as in the\nclassical Hopfield network. In general it is difficult to derive a writing rule\nfor a given dynamics and energy that is both compressive and fast. Thus, most\nresearch in energy-based memory has been limited either to tractable energy\nmodels not expressive enough to handle complex high-dimensional objects such as\nnatural images, or to models that do not offer fast writing. We present a novel\nmeta-learning approach to energy-based memory models (EBMM) that allows one to\nuse an arbitrary neural architecture as an energy model and quickly store\npatterns in its weights. We demonstrate experimentally that our EBMM approach\ncan build compressed memories for synthetic and natural data, and is capable of\nassociative retrieval that outperforms existing memory systems in terms of the\nreconstruction error and compression rate.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2019-10-07T10:58:08+00:00",
    "updated": "2021-04-20T08:34:53+00:00",
    "doi": null,
    "comment": "ICLR 2020",
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2012.05328v2": {
    "id": "http://arxiv.org/abs/2012.05328v2",
    "title": "GAN \"Steerability\" without optimization",
    "authors": [
      "Nurit Spingarn-Eliezer",
      "Ron Banner",
      "Tomer Michaeli"
    ],
    "abstract": "Recent research has shown remarkable success in revealing \"steering\"\ndirections in the latent spaces of pre-trained GANs. These directions\ncorrespond to semantically meaningful image transformations e.g., shift, zoom,\ncolor manipulations), and have similar interpretable effects across all\ncategories that the GAN can generate. Some methods focus on user-specified\ntransformations, while others discover transformations in an unsupervised\nmanner. However, all existing techniques rely on an optimization procedure to\nexpose those directions, and offer no control over the degree of allowed\ninteraction between different transformations. In this paper, we show that\n\"steering\" trajectories can be computed in closed form directly from the\ngenerator's weights without any form of training or optimization. This applies\nto user-prescribed geometric transformations, as well as to unsupervised\ndiscovery of more complex effects. Our approach allows determining both linear\nand nonlinear trajectories, and has many advantages over previous methods. In\nparticular, we can control whether one transformation is allowed to come on the\nexpense of another (e.g. zoom-in with or without allowing translation to keep\nthe object centered). Moreover, we can determine the natural end-point of the\ntrajectory, which corresponds to the largest extent to which a transformation\ncan be applied without incurring degradation. Finally, we show how transferring\nattributes between images can be achieved without optimization, even across\ndifferent categories.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-12-09T21:34:34+00:00",
    "updated": "2021-01-24T16:50:39+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1705.02364v5": {
    "id": "http://arxiv.org/abs/1705.02364v5",
    "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
    "authors": [
      "Alexis Conneau",
      "Douwe Kiela",
      "Holger Schwenk",
      "Loic Barrault",
      "Antoine Bordes"
    ],
    "abstract": "Many modern NLP systems rely on word embeddings, previously trained in an\nunsupervised manner on large corpora, as base features. Efforts to obtain\nembeddings for larger chunks of text, such as sentences, have however not been\nso successful. Several attempts at learning unsupervised representations of\nsentences have not reached satisfactory enough performance to be widely\nadopted. In this paper, we show how universal sentence representations trained\nusing the supervised data of the Stanford Natural Language Inference datasets\ncan consistently outperform unsupervised methods like SkipThought vectors on a\nwide range of transfer tasks. Much like how computer vision uses ImageNet to\nobtain features, which can then be transferred to other tasks, our work tends\nto indicate the suitability of natural language inference for transfer learning\nto other NLP tasks. Our encoder is publicly available.",
    "categories": [
      "cs.CL"
    ],
    "published": "2017-05-05T18:54:39+00:00",
    "updated": "2018-07-08T21:22:11+00:00",
    "doi": null,
    "comment": "EMNLP 2017",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1603.08983v6": {
    "id": "http://arxiv.org/abs/1603.08983v6",
    "title": "Adaptive Computation Time for Recurrent Neural Networks",
    "authors": [
      "Alex Graves"
    ],
    "abstract": "This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.",
    "categories": [
      "cs.NE"
    ],
    "published": "2016-03-29T22:09:00+00:00",
    "updated": "2017-02-21T16:21:21+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.NE"
  },
  "2304.05684v3": {
    "id": "http://arxiv.org/abs/2304.05684v3",
    "title": "InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions",
    "authors": [
      "Han Liang",
      "Wenqian Zhang",
      "Wenxuan Li",
      "Jingyi Yu",
      "Lan Xu"
    ],
    "abstract": "We have recently seen tremendous progress in diffusion advances for\ngenerating realistic human motions. Yet, they largely disregard the multi-human\ninteractions. In this paper, we present InterGen, an effective diffusion-based\napproach that incorporates human-to-human interactions into the motion\ndiffusion process, which enables layman users to customize high-quality\ntwo-person interaction motions, with only text guidance. We first contribute a\nmultimodal dataset, named InterHuman. It consists of about 107M frames for\ndiverse two-person interactions, with accurate skeletal motions and 23,337\nnatural language descriptions. For the algorithm side, we carefully tailor the\nmotion diffusion model to our two-person interaction setting. To handle the\nsymmetry of human identities during interactions, we propose two cooperative\ntransformer-based denoisers that explicitly share weights, with a mutual\nattention mechanism to further connect the two denoising processes. Then, we\npropose a novel representation for motion input in our interaction diffusion\nmodel, which explicitly formulates the global relations between the two\nperformers in the world frame. We further introduce two novel regularization\nterms to encode spatial relations, equipped with a corresponding damping scheme\nduring the training of our interaction diffusion model. Extensive experiments\nvalidate the effectiveness and generalizability of InterGen. Notably, it can\ngenerate more diverse and compelling two-person motions than previous methods\nand enables various downstream applications for human interactions.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-04-12T08:12:29+00:00",
    "updated": "2024-03-28T03:15:57+00:00",
    "doi": "10.1007/s11263-024-02042-6",
    "comment": "accepted by IJCV 2024",
    "journal_ref": "Int J Comput Vis (2024) 1-21",
    "primary_category": "cs.CV"
  },
  "2306.06109v1": {
    "id": "http://arxiv.org/abs/2306.06109v1",
    "title": "Learning to Quantize Vulnerability Patterns and Match to Locate Statement-Level Vulnerabilities",
    "authors": [
      "Michael Fu",
      "Trung Le",
      "Van Nguyen",
      "Chakkrit Tantithamthavorn",
      "Dinh Phung"
    ],
    "abstract": "Deep learning (DL) models have become increasingly popular in identifying\nsoftware vulnerabilities. Prior studies found that vulnerabilities across\ndifferent vulnerable programs may exhibit similar vulnerable scopes, implicitly\nforming discernible vulnerability patterns that can be learned by DL models\nthrough supervised training. However, vulnerable scopes still manifest in\nvarious spatial locations and formats within a program, posing challenges for\nmodels to accurately identify vulnerable statements. Despite this challenge,\nstate-of-the-art vulnerability detection approaches fail to exploit the\nvulnerability patterns that arise in vulnerable programs. To take full\nadvantage of vulnerability patterns and unleash the ability of DL models, we\npropose a novel vulnerability-matching approach in this paper, drawing\ninspiration from program analysis tools that locate vulnerabilities based on\npre-defined patterns. Specifically, a vulnerability codebook is learned, which\nconsists of quantized vectors representing various vulnerability patterns.\nDuring inference, the codebook is iterated to match all learned patterns and\npredict the presence of potential vulnerabilities within a given program. Our\napproach was extensively evaluated on a real-world dataset comprising more than\n188,000 C/C++ functions. The evaluation results show that our approach achieves\nan F1-score of 94% (6% higher than the previous best) and 82% (19% higher than\nthe previous best) for function and statement-level vulnerability\nidentification, respectively. These substantial enhancements highlight the\neffectiveness of our approach to identifying vulnerabilities. The training code\nand pre-trained models are available at https://github.com/optimatch/optimatch.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-05-26T04:13:31+00:00",
    "updated": "2023-05-26T04:13:31+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CR"
  },
  "2210.07124v1": {
    "id": "http://arxiv.org/abs/2210.07124v1",
    "title": "RTFormer: Efficient Design for Real-Time Semantic Segmentation with Transformer",
    "authors": [
      "Jian Wang",
      "Chenhui Gou",
      "Qiman Wu",
      "Haocheng Feng",
      "Junyu Han",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "abstract": "Recently, transformer-based networks have shown impressive results in\nsemantic segmentation. Yet for real-time semantic segmentation, pure CNN-based\napproaches still dominate in this field, due to the time-consuming computation\nmechanism of transformer. We propose RTFormer, an efficient dual-resolution\ntransformer for real-time semantic segmenation, which achieves better trade-off\nbetween performance and efficiency than CNN-based models. To achieve high\ninference efficiency on GPU-like devices, our RTFormer leverages GPU-Friendly\nAttention with linear complexity and discards the multi-head mechanism.\nBesides, we find that cross-resolution attention is more efficient to gather\nglobal context information for high-resolution branch by spreading the high\nlevel knowledge learned from low-resolution branch. Extensive experiments on\nmainstream benchmarks demonstrate the effectiveness of our proposed RTFormer,\nit achieves state-of-the-art on Cityscapes, CamVid and COCOStuff, and shows\npromising results on ADE20K. Code is available at PaddleSeg:\nhttps://github.com/PaddlePaddle/PaddleSeg.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-10-13T16:03:53+00:00",
    "updated": "2022-10-13T16:03:53+00:00",
    "doi": null,
    "comment": "NeurIPS2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2105.13255v1": {
    "id": "http://arxiv.org/abs/2105.13255v1",
    "title": "Measuring Fine-Grained Domain Relevance of Terms: A Hierarchical Core-Fringe Approach",
    "authors": [
      "Jie Huang",
      "Kevin Chen-Chuan Chang",
      "Jinjun Xiong",
      "Wen-mei Hwu"
    ],
    "abstract": "We propose to measure fine-grained domain relevance - the degree that a term\nis relevant to a broad (e.g., computer science) or narrow (e.g., deep learning)\ndomain. Such measurement is crucial for many downstream tasks in natural\nlanguage processing. To handle long-tail terms, we build a core-anchored\nsemantic graph, which uses core terms with rich description information to\nbridge the vast remaining fringe terms semantically. To support a fine-grained\ndomain without relying on a matching corpus for supervision, we develop\nhierarchical core-fringe learning, which learns core and fringe terms jointly\nin a semi-supervised manner contextualized in the hierarchy of the domain. To\nreduce expensive human efforts, we employ automatic annotation and hierarchical\npositive-unlabeled learning. Our approach applies to big or small domains,\ncovers head or tail terms, and requires little human effort. Extensive\nexperiments demonstrate that our methods outperform strong baselines and even\nsurpass professional human performance.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2021-05-27T15:52:34+00:00",
    "updated": "2021-05-27T15:52:34+00:00",
    "doi": null,
    "comment": "Accepted to ACL 2021",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1709.09602v2": {
    "id": "http://arxiv.org/abs/1709.09602v2",
    "title": "Exposure: A White-Box Photo Post-Processing Framework",
    "authors": [
      "Yuanming Hu",
      "Hao He",
      "Chenxi Xu",
      "Baoyuan Wang",
      "Stephen Lin"
    ],
    "abstract": "Retouching can significantly elevate the visual appeal of photos, but many\ncasual photographers lack the expertise to do this well. To address this\nproblem, previous works have proposed automatic retouching systems based on\nsupervised learning from paired training images acquired before and after\nmanual editing. As it is difficult for users to acquire paired images that\nreflect their retouching preferences, we present in this paper a deep learning\napproach that is instead trained on unpaired data, namely a set of photographs\nthat exhibits a retouching style the user likes, which is much easier to\ncollect. Our system is formulated using deep convolutional neural networks that\nlearn to apply different retouching operations on an input image. Network\ntraining with respect to various types of edits is enabled by modeling these\nretouching operations in a unified manner as resolution-independent\ndifferentiable filters. To apply the filters in a proper sequence and with\nsuitable parameters, we employ a deep reinforcement learning approach that\nlearns to make decisions on what action to take next, given the current state\nof the image. In contrast to many deep learning systems, ours provides users\nwith an understandable solution in the form of conventional retouching edits,\nrather than just a \"black-box\" result. Through quantitative comparisons and\nuser studies, we show that this technique generates retouching results\nconsistent with the provided photo set.",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "published": "2017-09-27T16:15:58+00:00",
    "updated": "2018-02-06T16:47:58+00:00",
    "doi": null,
    "comment": "ACM Transaction on Graphics (Accepted with minor revisions)",
    "journal_ref": null,
    "primary_category": "cs.GR"
  },
  "2204.02013v3": {
    "id": "http://arxiv.org/abs/2204.02013v3",
    "title": "RL4ReAl: Reinforcement Learning for Register Allocation",
    "authors": [
      "S. VenkataKeerthy",
      "Siddharth Jain",
      "Anilava Kundu",
      "Rohit Aggarwal",
      "Albert Cohen",
      "Ramakrishna Upadrasta"
    ],
    "abstract": "We aim to automate decades of research and experience in register allocation,\nleveraging machine learning. We tackle this problem by embedding a multi-agent\nreinforcement learning algorithm within LLVM, training it with the state of the\nart techniques. We formalize the constraints that precisely define the problem\nfor a given instruction-set architecture, while ensuring that the generated\ncode preserves semantic correctness. We also develop a gRPC based framework\nproviding a modular and efficient compiler interface for training and\ninference. Our approach is architecture independent: we show experimental\nresults targeting Intel x86 and ARM AArch64. Our results match or out-perform\nthe heavily tuned, production-grade register allocators of LLVM.",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.PL",
      "D.2; I.2.5"
    ],
    "published": "2022-04-05T06:30:03+00:00",
    "updated": "2023-02-06T05:22:34+00:00",
    "doi": "10.1145/3578360.3580273",
    "comment": "Published in CC'23",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2005.08100v1": {
    "id": "http://arxiv.org/abs/2005.08100v1",
    "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
    "authors": [
      "Anmol Gulati",
      "James Qin",
      "Chung-Cheng Chiu",
      "Niki Parmar",
      "Yu Zhang",
      "Jiahui Yu",
      "Wei Han",
      "Shibo Wang",
      "Zhengdong Zhang",
      "Yonghui Wu",
      "Ruoming Pang"
    ],
    "abstract": "Recently Transformer and Convolution neural network (CNN) based models have\nshown promising results in Automatic Speech Recognition (ASR), outperforming\nRecurrent neural networks (RNNs). Transformer models are good at capturing\ncontent-based global interactions, while CNNs exploit local features\neffectively. In this work, we achieve the best of both worlds by studying how\nto combine convolution neural networks and transformers to model both local and\nglobal dependencies of an audio sequence in a parameter-efficient way. To this\nregard, we propose the convolution-augmented transformer for speech\nrecognition, named Conformer. Conformer significantly outperforms the previous\nTransformer and CNN based models achieving state-of-the-art accuracies. On the\nwidely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without\nusing a language model and 1.9%/3.9% with an external language model on\ntest/testother. We also observe competitive performance of 2.7%/6.3% with a\nsmall model of only 10M parameters.",
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "published": "2020-05-16T20:56:25+00:00",
    "updated": "2020-05-16T20:56:25+00:00",
    "doi": null,
    "comment": "Submitted to Interspeech 2020",
    "journal_ref": null,
    "primary_category": "eess.AS"
  },
  "2311.18159v3": {
    "id": "http://arxiv.org/abs/2311.18159v3",
    "title": "CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization",
    "authors": [
      "KL Navaneet",
      "Kossar Pourahmadi Meibodi",
      "Soroush Abbasi Koohpayegani",
      "Hamed Pirsiavash"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) is a new method for modeling and rendering 3D\nradiance fields that achieves much faster learning and rendering time compared\nto SOTA NeRF methods. However, it comes with a drawback in the much larger\nstorage demand compared to NeRF methods since it needs to store the parameters\nfor several 3D Gaussians. We notice that many Gaussians may share similar\nparameters, so we introduce a simple vector quantization method based on\nK-means to quantize the Gaussian parameters while optimizing them. Then, we\nstore the small codebook along with the index of the code for each Gaussian. We\ncompress the indices further by sorting them and using a method similar to\nrun-length encoding. Moreover, we use a simple regularizer to encourage zero\nopacity (invisible Gaussians) to reduce the storage and rendering time by a\nlarge factor through reducing the number of Gaussians. We do extensive\nexperiments on standard benchmarks as well as an existing 3D dataset that is an\norder of magnitude larger than the standard benchmarks used in this field. We\nshow that our simple yet effective method can reduce the storage cost for 3DGS\nby 40 to 50x and rendering time by 2 to 3x with a very small drop in the\nquality of rendered images.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-11-30T00:29:13+00:00",
    "updated": "2024-09-26T18:45:20+00:00",
    "doi": null,
    "comment": "Code is available at https://github.com/UCDvision/compact3d",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2006.09252v3": {
    "id": "http://arxiv.org/abs/2006.09252v3",
    "title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting",
    "authors": [
      "Giorgos Bouritsas",
      "Fabrizio Frasca",
      "Stefanos Zafeiriou",
      "Michael M. Bronstein"
    ],
    "abstract": "While Graph Neural Networks (GNNs) have achieved remarkable results in a\nvariety of applications, recent studies exposed important shortcomings in their\nability to capture the structure of the underlying graph. It has been shown\nthat the expressive power of standard GNNs is bounded by the Weisfeiler-Leman\n(WL) graph isomorphism test, from which they inherit proven limitations such as\nthe inability to detect and count graph substructures. On the other hand, there\nis significant empirical evidence, e.g. in network science and bioinformatics,\nthat substructures are often intimately related to downstream tasks. To this\nend, we propose \"Graph Substructure Networks\" (GSN), a topologically-aware\nmessage passing scheme based on substructure encoding. We theoretically analyse\nthe expressive power of our architecture, showing that it is strictly more\nexpressive than the WL test, and provide sufficient conditions for\nuniversality. Importantly, we do not attempt to adhere to the WL hierarchy;\nthis allows us to retain multiple attractive properties of standard GNNs such\nas locality and linear network complexity, while being able to disambiguate\neven hard instances of graph isomorphism. We perform an extensive experimental\nevaluation on graph classification and regression tasks and obtain\nstate-of-the-art results in diverse real-world settings including molecular\ngraphs and social networks. The code is publicly available at\nhttps://github.com/gbouritsas/graph-substructure-networks.",
    "categories": [
      "cs.LG",
      "cs.SI",
      "stat.ML"
    ],
    "published": "2020-06-16T15:30:31+00:00",
    "updated": "2021-07-05T13:22:05+00:00",
    "doi": "10.1109/TPAMI.2022.3154319",
    "comment": null,
    "journal_ref": "IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI) vol. 45 (2023) pp.\n  657 - 668",
    "primary_category": "cs.LG"
  },
  "2312.10649v1": {
    "id": "http://arxiv.org/abs/2312.10649v1",
    "title": "PNeRFLoc: Visual Localization with Point-based Neural Radiance Fields",
    "authors": [
      "Boming Zhao",
      "Luwei Yang",
      "Mao Mao",
      "Hujun Bao",
      "Zhaopeng Cui"
    ],
    "abstract": "Due to the ability to synthesize high-quality novel views, Neural Radiance\nFields (NeRF) have been recently exploited to improve visual localization in a\nknown environment. However, the existing methods mostly utilize NeRFs for data\naugmentation to improve the regression model training, and the performance on\nnovel viewpoints and appearances is still limited due to the lack of geometric\nconstraints. In this paper, we propose a novel visual localization framework,\n\\ie, PNeRFLoc, based on a unified point-based representation. On the one hand,\nPNeRFLoc supports the initial pose estimation by matching 2D and 3D feature\npoints as traditional structure-based methods; on the other hand, it also\nenables pose refinement with novel view synthesis using rendering-based\noptimization. Specifically, we propose a novel feature adaption module to close\nthe gaps between the features for visual localization and neural rendering. To\nimprove the efficacy and efficiency of neural rendering-based optimization, we\nalso develop an efficient rendering-based framework with a warping loss\nfunction. Furthermore, several robustness techniques are developed to handle\nillumination changes and dynamic objects for outdoor scenarios. Experiments\ndemonstrate that PNeRFLoc performs the best on synthetic data when the NeRF\nmodel can be well learned and performs on par with the SOTA method on the\nvisual localization benchmark datasets.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-17T08:30:00+00:00",
    "updated": "2023-12-17T08:30:00+00:00",
    "doi": null,
    "comment": "Accepted to AAAI 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.12751v1": {
    "id": "http://arxiv.org/abs/2401.12751v1",
    "title": "PSDF: Prior-Driven Neural Implicit Surface Learning for Multi-view Reconstruction",
    "authors": [
      "Wanjuan Su",
      "Chen Zhang",
      "Qingshan Xu",
      "Wenbing Tao"
    ],
    "abstract": "Surface reconstruction has traditionally relied on the Multi-View Stereo\n(MVS)-based pipeline, which often suffers from noisy and incomplete geometry.\nThis is due to that although MVS has been proven to be an effective way to\nrecover the geometry of the scenes, especially for locally detailed areas with\nrich textures, it struggles to deal with areas with low texture and large\nvariations of illumination where the photometric consistency is unreliable.\nRecently, Neural Implicit Surface Reconstruction (NISR) combines surface\nrendering and volume rendering techniques and bypasses the MVS as an\nintermediate step, which has emerged as a promising alternative to overcome the\nlimitations of traditional pipelines. While NISR has shown impressive results\non simple scenes, it remains challenging to recover delicate geometry from\nuncontrolled real-world scenes which is caused by its underconstrained\noptimization. To this end, the framework PSDF is proposed which resorts to\nexternal geometric priors from a pretrained MVS network and internal geometric\npriors inherent in the NISR model to facilitate high-quality neural implicit\nsurface learning. Specifically, the visibility-aware feature consistency loss\nand depth prior-assisted sampling based on external geometric priors are\nintroduced. These proposals provide powerfully geometric consistency\nconstraints and aid in locating surface intersection points, thereby\nsignificantly improving the accuracy and delicate reconstruction of NISR.\nMeanwhile, the internal prior-guided importance rendering is presented to\nenhance the fidelity of the reconstructed surface mesh by mitigating the biased\nrendering issue in NISR. Extensive experiments on the Tanks and Temples dataset\nshow that PSDF achieves state-of-the-art performance on complex uncontrolled\nscenes.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-01-23T13:30:43+00:00",
    "updated": "2024-01-23T13:30:43+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2310.04959v1": {
    "id": "http://arxiv.org/abs/2310.04959v1",
    "title": "Towards Better Chain-of-Thought Prompting Strategies: A Survey",
    "authors": [
      "Zihan Yu",
      "Liang He",
      "Zhen Wu",
      "Xinyu Dai",
      "Jiajun Chen"
    ],
    "abstract": "Chain-of-Thought (CoT), a step-wise and coherent reasoning chain, shows its\nimpressive strength when used as a prompting strategy for large language models\n(LLM). Recent years, the prominent effect of CoT prompting has attracted\nemerging research. However, there still lacks of a systematic summary about key\nfactors of CoT prompting and comprehensive guide for prompts utilizing. For a\ndeeper understanding about CoT prompting, we survey on a wide range of current\nresearch, presenting a systematic and comprehensive analysis on several factors\nthat may influence the effect of CoT prompting, and introduce how to better\napply it in different applications under these discussions. We further analyze\nthe challenges and propose some future directions about CoT prompting. This\nsurvey could provide an overall reference on related research.",
    "categories": [
      "cs.CL"
    ],
    "published": "2023-10-08T01:16:55+00:00",
    "updated": "2023-10-08T01:16:55+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2104.06820v1": {
    "id": "http://arxiv.org/abs/2104.06820v1",
    "title": "Few-shot Image Generation via Cross-domain Correspondence",
    "authors": [
      "Utkarsh Ojha",
      "Yijun Li",
      "Jingwan Lu",
      "Alexei A. Efros",
      "Yong Jae Lee",
      "Eli Shechtman",
      "Richard Zhang"
    ],
    "abstract": "Training generative models, such as GANs, on a target domain containing\nlimited examples (e.g., 10) can easily result in overfitting. In this work, we\nseek to utilize a large source domain for pretraining and transfer the\ndiversity information from source to target. We propose to preserve the\nrelative similarities and differences between instances in the source via a\nnovel cross-domain distance consistency loss. To further reduce overfitting, we\npresent an anchor-based strategy to encourage different levels of realism over\ndifferent regions in the latent space. With extensive results in both\nphotorealistic and non-photorealistic domains, we demonstrate qualitatively and\nquantitatively that our few-shot model automatically discovers correspondences\nbetween source and target domains and generates more diverse and realistic\nimages than previous methods.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2021-04-13T17:59:35+00:00",
    "updated": "2021-04-13T17:59:35+00:00",
    "doi": null,
    "comment": "CVPR 2021",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2405.01593v1": {
    "id": "http://arxiv.org/abs/2405.01593v1",
    "title": "Large Language Model Agent for Fake News Detection",
    "authors": [
      "Xinyi Li",
      "Yongfeng Zhang",
      "Edward C. Malthouse"
    ],
    "abstract": "In the current digital era, the rapid spread of misinformation on online\nplatforms presents significant challenges to societal well-being, public trust,\nand democratic processes, influencing critical decision making and public\nopinion. To address these challenges, there is a growing need for automated\nfake news detection mechanisms. Pre-trained large language models (LLMs) have\ndemonstrated exceptional capabilities across various natural language\nprocessing (NLP) tasks, prompting exploration into their potential for\nverifying news claims. Instead of employing LLMs in a non-agentic way, where\nLLMs generate responses based on direct prompts in a single shot, our work\nintroduces FactAgent, an agentic approach of utilizing LLMs for fake news\ndetection. FactAgent enables LLMs to emulate human expert behavior in verifying\nnews claims without any model training, following a structured workflow. This\nworkflow breaks down the complex task of news veracity checking into multiple\nsub-steps, where LLMs complete simple tasks using their internal knowledge or\nexternal tools. At the final step of the workflow, LLMs integrate all findings\nthroughout the workflow to determine the news claim's veracity. Compared to\nmanual human verification, FactAgent offers enhanced efficiency. Experimental\nstudies demonstrate the effectiveness of FactAgent in verifying claims without\nthe need for any training process. Moreover, FactAgent provides transparent\nexplanations at each step of the workflow and during final decision-making,\noffering insights into the reasoning process of fake news detection for end\nusers. FactAgent is highly adaptable, allowing for straightforward updates to\nits tools that LLMs can leverage within the workflow, as well as updates to the\nworkflow itself using domain knowledge. This adaptability enables FactAgent's\napplication to news verification across various domains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-04-30T06:55:27+00:00",
    "updated": "2024-04-30T06:55:27+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1605.06796v2": {
    "id": "http://arxiv.org/abs/1605.06796v2",
    "title": "Interpretable Distribution Features with Maximum Testing Power",
    "authors": [
      "Wittawat Jitkrittum",
      "Zoltan Szabo",
      "Kacper Chwialkowski",
      "Arthur Gretton"
    ],
    "abstract": "Two semimetrics on probability distributions are proposed, given as the sum\nof differences of expectations of analytic functions evaluated at spatial or\nfrequency locations (i.e, features). The features are chosen so as to maximize\nthe distinguishability of the distributions, by optimizing a lower bound on\ntest power for a statistical test using these features. The result is a\nparsimonious and interpretable indication of how and where two distributions\ndiffer locally. An empirical estimate of the test power criterion converges\nwith increasing sample size, ensuring the quality of the returned features. In\nreal-world benchmarks on high-dimensional text and image data, linear-time\ntests using the proposed semimetrics achieve comparable performance to the\nstate-of-the-art quadratic-time maximum mean discrepancy test, while returning\nhuman-interpretable features that explain the test results.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "46E22, 62G10",
      "G.3; I.2.6"
    ],
    "published": "2016-05-22T14:10:13+00:00",
    "updated": "2016-10-28T10:48:05+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2304.04514v1": {
    "id": "http://arxiv.org/abs/2304.04514v1",
    "title": "DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via Word-Region Alignment",
    "authors": [
      "Lewei Yao",
      "Jianhua Han",
      "Xiaodan Liang",
      "Dan Xu",
      "Wei Zhang",
      "Zhenguo Li",
      "Hang Xu"
    ],
    "abstract": "This paper presents DetCLIPv2, an efficient and scalable training framework\nthat incorporates large-scale image-text pairs to achieve open-vocabulary\nobject detection (OVD). Unlike previous OVD frameworks that typically rely on a\npre-trained vision-language model (e.g., CLIP) or exploit image-text pairs via\na pseudo labeling process, DetCLIPv2 directly learns the fine-grained\nword-region alignment from massive image-text pairs in an end-to-end manner. To\naccomplish this, we employ a maximum word-region similarity between region\nproposals and textual words to guide the contrastive objective. To enable the\nmodel to gain localization capability while learning broad concepts, DetCLIPv2\nis trained with a hybrid supervision from detection, grounding and image-text\npair data under a unified data formulation. By jointly training with an\nalternating scheme and adopting low-resolution input for image-text pairs,\nDetCLIPv2 exploits image-text pair data efficiently and effectively: DetCLIPv2\nutilizes 13X more image-text pairs than DetCLIP with a similar training time\nand improves performance. With 13M image-text pairs for pre-training, DetCLIPv2\ndemonstrates superior open-vocabulary detection performance, e.g., DetCLIPv2\nwith Swin-T backbone achieves 40.4% zero-shot AP on the LVIS benchmark, which\noutperforms previous works GLIP/GLIPv2/DetCLIP by 14.4/11.4/4.5% AP,\nrespectively, and even beats its fully-supervised counterpart by a large\nmargin.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-04-10T11:08:15+00:00",
    "updated": "2023-04-10T11:08:15+00:00",
    "doi": null,
    "comment": "Accepted to CVPR2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.00757v1": {
    "id": "http://arxiv.org/abs/2210.00757v1",
    "title": "Fully Transformer Network for Change Detection of Remote Sensing Images",
    "authors": [
      "Tianyu Yan",
      "Zifu Wan",
      "Pingping Zhang"
    ],
    "abstract": "Recently, change detection (CD) of remote sensing images have achieved great\nprogress with the advances of deep learning. However, current methods generally\ndeliver incomplete CD regions and irregular CD boundaries due to the limited\nrepresentation ability of the extracted visual features. To relieve these\nissues, in this work we propose a novel learning framework named Fully\nTransformer Network (FTN) for remote sensing image CD, which improves the\nfeature extraction from a global view and combines multi-level visual features\nin a pyramid manner. More specifically, the proposed framework first utilizes\nthe advantages of Transformers in long-range dependency modeling. It can help\nto learn more discriminative global-level features and obtain complete CD\nregions. Then, we introduce a pyramid structure to aggregate multi-level visual\nfeatures from Transformers for feature enhancement. The pyramid structure\ngrafted with a Progressive Attention Module (PAM) can improve the feature\nrepresentation ability with additional interdependencies through channel\nattentions. Finally, to better train the framework, we utilize the\ndeeply-supervised learning with multiple boundaryaware loss functions.\nExtensive experiments demonstrate that our proposed method achieves a new\nstate-of-the-art performance on four public CD benchmarks. For model\nreproduction, the source code is released at https://github.com/AI-Zhpp/FTN.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2022-10-03T08:21:25+00:00",
    "updated": "2022-10-03T08:21:25+00:00",
    "doi": null,
    "comment": "18 pages, 6 figures and 5 tables. This work will appear in ACCV2022\n  as a poster paper",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1802.08435v2": {
    "id": "http://arxiv.org/abs/1802.08435v2",
    "title": "Efficient Neural Audio Synthesis",
    "authors": [
      "Nal Kalchbrenner",
      "Erich Elsen",
      "Karen Simonyan",
      "Seb Noury",
      "Norman Casagrande",
      "Edward Lockhart",
      "Florian Stimberg",
      "Aaron van den Oord",
      "Sander Dieleman",
      "Koray Kavukcuoglu"
    ],
    "abstract": "Sequential models achieve state-of-the-art results in audio, visual and\ntextual domains with respect to both estimating the data distribution and\ngenerating high-quality samples. Efficient sampling for this class of models\nhas however remained an elusive problem. With a focus on text-to-speech\nsynthesis, we describe a set of general techniques for reducing sampling time\nwhile maintaining high output quality. We first describe a single-layer\nrecurrent neural network, the WaveRNN, with a dual softmax layer that matches\nthe quality of the state-of-the-art WaveNet model. The compact form of the\nnetwork makes it possible to generate 24kHz 16-bit audio 4x faster than real\ntime on a GPU. Second, we apply a weight pruning technique to reduce the number\nof weights in the WaveRNN. We find that, for a constant number of parameters,\nlarge sparse networks perform better than small dense networks and this\nrelationship holds for sparsity levels beyond 96%. The small number of weights\nin a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile\nCPU in real time. Finally, we propose a new generation scheme based on\nsubscaling that folds a long sequence into a batch of shorter sequences and\nallows one to generate multiple samples at once. The Subscale WaveRNN produces\n16 samples per step without loss of quality and offers an orthogonal method for\nincreasing sampling efficiency.",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "published": "2018-02-23T08:20:23+00:00",
    "updated": "2018-06-25T19:45:25+00:00",
    "doi": null,
    "comment": "10 pages",
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "2201.08050v2": {
    "id": "http://arxiv.org/abs/2201.08050v2",
    "title": "TerViT: An Efficient Ternary Vision Transformer",
    "authors": [
      "Sheng Xu",
      "Yanjing Li",
      "Teli Ma",
      "Bohan Zeng",
      "Baochang Zhang",
      "Peng Gao",
      "Jinhu Lv"
    ],
    "abstract": "Vision transformers (ViTs) have demonstrated great potential in various\nvisual tasks, but suffer from expensive computational and memory cost problems\nwhen deployed on resource-constrained devices. In this paper, we introduce a\nternary vision transformer (TerViT) to ternarize the weights in ViTs, which are\nchallenged by the large loss surface gap between real-valued and ternary\nparameters. To address the issue, we introduce a progressive training scheme by\nfirst training 8-bit transformers and then TerViT, and achieve a better\noptimization than conventional methods. Furthermore, we introduce channel-wise\nternarization, by partitioning each matrix to different channels, each of which\nis with an unique distribution and ternarization interval. We apply our methods\nto popular DeiT and Swin backbones, and extensive results show that we can\nachieve competitive performance. For example, TerViT can quantize Swin-S to\n13.1MB model size while achieving above 79% Top-1 accuracy on ImageNet dataset.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2022-01-20T08:29:19+00:00",
    "updated": "2022-01-21T05:22:32+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.00436v3": {
    "id": "http://arxiv.org/abs/2308.00436v3",
    "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
    "authors": [
      "Ning Miao",
      "Yee Whye Teh",
      "Tom Rainforth"
    ],
    "abstract": "The recent progress in large language models (LLMs), especially the invention\nof chain-of-thought prompting, has made it possible to automatically answer\nquestions by stepwise reasoning. However, when faced with more complicated\nproblems that require non-linear thinking, even the strongest LLMs make\nmistakes. To address this, we explore whether LLMs are able to recognize errors\nin their own step-by-step reasoning, without resorting to external resources.\nTo this end, we propose SelfCheck, a general-purpose zero-shot verification\nschema for recognizing such errors. We then use the results of these checks to\nimprove question-answering performance by conducting weighted voting on\nmultiple solutions to the question. We test SelfCheck on three datasets (GSM8K,\nMathQA, and MATH) and find that it successfully recognizes errors and, in turn,\nincreases final answer accuracies.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-08-01T10:31:36+00:00",
    "updated": "2023-10-05T12:59:59+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2304.05060v2": {
    "id": "http://arxiv.org/abs/2304.05060v2",
    "title": "SPIRiT-Diffusion: Self-Consistency Driven Diffusion Model for Accelerated MRI",
    "authors": [
      "Zhuo-Xu Cui",
      "Chentao Cao",
      "Yue Wang",
      "Sen Jia",
      "Jing Cheng",
      "Xin Liu",
      "Hairong Zheng",
      "Dong Liang",
      "Yanjie Zhu"
    ],
    "abstract": "Diffusion models have emerged as a leading methodology for image generation\nand have proven successful in the realm of magnetic resonance imaging (MRI)\nreconstruction. However, existing reconstruction methods based on diffusion\nmodels are primarily formulated in the image domain, making the reconstruction\nquality susceptible to inaccuracies in coil sensitivity maps (CSMs). k-space\ninterpolation methods can effectively address this issue but conventional\ndiffusion models are not readily applicable in k-space interpolation. To\novercome this challenge, we introduce a novel approach called SPIRiT-Diffusion,\nwhich is a diffusion model for k-space interpolation inspired by the iterative\nself-consistent SPIRiT method. Specifically, we utilize the iterative solver of\nthe self-consistent term (i.e., k-space physical prior) in SPIRiT to formulate\na novel stochastic differential equation (SDE) governing the diffusion process.\nSubsequently, k-space data can be interpolated by executing the diffusion\nprocess. This innovative approach highlights the optimization model's role in\ndesigning the SDE in diffusion models, enabling the diffusion process to align\nclosely with the physics inherent in the optimization model, a concept referred\nto as model-driven diffusion. We evaluated the proposed SPIRiT-Diffusion method\nusing a 3D joint intracranial and carotid vessel wall imaging dataset. The\nresults convincingly demonstrate its superiority over image-domain\nreconstruction methods, achieving high reconstruction quality even at a\nsubstantial acceleration rate of 10.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-04-11T08:43:52+00:00",
    "updated": "2024-04-20T06:14:34+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2408.07055v1": {
    "id": "http://arxiv.org/abs/2408.07055v1",
    "title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs",
    "authors": [
      "Yushi Bai",
      "Jiajie Zhang",
      "Xin Lv",
      "Linzhi Zheng",
      "Siqi Zhu",
      "Lei Hou",
      "Yuxiao Dong",
      "Jie Tang",
      "Juanzi Li"
    ],
    "abstract": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-08-13T17:46:12+00:00",
    "updated": "2024-08-13T17:46:12+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2401.15077v2": {
    "id": "http://arxiv.org/abs/2401.15077v2",
    "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
    "authors": [
      "Yuhui Li",
      "Fangyun Wei",
      "Chao Zhang",
      "Hongyang Zhang"
    ],
    "abstract": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-01-26T18:59:01+00:00",
    "updated": "2024-02-04T17:18:34+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1808.01400v6": {
    "id": "http://arxiv.org/abs/1808.01400v6",
    "title": "code2seq: Generating Sequences from Structured Representations of Code",
    "authors": [
      "Uri Alon",
      "Shaked Brody",
      "Omer Levy",
      "Eran Yahav"
    ],
    "abstract": "The ability to generate natural language sequences from source code snippets\nhas a variety of applications such as code summarization, documentation, and\nretrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine\ntranslation (NMT), have achieved state-of-the-art performance on these tasks by\ntreating source code as a sequence of tokens. We present ${\\rm {\\scriptsize\nCODE2SEQ}}$: an alternative approach that leverages the syntactic structure of\nprogramming languages to better encode source code. Our model represents a code\nsnippet as the set of compositional paths in its abstract syntax tree (AST) and\nuses attention to select the relevant paths while decoding. We demonstrate the\neffectiveness of our approach for two tasks, two programming languages, and\nfour datasets of up to $16$M examples. Our model significantly outperforms\nprevious models that were specifically designed for programming languages, as\nwell as state-of-the-art NMT models. An interactive online demo of our model is\navailable at http://code2seq.org. Our code, data and trained models are\navailable at http://github.com/tech-srl/code2seq.",
    "categories": [
      "cs.LG",
      "cs.PL",
      "stat.ML"
    ],
    "published": "2018-08-04T01:26:07+00:00",
    "updated": "2019-02-21T14:12:56+00:00",
    "doi": null,
    "comment": "Accepted to ICLR'2019",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2312.03043v1": {
    "id": "http://arxiv.org/abs/2312.03043v1",
    "title": "Navigating the Synthetic Realm: Harnessing Diffusion-based Models for Laparoscopic Text-to-Image Generation",
    "authors": [
      "Simeon Allmendinger",
      "Patrick Hemmer",
      "Moritz Queisner",
      "Igor Sauer",
      "Leopold M\u00fcller",
      "Johannes Jakubik",
      "Michael V\u00f6ssing",
      "Niklas K\u00fchl"
    ],
    "abstract": "Recent advances in synthetic imaging open up opportunities for obtaining\nadditional data in the field of surgical imaging. This data can provide\nreliable supplements supporting surgical applications and decision-making\nthrough computer vision. Particularly the field of image-guided surgery, such\nas laparoscopic and robotic-assisted surgery, benefits strongly from synthetic\nimage datasets and virtual surgical training methods. Our study presents an\nintuitive approach for generating synthetic laparoscopic images from short text\nprompts using diffusion-based generative models. We demonstrate the usage of\nstate-of-the-art text-to-image architectures in the context of laparoscopic\nimaging with regard to the surgical removal of the gallbladder as an example.\nResults on fidelity and diversity demonstrate that diffusion-based models can\nacquire knowledge about the style and semantics in the field of image-guided\nsurgery. A validation study with a human assessment survey underlines the\nrealistic nature of our synthetic data, as medical personnel detects actual\nimages in a pool with generated images causing a false-positive rate of 66%. In\naddition, the investigation of a state-of-the-art machine learning model to\nrecognize surgical actions indicates enhanced results when trained with\nadditional generated images of up to 5.20%. Overall, the achieved image quality\ncontributes to the usage of computer-generated images in surgical applications\nand enhances its path to maturity.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.TO"
    ],
    "published": "2023-12-05T16:20:22+00:00",
    "updated": "2023-12-05T16:20:22+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "eess.IV"
  },
  "2310.03714v1": {
    "id": "http://arxiv.org/abs/2310.03714v1",
    "title": "DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines",
    "authors": [
      "Omar Khattab",
      "Arnav Singhvi",
      "Paridhi Maheshwari",
      "Zhiyuan Zhang",
      "Keshav Santhanam",
      "Sri Vardhamanan",
      "Saiful Haq",
      "Ashutosh Sharma",
      "Thomas T. Joshi",
      "Hanna Moazam",
      "Heather Miller",
      "Matei Zaharia",
      "Christopher Potts"
    ],
    "abstract": "The ML community is rapidly exploring techniques for prompting language\nmodels (LMs) and for stacking them into pipelines that solve complex tasks.\nUnfortunately, existing LM pipelines are typically implemented using hard-coded\n\"prompt templates\", i.e. lengthy strings discovered via trial and error. Toward\na more systematic approach for developing and optimizing LM pipelines, we\nintroduce DSPy, a programming model that abstracts LM pipelines as text\ntransformation graphs, i.e. imperative computational graphs where LMs are\ninvoked through declarative modules. DSPy modules are parameterized, meaning\nthey can learn (by creating and collecting demonstrations) how to apply\ncompositions of prompting, finetuning, augmentation, and reasoning techniques.\nWe design a compiler that will optimize any DSPy pipeline to maximize a given\nmetric. We conduct two case studies, showing that succinct DSPy programs can\nexpress and optimize sophisticated LM pipelines that reason about math word\nproblems, tackle multi-hop retrieval, answer complex questions, and control\nagent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and\nllama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot\nprompting (generally by over 25% and 65%, respectively) and pipelines with\nexpert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top\nof that, DSPy programs compiled to open and relatively small LMs like\n770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely\non expert-written prompt chains for proprietary GPT-3.5. DSPy is available at\nhttps://github.com/stanfordnlp/dspy",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2023-10-05T17:37:25+00:00",
    "updated": "2023-10-05T17:37:25+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2303.11301v1": {
    "id": "http://arxiv.org/abs/2303.11301v1",
    "title": "VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking",
    "authors": [
      "Yukang Chen",
      "Jianhui Liu",
      "Xiangyu Zhang",
      "Xiaojuan Qi",
      "Jiaya Jia"
    ],
    "abstract": "3D object detectors usually rely on hand-crafted proxies, e.g., anchors or\ncenters, and translate well-studied 2D frameworks to 3D. Thus, sparse voxel\nfeatures need to be densified and processed by dense prediction heads, which\ninevitably costs extra computation. In this paper, we instead propose VoxelNext\nfor fully sparse 3D object detection. Our core insight is to predict objects\ndirectly based on sparse voxel features, without relying on hand-crafted\nproxies. Our strong sparse convolutional network VoxelNeXt detects and tracks\n3D objects through voxel features entirely. It is an elegant and efficient\nframework, with no need for sparse-to-dense conversion or NMS post-processing.\nOur method achieves a better speed-accuracy trade-off than other mainframe\ndetectors on the nuScenes dataset. For the first time, we show that a fully\nsparse voxel-based representation works decently for LIDAR 3D object detection\nand tracking. Extensive experiments on nuScenes, Waymo, and Argoverse2\nbenchmarks validate the effectiveness of our approach. Without bells and\nwhistles, our model outperforms all existing LIDAR methods on the nuScenes\ntracking test benchmark.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-03-20T17:40:44+00:00",
    "updated": "2023-03-20T17:40:44+00:00",
    "doi": null,
    "comment": "In CVPR 2023, Code and models are available at\n  https://github.com/dvlab-research/VoxelNeXt",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1505.01596v2": {
    "id": "http://arxiv.org/abs/1505.01596v2",
    "title": "Learning to See by Moving",
    "authors": [
      "Pulkit Agrawal",
      "Joao Carreira",
      "Jitendra Malik"
    ],
    "abstract": "The dominant paradigm for feature learning in computer vision relies on\ntraining neural networks for the task of object recognition using millions of\nhand labelled images. Is it possible to learn useful features for a diverse set\nof visual tasks using any other form of supervision? In biology, living\norganisms developed the ability of visual perception for the purpose of moving\nand acting in the world. Drawing inspiration from this observation, in this\nwork we investigate if the awareness of egomotion can be used as a supervisory\nsignal for feature learning. As opposed to the knowledge of class labels,\ninformation about egomotion is freely available to mobile agents. We show that\ngiven the same number of training images, features learnt using egomotion as\nsupervision compare favourably to features learnt using class-label as\nsupervision on visual tasks of scene recognition, object recognition, visual\nodometry and keypoint matching.",
    "categories": [
      "cs.CV",
      "cs.NE",
      "cs.RO"
    ],
    "published": "2015-05-07T06:03:01+00:00",
    "updated": "2015-09-14T16:59:36+00:00",
    "doi": null,
    "comment": "12 pages",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2203.14104v1": {
    "id": "http://arxiv.org/abs/2203.14104v1",
    "title": "Bridge-Prompt: Towards Ordinal Action Understanding in Instructional Videos",
    "authors": [
      "Muheng Li",
      "Lei Chen",
      "Yueqi Duan",
      "Zhilan Hu",
      "Jianjiang Feng",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Action recognition models have shown a promising capability to classify human\nactions in short video clips. In a real scenario, multiple correlated human\nactions commonly occur in particular orders, forming semantically meaningful\nhuman activities. Conventional action recognition approaches focus on analyzing\nsingle actions. However, they fail to fully reason about the contextual\nrelations between adjacent actions, which provide potential temporal logic for\nunderstanding long videos. In this paper, we propose a prompt-based framework,\nBridge-Prompt (Br-Prompt), to model the semantics across adjacent actions, so\nthat it simultaneously exploits both out-of-context and contextual information\nfrom a series of ordinal actions in instructional videos. More specifically, we\nreformulate the individual action labels as integrated text prompts for\nsupervision, which bridge the gap between individual action semantics. The\ngenerated text prompts are paired with corresponding video clips, and together\nco-train the text encoder and the video encoder via a contrastive approach. The\nlearned vision encoder has a stronger capability for ordinal-action-related\ndownstream tasks, e.g. action segmentation and human activity recognition. We\nevaluate the performances of our approach on several video datasets: Georgia\nTech Egocentric Activities (GTEA), 50Salads, and the Breakfast dataset.\nBr-Prompt achieves state-of-the-art on multiple benchmarks. Code is available\nat https://github.com/ttlmh/Bridge-Prompt",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2022-03-26T15:52:27+00:00",
    "updated": "2022-03-26T15:52:27+00:00",
    "doi": null,
    "comment": "Accepted to CVPR 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2301.10048v2": {
    "id": "http://arxiv.org/abs/2301.10048v2",
    "title": "Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting",
    "authors": [
      "Kaidong Zhang",
      "Jialun Peng",
      "Jingjing Fu",
      "Dong Liu"
    ],
    "abstract": "Transformers have been widely used for video processing owing to the\nmulti-head self attention (MHSA) mechanism. However, the MHSA mechanism\nencounters an intrinsic difficulty for video inpainting, since the features\nassociated with the corrupted regions are degraded and incur inaccurate self\nattention. This problem, termed query degradation, may be mitigated by first\ncompleting optical flows and then using the flows to guide the self attention,\nwhich was verified in our previous work - flow-guided transformer (FGT). We\nfurther exploit the flow guidance and propose FGT++ to pursue more effective\nand efficient video inpainting. First, we design a lightweight flow completion\nnetwork by using local aggregation and edge loss. Second, to address the query\ndegradation, we propose a flow guidance feature integration module, which uses\nthe motion discrepancy to enhance the features, together with a flow-guided\nfeature propagation module that warps the features according to the flows.\nThird, we decouple the transformer along the temporal and spatial dimensions,\nwhere flows are used to select the tokens through a temporally deformable MHSA\nmechanism, and global tokens are combined with the inner-window local tokens\nthrough a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to\nbe outperforming the existing video inpainting networks qualitatively and\nquantitatively.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-01-24T14:44:44+00:00",
    "updated": "2024-03-19T04:02:28+00:00",
    "doi": "10.1109/TPAMI.2024.3361010",
    "comment": "Accepted to TPAMI. This manuscript is a journal extension of our ECCV\n  2022 paper (arXiv:2208.06768)",
    "journal_ref": "Zhang K, Peng J, Fu J, et al. Exploiting optical flow guidance for\n  transformer-based video inpainting[J]. IEEE Transactions on Pattern Analysis\n  and Machine Intelligence, 2024",
    "primary_category": "cs.CV"
  },
  "2106.14342v1": {
    "id": "http://arxiv.org/abs/2106.14342v1",
    "title": "Stabilizing Equilibrium Models by Jacobian Regularization",
    "authors": [
      "Shaojie Bai",
      "Vladlen Koltun",
      "J. Zico Kolter"
    ],
    "abstract": "Deep equilibrium networks (DEQs) are a new class of models that eschews\ntraditional depth in favor of finding the fixed point of a single nonlinear\nlayer. These models have been shown to achieve performance competitive with the\nstate-of-the-art deep networks while using significantly less memory. Yet they\nare also slower, brittle to architectural choices, and introduce potential\ninstability to the model. In this paper, we propose a regularization scheme for\nDEQ models that explicitly regularizes the Jacobian of the fixed-point update\nequations to stabilize the learning of equilibrium models. We show that this\nregularization adds only minimal computational cost, significantly stabilizes\nthe fixed-point convergence in both forward and backward passes, and scales\nwell to high-dimensional, realistic domains (e.g., WikiText-103 language\nmodeling and ImageNet classification). Using this method, we demonstrate, for\nthe first time, an implicit-depth model that runs with approximately the same\nspeed and level of performance as popular conventional deep networks such as\nResNet-101, while still maintaining the constant memory footprint and\narchitectural simplicity of DEQs. Code is available at\nhttps://github.com/locuslab/deq .",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2021-06-28T00:14:11+00:00",
    "updated": "2021-06-28T00:14:11+00:00",
    "doi": null,
    "comment": "ICML 2021 Short Oral",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2108.01390v5": {
    "id": "http://arxiv.org/abs/2108.01390v5",
    "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer",
    "authors": [
      "Yifan Xu",
      "Zhijie Zhang",
      "Mengdan Zhang",
      "Kekai Sheng",
      "Ke Li",
      "Weiming Dong",
      "Liqing Zhang",
      "Changsheng Xu",
      "Xing Sun"
    ],
    "abstract": "Vision transformers (ViTs) have recently received explosive popularity, but\nthe huge computational cost is still a severe issue. Since the computation\ncomplexity of ViT is quadratic with respect to the input sequence length, a\nmainstream paradigm for computation reduction is to reduce the number of\ntokens. Existing designs include structured spatial compression that uses a\nprogressive shrinking pyramid to reduce the computations of large feature maps,\nand unstructured token pruning that dynamically drops redundant tokens.\nHowever, the limitation of existing token pruning lies in two folds: 1) the\nincomplete spatial structure caused by pruning is not compatible with\nstructured spatial compression that is commonly used in modern deep-narrow\ntransformers; 2) it usually requires a time-consuming pre-training procedure.\nTo tackle the limitations and expand the applicable scenario of token pruning,\nwe present Evo-ViT, a self-motivated slow-fast token evolution approach for\nvision transformers. Specifically, we conduct unstructured instance-wise token\nselection by taking advantage of the simple and effective global class\nattention that is native to vision transformers. Then, we propose to update the\nselected informative tokens and uninformative tokens with different computation\npaths, namely, slow-fast updating. Since slow-fast updating mechanism maintains\nthe spatial structure and information flow, Evo-ViT can accelerate vanilla\ntransformers of both flat and deep-narrow structures from the very beginning of\nthe training process. Experimental results demonstrate that our method\nsignificantly reduces the computational cost of vision transformers while\nmaintaining comparable performance on image classification.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-08-03T09:56:07+00:00",
    "updated": "2021-12-06T15:28:49+00:00",
    "doi": null,
    "comment": "We propose a novel and effective design for dynamic vision\n  transformer to achieve better computational efficiency. The code is available\n  at https://github.com/YifanXu74/Evo-ViT",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.05595v2": {
    "id": "http://arxiv.org/abs/2404.05595v2",
    "title": "UniFL: Improve Stable Diffusion via Unified Feedback Learning",
    "authors": [
      "Jiacheng Zhang",
      "Jie Wu",
      "Yuxi Ren",
      "Xin Xia",
      "Huafeng Kuang",
      "Pan Xie",
      "Jiashi Li",
      "Xuefeng Xiao",
      "Min Zheng",
      "Lean Fu",
      "Guanbin Li"
    ],
    "abstract": "Diffusion models have revolutionized the field of image generation, leading\nto the proliferation of high-quality models and diverse downstream\napplications. However, despite these significant advancements, the current\ncompetitive solutions still suffer from several limitations, including inferior\nvisual quality, a lack of aesthetic appeal, and inefficient inference, without\na comprehensive solution in sight. To address these challenges, we present\nUniFL, a unified framework that leverages feedback learning to enhance\ndiffusion models comprehensively. UniFL stands out as a universal, effective,\nand generalizable solution applicable to various diffusion models, such as\nSD1.5 and SDXL. Notably, UniFL incorporates three key components: perceptual\nfeedback learning, which enhances visual quality; decoupled feedback learning,\nwhich improves aesthetic appeal; and adversarial feedback learning, which\noptimizes inference speed. In-depth experiments and extensive user studies\nvalidate the superior performance of our proposed method in enhancing both the\nquality of generated models and their acceleration. For instance, UniFL\nsurpasses ImageReward by 17% user preference in terms of generation quality and\noutperforms LCM and SDXL Turbo by 57% and 20% in 4-step inference. Moreover, we\nhave verified the efficacy of our approach in downstream tasks, including Lora,\nControlNet, and AnimateDiff.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-08T15:14:20+00:00",
    "updated": "2024-05-22T13:52:09+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2305.12827v3": {
    "id": "http://arxiv.org/abs/2305.12827v3",
    "title": "Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models",
    "authors": [
      "Guillermo Ortiz-Jimenez",
      "Alessandro Favero",
      "Pascal Frossard"
    ],
    "abstract": "Task arithmetic has recently emerged as a cost-effective and scalable\napproach to edit pre-trained models directly in weight space: By adding the\nfine-tuned weights of different tasks, the model's performance can be improved\non these tasks, while negating them leads to task forgetting. Yet, our\nunderstanding of the effectiveness of task arithmetic and its underlying\nprinciples remains limited. We present a comprehensive study of task arithmetic\nin vision-language models and show that weight disentanglement is the crucial\nfactor that makes it effective. This property arises during pre-training and\nmanifests when distinct directions in weight space govern separate, localized\nregions in function space associated with the tasks. Notably, we show that\nfine-tuning models in their tangent space by linearizing them amplifies weight\ndisentanglement. This leads to substantial performance improvements across\nmultiple task arithmetic benchmarks and diverse models. Building on these\nfindings, we provide theoretical and empirical analyses of the neural tangent\nkernel (NTK) of these models and establish a compelling link between task\narithmetic and the spatial localization of the NTK eigenfunctions. Overall, our\nwork uncovers novel insights into the fundamental mechanisms of task arithmetic\nand offers a more reliable and effective approach to edit pre-trained models\nthrough the NTK linearization.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2023-05-22T08:39:25+00:00",
    "updated": "2023-11-21T18:43:43+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": "Advances in Neural Information Processing Systems 36 (NeurIPS\n  2023)",
    "primary_category": "cs.LG"
  },
  "2404.00301v1": {
    "id": "http://arxiv.org/abs/2404.00301v1",
    "title": "Monocular Identity-Conditioned Facial Reflectance Reconstruction",
    "authors": [
      "Xingyu Ren",
      "Jiankang Deng",
      "Yuhao Cheng",
      "Jia Guo",
      "Chao Ma",
      "Yichao Yan",
      "Wenhan Zhu",
      "Xiaokang Yang"
    ],
    "abstract": "Recent 3D face reconstruction methods have made remarkable advancements, yet\nthere remain huge challenges in monocular high-quality facial reflectance\nreconstruction. Existing methods rely on a large amount of light-stage captured\ndata to learn facial reflectance models. However, the lack of subject diversity\nposes challenges in achieving good generalization and widespread applicability.\nIn this paper, we learn the reflectance prior in image space rather than UV\nspace and present a framework named ID2Reflectance. Our framework can directly\nestimate the reflectance maps of a single image while using limited reflectance\ndata for training. Our key insight is that reflectance data shares facial\nstructures with RGB faces, which enables obtaining expressive facial prior from\ninexpensive RGB data thus reducing the dependency on reflectance data. We first\nlearn a high-quality prior for facial reflectance. Specifically, we pretrain\nmulti-domain facial feature codebooks and design a codebook fusion method to\nalign the reflectance and RGB domains. Then, we propose an identity-conditioned\nswapping module that injects facial identity from the target image into the\npre-trained autoencoder to modify the identity of the source reflectance image.\nFinally, we stitch multi-view swapped reflectance images to obtain renderable\nassets. Extensive experiments demonstrate that our method exhibits excellent\ngeneralization capability and achieves state-of-the-art facial reflectance\nreconstruction results for in-the-wild faces. Our project page is\nhttps://xingyuren.github.io/id2reflectance/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-30T09:43:40+00:00",
    "updated": "2024-03-30T09:43:40+00:00",
    "doi": null,
    "comment": "Accepted by CVPR 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1802.09787v1": {
    "id": "http://arxiv.org/abs/1802.09787v1",
    "title": "Relational Reasoning for Markov Chains in a Probabilistic Guarded Lambda Calculus",
    "authors": [
      "Alejandro Aguirre",
      "Gilles Barthe",
      "Lars Birkedal",
      "Ale\u0161 Bizjak",
      "Marco Gaboardi",
      "Deepak Garg"
    ],
    "abstract": "We extend the simply-typed guarded $\\lambda$-calculus with discrete\nprobabilities and endow it with a program logic for reasoning about relational\nproperties of guarded probabilistic computations. This provides a framework for\nprogramming and reasoning about infinite stochastic processes like Markov\nchains. We demonstrate the logic sound by interpreting its judgements in the\ntopos of trees and by using probabilistic couplings for the semantics of\nrelational assertions over distributions on discrete types.\n  The program logic is designed to support syntax-directed proofs in the style\nof relational refinement types, but retains the expressiveness of higher-order\nlogic extended with discrete distributions, and the ability to reason\nrelationally about expressions that have different types or syntactic\nstructure. In addition, our proof system leverages a well-known theorem from\nthe coupling literature to justify better proof rules for relational reasoning\nabout probabilistic expressions. We illustrate these benefits with a broad\nrange of examples that were beyond the scope of previous systems, including\nshift couplings and lump couplings between random walks.",
    "categories": [
      "cs.PL"
    ],
    "published": "2018-02-27T09:19:18+00:00",
    "updated": "2018-02-27T09:19:18+00:00",
    "doi": null,
    "comment": "To appear at ESOP '18 (Extended version with appendix)",
    "journal_ref": null,
    "primary_category": "cs.PL"
  },
  "1804.00823v4": {
    "id": "http://arxiv.org/abs/1804.00823v4",
    "title": "Graph2Seq: Graph to Sequence Learning with Attention-based Neural Networks",
    "authors": [
      "Kun Xu",
      "Lingfei Wu",
      "Zhiguo Wang",
      "Yansong Feng",
      "Michael Witbrock",
      "Vadim Sheinin"
    ],
    "abstract": "The celebrated Sequence to Sequence learning (Seq2Seq) technique and its\nnumerous variants achieve excellent performance on many tasks. However, many\nmachine learning tasks have inputs naturally represented as graphs; existing\nSeq2Seq models face a significant challenge in achieving accurate conversion\nfrom graph form to the appropriate sequence. To address this challenge, we\nintroduce a novel general end-to-end graph-to-sequence neural encoder-decoder\nmodel that maps an input graph to a sequence of vectors and uses an\nattention-based LSTM method to decode the target sequence from these vectors.\nOur method first generates the node and graph embeddings using an improved\ngraph-based neural network with a novel aggregation strategy to incorporate\nedge direction information in the node embeddings. We further introduce an\nattention mechanism that aligns node embeddings and the decoding sequence to\nbetter cope with large graphs. Experimental results on bAbI, Shortest Path, and\nNatural Language Generation tasks demonstrate that our model achieves\nstate-of-the-art performance and significantly outperforms existing graph\nneural networks, Seq2Seq, and Tree2Seq models; using the proposed\nbi-directional node embedding aggregation strategy, the model can converge\nrapidly to the optimal performance.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2018-04-03T04:47:22+00:00",
    "updated": "2018-12-03T16:43:37+00:00",
    "doi": null,
    "comment": "16 pages, 3 figures, 4 tables",
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2004.04727v3": {
    "id": "http://arxiv.org/abs/2004.04727v3",
    "title": "3D Photography using Context-aware Layered Depth Inpainting",
    "authors": [
      "Meng-Li Shih",
      "Shih-Yang Su",
      "Johannes Kopf",
      "Jia-Bin Huang"
    ],
    "abstract": "We propose a method for converting a single RGB-D input image into a 3D photo\n- a multi-layer representation for novel view synthesis that contains\nhallucinated color and depth structures in regions occluded in the original\nview. We use a Layered Depth Image with explicit pixel connectivity as\nunderlying representation, and present a learning-based inpainting model that\nsynthesizes new local color-and-depth content into the occluded region in a\nspatial context-aware manner. The resulting 3D photos can be efficiently\nrendered with motion parallax using standard graphics engines. We validate the\neffectiveness of our method on a wide range of challenging everyday scenes and\nshow fewer artifacts compared with the state of the arts.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "published": "2020-04-09T17:59:06+00:00",
    "updated": "2020-06-10T14:21:03+00:00",
    "doi": null,
    "comment": "CVPR 2020. Project page:\n  https://shihmengli.github.io/3D-Photo-Inpainting/ Code:\n  https://github.com/vt-vl-lab/3d-photo-inpainting Demo:\n  https://colab.research.google.com/drive/1706ToQrkIZshRSJSHvZ1RuCiM__YX3Bz",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2104.08821v4": {
    "id": "http://arxiv.org/abs/2104.08821v4",
    "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
    "authors": [
      "Tianyu Gao",
      "Xingcheng Yao",
      "Danqi Chen"
    ],
    "abstract": "This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We find that dropout acts as minimal data augmentation, and\nremoving it leads to a representation collapse. Then, we propose a supervised\napproach, which incorporates annotated pairs from natural language inference\ndatasets into our contrastive learning framework by using \"entailment\" pairs as\npositives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on\nstandard semantic textual similarity (STS) tasks, and our unsupervised and\nsupervised models using BERT base achieve an average of 76.3% and 81.6%\nSpearman's correlation respectively, a 4.2% and 2.2% improvement compared to\nthe previous best results. We also show -- both theoretically and empirically\n-- that the contrastive learning objective regularizes pre-trained embeddings'\nanisotropic space to be more uniform, and it better aligns positive pairs when\nsupervised signals are available.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2021-04-18T11:27:08+00:00",
    "updated": "2022-05-18T12:29:49+00:00",
    "doi": null,
    "comment": "Accepted to EMNLP 2021. The code and pre-trained models are available\n  at https://github.com/princeton-nlp/simcse",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2403.12063v2": {
    "id": "http://arxiv.org/abs/2403.12063v2",
    "title": "Consistency Model is an Effective Posterior Sample Approximation for Diffusion Inverse Solvers",
    "authors": [
      "Tongda Xu",
      "Ziran Zhu",
      "Jian Li",
      "Dailan He",
      "Yuanyuan Wang",
      "Ming Sun",
      "Ling Li",
      "Hongwei Qin",
      "Yan Wang",
      "Jingjing Liu",
      "Ya-Qin Zhang"
    ],
    "abstract": "Diffusion Inverse Solvers (DIS) are designed to sample from the conditional\ndistribution $p_{\\theta}(X_0|y)$, with a predefined diffusion model\n$p_{\\theta}(X_0)$, an operator $f(\\cdot)$, and a measurement $y=f(x'_0)$\nderived from an unknown image $x'_0$. Existing DIS estimate the conditional\nscore function by evaluating $f(\\cdot)$ with an approximated posterior sample\ndrawn from $p_{\\theta}(X_0|X_t)$. However, most prior approximations rely on\nthe posterior means, which may not lie in the support of the image\ndistribution, thereby potentially diverge from the appearance of genuine\nimages. Such out-of-support samples may significantly degrade the performance\nof the operator $f(\\cdot)$, particularly when it is a neural network. In this\npaper, we introduces a novel approach for posterior approximation that\nguarantees to generate valid samples within the support of the image\ndistribution, and also enhances the compatibility with neural network-based\noperators $f(\\cdot)$. We first demonstrate that the solution of the Probability\nFlow Ordinary Differential Equation (PF-ODE) with an initial value $x_t$ yields\nan effective posterior sample $p_{\\theta}(X_0|X_t=x_t)$. Based on this\nobservation, we adopt the Consistency Model (CM), which is distilled from\nPF-ODE, for posterior sampling. Furthermore, we design a novel family of DIS\nusing only CM. Through extensive experiments, we show that our proposed method\nfor posterior sample approximation substantially enhance the effectiveness of\nDIS for neural network operators $f(\\cdot)$ (e.g., in semantic segmentation).\nAdditionally, our experiments demonstrate the effectiveness of the new CM-based\ninversion techniques. The source code is provided in the supplementary\nmaterial.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-02-09T02:23:47+00:00",
    "updated": "2024-06-01T10:54:50+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2406.11838v2": {
    "id": "http://arxiv.org/abs/2406.11838v2",
    "title": "Autoregressive Image Generation without Vector Quantization",
    "authors": [
      "Tianhong Li",
      "Yonglong Tian",
      "He Li",
      "Mingyang Deng",
      "Kaiming He"
    ],
    "abstract": "Conventional wisdom holds that autoregressive models for image generation are\ntypically accompanied by vector-quantized tokens. We observe that while a\ndiscrete-valued space can facilitate representing a categorical distribution,\nit is not a necessity for autoregressive modeling. In this work, we propose to\nmodel the per-token probability distribution using a diffusion procedure, which\nallows us to apply autoregressive models in a continuous-valued space. Rather\nthan using categorical cross-entropy loss, we define a Diffusion Loss function\nto model the per-token probability. This approach eliminates the need for\ndiscrete-valued tokenizers. We evaluate its effectiveness across a wide range\nof cases, including standard autoregressive models and generalized masked\nautoregressive (MAR) variants. By removing vector quantization, our image\ngenerator achieves strong results while enjoying the speed advantage of\nsequence modeling. We hope this work will motivate the use of autoregressive\ngeneration in other continuous-valued domains and applications. Code is\navailable at: https://github.com/LTH14/mar",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-06-17T17:59:58+00:00",
    "updated": "2024-07-28T05:30:08+00:00",
    "doi": null,
    "comment": "Tech report. Code: https://github.com/LTH14/mar",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2303.13076v1": {
    "id": "http://arxiv.org/abs/2303.13076v1",
    "title": "CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching",
    "authors": [
      "Xiaoshi Wu",
      "Feng Zhu",
      "Rui Zhao",
      "Hongsheng Li"
    ],
    "abstract": "Open-vocabulary detection (OVD) is an object detection task aiming at\ndetecting objects from novel categories beyond the base categories on which the\ndetector is trained. Recent OVD methods rely on large-scale visual-language\npre-trained models, such as CLIP, for recognizing novel objects. We identify\nthe two core obstacles that need to be tackled when incorporating these models\ninto detector training: (1) the distribution mismatch that happens when\napplying a VL-model trained on whole images to region recognition tasks; (2)\nthe difficulty of localizing objects of unseen classes. To overcome these\nobstacles, we propose CORA, a DETR-style framework that adapts CLIP for\nOpen-vocabulary detection by Region prompting and Anchor pre-matching. Region\nprompting mitigates the whole-to-region distribution gap by prompting the\nregion features of the CLIP-based region classifier. Anchor pre-matching helps\nlearning generalizable object localization by a class-aware matching mechanism.\nWe evaluate CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel\nclasses, which outperforms the previous SOTA by 2.4 AP50 even without resorting\nto extra training data. When extra training data is available, we train\nCORA$^+$ on both ground-truth base-category annotations and additional pseudo\nbounding box labels computed by CORA. CORA$^+$ achieves 43.1 AP50 on the COCO\nOVD benchmark and 28.1 box APr on the LVIS OVD benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-03-23T07:13:57+00:00",
    "updated": "2023-03-23T07:13:57+00:00",
    "doi": null,
    "comment": "11 pages, 4 figures. Accepted by CVPR 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.09195v1": {
    "id": "http://arxiv.org/abs/2401.09195v1",
    "title": "Training-Free Semantic Video Composition via Pre-trained Diffusion Model",
    "authors": [
      "Jiaqi Guo",
      "Sitong Su",
      "Junchen Zhu",
      "Lianli Gao",
      "Jingkuan Song"
    ],
    "abstract": "The video composition task aims to integrate specified foregrounds and\nbackgrounds from different videos into a harmonious composite. Current\napproaches, predominantly trained on videos with adjusted foreground color and\nlighting, struggle to address deep semantic disparities beyond superficial\nadjustments, such as domain gaps. Therefore, we propose a training-free\npipeline employing a pre-trained diffusion model imbued with semantic prior\nknowledge, which can process composite videos with broader semantic\ndisparities. Specifically, we process the video frames in a cascading manner\nand handle each frame in two processes with the diffusion model. In the\ninversion process, we propose Balanced Partial Inversion to obtain generation\ninitial points that balance reversibility and modifiability. Then, in the\ngeneration process, we further propose Inter-Frame Augmented attention to\naugment foreground continuity across frames. Experimental results reveal that\nour pipeline successfully ensures the visual harmony and inter-frame coherence\nof the outputs, demonstrating efficacy in managing broader semantic\ndisparities.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-01-17T13:07:22+00:00",
    "updated": "2024-01-17T13:07:22+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1703.01925v1": {
    "id": "http://arxiv.org/abs/1703.01925v1",
    "title": "Grammar Variational Autoencoder",
    "authors": [
      "Matt J. Kusner",
      "Brooks Paige",
      "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
    ],
    "abstract": "Deep generative models have been wildly successful at learning coherent\nlatent representations for continuous data such as video and audio. However,\ngenerative modeling of discrete data such as arithmetic expressions and\nmolecular structures still poses significant challenges. Crucially,\nstate-of-the-art methods often produce outputs that are not valid. We make the\nkey observation that frequently, discrete data can be represented as a parse\ntree from a context-free grammar. We propose a variational autoencoder which\nencodes and decodes directly to and from these parse trees, ensuring the\ngenerated outputs are always valid. Surprisingly, we show that not only does\nour model more often generate valid outputs, it also learns a more coherent\nlatent space in which nearby points decode to similar discrete outputs. We\ndemonstrate the effectiveness of our learned models by showing their improved\nperformance in Bayesian optimization for symbolic regression and molecular\nsynthesis.",
    "categories": [
      "stat.ML"
    ],
    "published": "2017-03-06T15:36:37+00:00",
    "updated": "2017-03-06T15:36:37+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2403.06775v1": {
    "id": "http://arxiv.org/abs/2403.06775v1",
    "title": "FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation",
    "authors": [
      "Pengchong Qiao",
      "Lei Shang",
      "Chang Liu",
      "Baigui Sun",
      "Xiangyang Ji",
      "Jie Chen"
    ],
    "abstract": "Subject-driven generation has garnered significant interest recently due to\nits ability to personalize text-to-image generation. Typical works focus on\nlearning the new subject's private attributes. However, an important fact has\nnot been taken seriously that a subject is not an isolated new concept but\nshould be a specialization of a certain category in the pre-trained model. This\nresults in the subject failing to comprehensively inherit the attributes in its\ncategory, causing poor attribute-related generations. In this paper, motivated\nby object-oriented programming, we model the subject as a derived class whose\nbase class is its semantic category. This modeling enables the subject to\ninherit public attributes from its category while learning its private\nattributes from the user-provided example. Specifically, we propose a\nplug-and-play method, Subject-Derived regularization (SuDe). It constructs the\nbase-derived class modeling by constraining the subject-driven generated images\nto semantically belong to the subject's category. Extensive experiments under\nthree baselines and two backbones on various subjects show that our SuDe\nenables imaginative attribute-related generations while maintaining subject\nfidelity. Codes will be open sourced soon at FaceChain\n(https://github.com/modelscope/facechain).",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-11T14:43:40+00:00",
    "updated": "2024-03-11T14:43:40+00:00",
    "doi": null,
    "comment": "accepted by CVPR2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1809.03839v3": {
    "id": "http://arxiv.org/abs/1809.03839v3",
    "title": "Unsupervised Domain Adaptation Based on Source-guided Discrepancy",
    "authors": [
      "Seiichi Kuroki",
      "Nontawat Charoenphakdee",
      "Han Bao",
      "Junya Honda",
      "Issei Sato",
      "Masashi Sugiyama"
    ],
    "abstract": "Unsupervised domain adaptation is the problem setting where data generating\ndistributions in the source and target domains are different, and labels in the\ntarget domain are unavailable. One important question in unsupervised domain\nadaptation is how to measure the difference between the source and target\ndomains. A previously proposed discrepancy that does not use the source domain\nlabels requires high computational cost to estimate and may lead to a loose\ngeneralization error bound in the target domain. To mitigate these problems, we\npropose a novel discrepancy called source-guided discrepancy (S-disc), which\nexploits labels in the source domain. As a consequence, S-disc can be computed\nefficiently with a finite sample convergence guarantee. In addition, we show\nthat S-disc can provide a tighter generalization error bound than the one based\non an existing discrepancy. Finally, we report experimental results that\ndemonstrate the advantages of S-disc over the existing discrepancies.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2018-09-11T13:11:30+00:00",
    "updated": "2018-11-19T09:54:32+00:00",
    "doi": null,
    "comment": "To appear in AAAI-19",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2203.09067v1": {
    "id": "http://arxiv.org/abs/2203.09067v1",
    "title": "UNIMO-2: End-to-End Unified Vision-Language Grounded Learning",
    "authors": [
      "Wei Li",
      "Can Gao",
      "Guocheng Niu",
      "Xinyan Xiao",
      "Hao Liu",
      "Jiachen Liu",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "abstract": "Vision-Language Pre-training (VLP) has achieved impressive performance on\nvarious cross-modal downstream tasks. However, most existing methods can only\nlearn from aligned image-caption data and rely heavily on expensive regional\nfeatures, which greatly limits their scalability and performance. In this\npaper, we propose an end-to-end unified-modal pre-training framework, namely\nUNIMO-2, for joint learning on both aligned image-caption data and unaligned\nimage-only and text-only corpus. We build a unified Transformer model to\njointly learn visual representations, textual representations and semantic\nalignment between images and texts. In particular, we propose to conduct\ngrounded learning on both images and texts via a sharing grounded space, which\nhelps bridge unaligned images and texts, and align the visual and textual\nsemantic spaces on different types of corpora. The experiments show that our\ngrounded learning method can improve textual and visual semantic alignment for\nimproving performance on various cross-modal tasks. Moreover, benefiting from\neffective joint modeling of different types of corpora, our model also achieves\nimpressive performance on single-modal visual and textual tasks. Our code and\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "published": "2022-03-17T03:53:11+00:00",
    "updated": "2022-03-17T03:53:11+00:00",
    "doi": null,
    "comment": "Accepted by ACL2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2205.11239v2": {
    "id": "http://arxiv.org/abs/2205.11239v2",
    "title": "Vision Transformer: Vit and its Derivatives",
    "authors": [
      "Zujun Fu"
    ],
    "abstract": "Transformer, an attention-based encoder-decoder architecture, has not only\nrevolutionized the field of natural language processing (NLP), but has also\ndone some pioneering work in the field of computer vision (CV). Compared to\nconvolutional neural networks (CNNs), the Vision Transformer (ViT) relies on\nexcellent modeling capabilities to achieve very good performance on several\nbenchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the\nself-attention mechanism in natural language processing, where word embeddings\nare replaced with patch embeddings.\n  This paper reviews the derivatives of ViT and the cross-applications of ViT\nwith other fields.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-05-12T14:02:39+00:00",
    "updated": "2022-05-24T14:08:01+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1704.01212v2": {
    "id": "http://arxiv.org/abs/1704.01212v2",
    "title": "Neural Message Passing for Quantum Chemistry",
    "authors": [
      "Justin Gilmer",
      "Samuel S. Schoenholz",
      "Patrick F. Riley",
      "Oriol Vinyals",
      "George E. Dahl"
    ],
    "abstract": "Supervised learning on molecules has incredible potential to be useful in\nchemistry, drug discovery, and materials science. Luckily, several promising\nand closely related neural network models invariant to molecular symmetries\nhave already been described in the literature. These models learn a message\npassing algorithm and aggregation procedure to compute a function of their\nentire input graph. At this point, the next step is to find a particularly\neffective variant of this general approach and apply it to chemical prediction\nbenchmarks until we either solve them or reach the limits of the approach. In\nthis paper, we reformulate existing models into a single common framework we\ncall Message Passing Neural Networks (MPNNs) and explore additional novel\nvariations within this framework. Using MPNNs we demonstrate state of the art\nresults on an important molecular property prediction benchmark; these results\nare strong enough that we believe future work should focus on datasets with\nlarger molecules or more accurate ground truth labels.",
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2017-04-04T23:00:44+00:00",
    "updated": "2017-06-12T20:52:56+00:00",
    "doi": null,
    "comment": "14 pages",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2304.11265v2": {
    "id": "http://arxiv.org/abs/2304.11265v2",
    "title": "Time Series Classification for Detecting Parkinson's Disease from Wrist Motions",
    "authors": [
      "Cedric Doni\u00e9",
      "Neha Das",
      "Satoshi Endo",
      "Sandra Hirche"
    ],
    "abstract": "Parkinson's disease (PD) is a neurodegenerative condition characterized by\nfrequently changing motor symptoms, necessitating continuous symptom monitoring\nfor more targeted treatment. Classical time series classification and deep\nlearning techniques have demonstrated limited efficacy in monitoring PD\nsymptoms using wearable accelerometer data due to complex PD movement patterns\nand the small size of available datasets. We investigate InceptionTime and\nRandOm Convolutional KErnel Transform (ROCKET) as they are promising for PD\nsymptom monitoring, with InceptionTime's high learning capacity being\nwell-suited to modeling complex movement patterns while ROCKET is suited to\nsmall datasets. With random search methodology, we identify the highest-scoring\nInceptionTime architecture and compare its performance to ROCKET with a ridge\nclassifier and a multi-layer perceptron (MLP) on wrist motion data from PD\npatients. Our findings indicate that all approaches are suitable for estimating\ntremor severity and bradykinesia presence but encounter challenges in detecting\ndyskinesia. ROCKET demonstrates superior performance in identifying dyskinesia,\nwhereas InceptionTime exhibits slightly better performance in tremor and\nbradykinesia detection. Notably, both methods outperform the multi-layer\nperceptron. In conclusion, InceptionTime exhibits the capability to classify\ncomplex wrist motion time series and holds the greatest potential for\ncontinuous symptom monitoring in PD.",
    "categories": [
      "cs.LG",
      "I.5; J.2; J.3"
    ],
    "published": "2023-04-21T22:38:44+00:00",
    "updated": "2024-05-20T20:59:05+00:00",
    "doi": null,
    "comment": "The source code is available under\n  https://github.com/cedricdonie/tsc-for-wrist-motion-pd-detection",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2202.09206v1": {
    "id": "http://arxiv.org/abs/2202.09206v1",
    "title": "Spatio-Temporal Outdoor Lighting Aggregation on Image Sequences using Transformer Networks",
    "authors": [
      "Haebom Lee",
      "Christian Homeyer",
      "Robert Herzog",
      "Jan Rexilius",
      "Carsten Rother"
    ],
    "abstract": "In this work, we focus on outdoor lighting estimation by aggregating\nindividual noisy estimates from images, exploiting the rich image information\nfrom wide-angle cameras and/or temporal image sequences. Photographs inherently\nencode information about the scene's lighting in the form of shading and\nshadows. Recovering the lighting is an inverse rendering problem and as that\nill-posed. Recent work based on deep neural networks has shown promising\nresults for single image lighting estimation, but suffers from robustness. We\ntackle this problem by combining lighting estimates from several image views\nsampled in the angular and temporal domain of an image sequence. For this task,\nwe introduce a transformer architecture that is trained in an end-2-end fashion\nwithout any statistical post-processing as required by previous work. Thereby,\nwe propose a positional encoding that takes into account the camera calibration\nand ego-motion estimation to globally register the individual estimates when\ncomputing attention between visual words. We show that our method leads to\nimproved lighting estimation while requiring less hyper-parameters compared to\nthe state-of-the-art.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-02-18T14:11:16+00:00",
    "updated": "2022-02-18T14:11:16+00:00",
    "doi": null,
    "comment": "11 pages, 7 figures, 1 table, currently under a review process",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2402.16607v2": {
    "id": "http://arxiv.org/abs/2402.16607v2",
    "title": "GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos",
    "authors": [
      "Xinqi Liu",
      "Chenming Wu",
      "Jialun Liu",
      "Xing Liu",
      "Jinbo Wu",
      "Chen Zhao",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "abstract": "In this paper, we present a novel method that facilitates the creation of\nvivid 3D Gaussian avatars from monocular video inputs (GVA). Our innovation\nlies in addressing the intricate challenges of delivering high-fidelity human\nbody reconstructions and aligning 3D Gaussians with human skin surfaces\naccurately. The key contributions of this paper are twofold. Firstly, we\nintroduce a pose refinement technique to improve hand and foot pose accuracy by\naligning normal maps and silhouettes. Precise pose is crucial for correct shape\nand appearance reconstruction. Secondly, we address the problems of unbalanced\naggregation and initialization bias that previously diminished the quality of\n3D Gaussian avatars, through a novel surface-guided re-initialization method\nthat ensures accurate alignment of 3D Gaussian points with avatar surfaces.\nExperimental results demonstrate that our proposed method achieves\nhigh-fidelity and vivid 3D Gaussian avatar reconstruction. Extensive\nexperimental analyses validate the performance qualitatively and\nquantitatively, demonstrating that it achieves state-of-the-art performance in\nphoto-realistic novel view synthesis while offering fine-grained control over\nthe human body and hand pose. Project page: https://3d-aigc.github.io/GVA/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-02-26T14:40:15+00:00",
    "updated": "2024-03-19T08:58:17+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2204.07316v3": {
    "id": "http://arxiv.org/abs/2204.07316v3",
    "title": "XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding",
    "authors": [
      "Chan-Jan Hsu",
      "Hung-yi Lee",
      "Yu Tsao"
    ],
    "abstract": "Transformer-based models are widely used in natural language understanding\n(NLU) tasks, and multimodal transformers have been effective in visual-language\ntasks. This study explores distilling visual information from pretrained\nmultimodal transformers to pretrained language encoders. Our framework is\ninspired by cross-modal encoders' success in visual-language tasks while we\nalter the learning objective to cater to the language-heavy characteristics of\nNLU. After training with a small number of extra adapting steps and finetuned,\nthe proposed XDBERT (cross-modal distilled BERT) outperforms pretrained-BERT in\ngeneral language understanding evaluation (GLUE), situations with adversarial\ngenerations (SWAG) benchmarks, and readability benchmarks. We analyze the\nperformance of XDBERT on GLUE to show that the improvement is likely visually\ngrounded.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2022-04-15T03:44:00+00:00",
    "updated": "2022-05-03T00:33:48+00:00",
    "doi": null,
    "comment": "ACL 2022",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1805.07036v1": {
    "id": "http://arxiv.org/abs/1805.07036v1",
    "title": "LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation",
    "authors": [
      "Tak-Wai Hui",
      "Xiaoou Tang",
      "Chen Change Loy"
    ],
    "abstract": "FlowNet2, the state-of-the-art convolutional neural network (CNN) for optical\nflow estimation, requires over 160M parameters to achieve accurate flow\nestimation. In this paper we present an alternative network that outperforms\nFlowNet2 on the challenging Sintel final pass and KITTI benchmarks, while being\n30 times smaller in the model size and 1.36 times faster in the running speed.\nThis is made possible by drilling down to architectural details that might have\nbeen missed in the current frameworks: (1) We present a more effective flow\ninference approach at each pyramid level through a lightweight cascaded\nnetwork. It not only improves flow estimation accuracy through early\ncorrection, but also permits seamless incorporation of descriptor matching in\nour network. (2) We present a novel flow regularization layer to ameliorate the\nissue of outliers and vague flow boundaries by using a feature-driven local\nconvolution. (3) Our network owns an effective structure for pyramidal feature\nextraction and embraces feature warping rather than image warping as practiced\nin FlowNet2. Our code and trained models are available at\nhttps://github.com/twhui/LiteFlowNet .",
    "categories": [
      "cs.CV"
    ],
    "published": "2018-05-18T03:29:54+00:00",
    "updated": "2018-05-18T03:29:54+00:00",
    "doi": null,
    "comment": "Accepted to CVPR 2018 (spotlight). Project page:\n  http://mmlab.ie.cuhk.edu.hk/projects/LiteFlowNet/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2008.05515v1": {
    "id": "http://arxiv.org/abs/2008.05515v1",
    "title": "Synergy between Machine/Deep Learning and Software Engineering: How Far Are We?",
    "authors": [
      "Simin Wang",
      "Liguo Huang",
      "Jidong Ge",
      "Tengfei Zhang",
      "Haitao Feng",
      "Ming Li",
      "He Zhang",
      "Vincent Ng"
    ],
    "abstract": "Since 2009, the deep learning revolution, which was triggered by the\nintroduction of ImageNet, has stimulated the synergy between Machine Learning\n(ML)/Deep Learning (DL) and Software Engineering (SE). Meanwhile, critical\nreviews have emerged that suggest that ML/DL should be used cautiously. To\nimprove the quality (especially the applicability and generalizability) of\nML/DL-related SE studies, and to stimulate and enhance future collaborations\nbetween SE/AI researchers and industry practitioners, we conducted a 10-year\nSystematic Literature Review (SLR) on 906 ML/DL-related SE papers published\nbetween 2009 and 2018. Our trend analysis demonstrated the mutual impacts that\nML/DL and SE have had on each other. At the same time, however, we also\nobserved a paucity of replicable and reproducible ML/DL-related SE studies and\nidentified five factors that influence their replicability and reproducibility.\nTo improve the applicability and generalizability of research results, we\nanalyzed what ingredients in a study would facilitate an understanding of why a\nML/DL technique was selected for a specific SE problem. In addition, we\nidentified the unique trends of impacts of DL models on SE tasks, as well as\nfive unique challenges that needed to be met in order to better leverage DL to\nimprove the productivity of SE tasks. Finally, we outlined a road-map that we\nbelieve can facilitate the transfer of ML/DL-based SE research results into\nreal-world industry practices.",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2020-08-12T18:19:30+00:00",
    "updated": "2020-08-12T18:19:30+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "2401.09794v1": {
    "id": "http://arxiv.org/abs/2401.09794v1",
    "title": "Wavelet-Guided Acceleration of Text Inversion in Diffusion-Based Image Editing",
    "authors": [
      "Gwanhyeong Koo",
      "Sunjae Yoon",
      "Chang D. Yoo"
    ],
    "abstract": "In the field of image editing, Null-text Inversion (NTI) enables fine-grained\nediting while preserving the structure of the original image by optimizing null\nembeddings during the DDIM sampling process. However, the NTI process is\ntime-consuming, taking more than two minutes per image. To address this, we\nintroduce an innovative method that maintains the principles of the NTI while\naccelerating the image editing process. We propose the WaveOpt-Estimator, which\ndetermines the text optimization endpoint based on frequency characteristics.\nUtilizing wavelet transform analysis to identify the image's frequency\ncharacteristics, we can limit text optimization to specific timesteps during\nthe DDIM sampling process. By adopting the Negative-Prompt Inversion (NPI)\nconcept, a target prompt representing the original image serves as the initial\ntext value for optimization. This approach maintains performance comparable to\nNTI while reducing the average editing time by over 80% compared to the NTI\nmethod. Our method presents a promising approach for efficient, high-quality\nimage editing based on diffusion models.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-01-18T08:26:37+00:00",
    "updated": "2024-01-18T08:26:37+00:00",
    "doi": null,
    "comment": "The International Conference on Acoustics, Speech, & Signal\n  Processing (ICASSP) 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.03462v3": {
    "id": "http://arxiv.org/abs/2401.03462v3",
    "title": "Long Context Compression with Activation Beacon",
    "authors": [
      "Peitian Zhang",
      "Zheng Liu",
      "Shitao Xiao",
      "Ninglu Shao",
      "Qiwei Ye",
      "Zhicheng Dou"
    ],
    "abstract": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-07T11:57:40+00:00",
    "updated": "2024-10-11T02:18:24+00:00",
    "doi": null,
    "comment": "Newer version of Activation Beacon",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2206.05737v2": {
    "id": "http://arxiv.org/abs/2206.05737v2",
    "title": "SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse Views",
    "authors": [
      "Xiaoxiao Long",
      "Cheng Lin",
      "Peng Wang",
      "Taku Komura",
      "Wenping Wang"
    ],
    "abstract": "We introduce SparseNeuS, a novel neural rendering based method for the task\nof surface reconstruction from multi-view images. This task becomes more\ndifficult when only sparse images are provided as input, a scenario where\nexisting neural reconstruction approaches usually produce incomplete or\ndistorted results. Moreover, their inability of generalizing to unseen new\nscenes impedes their application in practice. Contrarily, SparseNeuS can\ngeneralize to new scenes and work well with sparse images (as few as 2 or 3).\nSparseNeuS adopts signed distance function (SDF) as the surface representation,\nand learns generalizable priors from image features by introducing geometry\nencoding volumes for generic surface prediction. Moreover, several strategies\nare introduced to effectively leverage sparse views for high-quality\nreconstruction, including 1) a multi-level geometry reasoning framework to\nrecover the surfaces in a coarse-to-fine manner; 2) a multi-scale color\nblending scheme for more reliable color prediction; 3) a consistency-aware\nfine-tuning scheme to control the inconsistent regions caused by occlusion and\nnoise. Extensive experiments demonstrate that our approach not only outperforms\nthe state-of-the-art methods, but also exhibits good efficiency,\ngeneralizability, and flexibility.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-06-12T13:34:03+00:00",
    "updated": "2022-08-02T10:53:02+00:00",
    "doi": null,
    "comment": "Project page: https://www.xxlong.site/SparseNeuS/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2108.12409v2": {
    "id": "http://arxiv.org/abs/2108.12409v2",
    "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
    "authors": [
      "Ofir Press",
      "Noah A. Smith",
      "Mike Lewis"
    ],
    "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a\nfundamental question has yet to be answered: how does a model achieve\nextrapolation at inference time for sequences that are longer than it saw\nduring training? We first show that extrapolation can be enabled by simply\nchanging the position representation method, though we find that current\nmethods do not allow for efficient extrapolation. We therefore introduce a\nsimpler and more efficient position method, Attention with Linear Biases\n(ALiBi). ALiBi does not add positional embeddings to word embeddings; instead,\nit biases query-key attention scores with a penalty that is proportional to\ntheir distance. We show that this method trains a 1.3 billion parameter model\non input sequences of length 1024 that extrapolates to input sequences of\nlength 2048, achieving the same perplexity as a sinusoidal position embedding\nmodel trained on inputs of length 2048 but training 11% faster and using 11%\nless memory. ALiBi's inductive bias towards recency also leads it to outperform\nmultiple strong position methods on the WikiText-103 benchmark.",
    "categories": [
      "cs.CL"
    ],
    "published": "2021-08-27T17:35:06+00:00",
    "updated": "2022-04-22T18:20:48+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2307.04157v2": {
    "id": "http://arxiv.org/abs/2307.04157v2",
    "title": "DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer",
    "authors": [
      "Dan Ruta",
      "Gemma Canet Tarr\u00e9s",
      "Andrew Gilbert",
      "Eli Shechtman",
      "Nicholas Kolkin",
      "John Collomosse"
    ],
    "abstract": "Neural Style Transfer (NST) is the field of study applying neural techniques\nto modify the artistic appearance of a content image to match the style of a\nreference style image. Traditionally, NST methods have focused on texture-based\nimage edits, affecting mostly low level information and keeping most image\nstructures the same. However, style-based deformation of the content is\ndesirable for some styles, especially in cases where the style is abstract or\nthe primary concept of the style is in its deformed rendition of some content.\nWith the recent introduction of diffusion models, such as Stable Diffusion, we\ncan access far more powerful image generation techniques, enabling new\npossibilities. In our work, we propose using this new class of models to\nperform style transfer while enabling deformable style transfer, an elusive\ncapability in previous models. We show how leveraging the priors of these\nmodels can expose new artistic controls at inference time, and we document our\nfindings in exploring this new direction for the field of style transfer.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-07-09T12:13:43+00:00",
    "updated": "2023-07-11T09:28:36+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2211.01910v2": {
    "id": "http://arxiv.org/abs/2211.01910v2",
    "title": "Large Language Models Are Human-Level Prompt Engineers",
    "authors": [
      "Yongchao Zhou",
      "Andrei Ioan Muresanu",
      "Ziwen Han",
      "Keiran Paster",
      "Silviu Pitis",
      "Harris Chan",
      "Jimmy Ba"
    ],
    "abstract": "By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2022-11-03T15:43:03+00:00",
    "updated": "2023-03-10T17:20:17+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2010.07574v1": {
    "id": "http://arxiv.org/abs/2010.07574v1",
    "title": "Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses",
    "authors": [
      "Simon Flachs",
      "Oph\u00e9lie Lacroix",
      "Helen Yannakoudakis",
      "Marek Rei",
      "Anders S\u00f8gaard"
    ],
    "abstract": "Evaluation of grammatical error correction (GEC) systems has primarily\nfocused on essays written by non-native learners of English, which however is\nonly part of the full spectrum of GEC applications. We aim to broaden the\ntarget domain of GEC and release CWEB, a new benchmark for GEC consisting of\nwebsite text generated by English speakers of varying levels of proficiency.\nWebsite data is a common and important domain that contains far fewer\ngrammatical errors than learner essays, which we show presents a challenge to\nstate-of-the-art GEC systems. We demonstrate that a factor behind this is the\ninability of systems to rely on a strong internal language model in low error\ndensity domains. We hope this work shall facilitate the development of\nopen-domain GEC models that generalize to different topics and genres.",
    "categories": [
      "cs.CL"
    ],
    "published": "2020-10-15T07:52:01+00:00",
    "updated": "2020-10-15T07:52:01+00:00",
    "doi": null,
    "comment": "Accepted at EMNLP 2020",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2205.13524v3": {
    "id": "http://arxiv.org/abs/2205.13524v3",
    "title": "PREF: Phasorial Embedding Fields for Compact Neural Representations",
    "authors": [
      "Binbin Huang",
      "Xinhao Yan",
      "Anpei Chen",
      "Shenghua Gao",
      "Jingyi Yu"
    ],
    "abstract": "We present an efficient frequency-based neural representation termed PREF: a\nshallow MLP augmented with a phasor volume that covers significant border\nspectra than previous Fourier feature mapping or Positional Encoding. At the\ncore is our compact 3D phasor volume where frequencies distribute uniformly\nalong a 2D plane and dilate along a 1D axis. To this end, we develop a tailored\nand efficient Fourier transform that combines both Fast Fourier transform and\nlocal interpolation to accelerate na\\\"ive Fourier mapping. We also introduce a\nParsvel regularizer that stables frequency-based learning. In these ways, Our\nPREF reduces the costly MLP in the frequency-based representation, thereby\nsignificantly closing the efficiency gap between it and other hybrid\nrepresentations, and improving its interpretability. Comprehensive experiments\ndemonstrate that our PREF is able to capture high-frequency details while\nremaining compact and robust, including 2D image generalization, 3D signed\ndistance function regression and 5D neural radiance field reconstruction.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2022-05-26T17:43:03+00:00",
    "updated": "2022-10-02T11:28:10+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2111.11429v1": {
    "id": "http://arxiv.org/abs/2111.11429v1",
    "title": "Benchmarking Detection Transfer Learning with Vision Transformers",
    "authors": [
      "Yanghao Li",
      "Saining Xie",
      "Xinlei Chen",
      "Piotr Dollar",
      "Kaiming He",
      "Ross Girshick"
    ],
    "abstract": "Object detection is a central downstream task used to test if pre-trained\nnetwork parameters confer benefits, such as improved accuracy or training\nspeed. The complexity of object detection methods can make this benchmarking\nnon-trivial when new architectures, such as Vision Transformer (ViT) models,\narrive. These difficulties (e.g., architectural incompatibility, slow training,\nhigh memory consumption, unknown training formulae, etc.) have prevented recent\nstudies from benchmarking detection transfer learning with standard ViT models.\nIn this paper, we present training techniques that overcome these challenges,\nenabling the use of standard ViT models as the backbone of Mask R-CNN. These\ntools facilitate the primary goal of our study: we compare five ViT\ninitializations, including recent state-of-the-art self-supervised learning\nmethods, supervised initialization, and a strong random initialization\nbaseline. Our results show that recent masking-based unsupervised learning\nmethods may, for the first time, provide convincing transfer learning\nimprovements on COCO, increasing box AP up to 4% (absolute) over supervised and\nprior self-supervised pre-training methods. Moreover, these masking-based\ninitializations scale better, with the improvement growing as model size\nincreases.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-11-22T18:59:15+00:00",
    "updated": "2021-11-22T18:59:15+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2311.17461v1": {
    "id": "http://arxiv.org/abs/2311.17461v1",
    "title": "When StyleGAN Meets Stable Diffusion: a $\\mathscr{W}_+$ Adapter for Personalized Image Generation",
    "authors": [
      "Xiaoming Li",
      "Xinyu Hou",
      "Chen Change Loy"
    ],
    "abstract": "Text-to-image diffusion models have remarkably excelled in producing diverse,\nhigh-quality, and photo-realistic images. This advancement has spurred a\ngrowing interest in incorporating specific identities into generated content.\nMost current methods employ an inversion approach to embed a target visual\nconcept into the text embedding space using a single reference image. However,\nthe newly synthesized faces either closely resemble the reference image in\nterms of facial attributes, such as expression, or exhibit a reduced capacity\nfor identity preservation. Text descriptions intended to guide the facial\nattributes of the synthesized face may fall short, owing to the intricate\nentanglement of identity information with identity-irrelevant facial attributes\nderived from the reference image. To address these issues, we present the novel\nuse of the extended StyleGAN embedding space $\\mathcal{W}_+$, to achieve\nenhanced identity preservation and disentanglement for diffusion models. By\naligning this semantically meaningful human face latent space with\ntext-to-image diffusion models, we succeed in maintaining high fidelity in\nidentity preservation, coupled with the capacity for semantic editing.\nAdditionally, we propose new training objectives to balance the influences of\nboth prompt and identity conditions, ensuring that the identity-irrelevant\nbackground remains unaffected during facial attribute modifications. Extensive\nexperiments reveal that our method adeptly generates personalized text-to-image\noutputs that are not only compatible with prompt descriptions but also amenable\nto common StyleGAN editing directions in diverse settings. Our source code will\nbe available at \\url{https://github.com/csxmli2016/w-plus-adapter}.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-11-29T09:05:14+00:00",
    "updated": "2023-11-29T09:05:14+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.15117v1": {
    "id": "http://arxiv.org/abs/2309.15117v1",
    "title": "Generating Visual Scenes from Touch",
    "authors": [
      "Fengyu Yang",
      "Jiacheng Zhang",
      "Andrew Owens"
    ],
    "abstract": "An emerging line of work has sought to generate plausible imagery from touch.\nExisting approaches, however, tackle only narrow aspects of the visuo-tactile\nsynthesis problem, and lag significantly behind the quality of cross-modal\nsynthesis methods in other domains. We draw on recent advances in latent\ndiffusion to create a model for synthesizing images from tactile signals (and\nvice versa) and apply it to a number of visuo-tactile synthesis tasks. Using\nthis model, we significantly outperform prior work on the tactile-driven\nstylization problem, i.e., manipulating an image to match a touch signal, and\nwe are the first to successfully generate images from touch without additional\nsources of information about the scene. We also successfully use our model to\naddress two novel synthesis problems: generating images that do not contain the\ntouch sensor or the hand holding it, and estimating an image's shading from its\nreflectance and touch.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-09-26T17:59:52+00:00",
    "updated": "2023-09-26T17:59:52+00:00",
    "doi": null,
    "comment": "ICCV 2023; Project site:\n  https://fredfyyang.github.io/vision-from-touch/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1901.05708v1": {
    "id": "http://arxiv.org/abs/1901.05708v1",
    "title": "Efficient Matrix Profile Computation Using Different Distance Functions",
    "authors": [
      "Reza Akbarinia",
      "Bertrand Cloez"
    ],
    "abstract": "Matrix profile has been recently proposed as a promising technique to the\nproblem of all-pairs-similarity search on time series. Efficient algorithms\nhave been proposed for computing it, e.g., STAMP, STOMP and SCRIMP++. All these\nalgorithms use the z-normalized Euclidean distance to measure the distance\nbetween subsequences. However, as we observed, for some datasets other\nEuclidean measurements are more useful for knowledge discovery from time\nseries. In this paper, we propose efficient algorithms for computing matrix\nprofile for a general class of Euclidean distances. We first propose a simple\nbut efficient algorithm called AAMP for computing matrix profile with the\n\"pure\" (non-normalized) Euclidean distance. Then, we extend our algorithm for\nthe p-norm distance. We also propose an algorithm, called ACAMP, that uses the\nsame principle as AAMP, but for the case of z-normalized Euclidean distance. We\nimplemented our algorithms, and evaluated their performance through\nexperimentation. The experiments show excellent performance results. For\nexample, they show that AAMP is very efficient for computing matrix profile for\nnon-normalized Euclidean distances. The results also show that the ACAMP\nalgorithm is significantly faster than SCRIMP++ (the state of the art matrix\nprofile algorithm) for the case of z-normalized Euclidean distance.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2019-01-17T10:05:37+00:00",
    "updated": "2019-01-17T10:05:37+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2204.04083v2": {
    "id": "http://arxiv.org/abs/2204.04083v2",
    "title": "POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition",
    "authors": [
      "Ce Zheng",
      "Matias Mendieta",
      "Chen Chen"
    ],
    "abstract": "Facial expression recognition (FER) is an important task in computer vision,\nhaving practical applications in areas such as human-computer interaction,\neducation, healthcare, and online monitoring. In this challenging FER task,\nthere are three key issues especially prevalent: inter-class similarity,\nintra-class discrepancy, and scale sensitivity. While existing works typically\naddress some of these issues, none have fully addressed all three challenges in\na unified framework. In this paper, we propose a two-stream Pyramid\ncrOss-fuSion TransformER network (POSTER), that aims to holistically solve all\nthree issues. Specifically, we design a transformer-based cross-fusion method\nthat enables effective collaboration of facial landmark features and image\nfeatures to maximize proper attention to salient facial regions. Furthermore,\nPOSTER employs a pyramid structure to promote scale invariance. Extensive\nexperimental results demonstrate that our POSTER achieves new state-of-the-art\nresults on RAF-DB (92.05%), FERPlus (91.62%), as well as AffectNet 7 class\n(67.31%) and 8 class (63.34%). The code is available at\nhttps://github.com/zczcwh/POSTER.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2022-04-08T14:01:41+00:00",
    "updated": "2023-08-13T20:49:39+00:00",
    "doi": null,
    "comment": "ICCV Workshop (AMFG) 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2012.14240v2": {
    "id": "http://arxiv.org/abs/2012.14240v2",
    "title": "DeepSurfels: Learning Online Appearance Fusion",
    "authors": [
      "Marko Mihajlovic",
      "Silvan Weder",
      "Marc Pollefeys",
      "Martin R. Oswald"
    ],
    "abstract": "We present DeepSurfels, a novel hybrid scene representation for geometry and\nappearance information. DeepSurfels combines explicit and neural building\nblocks to jointly encode geometry and appearance information. In contrast to\nestablished representations, DeepSurfels better represents high-frequency\ntextures, is well-suited for online updates of appearance information, and can\nbe easily combined with machine learning methods. We further present an\nend-to-end trainable online appearance fusion pipeline that fuses information\nfrom RGB images into the proposed scene representation and is trained using\nself-supervision imposed by the reprojection error with respect to the input\nimages. Our method compares favorably to classical texture mapping approaches\nas well as recent learning-based techniques. Moreover, we demonstrate lower\nruntime, im-proved generalization capabilities, and better scalability to\nlarger scenes compared to existing methods.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-12-28T14:13:33+00:00",
    "updated": "2021-05-30T16:37:04+00:00",
    "doi": null,
    "comment": "In Proceedings IEEE Conference on Computer Vision and Pattern\n  Recognition. CVPR 2021",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2306.08757v1": {
    "id": "http://arxiv.org/abs/2306.08757v1",
    "title": "InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models",
    "authors": [
      "Yingheng Wang",
      "Yair Schiff",
      "Aaron Gokaslan",
      "Weishen Pan",
      "Fei Wang",
      "Christopher De Sa",
      "Volodymyr Kuleshov"
    ],
    "abstract": "While diffusion models excel at generating high-quality samples, their latent\nvariables typically lack semantic meaning and are not suitable for\nrepresentation learning. Here, we propose InfoDiffusion, an algorithm that\naugments diffusion models with low-dimensional latent variables that capture\nhigh-level factors of variation in the data. InfoDiffusion relies on a learning\nobjective regularized with the mutual information between observed and hidden\nvariables, which improves latent space quality and prevents the latents from\nbeing ignored by expressive diffusion-based decoders. Empirically, we find that\nInfoDiffusion learns disentangled and human-interpretable latent\nrepresentations that are competitive with state-of-the-art generative and\ncontrastive methods, while retaining the high sample quality of diffusion\nmodels. Our method enables manipulating the attributes of generated images and\nhas the potential to assist tasks that require exploring a learned latent space\nto generate quality samples, e.g., generative design.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2023-06-14T21:48:38+00:00",
    "updated": "2023-06-14T21:48:38+00:00",
    "doi": null,
    "comment": "ICML 2023",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2312.16378v1": {
    "id": "http://arxiv.org/abs/2312.16378v1",
    "title": "Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs",
    "authors": [
      "Sanjay Oruganti",
      "Sergei Nirenburg",
      "Jesse English",
      "Marjorie McShane"
    ],
    "abstract": "The paper describes a system that uses large language model (LLM) technology\nto support the automatic learning of new entries in an intelligent agent's\nsemantic lexicon. The process is bootstrapped by an existing non-toy lexicon\nand a natural language generator that converts formal, ontologically-grounded\nrepresentations of meaning into natural language sentences. The learning method\ninvolves a sequence of LLM requests and includes an automatic quality control\nstep. To date, this learning method has been applied to learning multiword\nexpressions whose meanings are equivalent to those of transitive verbs in the\nagent's lexicon. The experiment demonstrates the benefits of a hybrid learning\narchitecture that integrates knowledge-based methods and resources with both\ntraditional data analytics and LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-27T02:31:51+00:00",
    "updated": "2023-12-27T02:31:51+00:00",
    "doi": null,
    "comment": "7 pages, 7 figures, AAAI Fall Symposium Series 2023 on Integrating\n  Cognitive Architecture and Generative Models",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2205.01068v4": {
    "id": "http://arxiv.org/abs/2205.01068v4",
    "title": "OPT: Open Pre-trained Transformer Language Models",
    "authors": [
      "Susan Zhang",
      "Stephen Roller",
      "Naman Goyal",
      "Mikel Artetxe",
      "Moya Chen",
      "Shuohui Chen",
      "Christopher Dewan",
      "Mona Diab",
      "Xian Li",
      "Xi Victoria Lin",
      "Todor Mihaylov",
      "Myle Ott",
      "Sam Shleifer",
      "Kurt Shuster",
      "Daniel Simig",
      "Punit Singh Koura",
      "Anjali Sridhar",
      "Tianlu Wang",
      "Luke Zettlemoyer"
    ],
    "abstract": "Large language models, which are often trained for hundreds of thousands of\ncompute days, have shown remarkable capabilities for zero- and few-shot\nlearning. Given their computational cost, these models are difficult to\nreplicate without significant capital. For the few that are available through\nAPIs, no access is granted to the full model weights, making them difficult to\nstudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-only\npre-trained transformers ranging from 125M to 175B parameters, which we aim to\nfully and responsibly share with interested researchers. We show that OPT-175B\nis comparable to GPT-3, while requiring only 1/7th the carbon footprint to\ndevelop. We are also releasing our logbook detailing the infrastructure\nchallenges we faced, along with code for experimenting with all of the released\nmodels.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2022-05-02T17:49:50+00:00",
    "updated": "2022-06-21T17:04:40+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2406.00252v3": {
    "id": "http://arxiv.org/abs/2406.00252v3",
    "title": "Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey",
    "authors": [
      "Bowen Jiang",
      "Yangxinyu Xie",
      "Xiaomeng Wang",
      "Weijie J. Su",
      "Camillo J. Taylor",
      "Tanwi Mallick"
    ],
    "abstract": "Rationality is the quality of being guided by reason, characterized by\nlogical thinking and decision-making that align with evidence and logical\nrules. This quality is essential for effective problem-solving, as it ensures\nthat solutions are well-founded and systematically derived. Despite the\nadvancements of large language models (LLMs) in generating human-like text with\nremarkable accuracy, they present biases inherited from the training data,\ninconsistency across different contexts, and difficulty understanding complex\nscenarios involving multiple layers of context. Therefore, recent research\nattempts to leverage the strength of multiple agents working collaboratively\nwith various types of data and tools for enhanced consistency and reliability.\nTo that end, this paper aims to understand whether multi-modal and multi-agent\nsystems are advancing toward rationality by surveying the state-of-the-art\nworks, identifying advancements over single-agent and single-modal systems in\nterms of rationality, and discussing open problems and future directions. We\nmaintain an open repository at https://github.com/bowen-upenn/MMMA_Rationality.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MA"
    ],
    "published": "2024-06-01T01:17:25+00:00",
    "updated": "2024-06-18T04:22:39+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2403.09236v1": {
    "id": "http://arxiv.org/abs/2403.09236v1",
    "title": "Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph",
    "authors": [
      "Donglin Di",
      "Jiahui Yang",
      "Chaofan Luo",
      "Zhou Xue",
      "Wei Chen",
      "Xun Yang",
      "Yue Gao"
    ],
    "abstract": "Text-to-3D generation represents an exciting field that has seen rapid\nadvancements, facilitating the transformation of textual descriptions into\ndetailed 3D models. However, current progress often neglects the intricate\nhigh-order correlation of geometry and texture within 3D objects, leading to\nchallenges such as over-smoothness, over-saturation and the Janus problem. In\nthis work, we propose a method named ``3D Gaussian Generation via Hypergraph\n(Hyper-3DG)'', designed to capture the sophisticated high-order correlations\npresent within 3D objects. Our framework is anchored by a well-established\nmainflow and an essential module, named ``Geometry and Texture Hypergraph\nRefiner (HGRefiner)''. This module not only refines the representation of 3D\nGaussians but also accelerates the update process of these 3D Gaussians by\nconducting the Patch-3DGS Hypergraph Learning on both explicit attributes and\nlatent visual features. Our framework allows for the production of finely\ngenerated 3D objects within a cohesive optimization, effectively circumventing\ndegradation. Extensive experimentation has shown that our proposed method\nsignificantly enhances the quality of 3D generation while incurring no\nadditional computational overhead for the underlying framework. (Project code:\nhttps://github.com/yjhboy/Hyper3DG)",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-14T09:59:55+00:00",
    "updated": "2024-03-14T09:59:55+00:00",
    "doi": null,
    "comment": "27 pages, 14 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2301.09515v1": {
    "id": "http://arxiv.org/abs/2301.09515v1",
    "title": "StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis",
    "authors": [
      "Axel Sauer",
      "Tero Karras",
      "Samuli Laine",
      "Andreas Geiger",
      "Timo Aila"
    ],
    "abstract": "Text-to-image synthesis has recently seen significant progress thanks to\nlarge pretrained language models, large-scale training data, and the\nintroduction of scalable model families such as diffusion and autoregressive\nmodels. However, the best-performing models require iterative evaluation to\ngenerate a single sample. In contrast, generative adversarial networks (GANs)\nonly need a single forward pass. They are thus much faster, but they currently\nremain far behind the state-of-the-art in large-scale text-to-image synthesis.\nThis paper aims to identify the necessary steps to regain competitiveness. Our\nproposed model, StyleGAN-T, addresses the specific requirements of large-scale\ntext-to-image synthesis, such as large capacity, stable training on diverse\ndatasets, strong text alignment, and controllable variation vs. text alignment\ntradeoff. StyleGAN-T significantly improves over previous GANs and outperforms\ndistilled diffusion models - the previous state-of-the-art in fast\ntext-to-image synthesis - in terms of sample quality and speed.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2023-01-23T16:05:45+00:00",
    "updated": "2023-01-23T16:05:45+00:00",
    "doi": null,
    "comment": "Project page: https://sites.google.com/view/stylegan-t/",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1906.01829v1": {
    "id": "http://arxiv.org/abs/1906.01829v1",
    "title": "Binarized Collaborative Filtering with Distilling Graph Convolutional Networks",
    "authors": [
      "Haoyu Wang",
      "Defu Lian",
      "Yong Ge"
    ],
    "abstract": "The efficiency of top-K item recommendation based on implicit feedback are\nvital to recommender systems in real world, but it is very challenging due to\nthe lack of negative samples and the large number of candidate items. To\naddress the challenges, we firstly introduce an improved Graph Convolutional\nNetwork~(GCN) model with high-order feature interaction considered. Then we\ndistill the ranking information derived from GCN into binarized collaborative\nfiltering, which makes use of binary representation to improve the efficiency\nof online recommendation. However, binary codes are not only hard to be\noptimized but also likely to incur the loss of information during the training\nprocessing. Therefore, we propose a novel framework to convert the binary\nconstrained optimization problem into an equivalent continuous optimization\nproblem with a stochastic penalty. The binarized collaborative filtering model\nis then easily optimized by many popular solvers like SGD and Adam. The\nproposed algorithm is finally evaluated on three real-world datasets and shown\nthe superiority to the competing baselines.",
    "categories": [
      "cs.IR"
    ],
    "published": "2019-06-05T05:11:43+00:00",
    "updated": "2019-06-05T05:11:43+00:00",
    "doi": null,
    "comment": "7 pages, 3 figures,ijcai",
    "journal_ref": null,
    "primary_category": "cs.IR"
  },
  "2407.17847v1": {
    "id": "http://arxiv.org/abs/2407.17847v1",
    "title": "Move and Act: Enhanced Object Manipulation and Background Integrity for Image Editing",
    "authors": [
      "Pengfei Jiang",
      "Mingbao Lin",
      "Fei Chao",
      "Rongrong Ji"
    ],
    "abstract": "Current methods commonly utilize three-branch structures of inversion,\nreconstruction, and editing, to tackle consistent image editing task. However,\nthese methods lack control over the generation position of the edited object\nand have issues with background preservation. To overcome these limitations, we\npropose a tuning-free method with only two branches: inversion and editing.\nThis approach allows users to simultaneously edit the object's action and\ncontrol the generation position of the edited object. Additionally, it achieves\nimproved background preservation. Specifically, we transfer the edited object\ninformation to the target area and repair or preserve the background of other\nareas during the inversion process at a specific time step. In the editing\nstage, we use the image features in self-attention to query the key and value\nof the corresponding time step in the inversion to achieve consistent image\nediting. Impressive image editing results and quantitative evaluation\ndemonstrate the effectiveness of our method. The code is available at\nhttps://github.com/mobiushy/move-act.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-07-25T08:00:49+00:00",
    "updated": "2024-07-25T08:00:49+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2303.04360v2": {
    "id": "http://arxiv.org/abs/2303.04360v2",
    "title": "Does Synthetic Data Generation of LLMs Help Clinical Text Mining?",
    "authors": [
      "Ruixiang Tang",
      "Xiaotian Han",
      "Xiaoqian Jiang",
      "Xia Hu"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have led to the\ndevelopment of highly potent models like OpenAI's ChatGPT. These models have\nexhibited exceptional performance in a variety of tasks, such as question\nanswering, essay composition, and code generation. However, their effectiveness\nin the healthcare sector remains uncertain. In this study, we seek to\ninvestigate the potential of ChatGPT to aid in clinical text mining by\nexamining its ability to extract structured information from unstructured\nhealthcare texts, with a focus on biological named entity recognition and\nrelation extraction. However, our preliminary results indicate that employing\nChatGPT directly for these tasks resulted in poor performance and raised\nprivacy concerns associated with uploading patients' information to the ChatGPT\nAPI. To overcome these limitations, we propose a new training paradigm that\ninvolves generating a vast quantity of high-quality synthetic data with labels\nutilizing ChatGPT and fine-tuning a local model for the downstream task. Our\nmethod has resulted in significant improvements in the performance of\ndownstream tasks, improving the F1-score from 23.37% to 63.99% for the named\nentity recognition task and from 75.86% to 83.59% for the relation extraction\ntask. Furthermore, generating data using ChatGPT can significantly reduce the\ntime and effort required for data collection and labeling, as well as mitigate\ndata privacy concerns. In summary, the proposed framework presents a promising\nsolution to enhance the applicability of LLM models to clinical text mining.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-03-08T03:56:31+00:00",
    "updated": "2023-04-10T18:47:51+00:00",
    "doi": null,
    "comment": "10 pages, 8 tables, 4 figures",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2210.14986v2": {
    "id": "http://arxiv.org/abs/2210.14986v2",
    "title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters for Implicature Resolution by LLMs",
    "authors": [
      "Laura Ruis",
      "Akbir Khan",
      "Stella Biderman",
      "Sara Hooker",
      "Tim Rockt\u00e4schel",
      "Edward Grefenstette"
    ],
    "abstract": "Despite widespread use of LLMs as conversational agents, evaluations of\nperformance fail to capture a crucial aspect of communication: interpreting\nlanguage in context -- incorporating its pragmatics. Humans interpret language\nusing beliefs and prior knowledge about the world. For example, we intuitively\nunderstand the response \"I wore gloves\" to the question \"Did you leave\nfingerprints?\" as meaning \"No\". To investigate whether LLMs have the ability to\nmake this type of inference, known as an implicature, we design a simple task\nand evaluate four categories of widely used state-of-the-art models. We find\nthat, despite only evaluating on utterances that require a binary inference\n(yes or no), models in three of these categories perform close to random.\nHowever, LLMs instruction-tuned at the example-level perform significantly\nbetter. These results suggest that certain fine-tuning strategies are far\nbetter at inducing pragmatic understanding in models. We present our findings\nas the starting point for further research into evaluating how LLMs interpret\nlanguage in context and to drive the development of more pragmatic and useful\nmodels of human discourse.",
    "categories": [
      "cs.CL"
    ],
    "published": "2022-10-26T19:04:23+00:00",
    "updated": "2023-12-03T17:14:05+00:00",
    "doi": null,
    "comment": "Accepted as Spotlight at NeurIPS 2023",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2306.12907v1": {
    "id": "http://arxiv.org/abs/2306.12907v1",
    "title": "xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages",
    "authors": [
      "Mingda Chen",
      "Kevin Heffernan",
      "Onur \u00c7elebi",
      "Alex Mourachko",
      "Holger Schwenk"
    ],
    "abstract": "We introduce a new proxy score for evaluating bitext mining based on\nsimilarity in a multilingual embedding space: xSIM++. In comparison to xSIM,\nthis improved proxy leverages rule-based approaches to extend English sentences\nin any evaluation set with synthetic, hard-to-distinguish examples which more\nclosely mirror the scenarios we encounter during large-scale mining. We\nvalidate this proxy by running a significant number of bitext mining\nexperiments for a set of low-resource languages, and subsequently train NMT\nsystems on the mined data. In comparison to xSIM, we show that xSIM++ is better\ncorrelated with the downstream BLEU scores of translation systems trained on\nmined bitexts, providing a reliable proxy of bitext mining performance without\nneeding to run expensive bitext mining pipelines. xSIM++ also reports\nperformance for different error types, offering more fine-grained feedback for\nmodel development.",
    "categories": [
      "cs.CL"
    ],
    "published": "2023-06-22T14:20:15+00:00",
    "updated": "2023-06-22T14:20:15+00:00",
    "doi": null,
    "comment": "The first two authors contributed equally; ACL 2023 short; Code and\n  data are available at https://github.com/facebookresearch/LASER",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2102.09548v2": {
    "id": "http://arxiv.org/abs/2102.09548v2",
    "title": "Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development",
    "authors": [
      "Kexin Huang",
      "Tianfan Fu",
      "Wenhao Gao",
      "Yue Zhao",
      "Yusuf Roohani",
      "Jure Leskovec",
      "Connor W. Coley",
      "Cao Xiao",
      "Jimeng Sun",
      "Marinka Zitnik"
    ],
    "abstract": "Therapeutics machine learning is an emerging field with incredible\nopportunities for innovatiaon and impact. However, advancement in this field\nrequires formulation of meaningful learning tasks and careful curation of\ndatasets. Here, we introduce Therapeutics Data Commons (TDC), the first\nunifying platform to systematically access and evaluate machine learning across\nthe entire range of therapeutics. To date, TDC includes 66 AI-ready datasets\nspread across 22 learning tasks and spanning the discovery and development of\nsafe and effective medicines. TDC also provides an ecosystem of tools and\ncommunity resources, including 33 data functions and types of meaningful data\nsplits, 23 strategies for systematic model evaluation, 17 molecule generation\noracles, and 29 public leaderboards. All resources are integrated and\naccessible via an open Python library. We carry out extensive experiments on\nselected datasets, demonstrating that even the strongest algorithms fall short\nof solving key therapeutics challenges, including real dataset distributional\nshifts, multi-scale modeling of heterogeneous data, and robust generalization\nto novel data points. We envision that TDC can facilitate algorithmic and\nscientific advances and considerably accelerate machine-learning model\ndevelopment, validation and transition into biomedical and clinical\nimplementation. TDC is an open-science initiative available at\nhttps://tdcommons.ai.",
    "categories": [
      "cs.LG",
      "cs.CY",
      "q-bio.BM",
      "q-bio.QM"
    ],
    "published": "2021-02-18T18:50:31+00:00",
    "updated": "2021-08-28T19:59:03+00:00",
    "doi": null,
    "comment": "Published at NeurIPS 2021 Datasets and Benchmarks",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2301.13741v3": {
    "id": "http://arxiv.org/abs/2301.13741v3",
    "title": "UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers",
    "authors": [
      "Dachuan Shi",
      "Chaofan Tao",
      "Ying Jin",
      "Zhendong Yang",
      "Chun Yuan",
      "Jiaqi Wang"
    ],
    "abstract": "Real-world data contains a vast amount of multimodal information, among which\nvision and language are the two most representative modalities. Moreover,\nincreasingly heavier models, \\textit{e}.\\textit{g}., Transformers, have\nattracted the attention of researchers to model compression. However, how to\ncompress multimodal models, especially vison-language Transformers, is still\nunder-explored. This paper proposes the \\textbf{U}nified and\n\\textbf{P}r\\textbf{o}gressive \\textbf{P}runing (\\textbf{\\emph{UPop}}) as a\nuniversal vison-language Transformer compression framework, which incorporates\n1) unifiedly searching multimodal subnets in a continuous optimization space\nfrom the original model, which enables automatic assignment of pruning ratios\namong compressible modalities and structures; 2) progressively searching and\nretraining the subnet, which maintains convergence between the search and\nretrain to attain higher compression ratios. Experiments on various tasks,\ndatasets, and model architectures demonstrate the effectiveness and versatility\nof the proposed UPop framework. The code is available at\nhttps://github.com/sdc17/UPop.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-01-31T16:18:52+00:00",
    "updated": "2023-06-30T03:25:27+00:00",
    "doi": null,
    "comment": "ICML 2023. Website: https://dachuanshi.com/UPop-Project",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2201.08383v2": {
    "id": "http://arxiv.org/abs/2201.08383v2",
    "title": "MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition",
    "authors": [
      "Chao-Yuan Wu",
      "Yanghao Li",
      "Karttikeya Mangalam",
      "Haoqi Fan",
      "Bo Xiong",
      "Jitendra Malik",
      "Christoph Feichtenhofer"
    ],
    "abstract": "While today's video recognition systems parse snapshots or short clips\naccurately, they cannot connect the dots and reason across a longer range of\ntime yet. Most existing video architectures can only process <5 seconds of a\nvideo without hitting the computation or memory bottlenecks.\n  In this paper, we propose a new strategy to overcome this challenge. Instead\nof trying to process more frames at once like most existing methods, we propose\nto process videos in an online fashion and cache \"memory\" at each iteration.\nThrough the memory, the model can reference prior context for long-term\nmodeling, with only a marginal cost. Based on this idea, we build MeMViT, a\nMemory-augmented Multiscale Vision Transformer, that has a temporal support 30x\nlonger than existing models with only 4.5% more compute; traditional methods\nneed >3,000% more compute to do the same. On a wide range of settings, the\nincreased temporal support enabled by MeMViT brings large gains in recognition\naccuracy consistently. MeMViT obtains state-of-the-art results on the AVA,\nEPIC-Kitchens-100 action classification, and action anticipation datasets. Code\nand models are available at https://github.com/facebookresearch/memvit.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-01-20T18:59:54+00:00",
    "updated": "2022-11-30T19:40:55+00:00",
    "doi": null,
    "comment": "Technical report. arXiv v2: add link to code",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.09591v3": {
    "id": "http://arxiv.org/abs/2308.09591v3",
    "title": "O$^2$-Recon: Completing 3D Reconstruction of Occluded Objects in the Scene with a Pre-trained 2D Diffusion Model",
    "authors": [
      "Yubin Hu",
      "Sheng Ye",
      "Wang Zhao",
      "Matthieu Lin",
      "Yuze He",
      "Yu-Hui Wen",
      "Ying He",
      "Yong-Jin Liu"
    ],
    "abstract": "Occlusion is a common issue in 3D reconstruction from RGB-D videos, often\nblocking the complete reconstruction of objects and presenting an ongoing\nproblem. In this paper, we propose a novel framework, empowered by a 2D\ndiffusion-based in-painting model, to reconstruct complete surfaces for the\nhidden parts of objects. Specifically, we utilize a pre-trained diffusion model\nto fill in the hidden areas of 2D images. Then we use these in-painted images\nto optimize a neural implicit surface representation for each instance for 3D\nreconstruction. Since creating the in-painting masks needed for this process is\ntricky, we adopt a human-in-the-loop strategy that involves very little human\nengagement to generate high-quality masks. Moreover, some parts of objects can\nbe totally hidden because the videos are usually shot from limited\nperspectives. To ensure recovering these invisible areas, we develop a cascaded\nnetwork architecture for predicting signed distance field, making use of\ndifferent frequency bands of positional encoding and maintaining overall\nsmoothness. Besides the commonly used rendering loss, Eikonal loss, and\nsilhouette loss, we adopt a CLIP-based semantic consistency loss to guide the\nsurface from unseen camera angles. Experiments on ScanNet scenes show that our\nproposed framework achieves state-of-the-art accuracy and completeness in\nobject-level reconstruction from scene-level RGB-D videos. Code:\nhttps://github.com/THU-LYJ-Lab/O2-Recon.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-08-18T14:38:31+00:00",
    "updated": "2024-03-19T06:37:32+00:00",
    "doi": null,
    "comment": "AAAI 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2402.00763v1": {
    "id": "http://arxiv.org/abs/2402.00763v1",
    "title": "360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming",
    "authors": [
      "Jiayang Bai",
      "Letian Huang",
      "Jie Guo",
      "Wen Gong",
      "Yuanqi Li",
      "Yanwen Guo"
    ],
    "abstract": "3D Gaussian Splatting (3D-GS) has recently attracted great attention with\nreal-time and photo-realistic renderings. This technique typically takes\nperspective images as input and optimizes a set of 3D elliptical Gaussians by\nsplatting them onto the image planes, resulting in 2D Gaussians. However,\napplying 3D-GS to panoramic inputs presents challenges in effectively modeling\nthe projection onto the spherical surface of ${360^\\circ}$ images using 2D\nGaussians. In practical applications, input panoramas are often sparse, leading\nto unreliable initialization of 3D Gaussians and subsequent degradation of\n3D-GS quality. In addition, due to the under-constrained geometry of\ntexture-less planes (e.g., walls and floors), 3D-GS struggles to model these\nflat regions with elliptical Gaussians, resulting in significant floaters in\nnovel views. To address these issues, we propose 360-GS, a novel $360^{\\circ}$\nGaussian splatting for a limited set of panoramic inputs. Instead of splatting\n3D Gaussians directly onto the spherical surface, 360-GS projects them onto the\ntangent plane of the unit sphere and then maps them to the spherical\nprojections. This adaptation enables the representation of the projection using\nGaussians. We guide the optimization of 360-GS by exploiting layout priors\nwithin panoramas, which are simple to obtain and contain strong structural\ninformation about the indoor scene. Our experimental results demonstrate that\n360-GS allows panoramic rendering and outperforms state-of-the-art methods with\nfewer artifacts in novel view synthesis, thus providing immersive roaming in\nindoor scenarios.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2024-02-01T16:52:21+00:00",
    "updated": "2024-02-01T16:52:21+00:00",
    "doi": null,
    "comment": "11 pages, 10 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2206.11752v3": {
    "id": "http://arxiv.org/abs/2206.11752v3",
    "title": "CLAMP: Prompt-based Contrastive Learning for Connecting Language and Animal Pose",
    "authors": [
      "Xu Zhang",
      "Wen Wang",
      "Zhe Chen",
      "Yufei Xu",
      "Jing Zhang",
      "Dacheng Tao"
    ],
    "abstract": "Animal pose estimation is challenging for existing image-based methods\nbecause of limited training data and large intra- and inter-species variances.\nMotivated by the progress of visual-language research, we propose that\npre-trained language models (e.g., CLIP) can facilitate animal pose estimation\nby providing rich prior knowledge for describing animal keypoints in text.\nHowever, we found that building effective connections between pre-trained\nlanguage models and visual animal keypoints is non-trivial since the gap\nbetween text-based descriptions and keypoint-based visual features about animal\npose can be significant. To address this issue, we introduce a novel\nprompt-based Contrastive learning scheme for connecting Language and AniMal\nPose (CLAMP) effectively. The CLAMP attempts to bridge the gap by adapting the\ntext prompts to the animal keypoints during network training. The adaptation is\ndecomposed into spatial-aware and feature-aware processes, and two novel\ncontrastive losses are devised correspondingly. In practice, the CLAMP enables\nthe first cross-modal animal pose estimation paradigm. Experimental results\nshow that our method achieves state-of-the-art performance under the\nsupervised, few-shot, and zero-shot settings, outperforming image-based methods\nby a large margin.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-06-23T14:51:42+00:00",
    "updated": "2023-06-26T00:46:10+00:00",
    "doi": "10.1109/CVPR52729.2023.02229",
    "comment": "CVPR2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2311.00502v2": {
    "id": "http://arxiv.org/abs/2311.00502v2",
    "title": "Efficient LLM Inference on CPUs",
    "authors": [
      "Haihao Shen",
      "Hanwen Chang",
      "Bo Dong",
      "Yu Luo",
      "Hengyu Meng"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance and\ntremendous potential across a wide range of tasks. However, deploying these\nmodels has been challenging due to the astronomical amount of model parameters,\nwhich requires a demand for large memory capacity and high memory bandwidth. In\nthis paper, we propose an effective approach that can make the deployment of\nLLMs more efficiently. We support an automatic INT4 weight-only quantization\nflow and design a special LLM runtime with highly-optimized kernels to\naccelerate the LLM inference on CPUs. We demonstrate the general applicability\nof our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase\nthe extreme inference efficiency on CPUs. The code is publicly available at:\nhttps://github.com/intel/intel-extension-for-transformers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-11-01T13:08:50+00:00",
    "updated": "2023-12-07T12:16:42+00:00",
    "doi": null,
    "comment": "NeurIPS'2023 on Efficient Natural Language and Speech Processing",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2312.16733v1": {
    "id": "http://arxiv.org/abs/2312.16733v1",
    "title": "SuperServe: Fine-Grained Inference Serving for Unpredictable Workloads",
    "authors": [
      "Alind Khare",
      "Dhruv Garg",
      "Sukrit Kalra",
      "Snigdha Grandhi",
      "Ion Stoica",
      "Alexey Tumanov"
    ],
    "abstract": "The increasing deployment of ML models on the critical path of production\napplications in both datacenter and the edge requires ML inference serving\nsystems to serve these models under unpredictable and bursty request arrival\nrates. Serving models under such conditions requires these systems to strike a\ncareful balance between the latency and accuracy requirements of the\napplication and the overall efficiency of utilization of scarce resources.\nState-of-the-art systems resolve this tension by either choosing a static point\nin the latency-accuracy tradeoff space to serve all requests or load specific\nmodels on the critical path of request serving. In this work, we instead\nresolve this tension by simultaneously serving the entire-range of models\nspanning the latency-accuracy tradeoff space. Our novel mechanism, SubNetAct,\nachieves this by carefully inserting specialized operators in weight-shared\nSuperNetworks. These operators enable SubNetAct to dynamically route requests\nthrough the network to meet a latency and accuracy target. SubNetAct requires\nupto 2.6x lower memory to serve a vastly-higher number of models than prior\nstate-of-the-art. In addition, SubNetAct's near-instantaneous actuation of\nmodels unlocks the design space of fine-grained, reactive scheduling policies.\nWe explore the design of one such extremely effective policy, SlackFit and\ninstantiate both SubNetAct and SlackFit in a real system, SuperServe.\nSuperServe achieves 4.67% higher accuracy for the same SLO attainment and 2.85x\nhigher SLO attainment for the same accuracy on a trace derived from the\nreal-world Microsoft Azure Functions workload and yields the best trade-offs on\na wide range of extremely-bursty synthetic traces automatically.",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "published": "2023-12-27T22:24:11+00:00",
    "updated": "2023-12-27T22:24:11+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.DC"
  },
  "2206.13619v1": {
    "id": "http://arxiv.org/abs/2206.13619v1",
    "title": "DeepPERF: A Deep Learning-Based Approach For Improving Software Performance",
    "authors": [
      "Spandan Garg",
      "Roshanak Zilouchian Moghaddam",
      "Colin B. Clement",
      "Neel Sundaresan",
      "Chen Wu"
    ],
    "abstract": "Improving software performance is an important yet challenging part of the\nsoftware development cycle. Today, the majority of performance inefficiencies\nare identified and patched by performance experts. Recent advancements in deep\nlearning approaches and the wide-spread availability of open source data\ncreates a great opportunity to automate the identification and patching of\nperformance problems. In this paper, we present DeepPERF, a transformer-based\napproach to suggest performance improvements for C# applications. We pretrain\nDeepPERF on English and Source code corpora and followed by finetuning for the\ntask of generating performance improvement patches for C# applications. Our\nevaluation shows that our model can generate the same performance improvement\nsuggestion as the developer fix in ~53% of the cases, getting ~34% of them\nverbatim in our expert-verified dataset of performance changes made by C#\ndevelopers. Additionally, we evaluate DeepPERF on 50 open source C#\nrepositories on GitHub using both benchmark and unit tests and find that our\nmodel is able to suggest valid performance improvements that can improve both\nCPU usage and Memory allocations. So far we've submitted 19 pull-requests with\n28 different performance optimizations and 11 of these PRs have been approved\nby the project owners.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PF"
    ],
    "published": "2022-06-27T20:35:52+00:00",
    "updated": "2022-06-27T20:35:52+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "2308.16911v3": {
    "id": "http://arxiv.org/abs/2308.16911v3",
    "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds",
    "authors": [
      "Runsen Xu",
      "Xiaolong Wang",
      "Tai Wang",
      "Yilun Chen",
      "Jiangmiao Pang",
      "Dahua Lin"
    ],
    "abstract": "The unprecedented advancements in Large Language Models (LLMs) have shown a\nprofound impact on natural language processing but are yet to fully embrace the\nrealm of 3D understanding. This paper introduces PointLLM, a preliminary effort\nto fill this gap, enabling LLMs to understand point clouds and offering a new\navenue beyond 2D visual data. PointLLM understands colored object point clouds\nwith human instructions and generates contextually appropriate responses,\nillustrating its grasp of point clouds and common sense. Specifically, it\nleverages a point cloud encoder with a powerful LLM to effectively fuse\ngeometric, appearance, and linguistic information. We collect a novel dataset\ncomprising 660K simple and 70K complex point-text instruction pairs to enable a\ntwo-stage training strategy: aligning latent spaces and subsequently\ninstruction-tuning the unified model. To rigorously evaluate the perceptual and\ngeneralization capabilities of PointLLM, we establish two benchmarks:\nGenerative 3D Object Classification and 3D Object Captioning, assessed through\nthree different methods, including human evaluation, GPT-4/ChatGPT evaluation,\nand traditional metrics. Experimental results reveal PointLLM's superior\nperformance over existing 2D and 3D baselines, with a notable achievement in\nhuman-evaluated object captioning tasks where it surpasses human annotators in\nover 50% of the samples. Codes, datasets, and benchmarks are available at\nhttps://github.com/OpenRobotLab/PointLLM .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-08-31T17:59:46+00:00",
    "updated": "2024-09-06T22:45:43+00:00",
    "doi": null,
    "comment": "ECCV 2024 Oral Camera Ready. This version includes clearer writing\n  and additional experimental results compared to previous versions. Project\n  page: https://runsenxu.com/projects/PointLLM",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2104.08836v3": {
    "id": "http://arxiv.org/abs/2104.08836v3",
    "title": "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding",
    "authors": [
      "Yiheng Xu",
      "Tengchao Lv",
      "Lei Cui",
      "Guoxin Wang",
      "Yijuan Lu",
      "Dinei Florencio",
      "Cha Zhang",
      "Furu Wei"
    ],
    "abstract": "Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUND, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUND dataset. The pre-trained\nLayoutXLM model and the XFUND dataset are publicly available at\nhttps://aka.ms/layoutxlm.",
    "categories": [
      "cs.CL"
    ],
    "published": "2021-04-18T12:16:00+00:00",
    "updated": "2021-09-09T11:37:48+00:00",
    "doi": null,
    "comment": "Work in progress",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2301.05372v1": {
    "id": "http://arxiv.org/abs/2301.05372v1",
    "title": "Text to Point Cloud Localization with Relation-Enhanced Transformer",
    "authors": [
      "Guangzhi Wang",
      "Hehe Fan",
      "Mohan Kankanhalli"
    ],
    "abstract": "Automatically localizing a position based on a few natural language\ninstructions is essential for future robots to communicate and collaborate with\nhumans. To approach this goal, we focus on the text-to-point-cloud cross-modal\nlocalization problem. Given a textual query, it aims to identify the described\nlocation from city-scale point clouds. The task involves two challenges. 1) In\ncity-scale point clouds, similar ambient instances may exist in several\nlocations. Searching each location in a huge point cloud with only instances as\nguidance may lead to less discriminative signals and incorrect results. 2) In\ntextual descriptions, the hints are provided separately. In this case, the\nrelations among those hints are not explicitly described, leading to\ndifficulties of learning relations. To overcome these two challenges, we\npropose a unified Relation-Enhanced Transformer (RET) to improve representation\ndiscriminability for both point cloud and natural language queries. The core of\nthe proposed RET is a novel Relation-enhanced Self-Attention (RSA) mechanism,\nwhich explicitly encodes instance (hint)-wise relations for the two modalities.\nMoreover, we propose a fine-grained cross-modal matching method to further\nrefine the location predictions in a subsequent instance-hint matching stage.\nExperimental results on the KITTI360Pose dataset demonstrate that our approach\nsurpasses the previous state-of-the-art method by large margin.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-01-13T02:58:49+00:00",
    "updated": "2023-01-13T02:58:49+00:00",
    "doi": null,
    "comment": "9 pages, 5 figures, accepted to AAAI-2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2207.04296v2": {
    "id": "http://arxiv.org/abs/2207.04296v2",
    "title": "TensorIR: An Abstraction for Automatic Tensorized Program Optimization",
    "authors": [
      "Siyuan Feng",
      "Bohan Hou",
      "Hongyi Jin",
      "Wuwei Lin",
      "Junru Shao",
      "Ruihang Lai",
      "Zihao Ye",
      "Lianmin Zheng",
      "Cody Hao Yu",
      "Yong Yu",
      "Tianqi Chen"
    ],
    "abstract": "Deploying deep learning models on various devices has become an important\ntopic. The wave of hardware specialization brings a diverse set of acceleration\nprimitives for multi-dimensional tensor computations. These new acceleration\nprimitives, along with the emerging machine learning models, bring tremendous\nengineering challenges. In this paper, we present TensorIR, a compiler\nabstraction for optimizing programs with these tensor computation primitives.\nTensorIR generalizes the loop nest representation used in existing machine\nlearning compilers to bring tensor computation as the first-class citizen.\nFinally, we build an end-to-end framework on top of our abstraction to\nautomatically optimize deep learning models for given tensor computation\nprimitives. Experimental results show that TensorIR compilation automatically\nuses the tensor computation primitives for given hardware backends and delivers\nperformance that is competitive to state-of-art hand-optimized systems across\nplatforms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "published": "2022-07-09T16:28:57+00:00",
    "updated": "2022-10-27T20:53:23+00:00",
    "doi": null,
    "comment": "Accepted to ASPLOS 2023",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1707.06347v2": {
    "id": "http://arxiv.org/abs/1707.06347v2",
    "title": "Proximal Policy Optimization Algorithms",
    "authors": [
      "John Schulman",
      "Filip Wolski",
      "Prafulla Dhariwal",
      "Alec Radford",
      "Oleg Klimov"
    ],
    "abstract": "We propose a new family of policy gradient methods for reinforcement\nlearning, which alternate between sampling data through interaction with the\nenvironment, and optimizing a \"surrogate\" objective function using stochastic\ngradient ascent. Whereas standard policy gradient methods perform one gradient\nupdate per data sample, we propose a novel objective function that enables\nmultiple epochs of minibatch updates. The new methods, which we call proximal\npolicy optimization (PPO), have some of the benefits of trust region policy\noptimization (TRPO), but they are much simpler to implement, more general, and\nhave better sample complexity (empirically). Our experiments test PPO on a\ncollection of benchmark tasks, including simulated robotic locomotion and Atari\ngame playing, and we show that PPO outperforms other online policy gradient\nmethods, and overall strikes a favorable balance between sample complexity,\nsimplicity, and wall-time.",
    "categories": [
      "cs.LG"
    ],
    "published": "2017-07-20T02:32:33+00:00",
    "updated": "2017-08-28T09:20:06+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2310.03015v1": {
    "id": "http://arxiv.org/abs/2310.03015v1",
    "title": "Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day",
    "authors": [
      "Yifan Jiang",
      "Hao Tang",
      "Jen-Hao Rick Chang",
      "Liangchen Song",
      "Zhangyang Wang",
      "Liangliang Cao"
    ],
    "abstract": "The task of novel view synthesis aims to generate unseen perspectives of an\nobject or scene from a limited set of input images. Nevertheless, synthesizing\nnovel views from a single image still remains a significant challenge in the\nrealm of computer vision. Previous approaches tackle this problem by adopting\nmesh prediction, multi-plain image construction, or more advanced techniques\nsuch as neural radiance fields. Recently, a pre-trained diffusion model that is\nspecifically designed for 2D image synthesis has demonstrated its capability in\nproducing photorealistic novel views, if sufficiently optimized on a 3D\nfinetuning task. Although the fidelity and generalizability are greatly\nimproved, training such a powerful diffusion model requires a vast volume of\ntraining data and model parameters, resulting in a notoriously long time and\nhigh computational costs. To tackle this issue, we propose Efficient-3DiM, a\nsimple but effective framework to learn a single-image novel-view synthesizer.\nMotivated by our in-depth analysis of the inference process of diffusion\nmodels, we propose several pragmatic strategies to reduce the training overhead\nto a manageable scale, including a crafted timestep sampling strategy, a\nsuperior 3D feature extractor, and an enhanced training scheme. When combined,\nour framework is able to reduce the total training time from 10 days to less\nthan 1 day, significantly accelerating the training process under the same\ncomputational platform (one instance with 8 Nvidia A100 GPUs). Comprehensive\nexperiments are conducted to demonstrate the efficiency and generalizability of\nour proposed method.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-10-04T17:57:07+00:00",
    "updated": "2023-10-04T17:57:07+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1711.08416v2": {
    "id": "http://arxiv.org/abs/1711.08416v2",
    "title": "Equivalence of Equilibrium Propagation and Recurrent Backpropagation",
    "authors": [
      "Benjamin Scellier",
      "Yoshua Bengio"
    ],
    "abstract": "Recurrent Backpropagation and Equilibrium Propagation are supervised learning\nalgorithms for fixed point recurrent neural networks which differ in their\nsecond phase. In the first phase, both algorithms converge to a fixed point\nwhich corresponds to the configuration where the prediction is made. In the\nsecond phase, Equilibrium Propagation relaxes to another nearby fixed point\ncorresponding to smaller prediction error, whereas Recurrent Backpropagation\nuses a side network to compute error derivatives iteratively. In this work we\nestablish a close connection between these two algorithms. We show that, at\nevery moment in the second phase, the temporal derivatives of the neural\nactivities in Equilibrium Propagation are equal to the error derivatives\ncomputed iteratively by Recurrent Backpropagation in the side network. This\nwork shows that it is not required to have a side network for the computation\nof error derivatives, and supports the hypothesis that, in biological neural\nnetworks, temporal derivatives of neural activities may code for error signals.",
    "categories": [
      "cs.LG"
    ],
    "published": "2017-11-22T17:49:58+00:00",
    "updated": "2018-05-22T18:19:12+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2402.16906v6": {
    "id": "http://arxiv.org/abs/2402.16906v6",
    "title": "Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step",
    "authors": [
      "Li Zhong",
      "Zilong Wang",
      "Jingbo Shang"
    ],
    "abstract": "Large language models (LLMs) are leading significant progress in code\ngeneration. Beyond one-pass code generation, recent works further integrate\nunit tests and program verifiers into LLMs to iteratively refine the generated\nprograms. However, these works consider the generated programs as an\nindivisible entity, which falls short for LLMs in debugging the programs,\nespecially when the programs contain complex logic flows and data operations.\nIn contrast, when human developers debug programs, they typically set\nbreakpoints and selectively examine runtime execution information. The\nexecution flow and the intermediate variables play a crucial role in the\ndebugging process, yet they are underutilized in the existing literature on\ncode generation. In this study, we introduce Large Language Model Debugger\n(LDB), a novel debugging framework that enables LLMs to refine their generated\nprograms with the runtime execution information. Specifically, LDB segments the\nprograms into basic blocks and tracks the values of intermediate variables\nafter each block throughout the runtime execution. This allows LLMs to\nconcentrate on simpler code units within the overall execution flow, verify\ntheir correctness against the task description block by block, and efficiently\npinpoint any potential errors. Experiments demonstrate that LDB consistently\nenhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and\nTransCoder benchmarks, archiving new state-of-the-art performance in code\ndebugging for various LLM selections.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-25T00:56:27+00:00",
    "updated": "2024-06-06T06:01:41+00:00",
    "doi": null,
    "comment": "Preprint",
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "2312.06439v2": {
    "id": "http://arxiv.org/abs/2312.06439v2",
    "title": "DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior",
    "authors": [
      "Tianyu Huang",
      "Yihan Zeng",
      "Zhilu Zhang",
      "Wan Xu",
      "Hang Xu",
      "Songcen Xu",
      "Rynson W. H. Lau",
      "Wangmeng Zuo"
    ],
    "abstract": "3D generation has raised great attention in recent years. With the success of\ntext-to-image diffusion models, the 2D-lifting technique becomes a promising\nroute to controllable 3D generation. However, these methods tend to present\ninconsistent geometry, which is also known as the Janus problem. We observe\nthat the problem is caused mainly by two aspects, i.e., viewpoint bias in 2D\ndiffusion models and overfitting of the optimization objective. To address it,\nwe propose a two-stage 2D-lifting framework, namely DreamControl, which\noptimizes coarse NeRF scenes as 3D self-prior and then generates fine-grained\nobjects with control-based score distillation. Specifically, adaptive viewpoint\nsampling and boundary integrity metric are proposed to ensure the consistency\nof generated priors. The priors are then regarded as input conditions to\nmaintain reasonable geometries, in which conditional LoRA and weighted score\nare further proposed to optimize detailed textures. DreamControl can generate\nhigh-quality 3D content in terms of both geometry consistency and texture\nfidelity. Moreover, our control-based optimization guidance is applicable to\nmore downstream tasks, including user-guided generation and 3D animation. The\nproject page is available at https://github.com/tyhuang0428/DreamControl.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-11T15:12:50+00:00",
    "updated": "2024-03-12T09:47:06+00:00",
    "doi": null,
    "comment": "Accepted by CVPR 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2408.00735v1": {
    "id": "http://arxiv.org/abs/2408.00735v1",
    "title": "TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models",
    "authors": [
      "Gilad Deutch",
      "Rinon Gal",
      "Daniel Garibi",
      "Or Patashnik",
      "Daniel Cohen-Or"
    ],
    "abstract": "Diffusion models have opened the path to a wide range of text-based image\nediting frameworks. However, these typically build on the multi-step nature of\nthe diffusion backwards process, and adapting them to distilled, fast-sampling\nmethods has proven surprisingly challenging. Here, we focus on a popular line\nof text-based editing frameworks - the ``edit-friendly'' DDPM-noise inversion\napproach. We analyze its application to fast sampling methods and categorize\nits failures into two classes: the appearance of visual artifacts, and\ninsufficient editing strength. We trace the artifacts to mismatched noise\nstatistics between inverted noises and the expected noise schedule, and suggest\na shifted noise schedule which corrects for this offset. To increase editing\nstrength, we propose a pseudo-guidance approach that efficiently increases the\nmagnitude of edits without introducing new artifacts. All in all, our method\nenables text-based image editing with as few as three diffusion steps, while\nproviding novel insights into the mechanisms behind popular text-based editing\napproaches.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2024-08-01T17:27:28+00:00",
    "updated": "2024-08-01T17:27:28+00:00",
    "doi": null,
    "comment": "Project page: https://turboedit-paper.github.io/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.11899v1": {
    "id": "http://arxiv.org/abs/2403.11899v1",
    "title": "GNeRP: Gaussian-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors",
    "authors": [
      "LI Yang",
      "WU Ruizheng",
      "LI Jiyong",
      "CHEN Ying-cong"
    ],
    "abstract": "Learning surfaces from neural radiance field (NeRF) became a rising topic in\nMulti-View Stereo (MVS). Recent Signed Distance Function (SDF)-based methods\ndemonstrated their ability to reconstruct accurate 3D shapes of Lambertian\nscenes. However, their results on reflective scenes are unsatisfactory due to\nthe entanglement of specular radiance and complicated geometry. To address the\nchallenges, we propose a Gaussian-based representation of normals in SDF\nfields. Supervised by polarization priors, this representation guides the\nlearning of geometry behind the specular reflection and captures more details\nthan existing methods. Moreover, we propose a reweighting strategy in the\noptimization process to alleviate the noise issue of polarization priors. To\nvalidate the effectiveness of our design, we capture polarimetric information,\nand ground truth meshes in additional reflective scenes with various geometry.\nWe also evaluated our framework on the PANDORA dataset. Comparisons prove our\nmethod outperforms existing neural 3D reconstruction methods in reflective\nscenes by a large margin.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-18T15:58:03+00:00",
    "updated": "2024-03-18T15:58:03+00:00",
    "doi": null,
    "comment": "Accepted to ICLR 2024 Poster. For the Appendix, please see\n  http://yukiumi13.github.io/gnerp_page",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.13604v3": {
    "id": "http://arxiv.org/abs/2312.13604v3",
    "title": "Ponymation: Learning Articulated 3D Animal Motions from Unlabeled Online Videos",
    "authors": [
      "Keqiang Sun",
      "Dor Litvak",
      "Yunzhi Zhang",
      "Hongsheng Li",
      "Jiajun Wu",
      "Shangzhe Wu"
    ],
    "abstract": "We introduce a new method for learning a generative model of articulated 3D\nanimal motions from raw, unlabeled online videos. Unlike existing approaches\nfor 3D motion synthesis, our model requires no pose annotations or parametric\nshape models for training; it learns purely from a collection of unlabeled web\nvideo clips, leveraging semantic correspondences distilled from self-supervised\nimage features. At the core of our method is a video Photo-Geometric\nAuto-Encoding framework that decomposes each training video clip into a set of\nexplicit geometric and photometric representations, including a rest-pose 3D\nshape, an articulated pose sequence, and texture, with the objective of\nre-rendering the input video via a differentiable renderer. This decomposition\nallows us to learn a generative model over the underlying articulated pose\nsequences akin to a Variational Auto-Encoding (VAE) formulation, but without\nrequiring any external pose annotations. At inference time, we can generate new\nmotion sequences by sampling from the learned motion VAE, and create plausible\n4D animations of an animal automatically within seconds given a single input\nimage.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-21T06:44:18+00:00",
    "updated": "2024-07-31T18:40:17+00:00",
    "doi": null,
    "comment": "ECCV 2024. Project page:\n  https://keqiangsun.github.io/projects/ponymation. The first two authors\n  contributed equally. The last two authors contributed equally",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2104.02409v3": {
    "id": "http://arxiv.org/abs/2104.02409v3",
    "title": "Learning to Estimate Hidden Motions with Global Motion Aggregation",
    "authors": [
      "Shihao Jiang",
      "Dylan Campbell",
      "Yao Lu",
      "Hongdong Li",
      "Richard Hartley"
    ],
    "abstract": "Occlusions pose a significant challenge to optical flow algorithms that rely\non local evidences. We consider an occluded point to be one that is imaged in\nthe first frame but not in the next, a slight overloading of the standard\ndefinition since it also includes points that move out-of-frame. Estimating the\nmotion of these points is extremely difficult, particularly in the two-frame\nsetting. Previous work relies on CNNs to learn occlusions, without much\nsuccess, or requires multiple frames to reason about occlusions using temporal\nsmoothness. In this paper, we argue that the occlusion problem can be better\nsolved in the two-frame case by modelling image self-similarities. We introduce\na global motion aggregation module, a transformer-based approach to find\nlong-range dependencies between pixels in the first image, and perform global\naggregation on the corresponding motion features. We demonstrate that the\noptical flow estimates in the occluded regions can be significantly improved\nwithout damaging the performance in non-occluded regions. This approach obtains\nnew state-of-the-art results on the challenging Sintel dataset, improving the\naverage end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At\nthe time of submission, our method ranks first on these benchmarks among all\npublished and unpublished approaches. Code is available at\nhttps://github.com/zacjiang/GMA",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-04-06T10:32:03+00:00",
    "updated": "2021-07-29T20:59:31+00:00",
    "doi": null,
    "comment": "Accepted to ICCV 2021",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2009.09808v3": {
    "id": "http://arxiv.org/abs/2009.09808v3",
    "title": "On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes",
    "authors": [
      "Thomas Davies",
      "Derek Nowrouzezahrai",
      "Alec Jacobson"
    ],
    "abstract": "A neural implicit outputs a number indicating whether the given query point\nin space is inside, outside, or on a surface. Many prior works have focused on\n_latent-encoded_ neural implicits, where a latent vector encoding of a specific\nshape is also fed as input. While affording latent-space interpolation, this\ncomes at the cost of reconstruction accuracy for any _single_ shape. Training a\nspecific network for each 3D shape, a _weight-encoded_ neural implicit may\nforgo the latent vector and focus reconstruction accuracy on the details of a\nsingle shape. While previously considered as an intermediary representation for\n3D scanning tasks or as a toy-problem leading up to latent-encoding tasks,\nweight-encoded neural implicits have not yet been taken seriously as a 3D shape\nrepresentation. In this paper, we establish that weight-encoded neural\nimplicits meet the criteria of a first-class 3D shape representation. We\nintroduce a suite of technical contributions to improve reconstruction\naccuracy, convergence, and robustness when learning the signed distance field\ninduced by a polygonal mesh -- the _de facto_ standard representation. Viewed\nas a lossy compression, our conversion outperforms standard techniques from\ngeometry processing. Compared to previous latent- and weight-encoded neural\nimplicits we demonstrate superior robustness, scalability, and performance.",
    "categories": [
      "cs.GR",
      "cs.CG",
      "cs.CV"
    ],
    "published": "2020-09-17T23:10:19+00:00",
    "updated": "2021-01-17T21:27:01+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.GR"
  },
  "2205.11916v4": {
    "id": "http://arxiv.org/abs/2205.11916v4",
    "title": "Large Language Models are Zero-Shot Reasoners",
    "authors": [
      "Takeshi Kojima",
      "Shixiang Shane Gu",
      "Machel Reid",
      "Yutaka Matsuo",
      "Yusuke Iwasawa"
    ],
    "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2022-05-24T09:22:26+00:00",
    "updated": "2023-01-29T05:14:17+00:00",
    "doi": null,
    "comment": "Accepted to NeurIPS2022. Our code is available at\n  https://github.com/kojima-takeshi188/zero_shot_cot",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2311.13398v3": {
    "id": "http://arxiv.org/abs/2311.13398v3",
    "title": "Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images",
    "authors": [
      "Jaeyoung Chung",
      "Jeongtaek Oh",
      "Kyoung Mu Lee"
    ],
    "abstract": "In this paper, we present a method to optimize Gaussian splatting with a\nlimited number of images while avoiding overfitting. Representing a 3D scene by\ncombining numerous Gaussian splats has yielded outstanding visual quality.\nHowever, it tends to overfit the training views when only a small number of\nimages are available. To address this issue, we introduce a dense depth map as\na geometry guide to mitigate overfitting. We obtained the depth map using a\npre-trained monocular depth estimation model and aligning the scale and offset\nusing sparse COLMAP feature points. The adjusted depth aids in the color-based\noptimization of 3D Gaussian splatting, mitigating floating artifacts, and\nensuring adherence to geometric constraints. We verify the proposed method on\nthe NeRF-LLFF dataset with varying numbers of few images. Our approach\ndemonstrates robust geometry compared to the original method that relies solely\non images. Project page: robot0321.github.io/DepthRegGS",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2023-11-22T13:53:04+00:00",
    "updated": "2024-01-04T08:19:16+00:00",
    "doi": null,
    "comment": "10 pages, 5 figures; Project page: robot0321.github.io/DepthRegGS",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.06101v1": {
    "id": "http://arxiv.org/abs/2308.06101v1",
    "title": "Taming the Power of Diffusion Models for High-Quality Virtual Try-On with Appearance Flow",
    "authors": [
      "Junhong Gou",
      "Siyu Sun",
      "Jianfu Zhang",
      "Jianlou Si",
      "Chen Qian",
      "Liqing Zhang"
    ],
    "abstract": "Virtual try-on is a critical image synthesis task that aims to transfer\nclothes from one image to another while preserving the details of both humans\nand clothes. While many existing methods rely on Generative Adversarial\nNetworks (GANs) to achieve this, flaws can still occur, particularly at high\nresolutions. Recently, the diffusion model has emerged as a promising\nalternative for generating high-quality images in various applications.\nHowever, simply using clothes as a condition for guiding the diffusion model to\ninpaint is insufficient to maintain the details of the clothes. To overcome\nthis challenge, we propose an exemplar-based inpainting approach that leverages\na warping module to guide the diffusion model's generation effectively. The\nwarping module performs initial processing on the clothes, which helps to\npreserve the local details of the clothes. We then combine the warped clothes\nwith clothes-agnostic person image and add noise as the input of diffusion\nmodel. Additionally, the warped clothes is used as local conditions for each\ndenoising process to ensure that the resulting output retains as much detail as\npossible. Our approach, namely Diffusion-based Conditional Inpainting for\nVirtual Try-ON (DCI-VTON), effectively utilizes the power of the diffusion\nmodel, and the incorporation of the warping module helps to produce\nhigh-quality and realistic virtual try-on results. Experimental results on\nVITON-HD demonstrate the effectiveness and superiority of our method.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-08-11T12:23:09+00:00",
    "updated": "2023-08-11T12:23:09+00:00",
    "doi": "10.1145/3581783.3612255",
    "comment": "Accepted by ACMMM 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2204.06644v2": {
    "id": "http://arxiv.org/abs/2204.06644v2",
    "title": "METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals",
    "authors": [
      "Payal Bajaj",
      "Chenyan Xiong",
      "Guolin Ke",
      "Xiaodong Liu",
      "Di He",
      "Saurabh Tiwary",
      "Tie-Yan Liu",
      "Paul Bennett",
      "Xia Song",
      "Jianfeng Gao"
    ],
    "abstract": "We present an efficient method of pretraining large-scale autoencoding\nlanguage models using training signals generated by an auxiliary model.\nOriginated in ELECTRA, this training strategy has demonstrated\nsample-efficiency to pretrain models at the scale of hundreds of millions of\nparameters. In this work, we conduct a comprehensive empirical study, and\npropose a recipe, namely \"Model generated dEnoising TRaining Objective\"\n(METRO), which incorporates some of the best modeling techniques developed\nrecently to speed up, stabilize, and enhance pretrained language models without\ncompromising model effectiveness. The resultant models, METRO-LM, consisting of\nup to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,\nSuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in\nthat they often outperform previous large models with significantly smaller\nmodel sizes and lower pretraining cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2022-04-13T21:39:15+00:00",
    "updated": "2022-04-16T22:06:50+00:00",
    "doi": null,
    "comment": "Update details in scaled initialization and add acknowledgement",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2306.14895v1": {
    "id": "http://arxiv.org/abs/2306.14895v1",
    "title": "Large Multimodal Models: Notes on CVPR 2023 Tutorial",
    "authors": [
      "Chunyuan Li"
    ],
    "abstract": "This tutorial note summarizes the presentation on ``Large Multimodal Models:\nTowards Building and Surpassing Multimodal GPT-4'', a part of CVPR 2023\ntutorial on ``Recent Advances in Vision Foundation Models''. The tutorial\nconsists of three parts. We first introduce the background on recent GPT-like\nlarge models for vision-and-language modeling to motivate the research in\ninstruction-tuned large multimodal models (LMMs). As a pre-requisite, we\ndescribe the basics of instruction-tuning in large language models, which is\nfurther extended to the multimodal space. Lastly, we illustrate how to build\nthe minimum prototype of multimodal GPT-4 like models with the open-source\nresource, and review the recently emerged topics.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-06-26T17:59:31+00:00",
    "updated": "2023-06-26T17:59:31+00:00",
    "doi": null,
    "comment": "27 pages, 24 figures; Tutorial website:\n  https://vlp-tutorial.github.io/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1806.01482v2": {
    "id": "http://arxiv.org/abs/1806.01482v2",
    "title": "SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints",
    "authors": [
      "Amir Sadeghian",
      "Vineet Kosaraju",
      "Ali Sadeghian",
      "Noriaki Hirose",
      "S. Hamid Rezatofighi",
      "Silvio Savarese"
    ],
    "abstract": "This paper addresses the problem of path prediction for multiple interacting\nagents in a scene, which is a crucial step for many autonomous platforms such\nas self-driving cars and social robots. We present \\textit{SoPhie}; an\ninterpretable framework based on Generative Adversarial Network (GAN), which\nleverages two sources of information, the path history of all the agents in a\nscene, and the scene context information, using images of the scene. To predict\na future path for an agent, both physical and social information must be\nleveraged. Previous work has not been successful to jointly model physical and\nsocial interactions. Our approach blends a social attention mechanism with a\nphysical attention that helps the model to learn where to look in a large scene\nand extract the most salient parts of the image relevant to the path. Whereas,\nthe social attention component aggregates information across the different\nagent interactions and extracts the most important trajectory information from\nthe surrounding neighbors. SoPhie also takes advantage of GAN to generates more\nrealistic samples and to capture the uncertain nature of the future paths by\nmodeling its distribution. All these mechanisms enable our approach to predict\nsocially and physically plausible paths for the agents and to achieve\nstate-of-the-art performance on several different trajectory forecasting\nbenchmarks.",
    "categories": [
      "cs.CV"
    ],
    "published": "2018-06-05T03:49:46+00:00",
    "updated": "2018-09-20T17:42:42+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.07246v2": {
    "id": "http://arxiv.org/abs/2312.07246v2",
    "title": "Unifying Correspondence, Pose and NeRF for Pose-Free Novel View Synthesis from Stereo Pairs",
    "authors": [
      "Sunghwan Hong",
      "Jaewoo Jung",
      "Heeseong Shin",
      "Jiaolong Yang",
      "Seungryong Kim",
      "Chong Luo"
    ],
    "abstract": "This work delves into the task of pose-free novel view synthesis from stereo\npairs, a challenging and pioneering task in 3D vision. Our innovative\nframework, unlike any before, seamlessly integrates 2D correspondence matching,\ncamera pose estimation, and NeRF rendering, fostering a synergistic enhancement\nof these tasks. We achieve this through designing an architecture that utilizes\na shared representation, which serves as a foundation for enhanced 3D geometry\nunderstanding. Capitalizing on the inherent interplay between the tasks, our\nunified framework is trained end-to-end with the proposed training strategy to\nimprove overall model accuracy. Through extensive evaluations across diverse\nindoor and outdoor scenes from two real-world datasets, we demonstrate that our\napproach achieves substantial improvement over previous methodologies,\nespecially in scenarios characterized by extreme viewpoint changes and the\nabsence of accurate camera poses.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-12T13:22:44+00:00",
    "updated": "2024-04-08T07:07:02+00:00",
    "doi": null,
    "comment": "Project page: https://ku-cvlab.github.io/CoPoNeRF/ CVPR2024 camera\n  ready version (Highlight)",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2010.02983v2": {
    "id": "http://arxiv.org/abs/2010.02983v2",
    "title": "Plug and Play Autoencoders for Conditional Text Generation",
    "authors": [
      "Florian Mai",
      "Nikolaos Pappas",
      "Ivan Montero",
      "Noah A. Smith",
      "James Henderson"
    ],
    "abstract": "Text autoencoders are commonly used for conditional generation tasks such as\nstyle transfer. We propose methods which are plug and play, where any\npretrained autoencoder can be used, and only require learning a mapping within\nthe autoencoder's embedding space, training embedding-to-embedding (Emb2Emb).\nThis reduces the need for labeled training data for the task and makes the\ntraining procedure more efficient. Crucial to the success of this method is a\nloss term for keeping the mapped embedding on the manifold of the autoencoder\nand a mapping which is trained to navigate the manifold by learning offset\nvectors. Evaluations on style transfer tasks both with and without\nsequence-to-sequence supervision show that our method performs better than or\ncomparable to strong baselines while being up to four times faster.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2020-10-06T19:18:06+00:00",
    "updated": "2020-10-12T08:20:59+00:00",
    "doi": null,
    "comment": "To be published in EMNLP 2020",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2109.02079v1": {
    "id": "http://arxiv.org/abs/2109.02079v1",
    "title": "Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution",
    "authors": [
      "Jin-Fan Hu",
      "Ting-Zhu Huang",
      "Liang-Jian Deng"
    ],
    "abstract": "Hyperspectral image has become increasingly crucial due to its abundant\nspectral information. However, It has poor spatial resolution with the\nlimitation of the current imaging mechanism. Nowadays, many convolutional\nneural networks have been proposed for the hyperspectral image super-resolution\nproblem. However, convolutional neural network (CNN) based methods only\nconsider the local information instead of the global one with the limited\nkernel size of receptive field in the convolution operation. In this paper, we\ndesign a network based on the transformer for fusing the low-resolution\nhyperspectral images and high-resolution multispectral images to obtain the\nhigh-resolution hyperspectral images. Thanks to the representing ability of the\ntransformer, our approach is able to explore the intrinsic relationships of\nfeatures globally. Furthermore, considering the LR-HSIs hold the main spectral\nstructure, the network focuses on the spatial detail estimation releasing from\nthe burden of reconstructing the whole data. It reduces the mapping space of\nthe proposed network, which enhances the final performance. Various experiments\nand quality indexes show our approach's superiority compared with other\nstate-of-the-art methods.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-09-05T14:00:34+00:00",
    "updated": "2021-09-05T14:00:34+00:00",
    "doi": "10.1109/LGRS.2022.3194257",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2301.10227v2": {
    "id": "http://arxiv.org/abs/2301.10227v2",
    "title": "Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets",
    "authors": [
      "Dennis Eschweiler",
      "R\u00fcveyda Yilmaz",
      "Matisse Baumann",
      "Ina Laube",
      "Rijo Roy",
      "Abin Jose",
      "Daniel Br\u00fcckner",
      "Johannes Stegmaier"
    ],
    "abstract": "Recent advances in computer vision have led to significant progress in the\ngeneration of realistic image data, with denoising diffusion probabilistic\nmodels proving to be a particularly effective method. In this study, we\ndemonstrate that diffusion models can effectively generate fully-annotated\nmicroscopy image data sets through an unsupervised and intuitive approach,\nusing rough sketches of desired structures as the starting point. The proposed\npipeline helps to reduce the reliance on manual annotations when training deep\nlearning-based segmentation approaches and enables the segmentation of diverse\ndatasets without the need for human annotations. This approach holds great\npromise in streamlining the data generation process and enabling a more\nefficient and scalable training of segmentation models, as we show in the\nexample of different practical experiments involving various organisms and cell\ntypes.",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-01-02T14:17:08+00:00",
    "updated": "2023-08-08T10:18:04+00:00",
    "doi": null,
    "comment": "9 pages, 2 figures",
    "journal_ref": null,
    "primary_category": "eess.IV"
  },
  "1904.04536v1": {
    "id": "http://arxiv.org/abs/1904.04536v1",
    "title": "Graphonomy: Universal Human Parsing via Graph Transfer Learning",
    "authors": [
      "Ke Gong",
      "Yiming Gao",
      "Xiaodan Liang",
      "Xiaohui Shen",
      "Meng Wang",
      "Liang Lin"
    ],
    "abstract": "Prior highly-tuned human parsing models tend to fit towards each dataset in a\nspecific domain or with discrepant label granularity, and can hardly be adapted\nto other human parsing tasks without extensive re-training. In this paper, we\naim to learn a single universal human parsing model that can tackle all kinds\nof human parsing needs by unifying label annotations from different domains or\nat various levels of granularity. This poses many fundamental learning\nchallenges, e.g. discovering underlying semantic structures among different\nlabel granularity, performing proper transfer learning across different image\ndomains, and identifying and utilizing label redundancies across related tasks.\n  To address these challenges, we propose a new universal human parsing agent,\nnamed \"Graphonomy\", which incorporates hierarchical graph transfer learning\nupon the conventional parsing network to encode the underlying label semantic\nstructures and propagate relevant semantic information. In particular,\nGraphonomy first learns and propagates compact high-level graph representation\namong the labels within one dataset via Intra-Graph Reasoning, and then\ntransfers semantic information across multiple datasets via Inter-Graph\nTransfer. Various graph transfer dependencies (\\eg, similarity, linguistic\nknowledge) between different datasets are analyzed and encoded to enhance graph\ntransfer capability. By distilling universal semantic graph representation to\neach specific task, Graphonomy is able to predict all levels of parsing labels\nin one system without piling up the complexity. Experimental results show\nGraphonomy effectively achieves the state-of-the-art results on three human\nparsing benchmarks as well as advantageous universal human parsing performance.",
    "categories": [
      "cs.CV"
    ],
    "published": "2019-04-09T08:49:18+00:00",
    "updated": "2019-04-09T08:49:18+00:00",
    "doi": null,
    "comment": "Accepted to CVPR 2019. The Code is available at\n  https://github.com/Gaoyiminggithub/Graphonomy",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1412.1632v1": {
    "id": "http://arxiv.org/abs/1412.1632v1",
    "title": "Deep Learning for Answer Sentence Selection",
    "authors": [
      "Lei Yu",
      "Karl Moritz Hermann",
      "Phil Blunsom",
      "Stephen Pulman"
    ],
    "abstract": "Answer sentence selection is the task of identifying sentences that contain\nthe answer to a given question. This is an important problem in its own right\nas well as in the larger context of open domain question answering. We propose\na novel approach to solving this task via means of distributed representations,\nand learn to match questions with answers by considering their semantic\nencoding. This contrasts prior work on this task, which typically relies on\nclassifiers with large numbers of hand-crafted syntactic and semantic features\nand various external resources. Our approach does not require any feature\nengineering nor does it involve specialist linguistic data, making this model\neasily applicable to a wide range of domains and languages. Experimental\nresults on a standard benchmark dataset from TREC demonstrate that---despite\nits simplicity---our model matches state of the art performance on the answer\nsentence selection task.",
    "categories": [
      "cs.CL"
    ],
    "published": "2014-12-04T11:53:02+00:00",
    "updated": "2014-12-04T11:53:02+00:00",
    "doi": null,
    "comment": "9 pages, accepted by NIPS deep learning workshop",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2403.03542v4": {
    "id": "http://arxiv.org/abs/2403.03542v4",
    "title": "DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training",
    "authors": [
      "Zhongkai Hao",
      "Chang Su",
      "Songming Liu",
      "Julius Berner",
      "Chengyang Ying",
      "Hang Su",
      "Anima Anandkumar",
      "Jian Song",
      "Jun Zhu"
    ],
    "abstract": "Pre-training has been investigated to improve the efficiency and performance\nof training neural operators in data-scarce settings. However, it is largely in\nits infancy due to the inherent complexity and diversity, such as long\ntrajectories, multiple scales and varying dimensions of partial differential\nequations (PDEs) data. In this paper, we present a new auto-regressive\ndenoising pre-training strategy, which allows for more stable and efficient\npre-training on PDE data and generalizes to various downstream tasks. Moreover,\nby designing a flexible and scalable model architecture based on Fourier\nattention, we can easily scale up the model for large-scale pre-training. We\ntrain our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets\nwith more than 100k trajectories. Extensive experiments show that we achieve\nSOTA on these benchmarks and validate the strong generalizability of our model\nto significantly enhance performance on diverse downstream PDE tasks like 3D\ndata. Code is available at \\url{https://github.com/thu-ml/DPOT}.",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "published": "2024-03-06T08:38:34+00:00",
    "updated": "2024-05-07T01:57:00+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2112.06825v2": {
    "id": "http://arxiv.org/abs/2112.06825v2",
    "title": "VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks",
    "authors": [
      "Yi-Lin Sung",
      "Jaemin Cho",
      "Mohit Bansal"
    ],
    "abstract": "Recently, fine-tuning language models pre-trained on large text corpora have\nprovided huge improvements on vision-and-language (V&L) tasks as well as on\npure language tasks. However, fine-tuning the entire parameter set of\npre-trained models becomes impractical since the model size is growing rapidly.\nHence, in this paper, we introduce adapter-based parameter-efficient transfer\nlearning techniques to V&L models such as VL-BART and VLT5. We evaluate our\nmethods in a unified multi-task setup on both image-text and video-text\nbenchmarks. For the image-text tasks, we use four diverse V&L datasets: VQAv2,\nGQA, NLVR2 , and MSCOCO image captioning. For video-text tasks, we use TVQA,\nHow2QA, TVC, and YC2C. With careful training and thorough experiments, we\nbenchmark three popular adapter-based methods (Adapter, Hyperformer, Compacter)\nagainst the standard full fine-tuning and the recently proposed prompt-tuning\napproach. We also enhance the efficiency and performance of adapters by sharing\ntheir weights to attain knowledge across tasks. Our results demonstrate that\ntraining the adapter with the weight-sharing technique (4.18% of total\nparameters for image-text tasks and 3.39% for video-text tasks) can match the\nperformance of fine-tuning the entire model. Lastly, we present a comprehensive\nanalysis including the combination of adapter and task-specific prompts and the\nimpact of V&L pre-training on adapters. Our code is available at:\nhttps://github.com/ylsung/VL_adapter.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2021-12-13T17:35:26+00:00",
    "updated": "2022-03-24T17:33:07+00:00",
    "doi": null,
    "comment": "CVPR 2022 (15 pages; with new video-text and CLIP-ViL experiments)",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.16174v1": {
    "id": "http://arxiv.org/abs/2210.16174v1",
    "title": "Multimodal Transformer for Parallel Concatenated Variational Autoencoders",
    "authors": [
      "Stephen D. Liang",
      "Jerry M. Mendel"
    ],
    "abstract": "In this paper, we propose a multimodal transformer using parallel\nconcatenated architecture. Instead of using patches, we use column stripes for\nimages in R, G, B channels as the transformer input. The column stripes keep\nthe spatial relations of original image. We incorporate the multimodal\ntransformer with variational autoencoder for synthetic cross-modal data\ngeneration. The multimodal transformer is designed using multiple compression\nmatrices, and it serves as encoders for Parallel Concatenated Variational\nAutoEncoders (PC-VAE). The PC-VAE consists of multiple encoders, one latent\nspace, and two decoders. The encoders are based on random Gaussian matrices and\ndon't need any training. We propose a new loss function based on the\ninteraction information from partial information decomposition. The interaction\ninformation evaluates the input cross-modal information and decoder output. The\nPC-VAE are trained via minimizing the loss function. Experiments are performed\nto validate the proposed multimodal transformer for PC-VAE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2022-10-28T14:45:32+00:00",
    "updated": "2022-10-28T14:45:32+00:00",
    "doi": null,
    "comment": "NeurIPS 2022 Workshop on Vision Transformers: Theory and Application,\n  New Orleans, LA, December 2022",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2212.10544v2": {
    "id": "http://arxiv.org/abs/2212.10544v2",
    "title": "Pretraining Without Attention",
    "authors": [
      "Junxiong Wang",
      "Jing Nathan Yan",
      "Albert Gu",
      "Alexander M. Rush"
    ],
    "abstract": "Transformers have been essential to pretraining success in NLP. While other\narchitectures have been used, downstream accuracy is either significantly\nworse, or requires attention layers to match standard benchmarks such as GLUE.\nThis work explores pretraining without attention by using recent advances in\nsequence routing based on state-space models (SSMs). Our proposed model,\nBidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative\ngating architecture that has been effective in simplified sequence modeling\narchitectures. The model learns static layers that do not consider pair-wise\ninteractions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE\nand can be extended to long-form pretraining of 4096 tokens without\napproximation. Analysis shows that while the models have similar average\naccuracy, the approach has different inductive biases than BERT in terms of\ninteractions and syntactic representations. All models from this work are\navailable at https://github.com/jxiw/BiGS.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2022-12-20T18:50:08+00:00",
    "updated": "2023-05-09T01:08:20+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2405.16205v1": {
    "id": "http://arxiv.org/abs/2405.16205v1",
    "title": "GeneAgent: Self-verification Language Agent for Gene Set Knowledge Discovery using Domain Databases",
    "authors": [
      "Zhizheng Wang",
      "Qiao Jin",
      "Chih-Hsuan Wei",
      "Shubo Tian",
      "Po-Ting Lai",
      "Qingqing Zhu",
      "Chi-Ping Day",
      "Christina Ross",
      "Zhiyong Lu"
    ],
    "abstract": "Gene set knowledge discovery is essential for advancing human functional\ngenomics. Recent studies have shown promising performance by harnessing the\npower of Large Language Models (LLMs) on this task. Nonetheless, their results\nare subject to several limitations common in LLMs such as hallucinations. In\nresponse, we present GeneAgent, a first-of-its-kind language agent featuring\nself-verification capability. It autonomously interacts with various biological\ndatabases and leverages relevant domain knowledge to improve accuracy and\nreduce hallucination occurrences. Benchmarking on 1,106 gene sets from\ndifferent sources, GeneAgent consistently outperforms standard GPT-4 by a\nsignificant margin. Moreover, a detailed manual review confirms the\neffectiveness of the self-verification module in minimizing hallucinations and\ngenerating more reliable analytical narratives. To demonstrate its practical\nutility, we apply GeneAgent to seven novel gene sets derived from mouse B2905\nmelanoma cell lines, with expert evaluations showing that GeneAgent offers\nnovel insights into gene functions and subsequently expedites knowledge\ndiscovery.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-05-25T12:35:15+00:00",
    "updated": "2024-05-25T12:35:15+00:00",
    "doi": null,
    "comment": "30 pages with 10 figures and/or tables",
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2402.18396v1": {
    "id": "http://arxiv.org/abs/2402.18396v1",
    "title": "Deep Confident Steps to New Pockets: Strategies for Docking Generalization",
    "authors": [
      "Gabriele Corso",
      "Arthur Deng",
      "Benjamin Fry",
      "Nicholas Polizzi",
      "Regina Barzilay",
      "Tommi Jaakkola"
    ],
    "abstract": "Accurate blind docking has the potential to lead to new biological\nbreakthroughs, but for this promise to be realized, docking methods must\ngeneralize well across the proteome. Existing benchmarks, however, fail to\nrigorously assess generalizability. Therefore, we develop DockGen, a new\nbenchmark based on the ligand-binding domains of proteins, and we show that\nexisting machine learning-based docking models have very weak generalization\nabilities. We carefully analyze the scaling laws of ML-based docking and show\nthat, by scaling data and model size, as well as integrating synthetic data\nstrategies, we are able to significantly increase the generalization capacity\nand set new state-of-the-art performance across benchmarks. Further, we propose\nConfidence Bootstrapping, a new training paradigm that solely relies on the\ninteraction between diffusion and confidence models and exploits the\nmulti-resolution generation process of diffusion models. We demonstrate that\nConfidence Bootstrapping significantly improves the ability of ML-based docking\nmethods to dock to unseen protein classes, edging closer to accurate and\ngeneralizable blind docking methods.",
    "categories": [
      "q-bio.BM",
      "cs.LG"
    ],
    "published": "2024-02-28T15:15:23+00:00",
    "updated": "2024-02-28T15:15:23+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": "International Conference on Learning Representations 2024",
    "primary_category": "q-bio.BM"
  },
  "2306.03745v2": {
    "id": "http://arxiv.org/abs/2306.03745v2",
    "title": "Soft Merging of Experts with Adaptive Routing",
    "authors": [
      "Mohammed Muqeeth",
      "Haokun Liu",
      "Colin Raffel"
    ],
    "abstract": "Sparsely activated neural networks with conditional computation learn to\nroute their inputs through different \"expert\" subnetworks, providing a form of\nmodularity that densely activated models lack. Despite their possible benefits,\nmodels with learned routing often underperform their parameter-matched densely\nactivated counterparts as well as models that use non-learned heuristic routing\nstrategies. In this paper, we hypothesize that these shortcomings stem from the\ngradient estimation techniques used to train sparsely activated models that use\nnon-differentiable discrete routing decisions. To address this issue, we\nintroduce Soft Merging of Experts with Adaptive Routing (SMEAR), which avoids\ndiscrete routing by using a single \"merged\" expert constructed via a weighted\naverage of all of the experts' parameters. By routing activations through a\nsingle merged expert, SMEAR does not incur a significant increase in\ncomputational costs and enables standard gradient-based training. We\nempirically validate that models using SMEAR outperform models that route based\non metadata or learn sparse routing through gradient estimation. Furthermore,\nwe provide qualitative analysis demonstrating that the experts learned via\nSMEAR exhibit a significant amount of specialization. All of the code used in\nour experiments is publicly available.",
    "categories": [
      "cs.LG"
    ],
    "published": "2023-06-06T15:04:31+00:00",
    "updated": "2024-05-13T16:20:29+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2302.10307v1": {
    "id": "http://arxiv.org/abs/2302.10307v1",
    "title": "ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency",
    "authors": [
      "Pengzhen Ren",
      "Changlin Li",
      "Hang Xu",
      "Yi Zhu",
      "Guangrun Wang",
      "Jianzhuang Liu",
      "Xiaojun Chang",
      "Xiaodan Liang"
    ],
    "abstract": "Recently, great success has been made in learning visual representations from\ntext supervision, facilitating the emergence of text-supervised semantic\nsegmentation. However, existing works focus on pixel grouping and cross-modal\nsemantic alignment, while ignoring the correspondence among multiple augmented\nviews of the same image. To overcome such limitation, we propose\nmulti-\\textbf{View} \\textbf{Co}nsistent learning (ViewCo) for text-supervised\nsemantic segmentation. Specifically, we first propose text-to-views consistency\nmodeling to learn correspondence for multiple views of the same input image.\nAdditionally, we propose cross-view segmentation consistency modeling to\naddress the ambiguity issue of text supervision by contrasting the segment\nfeatures of Siamese visual encoders. The text-to-views consistency benefits the\ndense assignment of the visual features by encouraging different crops to align\nwith the same text, while the cross-view segmentation consistency modeling\nprovides additional self-supervision, overcoming the limitation of ambiguous\ntext supervision for segmentation masks. Trained with large-scale image-text\ndata, our model can directly segment objects of arbitrary categories in a\nzero-shot manner. Extensive experiments show that ViewCo outperforms\nstate-of-the-art methods on average by up to 2.9\\%, 1.6\\%, and 2.4\\% mIoU on\nPASCAL VOC2012, PASCAL Context, and COCO, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-01-31T01:57:52+00:00",
    "updated": "2023-01-31T01:57:52+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1409.1556v6": {
    "id": "http://arxiv.org/abs/1409.1556v6",
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "authors": [
      "Karen Simonyan",
      "Andrew Zisserman"
    ],
    "abstract": "In this work we investigate the effect of the convolutional network depth on\nits accuracy in the large-scale image recognition setting. Our main\ncontribution is a thorough evaluation of networks of increasing depth using an\narchitecture with very small (3x3) convolution filters, which shows that a\nsignificant improvement on the prior-art configurations can be achieved by\npushing the depth to 16-19 weight layers. These findings were the basis of our\nImageNet Challenge 2014 submission, where our team secured the first and the\nsecond places in the localisation and classification tracks respectively. We\nalso show that our representations generalise well to other datasets, where\nthey achieve state-of-the-art results. We have made our two best-performing\nConvNet models publicly available to facilitate further research on the use of\ndeep visual representations in computer vision.",
    "categories": [
      "cs.CV"
    ],
    "published": "2014-09-04T19:48:04+00:00",
    "updated": "2015-04-10T16:25:04+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1806.01376v1": {
    "id": "http://arxiv.org/abs/1806.01376v1",
    "title": "Factorized Adversarial Networks for Unsupervised Domain Adaptation",
    "authors": [
      "Jian Ren",
      "Jianchao Yang",
      "Ning Xu",
      "David J. Foran"
    ],
    "abstract": "In this paper, we propose Factorized Adversarial Networks (FAN) to solve\nunsupervised domain adaptation problems for image classification tasks. Our\nnetworks map the data distribution into a latent feature space, which is\nfactorized into a domain-specific subspace that contains domain-specific\ncharacteristics and a task-specific subspace that retains category information,\nfor both source and target domains, respectively. Unsupervised domain\nadaptation is achieved by adversarial training to minimize the discrepancy\nbetween the distributions of two task-specific subspaces from source and target\ndomains. We demonstrate that the proposed approach outperforms state-of-the-art\nmethods on multiple benchmark datasets used in the literature for unsupervised\ndomain adaptation. Furthermore, we collect two real-world tagging datasets that\nare much larger than existing benchmark datasets, and get significant\nimprovement upon baselines, proving the practical value of our approach.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2018-06-04T20:39:13+00:00",
    "updated": "2018-06-04T20:39:13+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1802.09070v1": {
    "id": "http://arxiv.org/abs/1802.09070v1",
    "title": "Attention-Aware Generative Adversarial Networks (ATA-GANs)",
    "authors": [
      "Dimitris Kastaniotis",
      "Ioanna Ntinou",
      "Dimitrios Tsourounis",
      "George Economou",
      "Spiros Fotopoulos"
    ],
    "abstract": "In this work, we present a novel approach for training Generative Adversarial\nNetworks (GANs). Using the attention maps produced by a Teacher- Network we are\nable to improve the quality of the generated images as well as perform weakly\nobject localization on the generated images. To this end, we generate images of\nHEp-2 cells captured with Indirect Imunofluoresence (IIF) and study the ability\nof our network to perform a weakly localization of the cell. Firstly, we\ndemonstrate that whilst GANs can learn the mapping between the input domain and\nthe target distribution efficiently, the discriminator network is not able to\ndetect the regions of interest. Secondly, we present a novel attention transfer\nmechanism which allows us to enforce the discriminator to put emphasis on the\nregions of interest via transfer learning. Thirdly, we show that this leads to\nmore realistic images, as the discriminator learns to put emphasis on the area\nof interest. Fourthly, the proposed method allows one to generate both images\nas well as attention maps which can be useful for data annotation e.g in object\ndetection.",
    "categories": [
      "cs.CV"
    ],
    "published": "2018-02-25T19:40:30+00:00",
    "updated": "2018-02-25T19:40:30+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.17258v1": {
    "id": "http://arxiv.org/abs/2401.17258v1",
    "title": "You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation",
    "authors": [
      "Mehdi Noroozi",
      "Isma Hadji",
      "Brais Martinez",
      "Adrian Bulat",
      "Georgios Tzimiropoulos"
    ],
    "abstract": "In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach\nfor image super-resolution that yields state-of-the-art results using only a\nsingle DDIM step. We propose a novel scale distillation approach to train our\nSR model. Instead of directly training our SR model on the scale factor of\ninterest, we start by training a teacher model on a smaller magnification\nscale, thereby making the SR problem simpler for the teacher. We then train a\nstudent model for a higher magnification scale, using the predictions of the\nteacher as a target during the training. This process is repeated iteratively\nuntil we reach the target scale factor of the final model. The rationale behind\nour scale distillation is that the teacher aids the student diffusion model\ntraining by i) providing a target adapted to the current noise level rather\nthan using the same target coming from ground truth data for all noise levels\nand ii) providing an accurate target as the teacher has a simpler task to\nsolve. We empirically show that the distilled model significantly outperforms\nthe model trained for high scales directly, specifically with few steps during\ninference. Having a strong diffusion model that requires only one step allows\nus to freeze the U-Net and fine-tune the decoder on top of it. We show that the\ncombination of spatially distilled U-Net and fine-tuned decoder outperforms\nstate-of-the-art methods requiring 200 steps with only one single step.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-01-30T18:49:44+00:00",
    "updated": "2024-01-30T18:49:44+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2402.01586v4": {
    "id": "http://arxiv.org/abs/2402.01586v4",
    "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents",
    "authors": [
      "Wenyue Hua",
      "Xianjun Yang",
      "Mingyu Jin",
      "Zelong Li",
      "Wei Cheng",
      "Ruixiang Tang",
      "Yongfeng Zhang"
    ],
    "abstract": "The rise of LLM-based agents shows great potential to revolutionize task\nplanning, capturing significant attention. Given that these agents will be\nintegrated into high-stake domains, ensuring their reliability and safety is\ncrucial. This paper presents an Agent-Constitution-based agent framework,\nTrustAgent, with a particular focus on improving the LLM-based agent safety.\nThe proposed framework ensures strict adherence to the Agent Constitution\nthrough three strategic components: pre-planning strategy which injects safety\nknowledge to the model before plan generation, in-planning strategy which\nenhances safety during plan generation, and post-planning strategy which\nensures safety by post-planning inspection. Our experimental results\ndemonstrate that the proposed framework can effectively enhance an LLM agent's\nsafety across multiple domains by identifying and mitigating potential dangers\nduring the planning. Further analysis reveals that the framework not only\nimproves safety but also enhances the helpfulness of the agent. Additionally,\nwe highlight the importance of the LLM reasoning ability in adhering to the\nConstitution. This paper sheds light on how to ensure the safe integration of\nLLM-based agents into human-centric environments. Data and code are available\nat https://github.com/agiresearch/TrustAgent.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2024-02-02T17:26:23+00:00",
    "updated": "2024-10-03T22:12:05+00:00",
    "doi": null,
    "comment": "In EMNLP 2024",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2010.10150v1": {
    "id": "http://arxiv.org/abs/2010.10150v1",
    "title": "Local Knowledge Powered Conversational Agents",
    "authors": [
      "Sashank Santhanam",
      "Wei Ping",
      "Raul Puri",
      "Mohammad Shoeybi",
      "Mostofa Patwary",
      "Bryan Catanzaro"
    ],
    "abstract": "State-of-the-art conversational agents have advanced significantly in\nconjunction with the use of large transformer-based language models. However,\neven with these advancements, conversational agents still lack the ability to\nproduce responses that are informative and coherent with the local context. In\nthis work, we propose a dialog framework that incorporates both local knowledge\nas well as users' past dialogues to generate high quality conversations. We\nintroduce an approach to build a dataset based on Reddit conversations, where\noutbound URL links are widely available in the conversations and the\nhyperlinked documents can be naturally included as local external knowledge.\nUsing our framework and dataset, we demonstrate that incorporating local\nknowledge can largely improve informativeness, coherency and realisticness\nmeasures using human evaluations. In particular, our approach consistently\noutperforms the state-of-the-art conversational model on the Reddit dataset\nacross all three measures. We also find that scaling the size of our models\nfrom 117M to 8.3B parameters yields consistent improvement of validation\nperplexity as well as human evaluated metrics. Our model with 8.3B parameters\ncan generate human-like responses as rated by various human evaluations in a\nsingle-turn dialog setting.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2020-10-20T09:34:40+00:00",
    "updated": "2020-10-20T09:34:40+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2403.16990v1": {
    "id": "http://arxiv.org/abs/2403.16990v1",
    "title": "Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation",
    "authors": [
      "Omer Dahary",
      "Or Patashnik",
      "Kfir Aberman",
      "Daniel Cohen-Or"
    ],
    "abstract": "Text-to-image diffusion models have an unprecedented ability to generate\ndiverse and high-quality images. However, they often struggle to faithfully\ncapture the intended semantics of complex input prompts that include multiple\nsubjects. Recently, numerous layout-to-image extensions have been introduced to\nimprove user control, aiming to localize subjects represented by specific\ntokens. Yet, these methods often produce semantically inaccurate images,\nespecially when dealing with multiple semantically or visually similar\nsubjects. In this work, we study and analyze the causes of these limitations.\nOur exploration reveals that the primary issue stems from inadvertent semantic\nleakage between subjects in the denoising process. This leakage is attributed\nto the diffusion model's attention layers, which tend to blend the visual\nfeatures of different subjects. To address these issues, we introduce Bounded\nAttention, a training-free method for bounding the information flow in the\nsampling process. Bounded Attention prevents detrimental leakage among subjects\nand enables guiding the generation to promote each subject's individuality,\neven with complex multi-subject conditioning. Through extensive\nexperimentation, we demonstrate that our method empowers the generation of\nmultiple subjects that better align with given prompts and layouts.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2024-03-25T17:52:07+00:00",
    "updated": "2024-03-25T17:52:07+00:00",
    "doi": null,
    "comment": "Project page: https://omer11a.github.io/bounded-attention/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.12574v2": {
    "id": "http://arxiv.org/abs/2308.12574v2",
    "title": "Modeling Uncertainty and Using Post-fusion as Fallback Improves Retrieval Augmented Generation with LLMs",
    "authors": [
      "Ye Liu",
      "Semih Yavuz",
      "Rui Meng",
      "Meghana Moorthy",
      "Shafiq Joty",
      "Caiming Xiong",
      "Yingbo Zhou"
    ],
    "abstract": "The integration of retrieved passages and large language models (LLMs), such\nas ChatGPTs, has significantly contributed to improving open-domain question\nanswering. However, there is still a lack of exploration regarding the optimal\napproach for incorporating retrieved passages into the answer generation\nprocess. This paper aims to fill this gap by investigating different methods of\ncombining retrieved passages with LLMs to enhance answer generation. We begin\nby examining the limitations of a commonly-used concatenation approach.\nSurprisingly, this approach often results in generating \"unknown\" outputs, even\nwhen the correct document is among the top-k retrieved passages. To address\nthis issue, we explore four alternative strategies for integrating the\nretrieved passages with the LLMs. These strategies include two single-round\nmethods that utilize chain-of-thought reasoning and two multi-round strategies\nthat incorporate feedback loops. Through comprehensive analyses and\nexperiments, we provide insightful observations on how to effectively leverage\nretrieved passages to enhance the answer generation capability of LLMs.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2023-08-24T05:26:54+00:00",
    "updated": "2024-04-08T03:37:00+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.IR"
  },
  "2204.02776v2": {
    "id": "http://arxiv.org/abs/2204.02776v2",
    "title": "3D face reconstruction with dense landmarks",
    "authors": [
      "Erroll Wood",
      "Tadas Baltrusaitis",
      "Charlie Hewitt",
      "Matthew Johnson",
      "Jingjing Shen",
      "Nikola Milosavljevic",
      "Daniel Wilde",
      "Stephan Garbin",
      "Chirag Raman",
      "Jamie Shotton",
      "Toby Sharp",
      "Ivan Stojiljkovic",
      "Tom Cashman",
      "Julien Valentin"
    ],
    "abstract": "Landmarks often play a key role in face analysis, but many aspects of\nidentity or expression cannot be represented by sparse landmarks alone. Thus,\nin order to reconstruct faces more accurately, landmarks are often combined\nwith additional signals like depth images or techniques like differentiable\nrendering. Can we keep things simple by just using more landmarks? In answer,\nwe present the first method that accurately predicts 10x as many landmarks as\nusual, covering the whole head, including the eyes and teeth. This is\naccomplished using synthetic training data, which guarantees perfect landmark\nannotations. By fitting a morphable model to these dense landmarks, we achieve\nstate-of-the-art results for monocular 3D face reconstruction in the wild. We\nshow that dense landmarks are an ideal signal for integrating face shape\ninformation across frames by demonstrating accurate and expressive facial\nperformance capture in both monocular and multi-view scenarios. This approach\nis also highly efficient: we can predict dense landmarks and fit our 3D face\nmodel at over 150FPS on a single CPU thread. Please see our website:\nhttps://microsoft.github.io/DenseLandmarks/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-04-06T12:43:34+00:00",
    "updated": "2022-07-20T22:07:30+00:00",
    "doi": null,
    "comment": "ECCV 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2004.03805v1": {
    "id": "http://arxiv.org/abs/2004.03805v1",
    "title": "State of the Art on Neural Rendering",
    "authors": [
      "Ayush Tewari",
      "Ohad Fried",
      "Justus Thies",
      "Vincent Sitzmann",
      "Stephen Lombardi",
      "Kalyan Sunkavalli",
      "Ricardo Martin-Brualla",
      "Tomas Simon",
      "Jason Saragih",
      "Matthias Nie\u00dfner",
      "Rohit Pandey",
      "Sean Fanello",
      "Gordon Wetzstein",
      "Jun-Yan Zhu",
      "Christian Theobalt",
      "Maneesh Agrawala",
      "Eli Shechtman",
      "Dan B Goldman",
      "Michael Zollh\u00f6fer"
    ],
    "abstract": "Efficient rendering of photo-realistic virtual worlds is a long standing\neffort of computer graphics. Modern graphics techniques have succeeded in\nsynthesizing photo-realistic images from hand-crafted scene representations.\nHowever, the automatic generation of shape, materials, lighting, and other\naspects of scenes remains a challenging problem that, if solved, would make\nphoto-realistic computer graphics more widely accessible. Concurrently,\nprogress in computer vision and machine learning have given rise to a new\napproach to image synthesis and editing, namely deep generative models. Neural\nrendering is a new and rapidly emerging field that combines generative machine\nlearning techniques with physical knowledge from computer graphics, e.g., by\nthe integration of differentiable rendering into network training. With a\nplethora of applications in computer graphics and vision, neural rendering is\npoised to become a new area in the graphics community, yet no survey of this\nemerging field exists. This state-of-the-art report summarizes the recent\ntrends and applications of neural rendering. We focus on approaches that\ncombine classic computer graphics techniques with deep generative models to\nobtain controllable and photo-realistic outputs. Starting with an overview of\nthe underlying computer graphics and machine learning concepts, we discuss\ncritical aspects of neural rendering approaches. This state-of-the-art report\nis focused on the many important use cases for the described algorithms such as\nnovel view synthesis, semantic photo manipulation, facial and body reenactment,\nrelighting, free-viewpoint video, and the creation of photo-realistic avatars\nfor virtual and augmented reality telepresence. Finally, we conclude with a\ndiscussion of the social implications of such technology and investigate open\nresearch problems.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2020-04-08T04:36:31+00:00",
    "updated": "2020-04-08T04:36:31+00:00",
    "doi": null,
    "comment": "Eurographics 2020 survey paper",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.01152v1": {
    "id": "http://arxiv.org/abs/2312.01152v1",
    "title": "Ultra-Resolution Cascaded Diffusion Model for Gigapixel Image Synthesis in Histopathology",
    "authors": [
      "Sarah Cechnicka",
      "Hadrien Reynaud",
      "James Ball",
      "Naomi Simmonds",
      "Catherine Horsfield",
      "Andrew Smith",
      "Candice Roufosse",
      "Bernhard Kainz"
    ],
    "abstract": "Diagnoses from histopathology images rely on information from both high and\nlow resolutions of Whole Slide Images. Ultra-Resolution Cascaded Diffusion\nModels (URCDMs) allow for the synthesis of high-resolution images that are\nrealistic at all magnification levels, focusing not only on fidelity but also\non long-distance spatial coherency. Our model beats existing methods, improving\nthe pFID-50k [2] score by 110.63 to 39.52 pFID-50k. Additionally, a human\nexpert evaluation study was performed, reaching a weighted Mean Absolute Error\n(MAE) of 0.11 for the Lower Resolution Diffusion Models and a weighted MAE of\n0.22 for the URCDM.",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "published": "2023-12-02T14:41:26+00:00",
    "updated": "2023-12-02T14:41:26+00:00",
    "doi": null,
    "comment": "MedNeurIPS 2023 poster",
    "journal_ref": null,
    "primary_category": "eess.IV"
  },
  "2201.03514v4": {
    "id": "http://arxiv.org/abs/2201.03514v4",
    "title": "Black-Box Tuning for Language-Model-as-a-Service",
    "authors": [
      "Tianxiang Sun",
      "Yunfan Shao",
      "Hong Qian",
      "Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "abstract": "Extremely large pre-trained language models (PTMs) such as GPT-3 are usually\nreleased as a service. It allows users to design task-specific prompts to query\nthe PTMs through some black-box APIs. In such a scenario, which we call\nLanguage-Model-as-a-Service (LMaaS), the gradients of PTMs are usually\nunavailable. Can we optimize the task prompts by only accessing the model\ninference APIs? This paper proposes the black-box tuning framework to optimize\nthe continuous prompt prepended to the input text via derivative-free\noptimization. Instead of optimizing in the original high-dimensional prompt\nspace, which is intractable for traditional derivative-free optimization, we\nperform optimization in a randomly generated subspace due to the low intrinsic\ndimensionality of large PTMs. The experimental results show that the black-box\ntuning with RoBERTa on a few labeled samples not only significantly outperforms\nmanual prompt and GPT-3's in-context learning, but also surpasses the\ngradient-based counterparts, i.e., prompt tuning and full model tuning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2022-01-10T18:17:05+00:00",
    "updated": "2022-06-27T08:14:54+00:00",
    "doi": null,
    "comment": "Accepted by ICML 2022. Camera-ready version",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2307.02598v2": {
    "id": "http://arxiv.org/abs/2307.02598v2",
    "title": "Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation",
    "authors": [
      "S\u00e9bastien Lachapelle",
      "Divyat Mahajan",
      "Ioannis Mitliagkas",
      "Simon Lacoste-Julien"
    ],
    "abstract": "We tackle the problems of latent variables identification and\n``out-of-support'' image generation in representation learning. We show that\nboth are possible for a class of decoders that we call additive, which are\nreminiscent of decoders used for object-centric representation learning (OCRL)\nand well suited for images that can be decomposed as a sum of object-specific\nimages. We provide conditions under which exactly solving the reconstruction\nproblem using an additive decoder is guaranteed to identify the blocks of\nlatent variables up to permutation and block-wise invertible transformations.\nThis guarantee relies only on very weak assumptions about the distribution of\nthe latent factors, which might present statistical dependencies and have an\nalmost arbitrarily shaped support. Our result provides a new setting where\nnonlinear independent component analysis (ICA) is possible and adds to our\ntheoretical understanding of OCRL methods. We also show theoretically that\nadditive decoders can generate novel images by recombining observed factors of\nvariations in novel ways, an ability we refer to as Cartesian-product\nextrapolation. We show empirically that additivity is crucial for both\nidentifiability and extrapolation on simulated data.",
    "categories": [
      "cs.LG",
      "stat.ML",
      "I.2.6; I.5.1"
    ],
    "published": "2023-07-05T18:48:20+00:00",
    "updated": "2023-11-02T16:02:39+00:00",
    "doi": null,
    "comment": "Appears in: Advances in Neural Information Processing Systems 37\n  (NeurIPS 2023). 39 pages",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2205.04066v3": {
    "id": "http://arxiv.org/abs/2205.04066v3",
    "title": "Multi-level Consistency Learning for Semi-supervised Domain Adaptation",
    "authors": [
      "Zizheng Yan",
      "Yushuang Wu",
      "Guanbin Li",
      "Yipeng Qin",
      "Xiaoguang Han",
      "Shuguang Cui"
    ],
    "abstract": "Semi-supervised domain adaptation (SSDA) aims to apply knowledge learned from\na fully labeled source domain to a scarcely labeled target domain. In this\npaper, we propose a Multi-level Consistency Learning (MCL) framework for SSDA.\nSpecifically, our MCL regularizes the consistency of different views of target\ndomain samples at three levels: (i) at inter-domain level, we robustly and\naccurately align the source and target domains using a prototype-based optimal\ntransport method that utilizes the pros and cons of different views of target\nsamples; (ii) at intra-domain level, we facilitate the learning of both\ndiscriminative and compact target feature representations by proposing a novel\nclass-wise contrastive clustering loss; (iii) at sample level, we follow\nstandard practice and improve the prediction accuracy by conducting a\nconsistency-based self-training. Empirically, we verified the effectiveness of\nour MCL framework on three popular SSDA benchmarks, i.e., VisDA2017, DomainNet,\nand Office-Home datasets, and the experimental results demonstrate that our MCL\nframework achieves the state-of-the-art performance.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-05-09T06:41:18+00:00",
    "updated": "2022-06-28T10:25:30+00:00",
    "doi": null,
    "comment": "IJCAI 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2310.00684v2": {
    "id": "http://arxiv.org/abs/2310.00684v2",
    "title": "How Many Views Are Needed to Reconstruct an Unknown Object Using NeRF?",
    "authors": [
      "Sicong Pan",
      "Liren Jin",
      "Hao Hu",
      "Marija Popovi\u0107",
      "Maren Bennewitz"
    ],
    "abstract": "Neural Radiance Fields (NeRFs) are gaining significant interest for online\nactive object reconstruction due to their exceptional memory efficiency and\nrequirement for only posed RGB inputs. Previous NeRF-based view planning\nmethods exhibit computational inefficiency since they rely on an iterative\nparadigm, consisting of (1) retraining the NeRF when new images arrive; and (2)\nplanning a path to the next best view only. To address these limitations, we\npropose a non-iterative pipeline based on the Prediction of the Required number\nof Views (PRV). The key idea behind our approach is that the required number of\nviews to reconstruct an object depends on its complexity. Therefore, we design\na deep neural network, named PRVNet, to predict the required number of views,\nallowing us to tailor the data acquisition based on the object complexity and\nplan a globally shortest path. To train our PRVNet, we generate supervision\nlabels using the ShapeNet dataset. Simulated experiments show that our\nPRV-based view planning method outperforms baselines, achieving good\nreconstruction quality while significantly reducing movement cost and planning\ntime. We further justify the generalization ability of our approach in a\nreal-world experiment.",
    "categories": [
      "cs.RO"
    ],
    "published": "2023-10-01T14:27:47+00:00",
    "updated": "2024-02-13T16:12:11+00:00",
    "doi": null,
    "comment": "Sicong Pan and Liren Jin have equal contribution. Publication to\n  appear in IEEE International Conference on Robotics and Automation (ICRA),\n  2024",
    "journal_ref": null,
    "primary_category": "cs.RO"
  },
  "1711.09485v2": {
    "id": "http://arxiv.org/abs/1711.09485v2",
    "title": "SkipNet: Learning Dynamic Routing in Convolutional Networks",
    "authors": [
      "Xin Wang",
      "Fisher Yu",
      "Zi-Yi Dou",
      "Trevor Darrell",
      "Joseph E. Gonzalez"
    ],
    "abstract": "While deeper convolutional networks are needed to achieve maximum accuracy in\nvisual perception tasks, for many inputs shallower networks are sufficient. We\nexploit this observation by learning to skip convolutional layers on a\nper-input basis. We introduce SkipNet, a modified residual network, that uses a\ngating network to selectively skip convolutional blocks based on the\nactivations of the previous layer. We formulate the dynamic skipping problem in\nthe context of sequential decision making and propose a hybrid learning\nalgorithm that combines supervised learning and reinforcement learning to\naddress the challenges of non-differentiable skipping decisions. We show\nSkipNet reduces computation by 30-90% while preserving the accuracy of the\noriginal model on four benchmark datasets and outperforms the state-of-the-art\ndynamic networks and static compression methods. We also qualitatively evaluate\nthe gating policy to reveal a relationship between image scale and saliency and\nthe number of layers skipped.",
    "categories": [
      "cs.CV"
    ],
    "published": "2017-11-26T23:03:37+00:00",
    "updated": "2018-07-25T06:13:24+00:00",
    "doi": null,
    "comment": "ECCV 2018 Camera ready version. Code is available at\n  https://github.com/ucbdrive/skipnet",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1901.08149v2": {
    "id": "http://arxiv.org/abs/1901.08149v2",
    "title": "TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents",
    "authors": [
      "Thomas Wolf",
      "Victor Sanh",
      "Julien Chaumond",
      "Clement Delangue"
    ],
    "abstract": "We introduce a new approach to generative data-driven dialogue systems (e.g.\nchatbots) called TransferTransfo which is a combination of a Transfer learning\nbased training scheme and a high-capacity Transformer model. Fine-tuning is\nperformed by using a multi-task objective which combines several unsupervised\nprediction tasks. The resulting fine-tuned model shows strong improvements over\nthe current state-of-the-art end-to-end conversational models like memory\naugmented seq2seq and information-retrieval models. On the privately held\nPERSONA-CHAT dataset of the Conversational Intelligence Challenge 2, this\napproach obtains a new state-of-the-art, with respective perplexity, Hits@1 and\nF1 metrics of 16.28 (45 % absolute improvement), 80.7 (46 % absolute\nimprovement) and 19.5 (20 % absolute improvement).",
    "categories": [
      "cs.CL"
    ],
    "published": "2019-01-23T22:08:01+00:00",
    "updated": "2019-02-04T11:38:52+00:00",
    "doi": null,
    "comment": "6 pages, 2 figures, 2 tables, NeurIPS 2018 CAI Workshop",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1710.09537v1": {
    "id": "http://arxiv.org/abs/1710.09537v1",
    "title": "Rotational Unit of Memory",
    "authors": [
      "Rumen Dangovski",
      "Li Jing",
      "Marin Soljacic"
    ],
    "abstract": "The concepts of unitary evolution matrices and associative memory have\nboosted the field of Recurrent Neural Networks (RNN) to state-of-the-art\nperformance in a variety of sequential tasks. However, RNN still have a limited\ncapacity to manipulate long-term memory. To bypass this weakness the most\nsuccessful applications of RNN use external techniques such as attention\nmechanisms. In this paper we propose a novel RNN model that unifies the\nstate-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM\nis its rotational operation, which is, naturally, a unitary matrix, providing\narchitectures with the power to learn long-term dependencies by overcoming the\nvanishing and exploding gradients problem. Moreover, the rotational unit also\nserves as associative memory. We evaluate our model on synthetic memorization,\nquestion answering and language modeling tasks. RUM learns the Copying Memory\ntask completely and improves the state-of-the-art result in the Recall task.\nRUM's performance in the bAbI Question Answering task is comparable to that of\nmodels with attention mechanism. We also improve the state-of-the-art result to\n1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB)\ntask, which is to signify the applications of RUM to real-world sequential\ndata. The universality of our construction, at the core of RNN, establishes RUM\nas a promising approach to language modeling, speech recognition and machine\ntranslation.",
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "published": "2017-10-26T04:36:35+00:00",
    "updated": "2017-10-26T04:36:35+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2004.12832v2": {
    "id": "http://arxiv.org/abs/2004.12832v2",
    "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
    "authors": [
      "Omar Khattab",
      "Matei Zaharia"
    ],
    "abstract": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced\nadvances in Information Retrieval (IR), largely owed to fine-tuning deep\nlanguage models (LMs) for document ranking. While remarkably effective, the\nranking models based on these LMs increase computational cost by orders of\nmagnitude over prior approaches, particularly as they must feed each\nquery-document pair through a massive neural network to compute a single\nrelevance score. To tackle this, we present ColBERT, a novel ranking model that\nadapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT\nintroduces a late interaction architecture that independently encodes the query\nand the document using BERT and then employs a cheap yet powerful interaction\nstep that models their fine-grained similarity. By delaying and yet retaining\nthis fine-granular interaction, ColBERT can leverage the expressiveness of deep\nLMs while simultaneously gaining the ability to pre-compute document\nrepresentations offline, considerably speeding up query processing. Beyond\nreducing the cost of re-ranking the documents retrieved by a traditional model,\nColBERT's pruning-friendly interaction mechanism enables leveraging\nvector-similarity indexes for end-to-end retrieval directly from a large\ndocument collection. We extensively evaluate ColBERT using two recent passage\nsearch datasets. Results show that ColBERT's effectiveness is competitive with\nexisting BERT-based models (and outperforms every non-BERT baseline), while\nexecuting two orders-of-magnitude faster and requiring four orders-of-magnitude\nfewer FLOPs per query.",
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "published": "2020-04-27T14:21:03+00:00",
    "updated": "2020-06-04T05:28:21+00:00",
    "doi": null,
    "comment": "Accepted at SIGIR 2020",
    "journal_ref": null,
    "primary_category": "cs.IR"
  },
  "2112.02244v2": {
    "id": "http://arxiv.org/abs/2112.02244v2",
    "title": "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation",
    "authors": [
      "Zhao Yang",
      "Jiaqi Wang",
      "Yansong Tang",
      "Kai Chen",
      "Hengshuang Zhao",
      "Philip H. S. Torr"
    ],
    "abstract": "Referring image segmentation is a fundamental vision-language task that aims\nto segment out an object referred to by a natural language expression from an\nimage. One of the key challenges behind this task is leveraging the referring\nexpression for highlighting relevant positions in the image. A paradigm for\ntackling this problem is to leverage a powerful vision-language (\"cross-modal\")\ndecoder to fuse features independently extracted from a vision encoder and a\nlanguage encoder. Recent methods have made remarkable advancements in this\nparadigm by exploiting Transformers as cross-modal decoders, concurrent to the\nTransformer's overwhelming success in many other vision-language tasks.\nAdopting a different approach in this work, we show that significantly better\ncross-modal alignments can be achieved through the early fusion of linguistic\nand visual features in intermediate layers of a vision Transformer encoder\nnetwork. By conducting cross-modal feature fusion in the visual feature\nencoding stage, we can leverage the well-proven correlation modeling power of a\nTransformer encoder for excavating helpful multi-modal context. This way,\naccurate segmentation results are readily harvested with a light-weight mask\npredictor. Without bells and whistles, our method surpasses the previous\nstate-of-the-art methods on RefCOCO, RefCOCO+, and G-Ref by large margins.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "published": "2021-12-04T04:53:35+00:00",
    "updated": "2022-04-05T21:42:27+00:00",
    "doi": null,
    "comment": "CVPR 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2402.11291v3": {
    "id": "http://arxiv.org/abs/2402.11291v3",
    "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
    "authors": [
      "Panagiotis Giadikiaroglou",
      "Maria Lymperaiou",
      "Giorgos Filandrianos",
      "Giorgos Stamou"
    ],
    "abstract": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving\nunveils critical insights into their potential and challenges in AI, marking a\nsignificant step towards understanding their applicability in complex reasoning\ntasks. This survey leverages a unique taxonomy -- dividing puzzles into\nrule-based and rule-less categories -- to critically assess LLMs through\nvarious methodologies, including prompting techniques, neuro-symbolic\napproaches, and fine-tuning. Through a critical review of relevant datasets and\nbenchmarks, we assess LLMs' performance, identifying significant challenges in\ncomplex puzzle scenarios. Our findings highlight the disparity between LLM\ncapabilities and human-like reasoning, particularly in those requiring advanced\nlogical inference. The survey underscores the necessity for novel strategies\nand richer datasets to advance LLMs' puzzle-solving proficiency and contribute\nto AI's logical reasoning and creative problem-solving advancements.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-17T14:19:38+00:00",
    "updated": "2024-09-14T06:12:36+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2008.05742v3": {
    "id": "http://arxiv.org/abs/2008.05742v3",
    "title": "SkeletonNet: A Topology-Preserving Solution for Learning Mesh Reconstruction of Object Surfaces from RGB Images",
    "authors": [
      "Jiapeng Tang",
      "Xiaoguang Han",
      "Mingkui Tan",
      "Xin Tong",
      "Kui Jia"
    ],
    "abstract": "This paper focuses on the challenging task of learning 3D object surface\nreconstructions from RGB images. Existingmethods achieve varying degrees of\nsuccess by using different surface representations. However, they all have\ntheir own drawbacks,and cannot properly reconstruct the surface shapes of\ncomplex topologies, arguably due to a lack of constraints on the\ntopologicalstructures in their learning frameworks. To this end, we propose to\nlearn and use the topology-preserved, skeletal shape representationto assist\nthe downstream task of object surface reconstruction from RGB images.\nTechnically, we propose the novelSkeletonNetdesign that learns a volumetric\nrepresentation of a skeleton via a bridged learning of a skeletal point set,\nwhere we use paralleldecoders each responsible for the learning of points on 1D\nskeletal curves and 2D skeletal sheets, as well as an efficient module\nofglobally guided subvolume synthesis for a refined, high-resolution skeletal\nvolume; we present a differentiablePoint2Voxellayer tomake SkeletonNet\nend-to-end and trainable. With the learned skeletal volumes, we propose two\nmodels, the Skeleton-Based GraphConvolutional Neural Network (SkeGCNN) and the\nSkeleton-Regularized Deep Implicit Surface Network (SkeDISN), which\nrespectivelybuild upon and improve over the existing frameworks of explicit\nmesh deformation and implicit field learning for the downstream\nsurfacereconstruction task. We conduct thorough experiments that verify the\nefficacy of our proposed SkeletonNet. SkeGCNN and SkeDISNoutperform existing\nmethods as well, and they have their own merits when measured by different\nmetrics. Additional results ingeneralized task settings further demonstrate the\nusefulness of our proposed methods. We have made both our implementation\ncodeand the ShapeNet-Skeleton dataset publicly available at ble at\nhttps://github.com/tangjiapeng/SkeletonNet.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-08-13T07:59:25+00:00",
    "updated": "2021-06-10T03:26:02+00:00",
    "doi": null,
    "comment": "17 pages, 13 figures; TPAMI 2021",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2206.01191v5": {
    "id": "http://arxiv.org/abs/2206.01191v5",
    "title": "EfficientFormer: Vision Transformers at MobileNet Speed",
    "authors": [
      "Yanyu Li",
      "Geng Yuan",
      "Yang Wen",
      "Ju Hu",
      "Georgios Evangelidis",
      "Sergey Tulyakov",
      "Yanzhi Wang",
      "Jian Ren"
    ],
    "abstract": "Vision Transformers (ViT) have shown rapid progress in computer vision tasks,\nachieving promising results on various benchmarks. However, due to the massive\nnumber of parameters and model design, \\textit{e.g.}, attention mechanism,\nViT-based models are generally times slower than lightweight convolutional\nnetworks. Therefore, the deployment of ViT for real-time applications is\nparticularly challenging, especially on resource-constrained hardware such as\nmobile devices. Recent efforts try to reduce the computation complexity of ViT\nthrough network architecture search or hybrid design with MobileNet block, yet\nthe inference speed is still unsatisfactory. This leads to an important\nquestion: can transformers run as fast as MobileNet while obtaining high\nperformance? To answer this, we first revisit the network architecture and\noperators used in ViT-based models and identify inefficient designs. Then we\nintroduce a dimension-consistent pure transformer (without MobileNet blocks) as\na design paradigm. Finally, we perform latency-driven slimming to get a series\nof final models dubbed EfficientFormer. Extensive experiments show the\nsuperiority of EfficientFormer in performance and speed on mobile devices. Our\nfastest model, EfficientFormer-L1, achieves $79.2\\%$ top-1 accuracy on\nImageNet-1K with only $1.6$ ms inference latency on iPhone 12 (compiled with\nCoreML), which runs as fast as MobileNetV2$\\times 1.4$ ($1.6$ ms, $74.7\\%$\ntop-1), and our largest model, EfficientFormer-L7, obtains $83.3\\%$ accuracy\nwith only $7.0$ ms latency. Our work proves that properly designed transformers\ncan reach extremely low latency on mobile devices while maintaining high\nperformance.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-06-02T17:51:03+00:00",
    "updated": "2022-10-11T03:06:16+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.19645v1": {
    "id": "http://arxiv.org/abs/2403.19645v1",
    "title": "GANTASTIC: GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models",
    "authors": [
      "Yusuf Dalva",
      "Hidir Yesiltepe",
      "Pinar Yanardag"
    ],
    "abstract": "The rapid advancement in image generation models has predominantly been\ndriven by diffusion models, which have demonstrated unparalleled success in\ngenerating high-fidelity, diverse images from textual prompts. Despite their\nsuccess, diffusion models encounter substantial challenges in the domain of\nimage editing, particularly in executing disentangled edits-changes that target\nspecific attributes of an image while leaving irrelevant parts untouched. In\ncontrast, Generative Adversarial Networks (GANs) have been recognized for their\nsuccess in disentangled edits through their interpretable latent spaces. We\nintroduce GANTASTIC, a novel framework that takes existing directions from\npre-trained GAN models-representative of specific, controllable attributes-and\ntransfers these directions into diffusion-based models. This novel approach not\nonly maintains the generative quality and diversity that diffusion models are\nknown for but also significantly enhances their capability to perform precise,\ntargeted image edits, thereby leveraging the best of both worlds.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-28T17:55:16+00:00",
    "updated": "2024-03-28T17:55:16+00:00",
    "doi": null,
    "comment": "Project page: https://gantastic.github.io",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.16954v1": {
    "id": "http://arxiv.org/abs/2403.16954v1",
    "title": "Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation Training-Freely with Isolated Diffusion Guidance",
    "authors": [
      "Jingyuan Zhu",
      "Huimin Ma",
      "Jiansheng Chen",
      "Jian Yuan"
    ],
    "abstract": "Large-scale text-to-image diffusion models have achieved great success in\nsynthesizing high-quality and diverse images given target text prompts. Despite\nthe revolutionary image generation ability, current state-of-the-art models\nstill struggle to deal with multi-concept generation accurately in many cases.\nThis phenomenon is known as ``concept bleeding\" and displays as the unexpected\noverlapping or merging of various concepts. This paper presents a general\napproach for text-to-image diffusion models to address the mutual interference\nbetween different subjects and their attachments in complex scenes, pursuing\nbetter text-image consistency. The core idea is to isolate the synthesizing\nprocesses of different concepts. We propose to bind each attachment to\ncorresponding subjects separately with split text prompts. Besides, we\nintroduce a revision method to fix the concept bleeding problem in\nmulti-subject synthesis. We first depend on pre-trained object detection and\nsegmentation models to obtain the layouts of subjects. Then we isolate and\nresynthesize each subject individually with corresponding text prompts to avoid\nmutual interference. Overall, we achieve a training-free strategy, named\nIsolated Diffusion, to optimize multi-concept text-to-image synthesis. It is\ncompatible with the latest Stable Diffusion XL (SDXL) and prior Stable\nDiffusion (SD) models. We compare our approach with alternative methods using a\nvariety of multi-concept text prompts and demonstrate its effectiveness with\nclear advantages in text-image consistency and user study.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-25T17:16:27+00:00",
    "updated": "2024-03-25T17:16:27+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2203.07436v4": {
    "id": "http://arxiv.org/abs/2203.07436v4",
    "title": "SuperAnimal pretrained pose estimation models for behavioral analysis",
    "authors": [
      "Shaokai Ye",
      "Anastasiia Filippova",
      "Jessy Lauer",
      "Steffen Schneider",
      "Maxime Vidal",
      "Tian Qiu",
      "Alexander Mathis",
      "Mackenzie Weygandt Mathis"
    ],
    "abstract": "Quantification of behavior is critical in applications ranging from\nneuroscience, veterinary medicine and animal conservation efforts. A common key\nstep for behavioral analysis is first extracting relevant keypoints on animals,\nknown as pose estimation. However, reliable inference of poses currently\nrequires domain knowledge and manual labeling effort to build supervised\nmodels. We present a series of technical innovations that enable a new method,\ncollectively called SuperAnimal, to develop unified foundation models that can\nbe used on over 45 species, without additional human labels. Concretely, we\nintroduce a method to unify the keypoint space across differently labeled\ndatasets (via our generalized data converter) and for training these diverse\ndatasets in a manner such that they don't catastrophically forget keypoints\ngiven the unbalanced inputs (via our keypoint gradient masking and memory\nreplay approaches). These models show excellent performance across six pose\nbenchmarks. Then, to ensure maximal usability for end-users, we demonstrate how\nto fine-tune the models on differently labeled data and provide tooling for\nunsupervised video adaptation to boost performance and decrease jitter across\nframes. If the models are fine-tuned, we show SuperAnimal models are\n10-100$\\times$ more data efficient than prior transfer-learning-based\napproaches. We illustrate the utility of our models in behavioral\nclassification in mice and gait analysis in horses. Collectively, this presents\na data-efficient solution for animal pose estimation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.QM"
    ],
    "published": "2022-03-14T18:46:57+00:00",
    "updated": "2023-12-31T01:17:27+00:00",
    "doi": null,
    "comment": "Models and demos available at http://modelzoo.deeplabcut.org",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1607.08022v3": {
    "id": "http://arxiv.org/abs/1607.08022v3",
    "title": "Instance Normalization: The Missing Ingredient for Fast Stylization",
    "authors": [
      "Dmitry Ulyanov",
      "Andrea Vedaldi",
      "Victor Lempitsky"
    ],
    "abstract": "It this paper we revisit the fast stylization method introduced in Ulyanov\net. al. (2016). We show how a small change in the stylization architecture\nresults in a significant qualitative improvement in the generated images. The\nchange is limited to swapping batch normalization with instance normalization,\nand to apply the latter both at training and testing times. The resulting\nmethod can be used to train high-performance architectures for real-time image\ngeneration. The code will is made available on github at\nhttps://github.com/DmitryUlyanov/texture_nets. Full paper can be found at\narXiv:1701.02096.",
    "categories": [
      "cs.CV"
    ],
    "published": "2016-07-27T10:23:00+00:00",
    "updated": "2017-11-06T14:21:43+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2106.01613v3": {
    "id": "http://arxiv.org/abs/2106.01613v3",
    "title": "NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning",
    "authors": [
      "Chun-Hao Chang",
      "Rich Caruana",
      "Anna Goldenberg"
    ],
    "abstract": "Deployment of machine learning models in real high-risk settings (e.g.\nhealthcare) often depends not only on the model's accuracy but also on its\nfairness, robustness, and interpretability. Generalized Additive Models (GAMs)\nare a class of interpretable models with a long history of use in these\nhigh-risk domains, but they lack desirable features of deep learning such as\ndifferentiability and scalability. In this work, we propose a neural GAM\n(NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that scale well and perform better\nthan other GAMs on large datasets, while remaining interpretable compared to\nother ensemble and deep learning models. We demonstrate that our models find\ninteresting patterns in the data. Lastly, we show that we improve model\naccuracy via self-supervised pre-training, an improvement that is not possible\nfor non-differentiable GAMs.",
    "categories": [
      "cs.LG"
    ],
    "published": "2021-06-03T06:20:18+00:00",
    "updated": "2022-03-16T04:05:02+00:00",
    "doi": null,
    "comment": "2022 ICLR Spotlight paper",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2206.01908v1": {
    "id": "http://arxiv.org/abs/2206.01908v1",
    "title": "Video-based Human-Object Interaction Detection from Tubelet Tokens",
    "authors": [
      "Danyang Tu",
      "Wei Sun",
      "Xiongkuo Min",
      "Guangtao Zhai",
      "Wei Shen"
    ],
    "abstract": "We present a novel vision Transformer, named TUTOR, which is able to learn\ntubelet tokens, served as highly-abstracted spatiotemporal representations, for\nvideo-based human-object interaction (V-HOI) detection. The tubelet tokens\nstructurize videos by agglomerating and linking semantically-related patch\ntokens along spatial and temporal domains, which enjoy two benefits: 1)\nCompactness: each tubelet token is learned by a selective attention mechanism\nto reduce redundant spatial dependencies from others; 2) Expressiveness: each\ntubelet token is enabled to align with a semantic instance, i.e., an object or\na human, across frames, thanks to agglomeration and linking. The effectiveness\nand efficiency of TUTOR are verified by extensive experiments. Results shows\nour method outperforms existing works by large margins, with a relative mAP\ngain of $16.14\\%$ on VidHOI and a 2 points gain on CAD-120 as well as a $4\n\\times$ speedup.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-06-04T04:27:59+00:00",
    "updated": "2022-06-04T04:27:59+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.16237v1": {
    "id": "http://arxiv.org/abs/2309.16237v1",
    "title": "Object Motion Guided Human Motion Synthesis",
    "authors": [
      "Jiaman Li",
      "Jiajun Wu",
      "C. Karen Liu"
    ],
    "abstract": "Modeling human behaviors in contextual environments has a wide range of\napplications in character animation, embodied AI, VR/AR, and robotics. In\nreal-world scenarios, humans frequently interact with the environment and\nmanipulate various objects to complete daily tasks. In this work, we study the\nproblem of full-body human motion synthesis for the manipulation of large-sized\nobjects. We propose Object MOtion guided human MOtion synthesis (OMOMO), a\nconditional diffusion framework that can generate full-body manipulation\nbehaviors from only the object motion. Since naively applying diffusion models\nfails to precisely enforce contact constraints between the hands and the\nobject, OMOMO learns two separate denoising processes to first predict hand\npositions from object motion and subsequently synthesize full-body poses based\non the predicted hand positions. By employing the hand positions as an\nintermediate representation between the two denoising processes, we can\nexplicitly enforce contact constraints, resulting in more physically plausible\nmanipulation motions. With the learned model, we develop a novel system that\ncaptures full-body human manipulation motions by simply attaching a smartphone\nto the object being manipulated. Through extensive experiments, we demonstrate\nthe effectiveness of our proposed pipeline and its ability to generalize to\nunseen objects. Additionally, as high-quality human-object interaction datasets\nare scarce, we collect a large-scale dataset consisting of 3D object geometry,\nobject motion, and human motion. Our dataset contains human-object interaction\nmotion for 15 objects, with a total duration of approximately 10 hours.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-09-28T08:22:00+00:00",
    "updated": "2023-09-28T08:22:00+00:00",
    "doi": null,
    "comment": "SIGGRAPH Asia 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2104.02008v1": {
    "id": "http://arxiv.org/abs/2104.02008v1",
    "title": "Domain Generalization with MixStyle",
    "authors": [
      "Kaiyang Zhou",
      "Yongxin Yang",
      "Yu Qiao",
      "Tao Xiang"
    ],
    "abstract": "Though convolutional neural networks (CNNs) have demonstrated remarkable\nability in learning discriminative features, they often generalize poorly to\nunseen domains. Domain generalization aims to address this problem by learning\nfrom a set of source domains a model that is generalizable to any unseen\ndomain. In this paper, a novel approach is proposed based on probabilistically\nmixing instance-level feature statistics of training samples across source\ndomains. Our method, termed MixStyle, is motivated by the observation that\nvisual domain is closely related to image style (e.g., photo vs.~sketch\nimages). Such style information is captured by the bottom layers of a CNN where\nour proposed style-mixing takes place. Mixing styles of training instances\nresults in novel domains being synthesized implicitly, which increase the\ndomain diversity of the source domains, and hence the generalizability of the\ntrained model. MixStyle fits into mini-batch training perfectly and is\nextremely easy to implement. The effectiveness of MixStyle is demonstrated on a\nwide range of tasks including category classification, instance retrieval and\nreinforcement learning.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2021-04-05T16:58:09+00:00",
    "updated": "2021-04-05T16:58:09+00:00",
    "doi": null,
    "comment": "ICLR 2021; Code is available at\n  https://github.com/KaiyangZhou/mixstyle-release",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2203.02378v3": {
    "id": "http://arxiv.org/abs/2203.02378v3",
    "title": "DiT: Self-supervised Pre-training for Document Image Transformer",
    "authors": [
      "Junlong Li",
      "Yiheng Xu",
      "Tengchao Lv",
      "Lei Cui",
      "Cha Zhang",
      "Furu Wei"
    ],
    "abstract": "Image Transformer has recently achieved significant progress for natural\nimage understanding, either using supervised (ViT, DeiT, etc.) or\nself-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we\npropose \\textbf{DiT}, a self-supervised pre-trained \\textbf{D}ocument\n\\textbf{I}mage \\textbf{T}ransformer model using large-scale unlabeled text\nimages for Document AI tasks, which is essential since no supervised\ncounterparts ever exist due to the lack of human-labeled document images. We\nleverage DiT as the backbone network in a variety of vision-based Document AI\ntasks, including document image classification, document layout analysis, table\ndetection as well as text detection for OCR. Experiment results have\nillustrated that the self-supervised pre-trained DiT model achieves new\nstate-of-the-art results on these downstream tasks, e.g. document image\nclassification (91.11 $\\rightarrow$ 92.69), document layout analysis (91.0\n$\\rightarrow$ 94.9), table detection (94.23 $\\rightarrow$ 96.55) and text\ndetection for OCR (93.07 $\\rightarrow$ 94.29). The code and pre-trained models\nare publicly available at \\url{https://aka.ms/msdit}.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-03-04T15:34:46+00:00",
    "updated": "2022-07-19T04:15:51+00:00",
    "doi": null,
    "comment": "ACM Multimedia 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2206.01626v2": {
    "id": "http://arxiv.org/abs/2206.01626v2",
    "title": "Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress",
    "authors": [
      "Rishabh Agarwal",
      "Max Schwarzer",
      "Pablo Samuel Castro",
      "Aaron Courville",
      "Marc G. Bellemare"
    ],
    "abstract": "Learning tabula rasa, that is without any prior knowledge, is the prevalent\nworkflow in reinforcement learning (RL) research. However, RL systems, when\napplied to large-scale settings, rarely operate tabula rasa. Such large-scale\nsystems undergo multiple design or algorithmic changes during their development\ncycle and use ad hoc approaches for incorporating these changes without\nre-training from scratch, which would have been prohibitively expensive.\nAdditionally, the inefficiency of deep RL typically excludes researchers\nwithout access to industrial-scale resources from tackling\ncomputationally-demanding problems. To address these issues, we present\nreincarnating RL as an alternative workflow or class of problem settings, where\nprior computational work (e.g., learned policies) is reused or transferred\nbetween design iterations of an RL agent, or from one RL agent to another. As a\nstep towards enabling reincarnating RL from any agent to any other agent, we\nfocus on the specific setting of efficiently transferring an existing\nsub-optimal policy to a standalone value-based RL agent. We find that existing\napproaches fail in this setting and propose a simple algorithm to address their\nlimitations. Equipped with this algorithm, we demonstrate reincarnating RL's\ngains over tabula rasa RL on Atari 2600 games, a challenging locomotion task,\nand the real-world problem of navigating stratospheric balloons. Overall, this\nwork argues for an alternative approach to RL research, which we believe could\nsignificantly improve real-world RL adoption and help democratize it further.\nOpen-sourced code and trained agents at\nhttps://agarwl.github.io/reincarnating_rl.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2022-06-03T15:11:10+00:00",
    "updated": "2022-10-04T05:58:07+00:00",
    "doi": null,
    "comment": "NeurIPS 2022. Code and agents at\n  https://agarwl.github.io/reincarnating_rl",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2407.13032v1": {
    "id": "http://arxiv.org/abs/2407.13032v1",
    "title": "Agent-E: From Autonomous Web Navigation to Foundational Design Principles in Agentic Systems",
    "authors": [
      "Tamer Abuelsaad",
      "Deepak Akkil",
      "Prasenjit Dey",
      "Ashish Jagmohan",
      "Aditya Vempaty",
      "Ravi Kokku"
    ],
    "abstract": "AI Agents are changing the way work gets done, both in consumer and\nenterprise domains. However, the design patterns and architectures to build\nhighly capable agents or multi-agent systems are still developing, and the\nunderstanding of the implication of various design choices and algorithms is\nstill evolving. In this paper, we present our work on building a novel web\nagent, Agent-E \\footnote{Our code is available at\n\\url{https://github.com/EmergenceAI/Agent-E}}. Agent-E introduces numerous\narchitectural improvements over prior state-of-the-art web agents such as\nhierarchical architecture, flexible DOM distillation and denoising method, and\nthe concept of \\textit{change observation} to guide the agent towards more\naccurate performance. We first present the results of an evaluation of Agent-E\non WebVoyager benchmark dataset and show that Agent-E beats other SOTA text and\nmulti-modal web agents on this benchmark in most categories by 10-30\\%. We then\nsynthesize our learnings from the development of Agent-E into general design\nprinciples for developing agentic systems. These include the use of\ndomain-specific primitive skills, the importance of distillation and de-noising\nof environmental observations, the advantages of a hierarchical architecture,\nand the role of agentic self-improvement to enhance agent efficiency and\nefficacy as the agent gathers experience.",
    "categories": [
      "cs.AI"
    ],
    "published": "2024-07-17T21:44:28+00:00",
    "updated": "2024-07-17T21:44:28+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2309.01728v3": {
    "id": "http://arxiv.org/abs/2309.01728v3",
    "title": "Generative-based Fusion Mechanism for Multi-Modal Tracking",
    "authors": [
      "Zhangyong Tang",
      "Tianyang Xu",
      "Xuefeng Zhu",
      "Xiao-Jun Wu",
      "Josef Kittler"
    ],
    "abstract": "Generative models (GMs) have received increasing research interest for their\nremarkable capacity to achieve comprehensive understanding. However, their\npotential application in the domain of multi-modal tracking has remained\nrelatively unexplored. In this context, we seek to uncover the potential of\nharnessing generative techniques to address the critical challenge, information\nfusion, in multi-modal tracking. In this paper, we delve into two prominent GM\ntechniques, namely, Conditional Generative Adversarial Networks (CGANs) and\nDiffusion Models (DMs). Different from the standard fusion process where the\nfeatures from each modality are directly fed into the fusion block, we\ncondition these multi-modal features with random noise in the GM framework,\neffectively transforming the original training samples into harder instances.\nThis design excels at extracting discriminative clues from the features,\nenhancing the ultimate tracking performance. To quantitatively gauge the\neffectiveness of our approach, we conduct extensive experiments across two\nmulti-modal tracking tasks, three baseline methods, and three challenging\nbenchmarks. The experimental results demonstrate that the proposed\ngenerative-based fusion mechanism achieves state-of-the-art performance,\nsetting new records on LasHeR and RGBD1K.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-09-04T17:22:10+00:00",
    "updated": "2023-11-30T15:21:01+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2112.09127v2": {
    "id": "http://arxiv.org/abs/2112.09127v2",
    "title": "ICON: Implicit Clothed humans Obtained from Normals",
    "authors": [
      "Yuliang Xiu",
      "Jinlong Yang",
      "Dimitrios Tzionas",
      "Michael J. Black"
    ],
    "abstract": "Current methods for learning realistic and animatable 3D clothed avatars need\neither posed 3D scans or 2D images with carefully controlled user poses. In\ncontrast, our goal is to learn an avatar from only 2D images of people in\nunconstrained poses. Given a set of images, our method estimates a detailed 3D\nsurface from each image and then combines these into an animatable avatar.\nImplicit functions are well suited to the first task, as they can capture\ndetails like hair and clothes. Current methods, however, are not robust to\nvaried human poses and often produce 3D surfaces with broken or disembodied\nlimbs, missing details, or non-human shapes. The problem is that these methods\nuse global feature encoders that are sensitive to global pose. To address this,\nwe propose ICON (\"Implicit Clothed humans Obtained from Normals\"), which,\ninstead, uses local features. ICON has two main modules, both of which exploit\nthe SMPL(-X) body model. First, ICON infers detailed clothed-human normals\n(front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware\nimplicit surface regressor produces an iso-surface of a human occupancy field.\nImportantly, at inference time, a feedback loop alternates between refining the\nSMPL(-X) mesh using the inferred clothed normals and then refining the normals.\nGiven multiple reconstructed frames of a subject in varied poses, we use\nSCANimate to produce an animatable avatar from them. Evaluation on the AGORA\nand CAPE datasets shows that ICON outperforms the state of the art in\nreconstruction, even with heavily limited training data. Additionally, it is\nmuch more robust to out-of-distribution samples, e.g., in-the-wild poses/images\nand out-of-frame cropping. ICON takes a step towards robust 3D clothed human\nreconstruction from in-the-wild images. This enables creating avatars directly\nfrom video with personalized and natural pose-dependent cloth deformation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "published": "2021-12-16T18:59:41+00:00",
    "updated": "2022-03-28T21:07:22+00:00",
    "doi": null,
    "comment": "Project page: https://icon.is.tue.mpg.de/. Accepted by CVPR 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.14327v3": {
    "id": "http://arxiv.org/abs/2309.14327v3",
    "title": "DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention",
    "authors": [
      "Zhewei Yao",
      "Xiaoxia Wu",
      "Conglong Li",
      "Minjia Zhang",
      "Heyang Qin",
      "Olatunji Ruwase",
      "Ammar Ahmad Awan",
      "Samyam Rajbhandari",
      "Yuxiong He"
    ],
    "abstract": "Most of the existing multi-modal models, hindered by their incapacity to\nadeptly manage interleaved image-and-text inputs in multi-image, multi-round\ndialogues, face substantial constraints in resource allocation for training and\ndata accessibility, impacting their adaptability and scalability across varied\ninteraction realms. To address this, we present the DeepSpeed-VisualChat\nframework, designed to optimize Large Language Models (LLMs) by incorporating\nmulti-modal capabilities, with a focus on enhancing the proficiency of Large\nVision and Language Models in handling interleaved inputs. Our framework is\nnotable for (1) its open-source support for multi-round and multi-image\ndialogues, (2) introducing an innovative multi-modal causal attention\nmechanism, and (3) utilizing data blending techniques on existing datasets to\nassure seamless interactions in multi-round, multi-image conversations.\nCompared to existing frameworks, DeepSpeed-VisualChat shows superior\nscalability up to 70B parameter language model size, representing a significant\nadvancement in multi-modal language models and setting a solid foundation for\nfuture explorations.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "published": "2023-09-25T17:53:29+00:00",
    "updated": "2023-11-29T07:52:18+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2003.07302v2": {
    "id": "http://arxiv.org/abs/2003.07302v2",
    "title": "Dash: Scalable Hashing on Persistent Memory",
    "authors": [
      "Baotong Lu",
      "Xiangpeng Hao",
      "Tianzheng Wang",
      "Eric Lo"
    ],
    "abstract": "Byte-addressable persistent memory (PM) brings hash tables the potential of\nlow latency, cheap persistence and instant recovery. The recent advent of Intel\nOptane DC Persistent Memory Modules (DCPMM) further accelerates this trend.\nMany new hash table designs have been proposed, but most of them were based on\nemulation and perform sub-optimally on real PM. They were also piece-wise and\npartial solutions that side-step many important properties, in particular good\nscalability, high load factor and instant recovery. We present Dash, a holistic\napproach to building dynamic and scalable hash tables on real PM hardware with\nall the aforementioned properties. Based on Dash, we adapted two popular\ndynamic hashing schemes (extendible hashing and linear hashing). On a 24-core\nmachine with Intel Optane DCPMM, we show that compared to state-of-the-art,\nDash-enabled hash tables can achieve up to ~3.9X higher performance with up to\nover 90% load factor and an instant recovery time of 57ms regardless of data\nsize.",
    "categories": [
      "cs.DB"
    ],
    "published": "2020-03-16T16:15:46+00:00",
    "updated": "2020-04-09T08:03:53+00:00",
    "doi": "10.14778/3389133.3389134",
    "comment": "To appear at VLDB 2020 (PVLDB Vol. 13 Issue 8)",
    "journal_ref": "PVLDB, 13(8): 1147-1161, 2020",
    "primary_category": "cs.DB"
  },
  "2212.09478v2": {
    "id": "http://arxiv.org/abs/2212.09478v2",
    "title": "MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation",
    "authors": [
      "Ludan Ruan",
      "Yiyang Ma",
      "Huan Yang",
      "Huiguo He",
      "Bei Liu",
      "Jianlong Fu",
      "Nicholas Jing Yuan",
      "Qin Jin",
      "Baining Guo"
    ],
    "abstract": "We propose the first joint audio-video generation framework that brings\nengaging watching and listening experiences simultaneously, towards\nhigh-quality realistic videos. To generate joint audio-video pairs, we propose\na novel Multi-Modal Diffusion model (i.e., MM-Diffusion), with two-coupled\ndenoising autoencoders. In contrast to existing single-modal diffusion models,\nMM-Diffusion consists of a sequential multi-modal U-Net for a joint denoising\nprocess by design. Two subnets for audio and video learn to gradually generate\naligned audio-video pairs from Gaussian noises. To ensure semantic consistency\nacross modalities, we propose a novel random-shift based attention block\nbridging over the two subnets, which enables efficient cross-modal alignment,\nand thus reinforces the audio-video fidelity for each other. Extensive\nexperiments show superior results in unconditional audio-video generation, and\nzero-shot conditional tasks (e.g., video-to-audio). In particular, we achieve\nthe best FVD and FAD on Landscape and AIST++ dancing datasets. Turing tests of\n10k votes further demonstrate dominant preferences for our model. The code and\npre-trained models can be downloaded at\nhttps://github.com/researchmm/MM-Diffusion.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-12-19T14:11:52+00:00",
    "updated": "2023-03-24T12:13:52+00:00",
    "doi": null,
    "comment": "Accepted by CVPR 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2402.16197v1": {
    "id": "http://arxiv.org/abs/2402.16197v1",
    "title": "Language Models for Code Completion: A Practical Evaluation",
    "authors": [
      "Maliheh Izadi",
      "Jonathan Katzy",
      "Tim van Dam",
      "Marc Otten",
      "Razvan Mihai Popescu",
      "Arie van Deursen"
    ],
    "abstract": "Transformer-based language models for automatic code completion have shown\ngreat promise so far, yet the evaluation of these models rarely uses real data.\nThis study provides both quantitative and qualitative assessments of three\npublic code language models when completing real-world code. We first developed\nan open-source IDE extension, Code4Me, for the online evaluation of the models.\nWe collected real auto-completion usage data for over a year from more than\n1200 users, resulting in over 600K valid completions. These models were then\nevaluated using six standard metrics across twelve programming languages. Next,\nwe conducted a qualitative study of 1690 real-world completion requests to\nidentify the reasons behind the poor model performance. A comparative analysis\nof the models' performance in online and offline settings was also performed,\nusing benchmark synthetic datasets and two masking strategies. Our findings\nsuggest that while developers utilize code completion across various languages,\nthe best results are achieved for mainstream languages such as Python and Java.\nInCoder outperformed the other models across all programming languages,\nhighlighting the significance of training data and objectives. Our study also\nrevealed that offline evaluations do not accurately reflect real-world\nscenarios. Upon qualitative analysis of the model's predictions, we found that\n66.3% of failures were due to the models' limitations, 24.4% occurred due to\ninappropriate model usage in a development context, and 9.3% were valid\nrequests that developers overwrote. Given these findings, we propose several\nstrategies to overcome the current limitations. These include refining training\nobjectives, improving resilience to typographical errors, adopting hybrid\napproaches, and enhancing implementations and usability.",
    "categories": [
      "cs.SE",
      "cs.LG",
      "cs.PL"
    ],
    "published": "2024-02-25T20:43:55+00:00",
    "updated": "2024-02-25T20:43:55+00:00",
    "doi": null,
    "comment": "To be published in the proceedings of the 46th IEEE/ACM International\n  Conference on Software Engineering (ICSE 2024)",
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "2408.13459v1": {
    "id": "http://arxiv.org/abs/2408.13459v1",
    "title": "Rethinking Video Deblurring with Wavelet-Aware Dynamic Transformer and Diffusion Model",
    "authors": [
      "Chen Rao",
      "Guangyuan Li",
      "Zehua Lan",
      "Jiakai Sun",
      "Junsheng Luan",
      "Wei Xing",
      "Lei Zhao",
      "Huaizhong Lin",
      "Jianfeng Dong",
      "Dalong Zhang"
    ],
    "abstract": "Current video deblurring methods have limitations in recovering\nhigh-frequency information since the regression losses are conservative with\nhigh-frequency details. Since Diffusion Models (DMs) have strong capabilities\nin generating high-frequency details, we consider introducing DMs into the\nvideo deblurring task. However, we found that directly applying DMs to the\nvideo deblurring task has the following problems: (1) DMs require many\niteration steps to generate videos from Gaussian noise, which consumes many\ncomputational resources. (2) DMs are easily misled by the blurry artifacts in\nthe video, resulting in irrational content and distortion of the deblurred\nvideo. To address the above issues, we propose a novel video deblurring\nframework VD-Diff that integrates the diffusion model into the Wavelet-Aware\nDynamic Transformer (WADT). Specifically, we perform the diffusion model in a\nhighly compact latent space to generate prior features containing\nhigh-frequency information that conforms to the ground truth distribution. We\ndesign the WADT to preserve and recover the low-frequency information in the\nvideo while utilizing the high-frequency information generated by the diffusion\nmodel. Extensive experiments show that our proposed VD-Diff outperforms SOTA\nmethods on GoPro, DVD, BSD, and Real-World Video datasets.",
    "categories": [
      "cs.CV",
      "I.4.4"
    ],
    "published": "2024-08-24T04:13:47+00:00",
    "updated": "2024-08-24T04:13:47+00:00",
    "doi": null,
    "comment": "accepted by ECCV2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2108.09373v4": {
    "id": "http://arxiv.org/abs/2108.09373v4",
    "title": "Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training",
    "authors": [
      "Mark Zhao",
      "Niket Agarwal",
      "Aarti Basant",
      "Bugra Gedik",
      "Satadru Pan",
      "Mustafa Ozdal",
      "Rakesh Komuravelli",
      "Jerry Pan",
      "Tianshu Bao",
      "Haowei Lu",
      "Sundaram Narayanan",
      "Jack Langman",
      "Kevin Wilfong",
      "Harsha Rastogi",
      "Carole-Jean Wu",
      "Christos Kozyrakis",
      "Parik Pol"
    ],
    "abstract": "Datacenter-scale AI training clusters consisting of thousands of\ndomain-specific accelerators (DSA) are used to train increasingly-complex deep\nlearning models. These clusters rely on a data storage and ingestion (DSI)\npipeline, responsible for storing exabytes of training data and serving it at\ntens of terabytes per second. As DSAs continue to push training efficiency and\nthroughput, the DSI pipeline is becoming the dominating factor that constrains\nthe overall training performance and capacity. Innovations that improve the\nefficiency and performance of DSI systems and hardware are urgent, demanding a\ndeep understanding of DSI characteristics and infrastructure at scale.\n  This paper presents Meta's end-to-end DSI pipeline, composed of a central\ndata warehouse built on distributed storage and a Data PreProcessing Service\nthat scales to eliminate data stalls. We characterize how hundreds of models\nare collaboratively trained across geo-distributed datacenters via diverse and\ncontinuous training jobs. These training jobs read and heavily filter massive\nand evolving datasets, resulting in popular features and samples used across\ntraining jobs. We measure the intense network, memory, and compute resources\nrequired by each training job to preprocess samples during training. Finally,\nwe synthesize key takeaways based on our production infrastructure\ncharacterization. These include identifying hardware bottlenecks, discussing\nopportunities for heterogeneous DSI hardware, motivating research in datacenter\nscheduling and benchmark datasets, and assimilating lessons learned in\noptimizing DSI infrastructure.",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "published": "2021-08-20T21:09:34+00:00",
    "updated": "2022-04-22T23:51:04+00:00",
    "doi": "10.1145/3470496.3533044",
    "comment": "In The 49th Annual International Symposium on Computer Architecture\n  (ISCA 2022)",
    "journal_ref": null,
    "primary_category": "cs.DC"
  },
  "2303.14569v1": {
    "id": "http://arxiv.org/abs/2303.14569v1",
    "title": "VisCo Grids: Surface Reconstruction with Viscosity and Coarea Grids",
    "authors": [
      "Albert Pumarola",
      "Artsiom Sanakoyeu",
      "Lior Yariv",
      "Ali Thabet",
      "Yaron Lipman"
    ],
    "abstract": "Surface reconstruction has been seeing a lot of progress lately by utilizing\nImplicit Neural Representations (INRs). Despite their success, INRs often\nintroduce hard to control inductive bias (i.e., the solution surface can\nexhibit unexplainable behaviours), have costly inference, and are slow to\ntrain. The goal of this work is to show that replacing neural networks with\nsimple grid functions, along with two novel geometric priors achieve comparable\nresults to INRs, with instant inference, and improved training times. To that\nend we introduce VisCo Grids: a grid-based surface reconstruction method\nincorporating Viscosity and Coarea priors. Intuitively, the Viscosity prior\nreplaces the smoothness inductive bias of INRs, while the Coarea favors a\nminimal area solution. Experimenting with VisCo Grids on a standard\nreconstruction baseline provided comparable results to the best performing INRs\non this dataset.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-03-25T21:32:17+00:00",
    "updated": "2023-03-25T21:32:17+00:00",
    "doi": null,
    "comment": "Published in NeurIPS 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2302.09187v3": {
    "id": "http://arxiv.org/abs/2302.09187v3",
    "title": "Video Action Recognition Collaborative Learning with Dynamics via PSO-ConvNet Transformer",
    "authors": [
      "Nguyen Huu Phong",
      "Bernardete Ribeiro"
    ],
    "abstract": "Recognizing human actions in video sequences, known as Human Action\nRecognition (HAR), is a challenging task in pattern recognition. While\nConvolutional Neural Networks (ConvNets) have shown remarkable success in image\nrecognition, they are not always directly applicable to HAR, as temporal\nfeatures are critical for accurate classification. In this paper, we propose a\nnovel dynamic PSO-ConvNet model for learning actions in videos, building on our\nrecent work in image recognition. Our approach leverages a framework where the\nweight vector of each neural network represents the position of a particle in\nphase space, and particles share their current weight vectors and gradient\nestimates of the Loss function. To extend our approach to video, we integrate\nConvNets with state-of-the-art temporal methods such as Transformer and\nRecurrent Neural Networks. Our experimental results on the UCF-101 dataset\ndemonstrate substantial improvements of up to 9% in accuracy, which confirms\nthe effectiveness of our proposed method. In addition, we conducted experiments\non larger and more variety of datasets including Kinetics-400 and HMDB-51 and\nobtained preference for Collaborative Learning in comparison with\nNon-Collaborative Learning (Individual Learning). Overall, our dynamic\nPSO-ConvNet model provides a promising direction for improving HAR by better\ncapturing the spatio-temporal dynamics of human actions in videos. The code is\navailable at\nhttps://github.com/leonlha/Video-Action-Recognition-Collaborative-Learning-with-Dynamics-via-PSO-ConvNet-Transformer.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-02-17T23:39:34+00:00",
    "updated": "2023-09-21T08:05:15+00:00",
    "doi": "10.1038/s41598-023-39744-9",
    "comment": "18 pages",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2206.02338v2": {
    "id": "http://arxiv.org/abs/2206.02338v2",
    "title": "OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression",
    "authors": [
      "Wanhua Li",
      "Xiaoke Huang",
      "Zheng Zhu",
      "Yansong Tang",
      "Xiu Li",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "This paper presents a language-powered paradigm for ordinal regression.\nExisting methods usually treat each rank as a category and employ a set of\nweights to learn these concepts. These methods are easy to overfit and usually\nattain unsatisfactory performance as the learned concepts are mainly derived\nfrom the training set. Recent large pre-trained vision-language models like\nCLIP have shown impressive performance on various visual tasks. In this paper,\nwe propose to learn the rank concepts from the rich semantic CLIP latent space.\nSpecifically, we reformulate this task as an image-language matching problem\nwith a contrastive objective, which regards labels as text and obtains a\nlanguage prototype from a text encoder for each rank. While prompt engineering\nfor CLIP is extremely time-consuming, we propose OrdinalCLIP, a differentiable\nprompting method for adapting CLIP for ordinal regression. OrdinalCLIP consists\nof learnable context tokens and learnable rank embeddings; The learnable rank\nembeddings are constructed by explicitly modeling numerical continuity,\nresulting in well-ordered, compact language prototypes in the CLIP space. Once\nlearned, we can only save the language prototypes and discard the huge language\nmodel, resulting in zero additional computational overhead compared with the\nlinear head counterpart. Experimental results show that our paradigm achieves\ncompetitive performance in general ordinal regression tasks, and gains\nimprovements in few-shot and distribution shift settings for age estimation.\nThe code is available at https://github.com/xk-huang/OrdinalCLIP.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-06-06T03:54:53+00:00",
    "updated": "2022-10-01T13:45:55+00:00",
    "doi": null,
    "comment": "Accepted by NeurIPS2022. Code is available at\n  https://github.com/xk-huang/OrdinalCLIP",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2109.12575v2": {
    "id": "http://arxiv.org/abs/2109.12575v2",
    "title": "Paradigm Shift in Natural Language Processing",
    "authors": [
      "Tianxiang Sun",
      "Xiangyang Liu",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "abstract": "In the era of deep learning, modeling for most NLP tasks has converged to\nseveral mainstream paradigms. For example, we usually adopt the sequence\nlabeling paradigm to solve a bundle of tasks such as POS-tagging, NER,\nChunking, and adopt the classification paradigm to solve tasks like sentiment\nanalysis. With the rapid progress of pre-trained language models, recent years\nhave observed a rising trend of Paradigm Shift, which is solving one NLP task\nby reformulating it as another one. Paradigm shift has achieved great success\non many tasks, becoming a promising way to improve model performance. Moreover,\nsome of these paradigms have shown great potential to unify a large number of\nNLP tasks, making it possible to build a single model to handle diverse tasks.\nIn this paper, we review such phenomenon of paradigm shifts in recent years,\nhighlighting several paradigms that have the potential to solve different NLP\ntasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2021-09-26T11:55:23+00:00",
    "updated": "2022-05-29T13:17:35+00:00",
    "doi": "10.1007/s11633-022-1331-6",
    "comment": "Accepted to Machine Intelligence Research. Published version:\n  https://link.springer.com/article/10.1007/s11633-022-1331-6. Website:\n  https://txsun1997.github.io/nlp-paradigm-shift",
    "journal_ref": "Mach. Intell. Res. 19, 169-183 (2022)",
    "primary_category": "cs.CL"
  },
  "2012.12877v2": {
    "id": "http://arxiv.org/abs/2012.12877v2",
    "title": "Training data-efficient image transformers & distillation through attention",
    "authors": [
      "Hugo Touvron",
      "Matthieu Cord",
      "Matthijs Douze",
      "Francisco Massa",
      "Alexandre Sablayrolles",
      "Herv\u00e9 J\u00e9gou"
    ],
    "abstract": "Recently, neural networks purely based on attention were shown to address\nimage understanding tasks such as image classification. However, these visual\ntransformers are pre-trained with hundreds of millions of images using an\nexpensive infrastructure, thereby limiting their adoption.\n  In this work, we produce a competitive convolution-free transformer by\ntraining on Imagenet only. We train them on a single computer in less than 3\ndays. Our reference vision transformer (86M parameters) achieves top-1 accuracy\nof 83.1% (single-crop evaluation) on ImageNet with no external data.\n  More importantly, we introduce a teacher-student strategy specific to\ntransformers. It relies on a distillation token ensuring that the student\nlearns from the teacher through attention. We show the interest of this\ntoken-based distillation, especially when using a convnet as a teacher. This\nleads us to report results competitive with convnets for both Imagenet (where\nwe obtain up to 85.2% accuracy) and when transferring to other tasks. We share\nour code and models.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-12-23T18:42:10+00:00",
    "updated": "2021-01-15T15:52:50+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2204.01692v3": {
    "id": "http://arxiv.org/abs/2204.01692v3",
    "title": "Long Movie Clip Classification with State-Space Video Models",
    "authors": [
      "Md Mohaiminul Islam",
      "Gedas Bertasius"
    ],
    "abstract": "Most modern video recognition models are designed to operate on short video\nclips (e.g., 5-10s in length). Thus, it is challenging to apply such models to\nlong movie understanding tasks, which typically require sophisticated\nlong-range temporal reasoning. The recently introduced video transformers\npartially address this issue by using long-range temporal self-attention.\nHowever, due to the quadratic cost of self-attention, such models are often\ncostly and impractical to use. Instead, we propose ViS4mer, an efficient\nlong-range video model that combines the strengths of self-attention and the\nrecently introduced structured state-space sequence (S4) layer. Our model uses\na standard Transformer encoder for short-range spatiotemporal feature\nextraction, and a multi-scale temporal S4 decoder for subsequent long-range\ntemporal reasoning. By progressively reducing the spatiotemporal feature\nresolution and channel dimension at each decoder layer, ViS4mer learns complex\nlong-range spatiotemporal dependencies in a video. Furthermore, ViS4mer is\n$2.63\\times$ faster and requires $8\\times$ less GPU memory than the\ncorresponding pure self-attention-based model. Additionally, ViS4mer achieves\nstate-of-the-art results in $6$ out of $9$ long-form movie video classification\ntasks on the Long Video Understanding (LVU) benchmark. Furthermore, we show\nthat our approach successfully generalizes to other domains, achieving\ncompetitive results on the Breakfast and the COIN procedural activity datasets.\nThe code is publicly available at: https://github.com/md-mohaiminul/ViS4mer.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-04-04T17:58:02+00:00",
    "updated": "2023-01-04T11:54:58+00:00",
    "doi": null,
    "comment": "Accepted by ECCV 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1706.00550v5": {
    "id": "http://arxiv.org/abs/1706.00550v5",
    "title": "On Unifying Deep Generative Models",
    "authors": [
      "Zhiting Hu",
      "Zichao Yang",
      "Ruslan Salakhutdinov",
      "Eric P. Xing"
    ],
    "abstract": "Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nemerging families for generative model learning, have largely been considered\nas two distinct paradigms and received extensive independent studies\nrespectively. This paper aims to establish formal connections between GANs and\nVAEs through a new formulation of them. We interpret sample generation in GANs\nas performing posterior inference, and show that GANs and VAEs involve\nminimizing KL divergences of respective posterior and inference distributions\nwith opposite directions, extending the two learning phases of classic\nwake-sleep algorithm, respectively. The unified view provides a powerful tool\nto analyze a diverse set of existing model variants, and enables to transfer\ntechniques across research lines in a principled way. For example, we apply the\nimportance weighting method in VAE literatures for improved GAN learning, and\nenhance VAEs with an adversarial mechanism that leverages generated samples.\nExperiments show generality and effectiveness of the transferred techniques.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2017-06-02T04:15:44+00:00",
    "updated": "2018-07-11T15:01:24+00:00",
    "doi": null,
    "comment": "Polished and extended content over the ICLR conference version:\n  https://openreview.net/pdf?id=rylSzl-R-",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2307.09742v1": {
    "id": "http://arxiv.org/abs/2307.09742v1",
    "title": "Improved Distribution Matching for Dataset Condensation",
    "authors": [
      "Ganlong Zhao",
      "Guanbin Li",
      "Yipeng Qin",
      "Yizhou Yu"
    ],
    "abstract": "Dataset Condensation aims to condense a large dataset into a smaller one\nwhile maintaining its ability to train a well-performing model, thus reducing\nthe storage cost and training effort in deep learning applications. However,\nconventional dataset condensation methods are optimization-oriented and\ncondense the dataset by performing gradient or parameter matching during model\noptimization, which is computationally intensive even on small datasets and\nmodels. In this paper, we propose a novel dataset condensation method based on\ndistribution matching, which is more efficient and promising. Specifically, we\nidentify two important shortcomings of naive distribution matching (i.e.,\nimbalanced feature numbers and unvalidated embeddings for distance computation)\nand address them with three novel techniques (i.e., partitioning and expansion\naugmentation, efficient and enriched model sampling, and class-aware\ndistribution regularization). Our simple yet effective method outperforms most\nprevious optimization-oriented methods with much fewer computational resources,\nthereby scaling data condensation to larger datasets and models. Extensive\nexperiments demonstrate the effectiveness of our method. Codes are available at\nhttps://github.com/uitrbn/IDM",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2023-07-19T04:07:33+00:00",
    "updated": "2023-07-19T04:07:33+00:00",
    "doi": null,
    "comment": "CVPR2023",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2302.01329v1": {
    "id": "http://arxiv.org/abs/2302.01329v1",
    "title": "Dreamix: Video Diffusion Models are General Video Editors",
    "authors": [
      "Eyal Molad",
      "Eliahu Horwitz",
      "Dani Valevski",
      "Alex Rav Acha",
      "Yossi Matias",
      "Yael Pritch",
      "Yaniv Leviathan",
      "Yedid Hoshen"
    ],
    "abstract": "Text-driven image and video diffusion models have recently achieved\nunprecedented generation realism. While diffusion models have been successfully\napplied for image editing, very few works have done so for video editing. We\npresent the first diffusion-based method that is able to perform text-based\nmotion and appearance editing of general videos. Our approach uses a video\ndiffusion model to combine, at inference time, the low-resolution\nspatio-temporal information from the original video with new, high resolution\ninformation that it synthesized to align with the guiding text prompt. As\nobtaining high-fidelity to the original video requires retaining some of its\nhigh-resolution information, we add a preliminary stage of finetuning the model\non the original video, significantly boosting fidelity. We propose to improve\nmotion editability by a new, mixed objective that jointly finetunes with full\ntemporal attention and with temporal attention masking. We further introduce a\nnew framework for image animation. We first transform the image into a coarse\nvideo by simple image processing operations such as replication and perspective\ngeometric projections, and then use our general video editor to animate it. As\na further application, we can use our method for subject-driven video\ngeneration. Extensive qualitative and numerical experiments showcase the\nremarkable editing ability of our method and establish its superior performance\ncompared to baseline methods.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-02-02T18:58:58+00:00",
    "updated": "2023-02-02T18:58:58+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2303.10158v3": {
    "id": "http://arxiv.org/abs/2303.10158v3",
    "title": "Data-centric Artificial Intelligence: A Survey",
    "authors": [
      "Daochen Zha",
      "Zaid Pervaiz Bhat",
      "Kwei-Herng Lai",
      "Fan Yang",
      "Zhimeng Jiang",
      "Shaochen Zhong",
      "Xia Hu"
    ],
    "abstract": "Artificial Intelligence (AI) is making a profound impact in almost every\ndomain. A vital enabler of its great success is the availability of abundant\nand high-quality data for building machine learning models. Recently, the role\nof data in AI has been significantly magnified, giving rise to the emerging\nconcept of data-centric AI. The attention of researchers and practitioners has\ngradually shifted from advancing model design to enhancing the quality and\nquantity of the data. In this survey, we discuss the necessity of data-centric\nAI, followed by a holistic view of three general data-centric goals (training\ndata development, inference data development, and data maintenance) and the\nrepresentative methods. We also organize the existing literature from\nautomation and collaboration perspectives, discuss the challenges, and tabulate\nthe benchmarks for various tasks. We believe this is the first comprehensive\nsurvey that provides a global view of a spectrum of tasks across various stages\nof the data lifecycle. We hope it can help the readers efficiently grasp a\nbroad picture of this field, and equip them with the techniques and further\nresearch ideas to systematically engineer data for building AI systems. A\ncompanion list of data-centric AI resources will be regularly updated on\nhttps://github.com/daochenzha/data-centric-AI",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "published": "2023-03-17T17:44:56+00:00",
    "updated": "2023-06-11T07:25:40+00:00",
    "doi": null,
    "comment": "38 pages, 6 figues, 5 tables. A companion list of data-centric AI\n  resources is available at https://github.com/daochenzha/data-centric-AI",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2306.14289v2": {
    "id": "http://arxiv.org/abs/2306.14289v2",
    "title": "Faster Segment Anything: Towards Lightweight SAM for Mobile Applications",
    "authors": [
      "Chaoning Zhang",
      "Dongshen Han",
      "Yu Qiao",
      "Jung Uk Kim",
      "Sung-Ho Bae",
      "Seungkyu Lee",
      "Choong Seon Hong"
    ],
    "abstract": "Segment Anything Model (SAM) has attracted significant attention due to its\nimpressive zero-shot transfer performance and high versatility for numerous\nvision applications (like image editing with fine-grained control). Many of\nsuch applications need to be run on resource-constraint edge devices, like\nmobile phones. In this work, we aim to make SAM mobile-friendly by replacing\nthe heavyweight image encoder with a lightweight one. A naive way to train such\na new SAM as in the original SAM paper leads to unsatisfactory performance,\nespecially when limited training sources are available. We find that this is\nmainly caused by the coupled optimization of the image encoder and mask\ndecoder, motivated by which we propose decoupled distillation. Concretely, we\ndistill the knowledge from the heavy image encoder (ViT-H in the original SAM)\nto a lightweight image encoder, which can be automatically compatible with the\nmask decoder in the original SAM. The training can be completed on a single GPU\nwithin less than one day, and the resulting lightweight SAM is termed MobileSAM\nwhich is more than 60 times smaller yet performs on par with the original SAM.\nFor inference speed, With a single GPU, MobileSAM runs around 10ms per image:\n8ms on the image encoder and 4ms on the mask decoder. With superior\nperformance, our MobileSAM is around 5 times faster than the concurrent FastSAM\nand 7 times smaller, making it more suitable for mobile applications. Moreover,\nwe show that MobileSAM can run relatively smoothly on CPU. The code for our\nproject is provided at\n\\href{https://github.com/ChaoningZhang/MobileSAM}{\\textcolor{red}{MobileSAM}}),\nwith a demo showing that MobileSAM can run relatively smoothly on CPU.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-06-25T16:37:25+00:00",
    "updated": "2023-07-01T07:26:22+00:00",
    "doi": null,
    "comment": "First work to make SAM lightweight for mobile applications",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2406.10373v1": {
    "id": "http://arxiv.org/abs/2406.10373v1",
    "title": "Wild-GS: Real-Time Novel View Synthesis from Unconstrained Photo Collections",
    "authors": [
      "Jiacong Xu",
      "Yiqun Mei",
      "Vishal M. Patel"
    ],
    "abstract": "Photographs captured in unstructured tourist environments frequently exhibit\nvariable appearances and transient occlusions, challenging accurate scene\nreconstruction and inducing artifacts in novel view synthesis. Although prior\napproaches have integrated the Neural Radiance Field (NeRF) with additional\nlearnable modules to handle the dynamic appearances and eliminate transient\nobjects, their extensive training demands and slow rendering speeds limit\npractical deployments. Recently, 3D Gaussian Splatting (3DGS) has emerged as a\npromising alternative to NeRF, offering superior training and inference\nefficiency along with better rendering quality. This paper presents Wild-GS, an\ninnovative adaptation of 3DGS optimized for unconstrained photo collections\nwhile preserving its efficiency benefits. Wild-GS determines the appearance of\neach 3D Gaussian by their inherent material attributes, global illumination and\ncamera properties per image, and point-level local variance of reflectance.\nUnlike previous methods that model reference features in image space, Wild-GS\nexplicitly aligns the pixel appearance features to the corresponding local\nGaussians by sampling the triplane extracted from the reference image. This\nnovel design effectively transfers the high-frequency detailed appearance of\nthe reference view to 3D space and significantly expedites the training\nprocess. Furthermore, 2D visibility maps and depth regularization are leveraged\nto mitigate the transient effects and constrain the geometry, respectively.\nExtensive experiments demonstrate that Wild-GS achieves state-of-the-art\nrendering performance and the highest efficiency in both training and inference\namong all the existing techniques.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2024-06-14T19:06:07+00:00",
    "updated": "2024-06-14T19:06:07+00:00",
    "doi": null,
    "comment": "15 pages, 7 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.11358v2": {
    "id": "http://arxiv.org/abs/2404.11358v2",
    "title": "DeblurGS: Gaussian Splatting for Camera Motion Blur",
    "authors": [
      "Jeongtaek Oh",
      "Jaeyoung Chung",
      "Dongwoo Lee",
      "Kyoung Mu Lee"
    ],
    "abstract": "Although significant progress has been made in reconstructing sharp 3D scenes\nfrom motion-blurred images, a transition to real-world applications remains\nchallenging. The primary obstacle stems from the severe blur which leads to\ninaccuracies in the acquisition of initial camera poses through\nStructure-from-Motion, a critical aspect often overlooked by previous\napproaches. To address this challenge, we propose DeblurGS, a method to\noptimize sharp 3D Gaussian Splatting from motion-blurred images, even with the\nnoisy camera pose initialization. We restore a fine-grained sharp scene by\nleveraging the remarkable reconstruction capability of 3D Gaussian Splatting.\nOur approach estimates the 6-Degree-of-Freedom camera motion for each blurry\nobservation and synthesizes corresponding blurry renderings for the\noptimization process. Furthermore, we propose Gaussian Densification Annealing\nstrategy to prevent the generation of inaccurate Gaussians at erroneous\nlocations during the early training stages when camera motion is still\nimprecise. Comprehensive experiments demonstrate that our DeblurGS achieves\nstate-of-the-art performance in deblurring and novel view synthesis for\nreal-world and synthetic benchmark datasets, as well as field-captured blurry\nsmartphone videos.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-17T13:14:52+00:00",
    "updated": "2024-04-18T03:18:36+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2202.03218v1": {
    "id": "http://arxiv.org/abs/2202.03218v1",
    "title": "Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition",
    "authors": [
      "Bethan Thomas",
      "Samuel Kessler",
      "Salah Karout"
    ],
    "abstract": "Self-supervised learning (SSL) is a powerful tool that allows learning of\nunderlying representations from unlabeled data. Transformer based models such\nas wav2vec 2.0 and HuBERT are leading the field in the speech domain. Generally\nthese models are fine-tuned on a small amount of labeled data for a downstream\ntask such as Automatic Speech Recognition (ASR). This involves re-training the\nmajority of the model for each task. Adapters are small lightweight modules\nwhich are commonly used in Natural Language Processing (NLP) to adapt\npre-trained models to new tasks. In this paper we propose applying adapters to\nwav2vec 2.0 to reduce the number of parameters required for downstream ASR\ntasks, and increase scalability of the model to multiple tasks or languages.\nUsing adapters we can perform ASR while training fewer than 10% of parameters\nper task compared to full fine-tuning with little degradation of performance.\nAblations show that applying adapters into just the top few layers of the\npre-trained network gives similar performance to full transfer, supporting the\ntheory that higher pre-trained layers encode more phonemic information, and\nfurther optimizing efficiency.",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2022-02-07T14:20:54+00:00",
    "updated": "2022-02-07T14:20:54+00:00",
    "doi": null,
    "comment": "5 Pages, 4 figures. Accepted to ICASSP 2022",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2207.07697v1": {
    "id": "http://arxiv.org/abs/2207.07697v1",
    "title": "POET: Training Neural Networks on Tiny Devices with Integrated Rematerialization and Paging",
    "authors": [
      "Shishir G. Patil",
      "Paras Jain",
      "Prabal Dutta",
      "Ion Stoica",
      "Joseph E. Gonzalez"
    ],
    "abstract": "Fine-tuning models on edge devices like mobile phones would enable\nprivacy-preserving personalization over sensitive data. However, edge training\nhas historically been limited to relatively small models with simple\narchitectures because training is both memory and energy intensive. We present\nPOET, an algorithm to enable training large neural networks on memory-scarce\nbattery-operated edge devices. POET jointly optimizes the integrated search\nsearch spaces of rematerialization and paging, two algorithms to reduce the\nmemory consumption of backpropagation. Given a memory budget and a run-time\nconstraint, we formulate a mixed-integer linear program (MILP) for\nenergy-optimal training. Our approach enables training significantly larger\nmodels on embedded devices while reducing energy consumption while not\nmodifying mathematical correctness of backpropagation. We demonstrate that it\nis possible to fine-tune both ResNet-18 and BERT within the memory constraints\nof a Cortex-M class embedded device while outperforming current edge training\nmethods in energy efficiency. POET is an open-source project available at\nhttps://github.com/ShishirPatil/poet",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.DC",
      "stat.ML"
    ],
    "published": "2022-07-15T18:36:29+00:00",
    "updated": "2022-07-15T18:36:29+00:00",
    "doi": null,
    "comment": "Proceedings of the 39th International Conference on Machine Learning\n  2022 (ICML 2022)",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2209.03665v2": {
    "id": "http://arxiv.org/abs/2209.03665v2",
    "title": "Generalized One-shot Domain Adaptation of Generative Adversarial Networks",
    "authors": [
      "Zicheng Zhang",
      "Yinglu Liu",
      "Congying Han",
      "Tiande Guo",
      "Ting Yao",
      "Tao Mei"
    ],
    "abstract": "The adaptation of a Generative Adversarial Network (GAN) aims to transfer a\npre-trained GAN to a target domain with limited training data. In this paper,\nwe focus on the one-shot case, which is more challenging and rarely explored in\nprevious works. We consider that the adaptation from a source domain to a\ntarget domain can be decoupled into two parts: the transfer of global style\nlike texture and color, and the emergence of new entities that do not belong to\nthe source domain. While previous works mainly focus on style transfer, we\npropose a novel and concise framework to address the \\textit{generalized\none-shot adaptation} task for both style and entity transfer, in which a\nreference image and its binary entity mask are provided. Our core idea is to\nconstrain the gap between the internal distributions of the reference and\nsyntheses by sliced Wasserstein distance. To better achieve it, style fixation\nis used at first to roughly obtain the exemplary style, and an auxiliary\nnetwork is introduced to the generator to disentangle entity and style\ntransfer. Besides, to realize cross-domain correspondence, we propose the\nvariational Laplacian regularization to constrain the smoothness of the adapted\ngenerator. Both quantitative and qualitative experiments demonstrate the\neffectiveness of our method in various scenarios. Code is available at\n\\url{https://github.com/zhangzc21/Generalized-One-shot-GAN-adaptation}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2022-09-08T09:24:44+00:00",
    "updated": "2022-10-14T03:05:20+00:00",
    "doi": null,
    "comment": "NeurIPS 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.14257v2": {
    "id": "http://arxiv.org/abs/2401.14257v2",
    "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation",
    "authors": [
      "Minglin Chen",
      "Weihao Yuan",
      "Yukun Wang",
      "Zhe Sheng",
      "Yisheng He",
      "Zilong Dong",
      "Liefeng Bo",
      "Yulan Guo"
    ],
    "abstract": "Recently, text-to-3D approaches have achieved high-fidelity 3D content\ngeneration using text description. However, the generated objects are\nstochastic and lack fine-grained control. Sketches provide a cheap approach to\nintroduce such fine-grained control. Nevertheless, it is challenging to achieve\nflexible control from these sketches due to their abstraction and ambiguity. In\nthis paper, we present a multi-view sketch-guided text-to-3D generation\nframework (namely, Sketch2NeRF) to add sketch control to 3D generation.\nSpecifically, our method leverages pretrained 2D diffusion models (e.g., Stable\nDiffusion and ControlNet) to supervise the optimization of a 3D scene\nrepresented by a neural radiance field (NeRF). We propose a novel synchronized\ngeneration and reconstruction method to effectively optimize the NeRF. In the\nexperiments, we collected two kinds of multi-view sketch datasets to evaluate\nthe proposed method. We demonstrate that our method can synthesize 3D\nconsistent contents with fine-grained sketch control while being high-fidelity\nto text prompts. Extensive results show that our method achieves\nstate-of-the-art performance in terms of sketch similarity and text alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-01-25T15:49:12+00:00",
    "updated": "2024-01-27T07:22:06+00:00",
    "doi": null,
    "comment": "11 pages, 9 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2211.07636v2": {
    "id": "http://arxiv.org/abs/2211.07636v2",
    "title": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
    "authors": [
      "Yuxin Fang",
      "Wen Wang",
      "Binhui Xie",
      "Quan Sun",
      "Ledell Wu",
      "Xinggang Wang",
      "Tiejun Huang",
      "Xinlong Wang",
      "Yue Cao"
    ],
    "abstract": "We launch EVA, a vision-centric foundation model to explore the limits of\nvisual representation at scale using only publicly accessible data. EVA is a\nvanilla ViT pre-trained to reconstruct the masked out image-text aligned vision\nfeatures conditioned on visible image patches. Via this pretext task, we can\nefficiently scale up EVA to one billion parameters, and sets new records on a\nbroad range of representative vision downstream tasks, such as image\nrecognition, video action recognition, object detection, instance segmentation\nand semantic segmentation without heavy supervised training. Moreover, we\nobserve quantitative changes in scaling EVA result in qualitative changes in\ntransfer learning performance that are not present in other models. For\ninstance, EVA takes a great leap in the challenging large vocabulary instance\nsegmentation task: our model achieves almost the same state-of-the-art\nperformance on LVISv1.0 dataset with over a thousand categories and COCO\ndataset with only eighty categories. Beyond a pure vision encoder, EVA can also\nserve as a vision-centric, multi-modal pivot to connect images and text. We\nfind initializing the vision tower of a giant CLIP from EVA can greatly\nstabilize the training and outperform the training from scratch counterpart\nwith much fewer samples and less compute, providing a new direction for scaling\nup and accelerating the costly training of multi-modal foundation models. To\nfacilitate future research, we release all the code and models at\nhttps://github.com/baaivision/EVA.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2022-11-14T18:59:52+00:00",
    "updated": "2022-12-05T13:53:51+00:00",
    "doi": null,
    "comment": "v2: (i) fix / update EVA IN-1K variants results. (ii) add / update\n  EVA-CLIP results. (iii) add Appendix. (iv) release all the code and models at\n  https://github.com/baaivision/EVA",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1704.01047v3": {
    "id": "http://arxiv.org/abs/1704.01047v3",
    "title": "OctNetFusion: Learning Depth Fusion from Data",
    "authors": [
      "Gernot Riegler",
      "Ali Osman Ulusoy",
      "Horst Bischof",
      "Andreas Geiger"
    ],
    "abstract": "In this paper, we present a learning based approach to depth fusion, i.e.,\ndense 3D reconstruction from multiple depth images. The most common approach to\ndepth fusion is based on averaging truncated signed distance functions, which\nwas originally proposed by Curless and Levoy in 1996. While this method is\nsimple and provides great results, it is not able to reconstruct (partially)\noccluded surfaces and requires a large number frames to filter out sensor noise\nand outliers. Motivated by the availability of large 3D model repositories and\nrecent advances in deep learning, we present a novel 3D CNN architecture that\nlearns to predict an implicit surface representation from the input depth maps.\nOur learning based method significantly outperforms the traditional volumetric\nfusion approach in terms of noise reduction and outlier suppression. By\nlearning the structure of real world 3D objects and scenes, our approach is\nfurther able to reconstruct occluded regions and to fill in gaps in the\nreconstruction. We demonstrate that our learning based approach outperforms\nboth vanilla TSDF fusion as well as TV-L1 fusion on the task of volumetric\nfusion. Further, we demonstrate state-of-the-art 3D shape completion results.",
    "categories": [
      "cs.CV"
    ],
    "published": "2017-04-04T15:02:05+00:00",
    "updated": "2017-10-31T20:58:32+00:00",
    "doi": null,
    "comment": "3DV 2017, https://github.com/griegler/octnetfusion",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2003.03773v3": {
    "id": "http://arxiv.org/abs/2003.03773v3",
    "title": "Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation",
    "authors": [
      "Zhedong Zheng",
      "Yi Yang"
    ],
    "abstract": "This paper focuses on the unsupervised domain adaptation of transferring the\nknowledge from the source domain to the target domain in the context of\nsemantic segmentation. Existing approaches usually regard the pseudo label as\nthe ground truth to fully exploit the unlabeled target-domain data. Yet the\npseudo labels of the target-domain data are usually predicted by the model\ntrained on the source domain. Thus, the generated labels inevitably contain the\nincorrect prediction due to the discrepancy between the training domain and the\ntest domain, which could be transferred to the final adapted model and largely\ncompromises the training process. To overcome the problem, this paper proposes\nto explicitly estimate the prediction uncertainty during training to rectify\nthe pseudo label learning for unsupervised semantic segmentation adaptation.\nGiven the input image, the model outputs the semantic segmentation prediction\nas well as the uncertainty of the prediction. Specifically, we model the\nuncertainty via the prediction variance and involve the uncertainty into the\noptimization objective. To verify the effectiveness of the proposed method, we\nevaluate the proposed method on two prevalent synthetic-to-real semantic\nsegmentation benchmarks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, as\nwell as one cross-city benchmark, i.e., Cityscapes -> Oxford RobotCar. We\ndemonstrate through extensive experiments that the proposed approach (1)\ndynamically sets different confidence thresholds according to the prediction\nvariance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves\nsignificant improvements over the conventional pseudo label learning and yields\ncompetitive performance on all three benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-03-08T12:37:19+00:00",
    "updated": "2020-10-15T06:07:14+00:00",
    "doi": null,
    "comment": "13 pages, 6 figures, 10 tables, accepted by IJCV",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1708.00284v2": {
    "id": "http://arxiv.org/abs/1708.00284v2",
    "title": "Dual Motion GAN for Future-Flow Embedded Video Prediction",
    "authors": [
      "Xiaodan Liang",
      "Lisa Lee",
      "Wei Dai",
      "Eric P. Xing"
    ],
    "abstract": "Future frame prediction in videos is a promising avenue for unsupervised\nvideo representation learning. Video frames are naturally generated by the\ninherent pixel flows from preceding frames based on the appearance and motion\ndynamics in the video. However, existing methods focus on directly\nhallucinating pixel values, resulting in blurry predictions. In this paper, we\ndevelop a dual motion Generative Adversarial Net (GAN) architecture, which\nlearns to explicitly enforce future-frame predictions to be consistent with the\npixel-wise flows in the video through a dual-learning mechanism. The primal\nfuture-frame prediction and dual future-flow prediction form a closed loop,\ngenerating informative feedback signals to each other for better video\nprediction. To make both synthesized future frames and flows indistinguishable\nfrom reality, a dual adversarial training method is proposed to ensure that the\nfuture-flow prediction is able to help infer realistic future-frames, while the\nfuture-frame prediction in turn leads to realistic optical flows. Our dual\nmotion GAN also handles natural motion uncertainty in different pixel locations\nwith a new probabilistic motion encoder, which is based on variational\nautoencoders. Extensive experiments demonstrate that the proposed dual motion\nGAN significantly outperforms state-of-the-art approaches on synthesizing new\nvideo frames and predicting future flows. Our model generalizes well across\ndiverse visual scenes and shows superiority in unsupervised video\nrepresentation learning.",
    "categories": [
      "cs.CV"
    ],
    "published": "2017-08-01T12:38:58+00:00",
    "updated": "2017-08-03T04:23:30+00:00",
    "doi": null,
    "comment": "ICCV 17 camera ready",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.02032v2": {
    "id": "http://arxiv.org/abs/2401.02032v2",
    "title": "DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection",
    "authors": [
      "Yunfan Ye",
      "Kai Xu",
      "Yuhang Huang",
      "Renjiao Yi",
      "Zhiping Cai"
    ],
    "abstract": "Limited by the encoder-decoder architecture, learning-based edge detectors\nusually have difficulty predicting edge maps that satisfy both correctness and\ncrispness. With the recent success of the diffusion probabilistic model (DPM),\nwe found it is especially suitable for accurate and crisp edge detection since\nthe denoising process is directly applied to the original image size.\nTherefore, we propose the first diffusion model for the task of general edge\ndetection, which we call DiffusionEdge. To avoid expensive computational\nresources while retaining the final performance, we apply DPM in the latent\nspace and enable the classic cross-entropy loss which is uncertainty-aware in\npixel level to directly optimize the parameters in latent space in a\ndistillation manner. We also adopt a decoupled architecture to speed up the\ndenoising process and propose a corresponding adaptive Fourier filter to adjust\nthe latent features of specific frequencies. With all the technical designs,\nDiffusionEdge can be stably trained with limited resources, predicting crisp\nand accurate edge maps with much fewer augmentation strategies. Extensive\nexperiments on four edge detection benchmarks demonstrate the superiority of\nDiffusionEdge both in correctness and crispness. On the NYUDv2 dataset,\ncompared to the second best, we increase the ODS, OIS (without post-processing)\nand AC by 30.2%, 28.1% and 65.1%, respectively. Code:\nhttps://github.com/GuHuangAI/DiffusionEdge.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-01-04T02:20:54+00:00",
    "updated": "2024-01-09T12:00:35+00:00",
    "doi": null,
    "comment": "AAAI 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1605.02276v3": {
    "id": "http://arxiv.org/abs/1605.02276v3",
    "title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks",
    "authors": [
      "Manaal Faruqui",
      "Yulia Tsvetkov",
      "Pushpendre Rastogi",
      "Chris Dyer"
    ],
    "abstract": "Lacking standardized extrinsic evaluation methods for vector representations\nof words, the NLP community has relied heavily on word similarity tasks as a\nproxy for intrinsic evaluation of word vectors. Word similarity evaluation,\nwhich correlates the distance between vectors and human judgments of semantic\nsimilarity is attractive, because it is computationally inexpensive and fast.\nIn this paper we present several problems associated with the evaluation of\nword vectors on word similarity datasets, and summarize existing solutions. Our\nstudy suggests that the use of word similarity tasks for evaluation of word\nvectors is not sustainable and calls for further research on evaluation\nmethods.",
    "categories": [
      "cs.CL"
    ],
    "published": "2016-05-08T05:09:28+00:00",
    "updated": "2016-06-22T02:41:04+00:00",
    "doi": null,
    "comment": "The First Workshop on Evaluating Vector Space Representations for NLP",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2405.14701v2": {
    "id": "http://arxiv.org/abs/2405.14701v2",
    "title": "High Fidelity Scene Text Synthesis",
    "authors": [
      "Yibin Wang",
      "Weizhong Zhang",
      "Changhai Zhou",
      "Cheng Jin"
    ],
    "abstract": "Scene text synthesis involves rendering specified texts onto arbitrary\nimages. Current methods typically formulate this task in an end-to-end manner\nbut lack effective character-level guidance during training. Besides, their\ntext encoders, pre-trained on a single font type, struggle to adapt to the\ndiverse font styles encountered in practical applications. Consequently, these\nmethods suffer from character distortion, repetition, and absence, particularly\nin polystylistic scenarios. To this end, this paper proposes DreamText for\nhigh-fidelity scene text synthesis. Our key idea is to reconstruct the\ndiffusion training process, introducing more refined guidance tailored to this\ntask, to expose and rectify the model's attention at the character level and\nstrengthen its learning of text regions. This transformation poses a hybrid\noptimization challenge, involving both discrete and continuous variables. To\neffectively tackle this challenge, we employ a heuristic alternate optimization\nstrategy. Meanwhile, we jointly train the text encoder and generator to\ncomprehensively learn and utilize the diverse font present in the training\ndataset. This joint training is seamlessly integrated into the alternate\noptimization process, fostering a synergistic relationship between learning\ncharacter embedding and re-estimating character attention. Specifically, in\neach step, we first encode potential character-generated position information\nfrom cross-attention maps into latent character masks. These masks are then\nutilized to update the representation of specific characters in the current\nstep, which, in turn, enables the generator to correct the character's\nattention in the subsequent steps. Both qualitative and quantitative results\ndemonstrate the superiority of our method to the state of the art.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-05-23T15:35:48+00:00",
    "updated": "2024-08-11T11:31:23+00:00",
    "doi": null,
    "comment": "Code: https://github.com/CodeGoat24/DreamText, Project page:\n  https://codegoat24.github.io/DreamText/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2112.03126v3": {
    "id": "http://arxiv.org/abs/2112.03126v3",
    "title": "Label-Efficient Semantic Segmentation with Diffusion Models",
    "authors": [
      "Dmitry Baranchuk",
      "Ivan Rubachev",
      "Andrey Voynov",
      "Valentin Khrulkov",
      "Artem Babenko"
    ],
    "abstract": "Denoising diffusion probabilistic models have recently received much research\nattention since they outperform alternative approaches, such as GANs, and\ncurrently provide state-of-the-art generative performance. The superior\nperformance of diffusion models has made them an appealing tool in several\napplications, including inpainting, super-resolution, and semantic editing. In\nthis paper, we demonstrate that diffusion models can also serve as an\ninstrument for semantic segmentation, especially in the setup when labeled data\nis scarce. In particular, for several pretrained diffusion models, we\ninvestigate the intermediate activations from the networks that perform the\nMarkov step of the reverse diffusion process. We show that these activations\neffectively capture the semantic information from an input image and appear to\nbe excellent pixel-level representations for the segmentation problem. Based on\nthese observations, we describe a simple segmentation method, which can work\neven if only a few training images are provided. Our approach significantly\noutperforms the existing alternatives on several datasets for the same amount\nof human supervision.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2021-12-06T15:55:30+00:00",
    "updated": "2022-03-16T01:35:49+00:00",
    "doi": null,
    "comment": "ICLR'2022; v3: camera ready",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2302.07351v2": {
    "id": "http://arxiv.org/abs/2302.07351v2",
    "title": "Constrained Decision Transformer for Offline Safe Reinforcement Learning",
    "authors": [
      "Zuxin Liu",
      "Zijian Guo",
      "Yihang Yao",
      "Zhepeng Cen",
      "Wenhao Yu",
      "Tingnan Zhang",
      "Ding Zhao"
    ],
    "abstract": "Safe reinforcement learning (RL) trains a constraint satisfaction policy by\ninteracting with the environment. We aim to tackle a more challenging problem:\nlearning a safe policy from an offline dataset. We study the offline safe RL\nproblem from a novel multi-objective optimization perspective and propose the\n$\\epsilon$-reducible concept to characterize problem difficulties. The inherent\ntrade-offs between safety and task performance inspire us to propose the\nconstrained decision transformer (CDT) approach, which can dynamically adjust\nthe trade-offs during deployment. Extensive experiments show the advantages of\nthe proposed method in learning an adaptive, safe, robust, and high-reward\npolicy. CDT outperforms its variants and strong offline safe RL baselines by a\nlarge margin with the same hyperparameters across all tasks, while keeping the\nzero-shot adaptation capability to different constraint thresholds, making our\napproach more suitable for real-world RL under constraints. The code is\navailable at https://github.com/liuzuxin/OSRL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2023-02-14T21:27:10+00:00",
    "updated": "2023-06-21T06:07:22+00:00",
    "doi": null,
    "comment": "Published at ICML 2023",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2403.20153v1": {
    "id": "http://arxiv.org/abs/2403.20153v1",
    "title": "Talk3D: High-Fidelity Talking Portrait Synthesis via Personalized 3D Generative Prior",
    "authors": [
      "Jaehoon Ko",
      "Kyusun Cho",
      "Joungbin Lee",
      "Heeji Yoon",
      "Sangmin Lee",
      "Sangjun Ahn",
      "Seungryong Kim"
    ],
    "abstract": "Recent methods for audio-driven talking head synthesis often optimize neural\nradiance fields (NeRF) on a monocular talking portrait video, leveraging its\ncapability to render high-fidelity and 3D-consistent novel-view frames.\nHowever, they often struggle to reconstruct complete face geometry due to the\nabsence of comprehensive 3D information in the input monocular videos. In this\npaper, we introduce a novel audio-driven talking head synthesis framework,\ncalled Talk3D, that can faithfully reconstruct its plausible facial geometries\nby effectively adopting the pre-trained 3D-aware generative prior. Given the\npersonalized 3D generative model, we present a novel audio-guided attention\nU-Net architecture that predicts the dynamic face variations in the NeRF space\ndriven by audio. Furthermore, our model is further modulated by audio-unrelated\nconditioning tokens which effectively disentangle variations unrelated to audio\nfeatures. Compared to existing methods, our method excels in generating\nrealistic facial geometries even under extreme head poses. We also conduct\nextensive experiments showing our approach surpasses state-of-the-art\nbenchmarks in terms of both quantitative and qualitative evaluations.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-29T12:49:40+00:00",
    "updated": "2024-03-29T12:49:40+00:00",
    "doi": null,
    "comment": "Project page: https://ku-cvlab.github.io/Talk3D/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.09047v1": {
    "id": "http://arxiv.org/abs/2401.09047v1",
    "title": "VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models",
    "authors": [
      "Haoxin Chen",
      "Yong Zhang",
      "Xiaodong Cun",
      "Menghan Xia",
      "Xintao Wang",
      "Chao Weng",
      "Ying Shan"
    ],
    "abstract": "Text-to-video generation aims to produce a video based on a given prompt.\nRecently, several commercial video models have been able to generate plausible\nvideos with minimal noise, excellent details, and high aesthetic scores.\nHowever, these models rely on large-scale, well-filtered, high-quality videos\nthat are not accessible to the community. Many existing research works, which\ntrain models using the low-quality WebVid-10M dataset, struggle to generate\nhigh-quality videos because the models are optimized to fit WebVid-10M. In this\nwork, we explore the training scheme of video models extended from Stable\nDiffusion and investigate the feasibility of leveraging low-quality videos and\nsynthesized high-quality images to obtain a high-quality video model. We first\nanalyze the connection between the spatial and temporal modules of video models\nand the distribution shift to low-quality videos. We observe that full training\nof all modules results in a stronger coupling between spatial and temporal\nmodules than only training temporal modules. Based on this stronger coupling,\nwe shift the distribution to higher quality without motion degradation by\nfinetuning spatial modules with high-quality images, resulting in a generic\nhigh-quality video model. Evaluations are conducted to demonstrate the\nsuperiority of the proposed method, particularly in picture quality, motion,\nand concept composition.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-01-17T08:30:32+00:00",
    "updated": "2024-01-17T08:30:32+00:00",
    "doi": null,
    "comment": "Homepage: https://ailab-cvc.github.io/videocrafter; Github:\n  https://github.com/AILab-CVC/VideoCrafter",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1708.04896v2": {
    "id": "http://arxiv.org/abs/1708.04896v2",
    "title": "Random Erasing Data Augmentation",
    "authors": [
      "Zhun Zhong",
      "Liang Zheng",
      "Guoliang Kang",
      "Shaozi Li",
      "Yi Yang"
    ],
    "abstract": "In this paper, we introduce Random Erasing, a new data augmentation method\nfor training the convolutional neural network (CNN). In training, Random\nErasing randomly selects a rectangle region in an image and erases its pixels\nwith random values. In this process, training images with various levels of\nocclusion are generated, which reduces the risk of over-fitting and makes the\nmodel robust to occlusion. Random Erasing is parameter learning free, easy to\nimplement, and can be integrated with most of the CNN-based recognition models.\nAlbeit simple, Random Erasing is complementary to commonly used data\naugmentation techniques such as random cropping and flipping, and yields\nconsistent improvement over strong baselines in image classification, object\ndetection and person re-identification. Code is available at:\nhttps://github.com/zhunzhong07/Random-Erasing.",
    "categories": [
      "cs.CV"
    ],
    "published": "2017-08-16T13:56:48+00:00",
    "updated": "2017-11-16T10:05:35+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2111.15158v1": {
    "id": "http://arxiv.org/abs/2111.15158v1",
    "title": "A Dataset-Dispersion Perspective on Reconstruction Versus Recognition in Single-View 3D Reconstruction Networks",
    "authors": [
      "Yefan Zhou",
      "Yiru Shen",
      "Yujun Yan",
      "Chen Feng",
      "Yaoqing Yang"
    ],
    "abstract": "Neural networks (NN) for single-view 3D reconstruction (SVR) have gained in\npopularity. Recent work points out that for SVR, most cutting-edge NNs have\nlimited performance on reconstructing unseen objects because they rely\nprimarily on recognition (i.e., classification-based methods) rather than shape\nreconstruction. To understand this issue in depth, we provide a systematic\nstudy on when and why NNs prefer recognition to reconstruction and vice versa.\nOur finding shows that a leading factor in determining recognition versus\nreconstruction is how dispersed the training data is. Thus, we introduce the\ndispersion score, a new data-driven metric, to quantify this leading factor and\nstudy its effect on NNs. We hypothesize that NNs are biased toward recognition\nwhen training images are more dispersed and training shapes are less dispersed.\nOur hypothesis is supported and the dispersion score is proved effective\nthrough our experiments on synthetic and benchmark datasets. We show that the\nproposed metric is a principal way to analyze reconstruction quality and\nprovides novel information in addition to the conventional reconstruction\nscore.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-11-30T06:33:35+00:00",
    "updated": "2021-11-30T06:33:35+00:00",
    "doi": null,
    "comment": "Accepted to 3DV 2021",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2307.00040v3": {
    "id": "http://arxiv.org/abs/2307.00040v3",
    "title": "DisCo: Disentangled Control for Realistic Human Dance Generation",
    "authors": [
      "Tan Wang",
      "Linjie Li",
      "Kevin Lin",
      "Yuanhao Zhai",
      "Chung-Ching Lin",
      "Zhengyuan Yang",
      "Hanwang Zhang",
      "Zicheng Liu",
      "Lijuan Wang"
    ],
    "abstract": "Generative AI has made significant strides in computer vision, particularly\nin text-driven image/video synthesis (T2I/T2V). Despite the notable\nadvancements, it remains challenging in human-centric content synthesis such as\nrealistic dance generation. Current methodologies, primarily tailored for human\nmotion transfer, encounter difficulties when confronted with real-world dance\nscenarios (e.g., social media dance), which require to generalize across a wide\nspectrum of poses and intricate human details. In this paper, we depart from\nthe traditional paradigm of human motion transfer and emphasize two additional\ncritical attributes for the synthesis of human dance content in social media\ncontexts: (i) Generalizability: the model should be able to generalize beyond\ngeneric human viewpoints as well as unseen human subjects, backgrounds, and\nposes; (ii) Compositionality: it should allow for the seamless composition of\nseen/unseen subjects, backgrounds, and poses from different sources. To address\nthese challenges, we introduce DISCO, which includes a novel model architecture\nwith disentangled control to improve the compositionality of dance synthesis,\nand an effective human attribute pre-training for better generalizability to\nunseen humans. Extensive qualitative and quantitative results demonstrate that\nDisCc can generate high-quality human dance images and videos with diverse\nappearances and flexible motions. Code is available at\nhttps://disco-dance.github.io/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-06-30T17:37:48+00:00",
    "updated": "2024-04-04T19:41:09+00:00",
    "doi": null,
    "comment": "Accepted by CVPR24",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1905.12506v3": {
    "id": "http://arxiv.org/abs/1905.12506v3",
    "title": "Are Disentangled Representations Helpful for Abstract Visual Reasoning?",
    "authors": [
      "Sjoerd van Steenkiste",
      "Francesco Locatello",
      "J\u00fcrgen Schmidhuber",
      "Olivier Bachem"
    ],
    "abstract": "A disentangled representation encodes information about the salient factors\nof variation in the data independently. Although it is often argued that this\nrepresentational format is useful in learning to solve many real-world\ndown-stream tasks, there is little empirical evidence that supports this claim.\nIn this paper, we conduct a large-scale study that investigates whether\ndisentangled representations are more suitable for abstract reasoning tasks.\nUsing two new tasks similar to Raven's Progressive Matrices, we evaluate the\nusefulness of the representations learned by 360 state-of-the-art unsupervised\ndisentanglement models. Based on these representations, we train 3600 abstract\nreasoning models and observe that disentangled representations do in fact lead\nto better down-stream performance. In particular, they enable quicker learning\nusing fewer samples.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.NE",
      "stat.ML",
      "I.2.6"
    ],
    "published": "2019-05-29T14:52:32+00:00",
    "updated": "2020-01-07T14:36:07+00:00",
    "doi": null,
    "comment": "Accepted to NeurIPS 2019",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1808.03715v1": {
    "id": "http://arxiv.org/abs/1808.03715v1",
    "title": "This Time with Feeling: Learning Expressive Musical Performance",
    "authors": [
      "Sageev Oore",
      "Ian Simon",
      "Sander Dieleman",
      "Douglas Eck",
      "Karen Simonyan"
    ],
    "abstract": "Music generation has generally been focused on either creating scores or\ninterpreting them. We discuss differences between these two problems and\npropose that, in fact, it may be valuable to work in the space of direct $\\it\nperformance$ generation: jointly predicting the notes $\\it and$ $\\it also$\ntheir expressive timing and dynamics. We consider the significance and\nqualities of the data set needed for this. Having identified both a problem\ndomain and characteristics of an appropriate data set, we show an LSTM-based\nrecurrent network model that subjectively performs quite well on this task.\nCritically, we provide generated examples. We also include feedback from\nprofessional composers and musicians about some of these examples.",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "published": "2018-08-10T21:53:51+00:00",
    "updated": "2018-08-10T21:53:51+00:00",
    "doi": null,
    "comment": "Includes links to urls for audio samples",
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "2112.02624v2": {
    "id": "http://arxiv.org/abs/2112.02624v2",
    "title": "Dynamic Token Normalization Improves Vision Transformers",
    "authors": [
      "Wenqi Shao",
      "Yixiao Ge",
      "Zhaoyang Zhang",
      "Xuyuan Xu",
      "Xiaogang Wang",
      "Ying Shan",
      "Ping Luo"
    ],
    "abstract": "Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved\ngreat success in various computer vision tasks, owing to their capability to\nlearn long-range contextual information. Layer Normalization (LN) is an\nessential ingredient in these models. However, we found that the ordinary LN\nmakes tokens at different positions similar in magnitude because it normalizes\nembeddings within each token. It is difficult for Transformers to capture\ninductive bias such as the positional context in an image with LN. We tackle\nthis problem by proposing a new normalizer, termed Dynamic Token Normalization\n(DTN), where normalization is performed both within each token (intra-token)\nand across different tokens (inter-token). DTN has several merits. Firstly, it\nis built on a unified formulation and thus can represent various existing\nnormalization methods. Secondly, DTN learns to normalize tokens in both\nintra-token and inter-token manners, enabling Transformers to capture both the\nglobal contextual information and the local positional context. {Thirdly, by\nsimply replacing LN layers, DTN can be readily plugged into various vision\ntransformers, such as ViT, Swin, PVT, LeViT, T2T-ViT, BigBird and Reformer.\nExtensive experiments show that the transformer equipped with DTN consistently\noutperforms baseline model with minimal extra parameters and computational\noverhead. For example, DTN outperforms LN by $0.5\\%$ - $1.2\\%$ top-1 accuracy\non ImageNet, by $1.2$ - $1.4$ box AP in object detection on COCO benchmark, by\n$2.3\\%$ - $3.9\\%$ mCE in robustness experiments on ImageNet-C, and by $0.5\\%$ -\n$0.8\\%$ accuracy in Long ListOps on Long-Range Arena.} Codes will be made\npublic at \\url{https://github.com/wqshao126/DTN}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2021-12-05T17:04:59+00:00",
    "updated": "2022-10-14T05:25:34+00:00",
    "doi": null,
    "comment": "Published at ICLR'22; 18 pages, 12 Tables, 9 Figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.19318v2": {
    "id": "http://arxiv.org/abs/2403.19318v2",
    "title": "TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios",
    "authors": [
      "Xiaokang Zhang",
      "Jing Zhang",
      "Zeyao Ma",
      "Yang Li",
      "Bohan Zhang",
      "Guanlin Li",
      "Zijun Yao",
      "Kangli Xu",
      "Jinchang Zhou",
      "Daniel Zhang-Li",
      "Jifan Yu",
      "Shu Zhao",
      "Juanzi Li",
      "Jie Tang"
    ],
    "abstract": "We introduce TableLLM, a robust large language model (LLM) with 13 billion\nparameters, purpose-built for proficiently handling tabular data manipulation\ntasks, whether they are embedded within documents or spreadsheets, catering to\nreal-world office scenarios. We propose a distant supervision method for\ntraining, which comprises a reasoning process extension strategy, aiding in\ntraining LLMs to understand reasoning patterns more effectively as well as a\ncross-way validation strategy, ensuring the quality of the automatically\ngenerated data. To evaluate the performance of TableLLM, we have crafted a\nbenchmark tailored to address both document and spreadsheet formats as well as\nconstructed a well-organized evaluation pipeline capable of handling both\nscenarios. Thorough evaluations underscore the advantages of TableLLM when\ncompared to various existing general-purpose and tabular data-focused LLMs. We\nhave publicly released the model checkpoint, source code, benchmarks, and a web\napplication for user interaction.Our codes and data are publicly available at\nhttps://github.com/TableLLM/TableLLM.",
    "categories": [
      "cs.CL"
    ],
    "published": "2024-03-28T11:21:12+00:00",
    "updated": "2024-04-01T05:10:56+00:00",
    "doi": null,
    "comment": "https://tablellm.github.io/",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2401.17121v1": {
    "id": "http://arxiv.org/abs/2401.17121v1",
    "title": "Physical Priors Augmented Event-Based 3D Reconstruction",
    "authors": [
      "Jiaxu Wang",
      "Junhao He",
      "Ziyi Zhang",
      "Renjing Xu"
    ],
    "abstract": "3D neural implicit representations play a significant component in many\nrobotic applications. However, reconstructing neural radiance fields (NeRF)\nfrom realistic event data remains a challenge due to the sparsities and the\nlack of information when only event streams are available. In this paper, we\nutilize motion, geometry, and density priors behind event data to impose strong\nphysical constraints to augment NeRF training. The proposed novel pipeline can\ndirectly benefit from those priors to reconstruct 3D scenes without additional\ninputs. Moreover, we present a novel density-guided patch-based sampling\nstrategy for robust and efficient learning, which not only accelerates training\nprocedures but also conduces to expressions of local geometries. More\nimportantly, we establish the first large dataset for event-based 3D\nreconstruction, which contains 101 objects with various materials and\ngeometries, along with the groundtruth of images and depth maps for all camera\nviewpoints, which significantly facilitates other research in the related\nfields. The code and dataset will be publicly available at\nhttps://github.com/Mercerai/PAEv3d.",
    "categories": [
      "cs.RO"
    ],
    "published": "2024-01-30T15:54:25+00:00",
    "updated": "2024-01-30T15:54:25+00:00",
    "doi": null,
    "comment": "6 pages, 6 figures, ICRA 2024",
    "journal_ref": null,
    "primary_category": "cs.RO"
  },
  "2406.06465v1": {
    "id": "http://arxiv.org/abs/2406.06465v1",
    "title": "AID: Adapting Image2Video Diffusion Models for Instruction-guided Video Prediction",
    "authors": [
      "Zhen Xing",
      "Qi Dai",
      "Zejia Weng",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "Text-guided video prediction (TVP) involves predicting the motion of future\nframes from the initial frame according to an instruction, which has wide\napplications in virtual reality, robotics, and content creation. Previous TVP\nmethods make significant breakthroughs by adapting Stable Diffusion for this\ntask. However, they struggle with frame consistency and temporal stability\nprimarily due to the limited scale of video datasets. We observe that\npretrained Image2Video diffusion models possess good priors for video dynamics\nbut they lack textual control. Hence, transferring Image2Video models to\nleverage their video dynamic priors while injecting instruction control to\ngenerate controllable videos is both a meaningful and challenging task. To\nachieve this, we introduce the Multi-Modal Large Language Model (MLLM) to\npredict future video states based on initial frames and text instructions. More\nspecifically, we design a dual query transformer (DQFormer) architecture, which\nintegrates the instructions and frames into the conditional embeddings for\nfuture frame prediction. Additionally, we develop Long-Short Term Temporal\nAdapters and Spatial Adapters that can quickly transfer general video diffusion\nmodels to specific scenarios with minimal training costs. Experimental results\nshow that our method significantly outperforms state-of-the-art techniques on\nfour datasets: Something Something V2, Epic Kitchen-100, Bridge Data, and\nUCF-101. Notably, AID achieves 91.2% and 55.5% FVD improvements on Bridge and\nSSv2 respectively, demonstrating its effectiveness in various domains. More\nexamples can be found at our website https://chenhsing.github.io/AID.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2024-06-10T17:02:08+00:00",
    "updated": "2024-06-10T17:02:08+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.17497v1": {
    "id": "http://arxiv.org/abs/2403.17497v1",
    "title": "Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies",
    "authors": [
      "Philipp Sadler",
      "Sherzod Hakimov",
      "David Schlangen"
    ],
    "abstract": "In collaborative goal-oriented settings, the participants are not only\ninterested in achieving a successful outcome, but do also implicitly negotiate\nthe effort they put into the interaction (by adapting to each other). In this\nwork, we propose a challenging interactive reference game that requires two\nplayers to coordinate on vision and language observations. The learning signal\nin this game is a score (given after playing) that takes into account the\nachieved goal and the players' assumed efforts during the interaction. We show\nthat a standard Proximal Policy Optimization (PPO) setup achieves a high\nsuccess rate when bootstrapped with heuristic partner behaviors that implement\ninsights from the analysis of human-human interactions. And we find that a\npairing of neural partners indeed reduces the measured joint effort when\nplaying together repeatedly. However, we observe that in comparison to a\nreasonable heuristic pairing there is still room for improvement -- which\ninvites further research in the direction of cost-sharing in collaborative\ninteractions.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "published": "2024-03-26T08:58:28+00:00",
    "updated": "2024-03-26T08:58:28+00:00",
    "doi": null,
    "comment": "9 pages, Accepted at LREC-COLING 2024",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1911.02497v2": {
    "id": "http://arxiv.org/abs/1911.02497v2",
    "title": "A Programmable Approach to Neural Network Compression",
    "authors": [
      "Vinu Joseph",
      "Saurav Muralidharan",
      "Animesh Garg",
      "Michael Garland",
      "Ganesh Gopalakrishnan"
    ],
    "abstract": "Deep neural networks (DNNs) frequently contain far more weights, represented\nat a higher precision, than are required for the specific task which they are\ntrained to perform. Consequently, they can often be compressed using techniques\nsuch as weight pruning and quantization that reduce both the model size and\ninference time without appreciable loss in accuracy. However, finding the best\ncompression strategy and corresponding target sparsity for a given DNN,\nhardware platform, and optimization objective currently requires expensive,\nfrequently manual, trial-and-error experimentation. In this paper, we introduce\na programmable system for model compression called Condensa. Users\nprogrammatically compose simple operators, in Python, to build more complex and\npractically interesting compression strategies. Given a strategy and\nuser-provided objective (such as minimization of running time), Condensa uses a\nnovel Bayesian optimization-based algorithm to automatically infer desirable\nsparsities. Our experiments on four real-world DNNs demonstrate memory\nfootprint and hardware runtime throughput improvements of 188x and 2.59x,\nrespectively, using at most ten samples per search. We have released a\nreference implementation of Condensa at https://github.com/NVlabs/condensa.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "published": "2019-11-06T17:14:32+00:00",
    "updated": "2020-12-01T22:55:11+00:00",
    "doi": "10.1109/MM.2020.3012391",
    "comment": "This is an updated version of a paper published in IEEE Micro, vol.\n  40, no. 5, pp. 17-25, Sept.-Oct. 2020 at\n  https://ieeexplore.ieee.org/document/9151283",
    "journal_ref": "IEEE Micro, Volume: 40, Issue: 5, Sept.-Oct. 2020, pp. 17-25",
    "primary_category": "cs.LG"
  },
  "2308.08545v2": {
    "id": "http://arxiv.org/abs/2308.08545v2",
    "title": "TeCH: Text-guided Reconstruction of Lifelike Clothed Humans",
    "authors": [
      "Yangyi Huang",
      "Hongwei Yi",
      "Yuliang Xiu",
      "Tingting Liao",
      "Jiaxiang Tang",
      "Deng Cai",
      "Justus Thies"
    ],
    "abstract": "Despite recent research advancements in reconstructing clothed humans from a\nsingle image, accurately restoring the \"unseen regions\" with high-level details\nremains an unsolved challenge that lacks attention. Existing methods often\ngenerate overly smooth back-side surfaces with a blurry texture. But how to\neffectively capture all visual attributes of an individual from a single image,\nwhich are sufficient to reconstruct unseen areas (e.g., the back view)?\nMotivated by the power of foundation models, TeCH reconstructs the 3D human by\nleveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles)\nwhich are automatically generated via a garment parsing model and Visual\nQuestion Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion\nmodel (T2I) which learns the \"indescribable\" appearance. To represent\nhigh-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D\nrepresentation based on DMTet, which consists of an explicit body shape grid\nand an implicit distance field. Guided by the descriptive prompts +\npersonalized T2I diffusion model, the geometry and texture of the 3D humans are\noptimized through multi-view Score Distillation Sampling (SDS) and\nreconstruction losses based on the original observation. TeCH produces\nhigh-fidelity 3D clothed humans with consistent & delicate texture, and\ndetailed full-body geometry. Quantitative and qualitative experiments\ndemonstrate that TeCH outperforms the state-of-the-art methods in terms of\nreconstruction accuracy and rendering quality. The code will be publicly\navailable for research purposes at https://huangyangyi.github.io/TeCH",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "published": "2023-08-16T17:59:13+00:00",
    "updated": "2023-08-19T20:08:54+00:00",
    "doi": null,
    "comment": "Project: https://huangyangyi.github.io/TeCH, Code:\n  https://github.com/huangyangyi/TeCH",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.09705v1": {
    "id": "http://arxiv.org/abs/2308.09705v1",
    "title": "Guide3D: Create 3D Avatars from Text and Image Guidance",
    "authors": [
      "Yukang Cao",
      "Yan-Pei Cao",
      "Kai Han",
      "Ying Shan",
      "Kwan-Yee K. Wong"
    ],
    "abstract": "Recently, text-to-image generation has exhibited remarkable advancements,\nwith the ability to produce visually impressive results. In contrast,\ntext-to-3D generation has not yet reached a comparable level of quality.\nExisting methods primarily rely on text-guided score distillation sampling\n(SDS), and they encounter difficulties in transferring 2D attributes of the\ngenerated images to 3D content. In this work, we aim to develop an effective 3D\ngenerative model capable of synthesizing high-resolution textured meshes by\nleveraging both textual and image information. To this end, we introduce\nGuide3D, a zero-shot text-and-image-guided generative model for 3D avatar\ngeneration based on diffusion models. Our model involves (1) generating\nsparse-view images of a text-consistent character using diffusion models, and\n(2) jointly optimizing multi-resolution differentiable marching tetrahedral\ngrids with pixel-aligned image features. We further propose a similarity-aware\nfeature fusion strategy for efficiently integrating features from different\nviews. Moreover, we introduce two novel training objectives as an alternative\nto calculating SDS, significantly enhancing the optimization process. We\nthoroughly evaluate the performance and components of our framework, which\noutperforms the current state-of-the-art in producing topologically and\nstructurally correct geometry and high-resolution textures. Guide3D enables the\ndirect transfer of 2D-generated images to the 3D space. Our code will be made\npublicly available.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-08-18T17:55:47+00:00",
    "updated": "2023-08-18T17:55:47+00:00",
    "doi": null,
    "comment": "25 pages, 22 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2211.09795v1": {
    "id": "http://arxiv.org/abs/2211.09795v1",
    "title": "Conffusion: Confidence Intervals for Diffusion Models",
    "authors": [
      "Eliahu Horwitz",
      "Yedid Hoshen"
    ],
    "abstract": "Diffusion models have become the go-to method for many generative tasks,\nparticularly for image-to-image generation tasks such as super-resolution and\ninpainting. Current diffusion-based methods do not provide statistical\nguarantees regarding the generated results, often preventing their use in\nhigh-stakes situations. To bridge this gap, we construct a confidence interval\naround each generated pixel such that the true value of the pixel is guaranteed\nto fall within the interval with a probability set by the user. Since diffusion\nmodels parametrize the data distribution, a straightforward way of constructing\nsuch intervals is by drawing multiple samples and calculating their bounds.\nHowever, this method has several drawbacks: i) slow sampling speeds ii)\nsuboptimal bounds iii) requires training a diffusion model per task. To\nmitigate these shortcomings we propose Conffusion, wherein we fine-tune a\npre-trained diffusion model to predict interval bounds in a single forward\npass. We show that Conffusion outperforms the baseline method while being three\norders of magnitude faster.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-11-17T18:58:15+00:00",
    "updated": "2022-11-17T18:58:15+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1905.00534v1": {
    "id": "http://arxiv.org/abs/1905.00534v1",
    "title": "Drug-Drug Adverse Effect Prediction with Graph Co-Attention",
    "authors": [
      "Andreea Deac",
      "Yu-Hsiang Huang",
      "Petar Veli\u010dkovi\u0107",
      "Pietro Li\u00f2",
      "Jian Tang"
    ],
    "abstract": "Complex or co-existing diseases are commonly treated using drug combinations,\nwhich can lead to higher risk of adverse side effects. The detection of\npolypharmacy side effects is usually done in Phase IV clinical trials, but\nthere are still plenty which remain undiscovered when the drugs are put on the\nmarket. Such accidents have been affecting an increasing proportion of the\npopulation (15% in the US now) and it is thus of high interest to be able to\npredict the potential side effects as early as possible. Systematic\ncombinatorial screening of possible drug-drug interactions (DDI) is challenging\nand expensive. However, the recent significant increases in data availability\nfrom pharmaceutical research and development efforts offer a novel paradigm for\nrecovering relevant insights for DDI prediction. Accordingly, several recent\napproaches focus on curating massive DDI datasets (with millions of examples)\nand training machine learning models on them. Here we propose a neural network\narchitecture able to set state-of-the-art results on this task---using the type\nof the side-effect and the molecular structure of the drugs alone---by\nleveraging a co-attentional mechanism. In particular, we show the importance of\nintegrating joint information from the drug pairs early on when learning each\ndrug's representation.",
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.SI",
      "q-bio.QM"
    ],
    "published": "2019-05-02T00:10:20+00:00",
    "updated": "2019-05-02T00:10:20+00:00",
    "doi": null,
    "comment": "8 pages, 5 figures",
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2109.14379v2": {
    "id": "http://arxiv.org/abs/2109.14379v2",
    "title": "Infrared Small-Dim Target Detection with Transformer under Complex Backgrounds",
    "authors": [
      "Fangcen Liu",
      "Chenqiang Gao",
      "Fang Chen",
      "Deyu Meng",
      "Wangmeng Zuo",
      "Xinbo Gao"
    ],
    "abstract": "The infrared small-dim target detection is one of the key techniques in the\ninfrared search and tracking system. Since the local regions similar to\ninfrared small-dim targets spread over the whole background, exploring the\ninteraction information amongst image features in large-range dependencies to\nmine the difference between the target and background is crucial for robust\ndetection. However, existing deep learning-based methods are limited by the\nlocality of convolutional neural networks, which impairs the ability to capture\nlarge-range dependencies. Additionally, the small-dim appearance of the\ninfrared target makes the detection model highly possible to miss detection. To\nthis end, we propose a robust and general infrared small-dim target detection\nmethod with the transformer. We adopt the self-attention mechanism of the\ntransformer to learn the interaction information of image features in a larger\nrange. Moreover, we design a feature enhancement module to learn discriminative\nfeatures of small-dim targets to avoid miss detection. After that, to avoid the\nloss of the target information, we adopt a decoder with the U-Net-like skip\nconnection operation to contain more information of small-dim targets. Finally,\nwe get the detection result by a segmentation head. Extensive experiments on\ntwo public datasets show the obvious superiority of the proposed method over\nstate-of-the-art methods and the proposed method has stronger cross-scene\ngeneralization and anti-noise performance.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-09-29T12:23:41+00:00",
    "updated": "2021-11-11T07:15:10+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2302.14165v2": {
    "id": "http://arxiv.org/abs/2302.14165v2",
    "title": "GAM Coach: Towards Interactive and User-centered Algorithmic Recourse",
    "authors": [
      "Zijie J. Wang",
      "Jennifer Wortman Vaughan",
      "Rich Caruana",
      "Duen Horng Chau"
    ],
    "abstract": "Machine learning (ML) recourse techniques are increasingly used in\nhigh-stakes domains, providing end users with actions to alter ML predictions,\nbut they assume ML developers understand what input variables can be changed.\nHowever, a recourse plan's actionability is subjective and unlikely to match\ndevelopers' expectations completely. We present GAM Coach, a novel open-source\nsystem that adapts integer linear programming to generate customizable\ncounterfactual explanations for Generalized Additive Models (GAMs), and\nleverages interactive visualizations to enable end users to iteratively\ngenerate recourse plans meeting their needs. A quantitative user study with 41\nparticipants shows our tool is usable and useful, and users prefer personalized\nrecourse plans over generic plans. Through a log analysis, we explore how users\ndiscover satisfactory recourse plans, and provide empirical evidence that\ntransparency can lead to more opportunities for everyday users to discover\ncounterintuitive patterns in ML models. GAM Coach is available at:\nhttps://poloclub.github.io/gam-coach/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2023-02-27T21:57:42+00:00",
    "updated": "2023-03-01T01:36:37+00:00",
    "doi": "10.1145/3544548.3580816",
    "comment": "Accepted to CHI 2023. 20 pages, 12 figures. For a demo video, see\n  https://youtu.be/ubacP34H9XE. For a live demo, visit\n  https://poloclub.github.io/gam-coach/",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2304.13350v1": {
    "id": "http://arxiv.org/abs/2304.13350v1",
    "title": "Neuro-symbolic Zero-Shot Code Cloning with Cross-Language Intermediate Representation",
    "authors": [
      "Krishnam Hasija",
      "Shrishti Pradhan",
      "Manasi Patwardhan",
      "Raveendra Kumar Medicherla",
      "Lovekesh Vig",
      "Ravindra Naik"
    ],
    "abstract": "In this paper, we define a neuro-symbolic approach to address the task of\nfinding semantically similar clones for the codes of the legacy programming\nlanguage COBOL, without training data. We define a meta-model that is\ninstantiated to have an Intermediate Representation (IR) in the form of\nAbstract Syntax Trees (ASTs) common across codes in C and COBOL. We linearize\nthe IRs using Structure Based Traversal (SBT) to create sequential inputs. We\nfurther fine-tune UnixCoder, the best-performing model for zero-shot\ncross-programming language code search, for the Code Cloning task with the SBT\nIRs of C code-pairs, available in the CodeNet dataset. This allows us to learn\nlatent representations for the IRs of the C codes, which are transferable to\nthe IRs of the COBOL codes. With this fine-tuned UnixCoder, we get a\nperformance improvement of 12.85 MAP@2 over the pre-trained UniXCoder model, in\na zero-shot setting, on the COBOL test split synthesized from the CodeNet\ndataset. This demonstrates the efficacy of our meta-model based approach to\nfacilitate cross-programming language transfer.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published": "2023-04-26T07:41:26+00:00",
    "updated": "2023-04-26T07:41:26+00:00",
    "doi": null,
    "comment": "10 pages, 4 tables, 2 figures",
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "1906.06033v4": {
    "id": "http://arxiv.org/abs/1906.06033v4",
    "title": "Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks",
    "authors": [
      "Ahmed T. Elthakeb",
      "Prannoy Pilligundla",
      "Alex Cloninger",
      "Hadi Esmaeilzadeh"
    ],
    "abstract": "The deep layers of modern neural networks extract a rather rich set of\nfeatures as an input propagates through the network. This paper sets out to\nharvest these rich intermediate representations for quantization with minimal\naccuracy loss while significantly reducing the memory footprint and compute\nintensity of the DNN. This paper utilizes knowledge distillation through\nteacher-student paradigm (Hinton et al., 2015) in a novel setting that exploits\nthe feature extraction capability of DNNs for higher-accuracy quantization. As\nsuch, our algorithm logically divides a pretrained full-precision DNN to\nmultiple sections, each of which exposes intermediate features to train a team\nof students independently in the quantized domain. This divide and conquer\nstrategy, in fact, makes the training of each student section possible in\nisolation while all these independently trained sections are later stitched\ntogether to form the equivalent fully quantized network. Our algorithm is a\nsectional approach towards knowledge distillation and is not treating the\nintermediate representation as a hint for pretraining before one knowledge\ndistillation pass over the entire network (Romero et al., 2015). Experiments on\nvarious DNNs (AlexNet, LeNet, MobileNet, ResNet-18, ResNet-20, SVHN and VGG-11)\nshow that, this approach -- called DCQ (Divide and Conquer Quantization) -- on\naverage, improves the performance of a state-of-the-art quantized training\ntechnique, DoReFa-Net (Zhou et al., 2016) by 21.6% and 9.3% for binary and\nternary quantization, respectively. Additionally, we show that incorporating\nDCQ to existing quantized training methods leads to improved accuracies as\ncompared to previously reported by multiple state-of-the-art quantized training\nmethods.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2019-06-14T05:53:30+00:00",
    "updated": "2020-03-02T20:06:26+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1912.08795v2": {
    "id": "http://arxiv.org/abs/1912.08795v2",
    "title": "Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion",
    "authors": [
      "Hongxu Yin",
      "Pavlo Molchanov",
      "Zhizhong Li",
      "Jose M. Alvarez",
      "Arun Mallya",
      "Derek Hoiem",
      "Niraj K. Jha",
      "Jan Kautz"
    ],
    "abstract": "We introduce DeepInversion, a new method for synthesizing images from the\nimage distribution used to train a deep neural network. We 'invert' a trained\nnetwork (teacher) to synthesize class-conditional input images starting from\nrandom noise, without using any additional information about the training\ndataset. Keeping the teacher fixed, our method optimizes the input while\nregularizing the distribution of intermediate feature maps using information\nstored in the batch normalization layers of the teacher. Further, we improve\nthe diversity of synthesized images using Adaptive DeepInversion, which\nmaximizes the Jensen-Shannon divergence between the teacher and student network\nlogits. The resulting synthesized images from networks trained on the CIFAR-10\nand ImageNet datasets demonstrate high fidelity and degree of realism, and help\nenable a new breed of data-free applications - ones that do not require any\nreal images or labeled data. We demonstrate the applicability of our proposed\nmethod to three tasks of immense practical importance -- (i) data-free network\npruning, (ii) data-free knowledge transfer, and (iii) data-free continual\nlearning. Code is available at https://github.com/NVlabs/DeepInversion",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "published": "2019-12-18T18:50:10+00:00",
    "updated": "2020-06-16T03:30:21+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2209.11302v1": {
    "id": "http://arxiv.org/abs/2209.11302v1",
    "title": "ProgPrompt: Generating Situated Robot Task Plans using Large Language Models",
    "authors": [
      "Ishika Singh",
      "Valts Blukis",
      "Arsalan Mousavian",
      "Ankit Goyal",
      "Danfei Xu",
      "Jonathan Tremblay",
      "Dieter Fox",
      "Jesse Thomason",
      "Animesh Garg"
    ],
    "abstract": "Task planning can require defining myriad domain knowledge about the world in\nwhich a robot needs to act. To ameliorate that effort, large language models\n(LLMs) can be used to score potential next actions during task planning, and\neven generate action sequences directly, given an instruction in natural\nlanguage with no additional domain information. However, such methods either\nrequire enumerating all possible next steps for scoring, or generate free-form\ntext that may contain actions not possible on a given robot in its current\ncontext. We present a programmatic LLM prompt structure that enables plan\ngeneration functional across situated environments, robot capabilities, and\ntasks. Our key insight is to prompt the LLM with program-like specifications of\nthe available actions and objects in an environment, as well as with example\nprograms that can be executed. We make concrete recommendations about prompt\nstructure and generation constraints through ablation experiments, demonstrate\nstate of the art success rates in VirtualHome household tasks, and deploy our\nmethod on a physical robot arm for tabletop tasks. Website at\nprogprompt.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2022-09-22T20:29:49+00:00",
    "updated": "2022-09-22T20:29:49+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.RO"
  },
  "2406.19298v1": {
    "id": "http://arxiv.org/abs/2406.19298v1",
    "title": "Compositional Image Decomposition with Diffusion Models",
    "authors": [
      "Jocelin Su",
      "Nan Liu",
      "Yanbo Wang",
      "Joshua B. Tenenbaum",
      "Yilun Du"
    ],
    "abstract": "Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo\nunder the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different\ncomponents in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be\nflexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-06-27T16:13:34+00:00",
    "updated": "2024-06-27T16:13:34+00:00",
    "doi": null,
    "comment": "ICML 2024, Webpage:\n  https://energy-based-model.github.io/decomp-diffusion",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.03420v1": {
    "id": "http://arxiv.org/abs/2312.03420v1",
    "title": "Artist-Friendly Relightable and Animatable Neural Heads",
    "authors": [
      "Yingyan Xu",
      "Prashanth Chandran",
      "Sebastian Weiss",
      "Markus Gross",
      "Gaspard Zoss",
      "Derek Bradley"
    ],
    "abstract": "An increasingly common approach for creating photo-realistic digital avatars\nis through the use of volumetric neural fields. The original neural radiance\nfield (NeRF) allowed for impressive novel view synthesis of static heads when\ntrained on a set of multi-view images, and follow up methods showed that these\nneural representations can be extended to dynamic avatars. Recently, new\nvariants also surpassed the usual drawback of baked-in illumination in neural\nrepresentations, showing that static neural avatars can be relit in any\nenvironment. In this work we simultaneously tackle both the motion and\nillumination problem, proposing a new method for relightable and animatable\nneural heads. Our method builds on a proven dynamic avatar approach based on a\nmixture of volumetric primitives, combined with a recently-proposed lightweight\nhardware setup for relightable neural fields, and includes a novel architecture\nthat allows relighting dynamic neural avatars performing unseen expressions in\nany environment, even with nearfield illumination and viewpoints.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2023-12-06T11:06:46+00:00",
    "updated": "2023-12-06T11:06:46+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2007.15779v6": {
    "id": "http://arxiv.org/abs/2007.15779v6",
    "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
    "authors": [
      "Yu Gu",
      "Robert Tinn",
      "Hao Cheng",
      "Michael Lucas",
      "Naoto Usuyama",
      "Xiaodong Liu",
      "Tristan Naumann",
      "Jianfeng Gao",
      "Hoifung Poon"
    ],
    "abstract": "Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding & Reasoning\nBenchmark) at https://aka.ms/BLURB.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2020-07-31T00:04:15+00:00",
    "updated": "2021-09-16T21:26:07+00:00",
    "doi": "10.1145/3458754",
    "comment": "ACM Transactions on Computing for Healthcare (HEALTH)",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2208.02245v1": {
    "id": "http://arxiv.org/abs/2208.02245v1",
    "title": "MinVIS: A Minimal Video Instance Segmentation Framework without Video-based Training",
    "authors": [
      "De-An Huang",
      "Zhiding Yu",
      "Anima Anandkumar"
    ],
    "abstract": "We propose MinVIS, a minimal video instance segmentation (VIS) framework that\nachieves state-of-the-art VIS performance with neither video-based\narchitectures nor training procedures. By only training a query-based image\ninstance segmentation model, MinVIS outperforms the previous best result on the\nchallenging Occluded VIS dataset by over 10% AP. Since MinVIS treats frames in\ntraining videos as independent images, we can drastically sub-sample the\nannotated frames in training videos without any modifications. With only 1% of\nlabeled frames, MinVIS outperforms or is comparable to fully-supervised\nstate-of-the-art approaches on YouTube-VIS 2019/2021. Our key observation is\nthat queries trained to be discriminative between intra-frame object instances\nare temporally consistent and can be used to track instances without any\nmanually designed heuristics. MinVIS thus has the following inference pipeline:\nwe first apply the trained query-based image instance segmentation to video\nframes independently. The segmented instances are then tracked by bipartite\nmatching of the corresponding queries. This inference is done in an online\nfashion and does not need to process the whole video at once. MinVIS thus has\nthe practical advantages of reducing both the labeling costs and the memory\nrequirements, while not sacrificing the VIS performance. Code is available at:\nhttps://github.com/NVlabs/MinVIS",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2022-08-03T17:50:42+00:00",
    "updated": "2022-08-03T17:50:42+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2203.10712v3": {
    "id": "http://arxiv.org/abs/2203.10712v3",
    "title": "Disentangling Architecture and Training for Optical Flow",
    "authors": [
      "Deqing Sun",
      "Charles Herrmann",
      "Fitsum Reda",
      "Michael Rubinstein",
      "David Fleet",
      "William T. Freeman"
    ],
    "abstract": "How important are training details and datasets to recent optical flow models\nlike RAFT? And do they generalize? To explore these questions, rather than\ndevelop a new model, we revisit three prominent models, PWC-Net, IRR-PWC and\nRAFT, with a common set of modern training techniques and datasets, and observe\nsignificant performance gains, demonstrating the importance and generality of\nthese training details. Our newly trained PWC-Net and IRR-PWC models show\nsurprisingly large improvements, up to 30% versus original published results on\nSintel and KITTI 2015 benchmarks. They outperform the more recent Flow1D on\nKITTI 2015 while being 3x faster during inference. Our newly trained RAFT\nachieves an Fl-all score of 4.31% on KITTI 2015, more accurate than all\npublished optical flow methods at the time of writing. Our results demonstrate\nthe benefits of separating the contributions of models, training techniques and\ndatasets when analyzing performance gains of optical flow methods. Our source\ncode will be publicly available.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-03-21T03:15:18+00:00",
    "updated": "2022-09-19T20:41:23+00:00",
    "doi": null,
    "comment": "Accepted to ECCV22. 33 pages, including supplementals. Website at:\n  https://autoflow-google.github.io/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2103.17249v1": {
    "id": "http://arxiv.org/abs/2103.17249v1",
    "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery",
    "authors": [
      "Or Patashnik",
      "Zongze Wu",
      "Eli Shechtman",
      "Daniel Cohen-Or",
      "Dani Lischinski"
    ],
    "abstract": "Inspired by the ability of StyleGAN to generate highly realistic images in a\nvariety of domains, much recent work has focused on understanding how to use\nthe latent spaces of StyleGAN to manipulate generated and real images. However,\ndiscovering semantically meaningful latent manipulations typically involves\npainstaking human examination of the many degrees of freedom, or an annotated\ncollection of images for each desired manipulation. In this work, we explore\nleveraging the power of recently introduced Contrastive Language-Image\nPre-training (CLIP) models in order to develop a text-based interface for\nStyleGAN image manipulation that does not require such manual effort. We first\nintroduce an optimization scheme that utilizes a CLIP-based loss to modify an\ninput latent vector in response to a user-provided text prompt. Next, we\ndescribe a latent mapper that infers a text-guided latent manipulation step for\na given input image, allowing faster and more stable text-based manipulation.\nFinally, we present a method for mapping a text prompts to input-agnostic\ndirections in StyleGAN's style space, enabling interactive text-driven image\nmanipulation. Extensive results and comparisons demonstrate the effectiveness\nof our approaches.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2021-03-31T17:51:25+00:00",
    "updated": "2021-03-31T17:51:25+00:00",
    "doi": null,
    "comment": "18 pages, 24 figures, code and video may be found here:\n  https://github.com/orpatashnik/StyleCLIP",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2209.15258v1": {
    "id": "http://arxiv.org/abs/2209.15258v1",
    "title": "Transformers for Object Detection in Large Point Clouds",
    "authors": [
      "Felicia Ruppel",
      "Florian Faion",
      "Claudius Gl\u00e4ser",
      "Klaus Dietmayer"
    ],
    "abstract": "We present TransLPC, a novel detection model for large point clouds that is\nbased on a transformer architecture. While object detection with transformers\nhas been an active field of research, it has proved difficult to apply such\nmodels to point clouds that span a large area, e.g. those that are common in\nautonomous driving, with lidar or radar data. TransLPC is able to remedy these\nissues: The structure of the transformer model is modified to allow for larger\ninput sequence lengths, which are sufficient for large point clouds. Besides\nthis, we propose a novel query refinement technique to improve detection\naccuracy, while retaining a memory-friendly number of transformer decoder\nqueries. The queries are repositioned between layers, moving them closer to the\nbounding box they are estimating, in an efficient manner. This simple technique\nhas a significant effect on detection accuracy, which is evaluated on the\nchallenging nuScenes dataset on real-world lidar data. Besides this, the\nproposed method is compatible with existing transformer-based solutions that\nrequire object detection, e.g. for joint multi-object tracking and detection,\nand enables them to be used in conjunction with large point clouds.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2022-09-30T06:35:43+00:00",
    "updated": "2022-09-30T06:35:43+00:00",
    "doi": null,
    "comment": "Accepted for publication at the 2022 25th IEEE International\n  Conference on Intelligent Transportation Systems (ITSC 2022), Sep 18- Oct 12,\n  2022, in Macau, China",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2111.12994v2": {
    "id": "http://arxiv.org/abs/2111.12994v2",
    "title": "NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition",
    "authors": [
      "Hao Liu",
      "Xinghua Jiang",
      "Xin Li",
      "Zhimin Bao",
      "Deqiang Jiang",
      "Bo Ren"
    ],
    "abstract": "Recently, Vision Transformers (ViT), with the self-attention (SA) as the de\nfacto ingredients, have demonstrated great potential in the computer vision\ncommunity. For the sake of trade-off between efficiency and performance, a\ngroup of works merely perform SA operation within local patches, whereas the\nglobal contextual information is abandoned, which would be indispensable for\nvisual recognition tasks. To solve the issue, the subsequent global-local ViTs\ntake a stab at marrying local SA with global one in parallel or alternative way\nin the model. Nevertheless, the exhaustively combined local and global context\nmay exist redundancy for various visual data, and the receptive field within\neach layer is fixed. Alternatively, a more graceful way is that global and\nlocal context can adaptively contribute per se to accommodate different visual\ndata. To achieve this goal, we in this paper propose a novel ViT architecture,\ntermed NomMer, which can dynamically Nominate the synergistic global-local\ncontext in vision transforMer. By investigating the working pattern of our\nproposed NomMer, we further explore what context information is focused.\nBeneficial from this \"dynamic nomination\" mechanism, without bells and\nwhistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy\non ImageNet with only 73M parameters, but also show promising performance on\ndense prediction tasks, i.e., object detection and semantic segmentation. The\ncode and models will be made publicly available at\nhttps://github.com/TencentYoutuResearch/VisualRecognition-NomMer",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-11-25T10:07:54+00:00",
    "updated": "2022-03-14T15:02:52+00:00",
    "doi": null,
    "comment": "Accepted to CVPR2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1712.06760v2": {
    "id": "http://arxiv.org/abs/1712.06760v2",
    "title": "Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling",
    "authors": [
      "Yiru Shen",
      "Chen Feng",
      "Yaoqing Yang",
      "Dong Tian"
    ],
    "abstract": "Unlike on images, semantic learning on 3D point clouds using a deep network\nis challenging due to the naturally unordered data structure. Among existing\nworks, PointNet has achieved promising results by directly learning on point\nsets. However, it does not take full advantage of a point's local neighborhood\nthat contains fine-grained structural information which turns out to be helpful\ntowards better semantic learning. In this regard, we present two new operations\nto improve PointNet with a more efficient exploitation of local structures. The\nfirst one focuses on local 3D geometric structures. In analogy to a convolution\nkernel for images, we define a point-set kernel as a set of learnable 3D points\nthat jointly respond to a set of neighboring data points according to their\ngeometric affinities measured by kernel correlation, adapted from a similar\ntechnique for point cloud registration. The second one exploits local\nhigh-dimensional feature structures by recursive feature aggregation on a\nnearest-neighbor-graph computed from 3D positions. Experiments show that our\nnetwork can efficiently capture local information and robustly achieve better\nperformances on major datasets. Our code is available at\nhttp://www.merl.com/research/license#KCNet",
    "categories": [
      "cs.CV"
    ],
    "published": "2017-12-19T03:06:42+00:00",
    "updated": "2018-04-03T21:23:26+00:00",
    "doi": null,
    "comment": "Accepted in CVPR'18. *indicates equal contribution",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2310.16818v2": {
    "id": "http://arxiv.org/abs/2310.16818v2",
    "title": "DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior",
    "authors": [
      "Jingxiang Sun",
      "Bo Zhang",
      "Ruizhi Shao",
      "Lizhen Wang",
      "Wen Liu",
      "Zhenda Xie",
      "Yebin Liu"
    ],
    "abstract": "We present DreamCraft3D, a hierarchical 3D content generation method that\nproduces high-fidelity and coherent 3D objects. We tackle the problem by\nleveraging a 2D reference image to guide the stages of geometry sculpting and\ntexture boosting. A central focus of this work is to address the consistency\nissue that existing works encounter. To sculpt geometries that render\ncoherently, we perform score distillation sampling via a view-dependent\ndiffusion model. This 3D prior, alongside several training strategies,\nprioritizes the geometry consistency but compromises the texture fidelity. We\nfurther propose Bootstrapped Score Distillation to specifically boost the\ntexture. We train a personalized diffusion model, Dreambooth, on the augmented\nrenderings of the scene, imbuing it with 3D knowledge of the scene being\noptimized. The score distillation from this 3D-aware diffusion prior provides\nview-consistent guidance for the scene. Notably, through an alternating\noptimization of the diffusion prior and 3D scene representation, we achieve\nmutually reinforcing improvements: the optimized 3D scene aids in training the\nscene-specific diffusion model, which offers increasingly view-consistent\nguidance for 3D optimization. The optimization is thus bootstrapped and leads\nto substantial texture boosting. With tailored 3D priors throughout the\nhierarchical generation, DreamCraft3D generates coherent 3D objects with\nphotorealistic renderings, advancing the state-of-the-art in 3D content\ngeneration. Code available at https://github.com/deepseek-ai/DreamCraft3D.",
    "categories": [
      "cs.CV",
      "cs.CG"
    ],
    "published": "2023-10-25T17:50:10+00:00",
    "updated": "2023-10-26T06:54:22+00:00",
    "doi": null,
    "comment": "Project Page: https://mrtornado24.github.io/DreamCraft3D/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.07037v5": {
    "id": "http://arxiv.org/abs/2308.07037v5",
    "title": "Bayesian Flow Networks",
    "authors": [
      "Alex Graves",
      "Rupesh Kumar Srivastava",
      "Timothy Atkinson",
      "Faustino Gomez"
    ],
    "abstract": "This paper introduces Bayesian Flow Networks (BFNs), a new class of\ngenerative model in which the parameters of a set of independent distributions\nare modified with Bayesian inference in the light of noisy data samples, then\npassed as input to a neural network that outputs a second, interdependent\ndistribution. Starting from a simple prior and iteratively updating the two\ndistributions yields a generative procedure similar to the reverse process of\ndiffusion models; however it is conceptually simpler in that no forward process\nis required. Discrete and continuous-time loss functions are derived for\ncontinuous, discretised and discrete data, along with sample generation\nprocedures. Notably, the network inputs for discrete data lie on the\nprobability simplex, and are therefore natively differentiable, paving the way\nfor gradient-based sample guidance and few-step generation in discrete domains\nsuch as language modelling. The loss function directly optimises data\ncompression and places no restrictions on the network architecture. In our\nexperiments BFNs achieve competitive log-likelihoods for image modelling on\ndynamically binarized MNIST and CIFAR-10, and outperform all known discrete\ndiffusion models on the text8 character-level language modelling task.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-08-14T09:56:35+00:00",
    "updated": "2024-02-03T20:22:57+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2303.14412v1": {
    "id": "http://arxiv.org/abs/2303.14412v1",
    "title": "Freestyle Layout-to-Image Synthesis",
    "authors": [
      "Han Xue",
      "Zhiwu Huang",
      "Qianru Sun",
      "Li Song",
      "Wenjun Zhang"
    ],
    "abstract": "Typical layout-to-image synthesis (LIS) models generate images for a closed\nset of semantic classes, e.g., 182 common objects in COCO-Stuff. In this work,\nwe explore the freestyle capability of the model, i.e., how far can it generate\nunseen semantics (e.g., classes, attributes, and styles) onto a given layout,\nand call the task Freestyle LIS (FLIS). Thanks to the development of\nlarge-scale pre-trained language-image models, a number of discriminative\nmodels (e.g., image classification and object detection) trained on limited\nbase classes are empowered with the ability of unseen class prediction.\nInspired by this, we opt to leverage large-scale pre-trained text-to-image\ndiffusion models to achieve the generation of unseen semantics. The key\nchallenge of FLIS is how to enable the diffusion model to synthesize images\nfrom a specific layout which very likely violates its pre-learned knowledge,\ne.g., the model never sees \"a unicorn sitting on a bench\" during its\npre-training. To this end, we introduce a new module called Rectified\nCross-Attention (RCA) that can be conveniently plugged in the diffusion model\nto integrate semantic masks. This \"plug-in\" is applied in each cross-attention\nlayer of the model to rectify the attention maps between image and text tokens.\nThe key idea of RCA is to enforce each text token to act on the pixels in a\nspecified region, allowing us to freely put a wide variety of semantics from\npre-trained knowledge (which is general) onto the given layout (which is\nspecific). Extensive experiments show that the proposed diffusion network\nproduces realistic and freestyle layout-to-image generation results with\ndiverse text inputs, which has a high potential to spawn a bunch of interesting\napplications. Code is available at https://github.com/essunny310/FreestyleNet.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-03-25T09:37:41+00:00",
    "updated": "2023-03-25T09:37:41+00:00",
    "doi": null,
    "comment": "Accepted to CVPR 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2111.03936v3": {
    "id": "http://arxiv.org/abs/2111.03936v3",
    "title": "SOPE: Spectrum of Off-Policy Estimators",
    "authors": [
      "Christina J. Yuan",
      "Yash Chandak",
      "Stephen Giguere",
      "Philip S. Thomas",
      "Scott Niekum"
    ],
    "abstract": "Many sequential decision making problems are high-stakes and require\noff-policy evaluation (OPE) of a new policy using historical data collected\nusing some other policy. One of the most common OPE techniques that provides\nunbiased estimates is trajectory based importance sampling (IS). However, due\nto the high variance of trajectory IS estimates, importance sampling methods\nbased on state-action visitation distributions (SIS) have recently been\nadopted. Unfortunately, while SIS often provides lower variance estimates for\nlong horizons, estimating the state-action distribution ratios can be\nchallenging and lead to biased estimates. In this paper, we present a new\nperspective on this bias-variance trade-off and show the existence of a\nspectrum of estimators whose endpoints are SIS and IS. Additionally, we also\nestablish a spectrum for doubly-robust and weighted version of these\nestimators. We provide empirical evidence that estimators in this spectrum can\nbe used to trade-off between the bias and variance of IS and SIS and can\nachieve lower mean-squared error than both IS and SIS.",
    "categories": [
      "cs.LG"
    ],
    "published": "2021-11-06T18:29:21+00:00",
    "updated": "2021-12-02T23:56:51+00:00",
    "doi": null,
    "comment": "Accepted at Thirty-fifth Conference on Neural Information Processing\n  Systems (NeurIPS 2021)",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2309.00267v3": {
    "id": "http://arxiv.org/abs/2309.00267v3",
    "title": "RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback",
    "authors": [
      "Harrison Lee",
      "Samrat Phatale",
      "Hassan Mansoor",
      "Thomas Mesnard",
      "Johan Ferret",
      "Kellie Lu",
      "Colton Bishop",
      "Ethan Hall",
      "Victor Carbune",
      "Abhinav Rastogi",
      "Sushant Prakash"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but gathering\nhigh-quality preference labels is expensive. RL from AI Feedback (RLAIF),\nintroduced in Bai et al., offers a promising alternative that trains the reward\nmodel (RM) on preferences generated by an off-the-shelf LLM. Across the tasks\nof summarization, helpful dialogue generation, and harmless dialogue\ngeneration, we show that RLAIF achieves comparable performance to RLHF.\nFurthermore, we take a step towards \"self-improvement\" by demonstrating that\nRLAIF can outperform a supervised fine-tuned baseline even when the AI labeler\nis the same size as the policy, or even the exact same checkpoint as the\ninitial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that\ncircumvents RM training by obtaining rewards directly from an off-the-shelf LLM\nduring RL, which achieves superior performance to canonical RLAIF. Our results\nsuggest that RLAIF can achieve performance on-par with using human feedback,\noffering a potential solution to the scalability limitations of RLHF.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-01T05:53:33+00:00",
    "updated": "2024-09-03T14:01:54+00:00",
    "doi": null,
    "comment": "Presented at ICML 2024",
    "journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning, PMLR 235:26874-26901, 2024",
    "primary_category": "cs.CL"
  },
  "2111.03042v1": {
    "id": "http://arxiv.org/abs/2111.03042v1",
    "title": "Unsupervised Learning of Compositional Energy Concepts",
    "authors": [
      "Yilun Du",
      "Shuang Li",
      "Yash Sharma",
      "Joshua B. Tenenbaum",
      "Igor Mordatch"
    ],
    "abstract": "Humans are able to rapidly understand scenes by utilizing concepts extracted\nfrom prior experience. Such concepts are diverse, and include global scene\ndescriptors, such as the weather or lighting, as well as local scene\ndescriptors, such as the color or size of a particular object. So far,\nunsupervised discovery of concepts has focused on either modeling the global\nscene-level or the local object-level factors of variation, but not both. In\nthis work, we propose COMET, which discovers and represents concepts as\nseparate energy functions, enabling us to represent both global concepts as\nwell as objects under a unified framework. COMET discovers energy functions\nthrough recomposing the input image, which we find captures independent factors\nwithout additional supervision. Sample generation in COMET is formulated as an\noptimization process on underlying energy functions, enabling us to generate\nimages with permuted and composed concepts. Finally, discovered visual concepts\nin COMET generalize well, enabling us to compose concepts between separate\nmodalities of images as well as with other concepts discovered by a separate\ninstance of COMET trained on a different dataset. Code and data available at\nhttps://energy-based-model.github.io/comet/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2021-11-04T17:46:12+00:00",
    "updated": "2021-11-04T17:46:12+00:00",
    "doi": null,
    "comment": "NeurIPS 2021, website and code at\n  https://energy-based-model.github.io/comet/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2208.11253v1": {
    "id": "http://arxiv.org/abs/2208.11253v1",
    "title": "FashionVQA: A Domain-Specific Visual Question Answering System",
    "authors": [
      "Min Wang",
      "Ata Mahjoubfar",
      "Anupama Joshi"
    ],
    "abstract": "Humans apprehend the world through various sensory modalities, yet language\nis their predominant communication channel. Machine learning systems need to\ndraw on the same multimodal richness to have informed discourses with humans in\nnatural language; this is particularly true for systems specialized in\nvisually-dense information, such as dialogue, recommendation, and search\nengines for clothing. To this end, we train a visual question answering (VQA)\nsystem to answer complex natural language questions about apparel in fashion\nphotoshoot images. The key to the successful training of our VQA model is the\nautomatic creation of a visual question-answering dataset with 168 million\nsamples from item attributes of 207 thousand images using diverse templates.\nThe sample generation employs a strategy that considers the difficulty of the\nquestion-answer pairs to emphasize challenging concepts. Contrary to the recent\ntrends in using several datasets for pretraining the visual question answering\nmodels, we focused on keeping the dataset fixed while training various models\nfrom scratch to isolate the improvements from model architecture changes. We\nsee that using the same transformer for encoding the question and decoding the\nanswer, as in language models, achieves maximum accuracy, showing that visual\nlanguage models (VLMs) make the best visual question answering systems for our\ndataset. The accuracy of the best model surpasses the human expert level, even\nwhen answering human-generated questions that are not confined to the template\nformats. Our approach for generating a large-scale multimodal domain-specific\ndataset provides a path for training specialized models capable of\ncommunicating in natural language. The training of such domain-expert models,\ne.g., our fashion VLM model, cannot rely solely on the large-scale\ngeneral-purpose datasets collected from the web.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2022-08-24T01:18:13+00:00",
    "updated": "2022-08-24T01:18:13+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.13556v1": {
    "id": "http://arxiv.org/abs/2404.13556v1",
    "title": "ChatRetriever: Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval",
    "authors": [
      "Kelong Mao",
      "Chenlong Deng",
      "Haonan Chen",
      "Fengran Mo",
      "Zheng Liu",
      "Tetsuya Sakai",
      "Zhicheng Dou"
    ],
    "abstract": "Conversational search requires accurate interpretation of user intent from\ncomplex multi-turn contexts. This paper presents ChatRetriever, which inherits\nthe strong generalization capability of large language models to robustly\nrepresent complex conversational sessions for dense retrieval. To achieve this,\nwe propose a simple and effective dual-learning approach that adapts LLM for\nretrieval via contrastive learning while enhancing the complex session\nunderstanding through masked instruction tuning on high-quality conversational\ninstruction tuning data. Extensive experiments on five conversational search\nbenchmarks demonstrate that ChatRetriever substantially outperforms existing\nconversational dense retrievers, achieving state-of-the-art performance on par\nwith LLM-based rewriting approaches. Furthermore, ChatRetriever exhibits\nsuperior robustness in handling diverse conversational contexts. Our work\nhighlights the potential of adapting LLMs for retrieval with complex inputs\nlike conversational search sessions and proposes an effective approach to\nadvance this research direction.",
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "published": "2024-04-21T07:03:55+00:00",
    "updated": "2024-04-21T07:03:55+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.IR"
  },
  "2111.09734v1": {
    "id": "http://arxiv.org/abs/2111.09734v1",
    "title": "ClipCap: CLIP Prefix for Image Captioning",
    "authors": [
      "Ron Mokady",
      "Amir Hertz",
      "Amit H. Bermano"
    ],
    "abstract": "Image captioning is a fundamental task in vision-language understanding,\nwhere the model predicts a textual informative caption to a given input image.\nIn this paper, we present a simple approach to address this task. We use CLIP\nencoding as a prefix to the caption, by employing a simple mapping network, and\nthen fine-tunes a language model to generate the image captions. The recently\nproposed CLIP model contains rich semantic features which were trained with\ntextual context, making it best for vision-language perception. Our key idea is\nthat together with a pre-trained language model (GPT2), we obtain a wide\nunderstanding of both visual and textual data. Hence, our approach only\nrequires rather quick training to produce a competent captioning model. Without\nadditional annotations or pre-training, it efficiently generates meaningful\ncaptions for large-scale and diverse datasets. Surprisingly, our method works\nwell even when only the mapping network is trained, while both CLIP and the\nlanguage model remain frozen, allowing a lighter architecture with less\ntrainable parameters. Through quantitative evaluation, we demonstrate our model\nachieves comparable results to state-of-the-art methods on the challenging\nConceptual Captions and nocaps datasets, while it is simpler, faster, and\nlighter. Our code is available in\nhttps://github.com/rmokady/CLIP_prefix_caption.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-11-18T14:49:15+00:00",
    "updated": "2021-11-18T14:49:15+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2211.03044v2": {
    "id": "http://arxiv.org/abs/2211.03044v2",
    "title": "Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning",
    "authors": [
      "Yu Meng",
      "Martin Michalski",
      "Jiaxin Huang",
      "Yu Zhang",
      "Tarek Abdelzaher",
      "Jiawei Han"
    ],
    "abstract": "Recent studies have revealed the intriguing few-shot learning ability of\npretrained language models (PLMs): They can quickly adapt to a new task when\nfine-tuned on a small amount of labeled data formulated as prompts, without\nrequiring abundant task-specific annotations. Despite their promising\nperformance, most existing few-shot approaches that only learn from the small\ntraining set still underperform fully supervised training by nontrivial\nmargins. In this work, we study few-shot learning with PLMs from a different\nperspective: We first tune an autoregressive PLM on the few-shot samples and\nthen use it as a generator to synthesize a large amount of novel training\nsamples which augment the original training set. To encourage the generator to\nproduce label-discriminative samples, we train it via weighted maximum\nlikelihood where the weight of each token is automatically adjusted based on a\ndiscriminative meta-learning objective. A classification PLM can then be\nfine-tuned on both the few-shot and the synthetic samples with regularization\nfor better generalization and stability. Our approach FewGen achieves an\noverall better result across seven classification tasks of the GLUE benchmark\nthan existing few-shot learning methods, improving no-augmentation methods by\n5+ average points, and outperforming augmentation methods by 3+ average points.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2022-11-06T06:46:47+00:00",
    "updated": "2023-05-12T06:06:13+00:00",
    "doi": null,
    "comment": "ICML 2023. (Code: https://github.com/yumeng5/FewGen)",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2304.06706v3": {
    "id": "http://arxiv.org/abs/2304.06706v3",
    "title": "Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields",
    "authors": [
      "Jonathan T. Barron",
      "Ben Mildenhall",
      "Dor Verbin",
      "Pratul P. Srinivasan",
      "Peter Hedman"
    ],
    "abstract": "Neural Radiance Field training can be accelerated through the use of\ngrid-based representations in NeRF's learned mapping from spatial coordinates\nto colors and volumetric density. However, these grid-based approaches lack an\nexplicit understanding of scale and therefore often introduce aliasing, usually\nin the form of jaggies or missing scene content. Anti-aliasing has previously\nbeen addressed by mip-NeRF 360, which reasons about sub-volumes along a cone\nrather than points along a ray, but this approach is not natively compatible\nwith current grid-based techniques. We show how ideas from rendering and signal\nprocessing can be used to construct a technique that combines mip-NeRF 360 and\ngrid-based models such as Instant NGP to yield error rates that are 8% - 77%\nlower than either prior technique, and that trains 24x faster than mip-NeRF\n360.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2023-04-13T17:55:12+00:00",
    "updated": "2023-10-26T22:19:56+00:00",
    "doi": null,
    "comment": "Project page: https://jonbarron.info/zipnerf/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2009.13586v6": {
    "id": "http://arxiv.org/abs/2009.13586v6",
    "title": "Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization",
    "authors": [
      "Xuezhe Ma"
    ],
    "abstract": "In this paper, we introduce Apollo, a quasi-Newton method for nonconvex\nstochastic optimization, which dynamically incorporates the curvature of the\nloss function by approximating the Hessian via a diagonal matrix. Importantly,\nthe update and storage of the diagonal approximation of Hessian is as efficient\nas adaptive first-order optimization methods with linear complexity for both\ntime and memory. To handle nonconvexity, we replace the Hessian with its\nrectified absolute value, which is guaranteed to be positive-definite.\nExperiments on three tasks of vision and language show that Apollo achieves\nsignificant improvements over other stochastic optimization methods, including\nSGD and variants of Adam, in term of both convergence speed and generalization\nperformance. The implementation of the algorithm is available at\nhttps://github.com/XuezheMax/apollo.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2020-09-28T19:07:02+00:00",
    "updated": "2021-08-20T05:31:09+00:00",
    "doi": null,
    "comment": "Fixed errors in convergence analysis. 29 pages (plus appendix), 6\n  figures, 7 tables",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2312.15166v3": {
    "id": "http://arxiv.org/abs/2312.15166v3",
    "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling",
    "authors": [
      "Dahyun Kim",
      "Chanjun Park",
      "Sanghoon Kim",
      "Wonsung Lee",
      "Wonho Song",
      "Yunsu Kim",
      "Hyeonwoo Kim",
      "Yungi Kim",
      "Hyeonju Lee",
      "Jihoo Kim",
      "Changbae Ahn",
      "Seonghoon Yang",
      "Sukyung Lee",
      "Hyunbyung Park",
      "Gyoungjin Gim",
      "Mikyoung Cha",
      "Hwalsuk Lee",
      "Sunghun Kim"
    ],
    "abstract": "We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion\nparameters, demonstrating superior performance in various natural language\nprocessing (NLP) tasks. Inspired by recent efforts to efficiently up-scale\nLLMs, we present a method for scaling LLMs called depth up-scaling (DUS), which\nencompasses depthwise scaling and continued pretraining. In contrast to other\nLLM up-scaling methods that use mixture-of-experts, DUS does not require\ncomplex changes to train and inference efficiently. We show experimentally that\nDUS is simple yet effective in scaling up high-performance LLMs from small\nones. Building on the DUS model, we additionally present SOLAR 10.7B-Instruct,\na variant fine-tuned for instruction-following capabilities, surpassing\nMixtral-8x7B-Instruct. SOLAR 10.7B is publicly available under the Apache 2.0\nlicense, promoting broad access and application in the LLM field.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-12-23T05:11:37+00:00",
    "updated": "2024-04-04T01:53:38+00:00",
    "doi": null,
    "comment": "accepted to NAACL 2024 Industry Track",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2206.09178v1": {
    "id": "http://arxiv.org/abs/2206.09178v1",
    "title": "REVECA -- Rich Encoder-decoder framework for Video Event CAptioner",
    "authors": [
      "Jaehyuk Heo",
      "YongGi Jeong",
      "Sunwoo Kim",
      "Jaehee Kim",
      "Pilsung Kang"
    ],
    "abstract": "We describe an approach used in the Generic Boundary Event Captioning\nchallenge at the Long-Form Video Understanding Workshop held at CVPR 2022. We\ndesigned a Rich Encoder-decoder framework for Video Event CAptioner (REVECA)\nthat utilizes spatial and temporal information from the video to generate a\ncaption for the corresponding the event boundary. REVECA uses frame position\nembedding to incorporate information before and after the event boundary.\nFurthermore, it employs features extracted using the temporal segment network\nand temporal-based pairwise difference method to learn temporal information. A\nsemantic segmentation mask for the attentional pooling process is adopted to\nlearn the subject of an event. Finally, LoRA is applied to fine-tune the image\nencoder to enhance the learning efficiency. REVECA yielded an average score of\n50.97 on the Kinetics-GEBC test data, which is an improvement of 10.17 over the\nbaseline method. Our code is available in https://github.com/TooTouch/REVECA.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2022-06-18T11:10:12+00:00",
    "updated": "2022-06-18T11:10:12+00:00",
    "doi": null,
    "comment": "The IEEE/CVF Computer Vision and Pattern Recognition Conference\n  (CVPR). LOng-form VidEo Understanding (LOVEU) workshop",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.01300v3": {
    "id": "http://arxiv.org/abs/2404.01300v3",
    "title": "NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation Learning for Neural Radiance Fields",
    "authors": [
      "Muhammad Zubair Irshad",
      "Sergey Zakharov",
      "Vitor Guizilini",
      "Adrien Gaidon",
      "Zsolt Kira",
      "Rares Ambrus"
    ],
    "abstract": "Neural fields excel in computer vision and robotics due to their ability to\nunderstand the 3D visual world such as inferring semantics, geometry, and\ndynamics. Given the capabilities of neural fields in densely representing a 3D\nscene from 2D images, we ask the question: Can we scale their self-supervised\npretraining, specifically using masked autoencoders, to generate effective 3D\nrepresentations from posed RGB images. Owing to the astounding success of\nextending transformers to novel data modalities, we employ standard 3D Vision\nTransformers to suit the unique formulation of NeRFs. We leverage NeRF's\nvolumetric grid as a dense input to the transformer, contrasting it with other\n3D representations such as pointclouds where the information density can be\nuneven, and the representation is irregular. Due to the difficulty of applying\nmasked autoencoders to an implicit representation, such as NeRF, we opt for\nextracting an explicit representation that canonicalizes scenes across domains\nby employing the camera trajectory for sampling. Our goal is made possible by\nmasking random patches from NeRF's radiance and density grid and employing a\nstandard 3D Swin Transformer to reconstruct the masked patches. In doing so,\nthe model can learn the semantic and spatial structure of complete scenes. We\npretrain this representation at scale on our proposed curated posed-RGB data,\ntotaling over 1.8 million images. Once pretrained, the encoder is used for\neffective 3D transfer learning. Our novel self-supervised pretraining for\nNeRFs, NeRF-MAE, scales remarkably well and improves performance on various\nchallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,\nNeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF\nscene understanding baselines on Front3D and ScanNet datasets with an absolute\nperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-04-01T17:59:55+00:00",
    "updated": "2024-07-18T17:59:48+00:00",
    "doi": null,
    "comment": "Accepted to ECCV 2024. Project Page: https://nerf-mae.github.io/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2104.14786v1": {
    "id": "http://arxiv.org/abs/2104.14786v1",
    "title": "Editable Free-viewpoint Video Using a Layered Neural Representation",
    "authors": [
      "Jiakai Zhang",
      "Xinhang Liu",
      "Xinyi Ye",
      "Fuqiang Zhao",
      "Yanshun Zhang",
      "Minye Wu",
      "Yingliang Zhang",
      "Lan Xu",
      "Jingyi Yu"
    ],
    "abstract": "Generating free-viewpoint videos is critical for immersive VR/AR experience\nbut recent neural advances still lack the editing ability to manipulate the\nvisual perception for large dynamic scenes. To fill this gap, in this paper we\npropose the first approach for editable photo-realistic free-viewpoint video\ngeneration for large-scale dynamic scenes using only sparse 16 cameras. The\ncore of our approach is a new layered neural representation, where each dynamic\nentity including the environment itself is formulated into a space-time\ncoherent neural layered radiance representation called ST-NeRF. Such layered\nrepresentation supports fully perception and realistic manipulation of the\ndynamic scene whilst still supporting a free viewing experience in a wide\nrange. In our ST-NeRF, the dynamic entity/layer is represented as continuous\nfunctions, which achieves the disentanglement of location, deformation as well\nas the appearance of the dynamic entity in a continuous and self-supervised\nmanner. We propose a scene parsing 4D label map tracking to disentangle the\nspatial information explicitly, and a continuous deform module to disentangle\nthe temporal motion implicitly. An object-aware volume rendering scheme is\nfurther introduced for the re-assembling of all the neural layers. We adopt a\nnovel layered loss and motion-aware ray sampling strategy to enable efficient\ntraining for a large dynamic scene with multiple performers, Our framework\nfurther enables a variety of editing functions, i.e., manipulating the scale\nand location, duplicating or retiming individual neural layers to create\nnumerous visual effects while preserving high realism. Extensive experiments\ndemonstrate the effectiveness of our approach to achieve high-quality,\nphoto-realistic, and editable free-viewpoint video generation for dynamic\nscenes.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2021-04-30T06:50:45+00:00",
    "updated": "2021-04-30T06:50:45+00:00",
    "doi": "10.1145/3450626.3459756",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2009.08044v3": {
    "id": "http://arxiv.org/abs/2009.08044v3",
    "title": "Large-Scale Intelligent Microservices",
    "authors": [
      "Mark Hamilton",
      "Nick Gonsalves",
      "Christina Lee",
      "Anand Raman",
      "Brendan Walsh",
      "Siddhartha Prasad",
      "Dalitso Banda",
      "Lucy Zhang",
      "Mei Gao",
      "Lei Zhang",
      "William T. Freeman"
    ],
    "abstract": "Deploying Machine Learning (ML) algorithms within databases is a challenge\ndue to the varied computational footprints of modern ML algorithms and the\nmyriad of database technologies each with its own restrictive syntax. We\nintroduce an Apache Spark-based micro-service orchestration framework that\nextends database operations to include web service primitives. Our system can\norchestrate web services across hundreds of machines and takes full advantage\nof cluster, thread, and asynchronous parallelism. Using this framework, we\nprovide large scale clients for intelligent services such as speech, vision,\nsearch, anomaly detection, and text analysis. This allows users to integrate\nready-to-use intelligence into any datastore with an Apache Spark connector. To\neliminate the majority of overhead from network communication, we also\nintroduce a low-latency containerized version of our architecture. Finally, we\ndemonstrate that the services we investigate are competitive on a variety of\nbenchmarks, and present two applications of this framework to create\nintelligent search engines, and real-time auto race analytics systems.",
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.DC",
      "cs.LG",
      "cs.NI"
    ],
    "published": "2020-09-17T03:38:28+00:00",
    "updated": "2021-12-02T20:09:30+00:00",
    "doi": "10.1109/BigData50022.2020.9378270",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "1605.07146v4": {
    "id": "http://arxiv.org/abs/1605.07146v4",
    "title": "Wide Residual Networks",
    "authors": [
      "Sergey Zagoruyko",
      "Nikos Komodakis"
    ],
    "abstract": "Deep residual networks were shown to be able to scale up to thousands of\nlayers and still have improving performance. However, each fraction of a\npercent of improved accuracy costs nearly doubling the number of layers, and so\ntraining very deep residual networks has a problem of diminishing feature\nreuse, which makes these networks very slow to train. To tackle these problems,\nin this paper we conduct a detailed experimental study on the architecture of\nResNet blocks, based on which we propose a novel architecture where we decrease\ndepth and increase width of residual networks. We call the resulting network\nstructures wide residual networks (WRNs) and show that these are far superior\nover their commonly used thin and very deep counterparts. For example, we\ndemonstrate that even a simple 16-layer-deep wide residual network outperforms\nin accuracy and efficiency all previous deep residual networks, including\nthousand-layer-deep networks, achieving new state-of-the-art results on CIFAR,\nSVHN, COCO, and significant improvements on ImageNet. Our code and models are\navailable at https://github.com/szagoruyko/wide-residual-networks",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2016-05-23T19:27:13+00:00",
    "updated": "2017-06-14T06:06:48+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2311.12202v1": {
    "id": "http://arxiv.org/abs/2311.12202v1",
    "title": "Nepotistically Trained Generative-AI Models Collapse",
    "authors": [
      "Matyas Bohacek",
      "Hany Farid"
    ],
    "abstract": "Trained on massive amounts of human-generated content, AI (artificial\nintelligence) image synthesis is capable of reproducing semantically coherent\nimages that match the visual appearance of its training data. We show that when\nretrained on even small amounts of their own creation, these generative-AI\nmodels produce highly distorted images. We also show that this distortion\nextends beyond the text prompts used in retraining, and that once poisoned, the\nmodels struggle to fully heal even after retraining on only real images.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2023-11-20T21:43:32+00:00",
    "updated": "2023-11-20T21:43:32+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2203.16634v2": {
    "id": "http://arxiv.org/abs/2203.16634v2",
    "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information",
    "authors": [
      "Adi Haviv",
      "Ori Ram",
      "Ofir Press",
      "Peter Izsak",
      "Omer Levy"
    ],
    "abstract": "Causal transformer language models (LMs), such as GPT-3, typically require\nsome form of positional encoding, such as positional embeddings. However, we\nshow that LMs without any explicit positional encoding are still competitive\nwith standard models, and that this phenomenon is robust across different\ndatasets, model sizes, and sequence lengths. Probing experiments reveal that\nsuch models acquire an implicit notion of absolute positions throughout the\nnetwork, effectively compensating for the missing information. We conjecture\nthat causal attention enables the model to infer the number of predecessors\nthat each token can attend to, thereby approximating its absolute position. Our\nfindings indicate that causal LMs might derive positional awareness not only\nfrom the explicit positioning mechanism, but also from the effects of the\ncausal mask.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2022-03-30T19:37:07+00:00",
    "updated": "2022-12-05T22:10:52+00:00",
    "doi": null,
    "comment": "Findings of EMNLP 2022",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2012.14740v4": {
    "id": "http://arxiv.org/abs/2012.14740v4",
    "title": "LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding",
    "authors": [
      "Yang Xu",
      "Yiheng Xu",
      "Tengchao Lv",
      "Lei Cui",
      "Furu Wei",
      "Guoxin Wang",
      "Yijuan Lu",
      "Dinei Florencio",
      "Cha Zhang",
      "Wanxiang Che",
      "Min Zhang",
      "Lidong Zhou"
    ],
    "abstract": "Pre-training of text and layout has proved effective in a variety of\nvisually-rich document understanding tasks due to its effective model\narchitecture and the advantage of large-scale unlabeled scanned/digital-born\ndocuments. We propose LayoutLMv2 architecture with new pre-training tasks to\nmodel the interaction among text, layout, and image in a single multi-modal\nframework. Specifically, with a two-stream multi-modal Transformer encoder,\nLayoutLMv2 uses not only the existing masked visual-language modeling task but\nalso the new text-image alignment and text-image matching tasks, which make it\nbetter capture the cross-modality interaction in the pre-training stage.\nMeanwhile, it also integrates a spatial-aware self-attention mechanism into the\nTransformer architecture so that the model can fully understand the relative\npositional relationship among different text blocks. Experiment results show\nthat LayoutLMv2 outperforms LayoutLM by a large margin and achieves new\nstate-of-the-art results on a wide variety of downstream visually-rich document\nunderstanding tasks, including FUNSD (0.7895 $\\to$ 0.8420), CORD (0.9493 $\\to$\n0.9601), SROIE (0.9524 $\\to$ 0.9781), Kleister-NDA (0.8340 $\\to$ 0.8520),\nRVL-CDIP (0.9443 $\\to$ 0.9564), and DocVQA (0.7295 $\\to$ 0.8672). We made our\nmodel and code publicly available at \\url{https://aka.ms/layoutlmv2}.",
    "categories": [
      "cs.CL"
    ],
    "published": "2020-12-29T13:01:52+00:00",
    "updated": "2022-01-10T04:08:10+00:00",
    "doi": null,
    "comment": "ACL 2021 main conference",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2305.05432v1": {
    "id": "http://arxiv.org/abs/2305.05432v1",
    "title": "WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset",
    "authors": [
      "Andrea Burns",
      "Krishna Srinivasan",
      "Joshua Ainslie",
      "Geoff Brown",
      "Bryan A. Plummer",
      "Kate Saenko",
      "Jianmo Ni",
      "Mandy Guo"
    ],
    "abstract": "Webpages have been a rich resource for language and vision-language tasks.\nYet only pieces of webpages are kept: image-caption pairs, long text articles,\nor raw HTML, never all in one place. Webpage tasks have resultingly received\nlittle attention and structured image-text data underused. To study multimodal\nwebpage understanding, we introduce the Wikipedia Webpage 2M (WikiWeb2M) suite;\nthe first to retain the full set of images, text, and structure data available\nin a page. WikiWeb2M can be used for tasks like page description generation,\nsection summarization, and contextual image captioning.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "published": "2023-05-09T13:20:59+00:00",
    "updated": "2023-05-09T13:20:59+00:00",
    "doi": null,
    "comment": "Accepted at the WikiWorkshop 2023. Data is readily available at\n  https://github.com/google-research-datasets/wit/blob/main/wikiweb2m.md. arXiv\n  admin note: text overlap with arXiv:2305.03668",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2311.17043v1": {
    "id": "http://arxiv.org/abs/2311.17043v1",
    "title": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
    "authors": [
      "Yanwei Li",
      "Chengyao Wang",
      "Jiaya Jia"
    ],
    "abstract": "In this work, we present a novel method to tackle the token generation\nchallenge in Vision Language Models (VLMs) for video and image understanding,\ncalled LLaMA-VID. Current VLMs, while proficient in tasks like image captioning\nand visual question answering, face computational burdens when processing long\nvideos due to the excessive visual tokens. LLaMA-VID addresses this issue by\nrepresenting each frame with two distinct tokens, namely context token and\ncontent token. The context token encodes the overall image context based on\nuser input, whereas the content token encapsulates visual cues in each frame.\nThis dual-token strategy significantly reduces the overload of long videos\nwhile preserving critical information. Generally, LLaMA-VID empowers existing\nframeworks to support hour-long videos and pushes their upper limit with an\nextra context token. It is proved to surpass previous methods on most of video-\nor image-based benchmarks. Code is available\nhttps://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "published": "2023-11-28T18:53:43+00:00",
    "updated": "2023-11-28T18:53:43+00:00",
    "doi": null,
    "comment": "Code is available at https://github.com/dvlab-research/LLaMA-VID",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2406.09417v1": {
    "id": "http://arxiv.org/abs/2406.09417v1",
    "title": "Rethinking Score Distillation as a Bridge Between Image Distributions",
    "authors": [
      "David McAllister",
      "Songwei Ge",
      "Jia-Bin Huang",
      "David W. Jacobs",
      "Alexei A. Efros",
      "Aleksander Holynski",
      "Angjoo Kanazawa"
    ],
    "abstract": "Score distillation sampling (SDS) has proven to be an important tool,\nenabling the use of large-scale diffusion priors for tasks operating in\ndata-poor domains. Unfortunately, SDS has a number of characteristic artifacts\nthat limit its usefulness in general-purpose applications. In this paper, we\nmake progress toward understanding the behavior of SDS and its variants by\nviewing them as solving an optimal-cost transport path from a source\ndistribution to a target distribution. Under this new interpretation, these\nmethods seek to transport corrupted images (source) to the natural image\ndistribution (target). We argue that current methods' characteristic artifacts\nare caused by (1) linear approximation of the optimal path and (2) poor\nestimates of the source distribution. We show that calibrating the text\nconditioning of the source distribution can produce high-quality generation and\ntranslation results with little extra overhead. Our method can be easily\napplied across many domains, matching or beating the performance of specialized\nmethods. We demonstrate its utility in text-to-2D, text-based NeRF\noptimization, translating paintings to real images, optical illusion\ngeneration, and 3D sketch-to-real. We compare our method to existing approaches\nfor score distillation sampling and show that it can produce high-frequency\ndetails with realistic colors.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2024-06-13T17:59:58+00:00",
    "updated": "2024-06-13T17:59:58+00:00",
    "doi": null,
    "comment": "Project webpage: https://sds-bridge.github.io/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.01241v3": {
    "id": "http://arxiv.org/abs/2210.01241v3",
    "title": "Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization",
    "authors": [
      "Rajkumar Ramamurthy",
      "Prithviraj Ammanabrolu",
      "Kiant\u00e9 Brantley",
      "Jack Hessel",
      "Rafet Sifa",
      "Christian Bauckhage",
      "Hannaneh Hajishirzi",
      "Yejin Choi"
    ],
    "abstract": "We tackle the problem of aligning pre-trained large language models (LMs)\nwith human preferences. If we view text generation as a sequential\ndecision-making problem, reinforcement learning (RL) appears to be a natural\nconceptual framework. However, using RL for LM-based generation faces empirical\nchallenges, including training instability due to the combinatorial action\nspace, as well as a lack of open-source libraries and benchmarks customized for\nLM alignment. Thus, a question rises in the research community: is RL a\npractical paradigm for NLP?\n  To help answer this, we first introduce an open-source modular library,\nRL4LMs (Reinforcement Learning for Language Models), for optimizing language\ngenerators with RL. The library consists of on-policy RL algorithms that can be\nused to train any encoder or encoder-decoder LM in the HuggingFace library\n(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE\n(General Reinforced-language Understanding Evaluation) benchmark, a set of 6\nlanguage generation tasks which are supervised not by target strings, but by\nreward functions which capture automated measures of human preference. GRUE is\nthe first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,\nwe introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language\nPolicy Optimization) that learns to effectively reduce the combinatorial action\nspace in language generation. We show 1) that RL techniques are generally\nbetter than supervised methods at aligning LMs to human preferences; and 2)\nthat NLPO exhibits greater stability and performance than previous policy\ngradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic\nand human evaluations.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2022-10-03T21:38:29+00:00",
    "updated": "2023-03-01T01:31:17+00:00",
    "doi": null,
    "comment": "In Proceedings of ICLR 2023. Code found at\n  https://github.com/allenai/rl4lms and Project website at\n  https://rl4lms.apps.allenai.org/",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2211.06828v3": {
    "id": "http://arxiv.org/abs/2211.06828v3",
    "title": "Enhancing Few-shot Image Classification with Cosine Transformer",
    "authors": [
      "Quang-Huy Nguyen",
      "Cuong Q. Nguyen",
      "Dung D. Le",
      "Hieu H. Pham"
    ],
    "abstract": "This paper addresses the few-shot image classification problem, where the\nclassification task is performed on unlabeled query samples given a small\namount of labeled support samples only. One major challenge of the few-shot\nlearning problem is the large variety of object visual appearances that\nprevents the support samples to represent that object comprehensively. This\nmight result in a significant difference between support and query samples,\ntherefore undermining the performance of few-shot algorithms. In this paper, we\ntackle the problem by proposing Few-shot Cosine Transformer (FS-CT), where the\nrelational map between supports and queries is effectively obtained for the\nfew-shot tasks. The FS-CT consists of two parts, a learnable prototypical\nembedding network to obtain categorical representations from support samples\nwith hard cases, and a transformer encoder to effectively achieve the\nrelational map from two different support and query samples. We introduce\nCosine Attention, a more robust and stable attention module that enhances the\ntransformer module significantly and therefore improves FS-CT performance from\n5% to over 20% in accuracy compared to the default scaled dot-product\nmechanism. Our method performs competitive results in mini-ImageNet, CUB-200,\nand CIFAR-FS on 1-shot learning and 5-shot learning tasks across backbones and\nfew-shot configurations. We also developed a custom few-shot dataset for Yoga\npose recognition to demonstrate the potential of our algorithm for practical\napplication. Our FS-CT with cosine attention is a lightweight, simple few-shot\nalgorithm that can be applied for a wide range of applications, such as\nhealthcare, medical, and security surveillance. The official implementation\ncode of our Few-shot Cosine Transformer is available at\nhttps://github.com/vinuni-vishc/Few-Shot-Cosine-Transformer",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-11-13T06:03:28+00:00",
    "updated": "2023-07-21T16:54:18+00:00",
    "doi": "10.1109/ACCESS.2023.3298299",
    "comment": null,
    "journal_ref": "IEEE Access (2023)",
    "primary_category": "cs.CV"
  },
  "2310.11142v2": {
    "id": "http://arxiv.org/abs/2310.11142v2",
    "title": "BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference",
    "authors": [
      "Siqi Kou",
      "Lei Gan",
      "Dequan Wang",
      "Chongxuan Li",
      "Zhijie Deng"
    ],
    "abstract": "Diffusion models have impressive image generation capability, but low-quality\ngenerations still exist, and their identification remains challenging due to\nthe lack of a proper sample-wise metric. To address this, we propose BayesDiff,\na pixel-wise uncertainty estimator for generations from diffusion models based\non Bayesian inference. In particular, we derive a novel uncertainty iteration\nprinciple to characterize the uncertainty dynamics in diffusion, and leverage\nthe last-layer Laplace approximation for efficient Bayesian inference. The\nestimated pixel-wise uncertainty can not only be aggregated into a sample-wise\nmetric to filter out low-fidelity images but also aids in augmenting successful\ngenerations and rectifying artifacts in failed generations in text-to-image\ntasks. Extensive experiments demonstrate the efficacy of BayesDiff and its\npromise for practical applications.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-10-17T10:45:28+00:00",
    "updated": "2024-03-04T09:07:44+00:00",
    "doi": null,
    "comment": "ICLR 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.11419v1": {
    "id": "http://arxiv.org/abs/2404.11419v1",
    "title": "SLAIM: Robust Dense Neural SLAM for Online Tracking and Mapping",
    "authors": [
      "Vincent Cartillier",
      "Grant Schindler",
      "Irfan Essa"
    ],
    "abstract": "We present SLAIM - Simultaneous Localization and Implicit Mapping. We propose\na novel coarse-to-fine tracking model tailored for Neural Radiance Field SLAM\n(NeRF-SLAM) to achieve state-of-the-art tracking performance. Notably, existing\nNeRF-SLAM systems consistently exhibit inferior tracking performance compared\nto traditional SLAM algorithms. NeRF-SLAM methods solve camera tracking via\nimage alignment and photometric bundle-adjustment. Such optimization processes\nare difficult to optimize due to the narrow basin of attraction of the\noptimization loss in image space (local minima) and the lack of initial\ncorrespondences. We mitigate these limitations by implementing a Gaussian\npyramid filter on top of NeRF, facilitating a coarse-to-fine tracking\noptimization strategy. Furthermore, NeRF systems encounter challenges in\nconverging to the right geometry with limited input views. While prior\napproaches use a Signed-Distance Function (SDF)-based NeRF and directly\nsupervise SDF values by approximating ground truth SDF through depth\nmeasurements, this often results in suboptimal geometry. In contrast, our\nmethod employs a volume density representation and introduces a novel KL\nregularizer on the ray termination distribution, constraining scene geometry to\nconsist of empty space and opaque surfaces. Our solution implements both local\nand global bundle-adjustment to produce a robust (coarse-to-fine) and accurate\n(KL regularizer) SLAM solution. We conduct experiments on multiple datasets\n(ScanNet, TUM, Replica) showing state-of-the-art results in tracking and in\nreconstruction accuracy.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-17T14:23:28+00:00",
    "updated": "2024-04-17T14:23:28+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2211.05654v1": {
    "id": "http://arxiv.org/abs/2211.05654v1",
    "title": "Efficient Joint Detection and Multiple Object Tracking with Spatially Aware Transformer",
    "authors": [
      "Siddharth Sagar Nijhawan",
      "Leo Hoshikawa",
      "Atsushi Irie",
      "Masakazu Yoshimura",
      "Junji Otsuka",
      "Takeshi Ohashi"
    ],
    "abstract": "We propose a light-weight and highly efficient Joint Detection and Tracking\npipeline for the task of Multi-Object Tracking using a fully-transformer\narchitecture. It is a modified version of TransTrack, which overcomes the\ncomputational bottleneck associated with its design, and at the same time,\nachieves state-of-the-art MOTA score of 73.20%. The model design is driven by a\ntransformer based backbone instead of CNN, which is highly scalable with the\ninput resolution. We also propose a drop-in replacement for Feed Forward\nNetwork of transformer encoder layer, by using Butterfly Transform Operation to\nperform channel fusion and depth-wise convolution to learn spatial context\nwithin the feature maps, otherwise missing within the attention maps of the\ntransformer. As a result of our modifications, we reduce the overall model size\nof TransTrack by 58.73% and the complexity by 78.72%. Therefore, we expect our\ndesign to provide novel perspectives for architecture optimization in future\nresearch related to multi-object tracking.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-11-09T07:19:33+00:00",
    "updated": "2022-11-09T07:19:33+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2402.03686v3": {
    "id": "http://arxiv.org/abs/2402.03686v3",
    "title": "Are Machines Better at Complex Reasoning? Unveiling Human-Machine Inference Gaps in Entailment Verification",
    "authors": [
      "Soumya Sanyal",
      "Tianyi Xiao",
      "Jiacheng Liu",
      "Wenya Wang",
      "Xiang Ren"
    ],
    "abstract": "Making inferences in text comprehension to understand the meaning is\nessential in language processing. This work studies the entailment verification\n(EV) problem of multi-sentence premises that requires a system to make multiple\ninferences implicitly. Studying EV for such complex premises is important\nbecause modern NLP problems, such as detecting inconsistent model-generated\nrationales, require complex multi-hop reasoning. However, current textual\ninference datasets mostly contain short premises that only partially focus on\nthese challenges. To address this, we compile an EV benchmark that includes\ndatasets from three NLP domains (NLI, contextual QA, and rationales) containing\nmulti-sentence premises. On benchmarking humans and LLMs, we find that LLMs are\nbetter than humans in multi-hop reasoning across extended contexts, while\nhumans perform better in simple deductive reasoning tasks. We also finetune a\nFlan-T5 model for EV using two training objectives to obtain a strong\nopen-source model that outperforms GPT-3.5 and rivals GPT-4. Finally, we use\nthis model to filter out inconsistent model-generated rationales in\nself-consistency decoding, resulting in a 6% accuracy improvement on average\nacross three MCQ datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-06T04:14:09+00:00",
    "updated": "2024-05-27T18:44:14+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2111.11432v1": {
    "id": "http://arxiv.org/abs/2111.11432v1",
    "title": "Florence: A New Foundation Model for Computer Vision",
    "authors": [
      "Lu Yuan",
      "Dongdong Chen",
      "Yi-Ling Chen",
      "Noel Codella",
      "Xiyang Dai",
      "Jianfeng Gao",
      "Houdong Hu",
      "Xuedong Huang",
      "Boxin Li",
      "Chunyuan Li",
      "Ce Liu",
      "Mengchen Liu",
      "Zicheng Liu",
      "Yumao Lu",
      "Yu Shi",
      "Lijuan Wang",
      "Jianfeng Wang",
      "Bin Xiao",
      "Zhen Xiao",
      "Jianwei Yang",
      "Michael Zeng",
      "Luowei Zhou",
      "Pengchuan Zhang"
    ],
    "abstract": "Automated visual understanding of our diverse and open world demands computer\nvision models to generalize well with minimal customization for specific tasks,\nsimilar to human vision. Computer vision foundation models, which are trained\non diverse, large-scale dataset and can be adapted to a wide range of\ndownstream tasks, are critical for this mission to solve real-world computer\nvision applications. While existing vision foundation models such as CLIP,\nALIGN, and Wu Dao 2.0 focus mainly on mapping images and textual\nrepresentations to a cross-modal shared representation, we introduce a new\ncomputer vision foundation model, Florence, to expand the representations from\ncoarse (scene) to fine (object), from static (images) to dynamic (videos), and\nfrom RGB to multiple modalities (caption, depth). By incorporating universal\nvisual-language representations from Web-scale image-text data, our Florence\nmodel can be easily adapted for various computer vision tasks, such as\nclassification, retrieval, object detection, VQA, image caption, video\nretrieval and action recognition. Moreover, Florence demonstrates outstanding\nperformance in many types of transfer learning: fully sampled fine-tuning,\nlinear probing, few-shot transfer and zero-shot transfer for novel images and\nobjects. All of these properties are critical for our vision foundation model\nto serve general purpose vision tasks. Florence achieves new state-of-the-art\nresults in majority of 44 representative benchmarks, e.g., ImageNet-1K\nzero-shot classification with top-1 accuracy of 83.74 and the top-5 accuracy of\n97.18, 62.4 mAP on COCO fine tuning, 80.36 on VQA, and 87.8 on Kinetics-600.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2021-11-22T18:59:55+00:00",
    "updated": "2021-11-22T18:59:55+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.09977v1": {
    "id": "http://arxiv.org/abs/2404.09977v1",
    "title": "MaxFusion: Plug&Play Multi-Modal Generation in Text-to-Image Diffusion Models",
    "authors": [
      "Nithin Gopalakrishnan Nair",
      "Jeya Maria Jose Valanarasu",
      "Vishal M Patel"
    ],
    "abstract": "Large diffusion-based Text-to-Image (T2I) models have shown impressive\ngenerative powers for text-to-image generation as well as spatially conditioned\nimage generation. For most applications, we can train the model end-toend with\npaired data to obtain photorealistic generation quality. However, to add an\nadditional task, one often needs to retrain the model from scratch using paired\ndata across all modalities to retain good generation performance. In this\npaper, we tackle this issue and propose a novel strategy to scale a generative\nmodel across new tasks with minimal compute. During our experiments, we\ndiscovered that the variance maps of intermediate feature maps of diffusion\nmodels capture the intensity of conditioning. Utilizing this prior information,\nwe propose MaxFusion, an efficient strategy to scale up text-to-image\ngeneration models to accommodate new modality conditions. Specifically, we\ncombine aligned features of multiple models, hence bringing a compositional\neffect. Our fusion strategy can be integrated into off-the-shelf models to\nenhance their generative prowess.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-15T17:55:56+00:00",
    "updated": "2024-04-15T17:55:56+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.00208v1": {
    "id": "http://arxiv.org/abs/2401.00208v1",
    "title": "Inpaint4DNeRF: Promptable Spatio-Temporal NeRF Inpainting with Generative Diffusion Models",
    "authors": [
      "Han Jiang",
      "Haosen Sun",
      "Ruoxuan Li",
      "Chi-Keung Tang",
      "Yu-Wing Tai"
    ],
    "abstract": "Current Neural Radiance Fields (NeRF) can generate photorealistic novel\nviews. For editing 3D scenes represented by NeRF, with the advent of generative\nmodels, this paper proposes Inpaint4DNeRF to capitalize on state-of-the-art\nstable diffusion models (e.g., ControlNet) for direct generation of the\nunderlying completed background content, regardless of static or dynamic. The\nkey advantages of this generative approach for NeRF inpainting are twofold.\nFirst, after rough mask propagation, to complete or fill in previously occluded\ncontent, we can individually generate a small subset of completed images with\nplausible content, called seed images, from which simple 3D geometry proxies\ncan be derived. Second and the remaining problem is thus 3D multiview\nconsistency among all completed images, now guided by the seed images and their\n3D proxies. Without other bells and whistles, our generative Inpaint4DNeRF\nbaseline framework is general which can be readily extended to 4D dynamic\nNeRFs, where temporal consistency can be naturally handled in a similar way as\nour multiview consistency.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-30T11:26:55+00:00",
    "updated": "2023-12-30T11:26:55+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1811.05042v2": {
    "id": "http://arxiv.org/abs/1811.05042v2",
    "title": "Exploiting Local Feature Patterns for Unsupervised Domain Adaptation",
    "authors": [
      "Jun Wen",
      "Risheng Liu",
      "Nenggan Zheng",
      "Qian Zheng",
      "Zhefeng Gong",
      "Junsong Yuan"
    ],
    "abstract": "Unsupervised domain adaptation methods aim to alleviate performance\ndegradation caused by domain-shift by learning domain-invariant\nrepresentations. Existing deep domain adaptation methods focus on holistic\nfeature alignment by matching source and target holistic feature distributions,\nwithout considering local features and their multi-mode statistics. We show\nthat the learned local feature patterns are more generic and transferable and a\nfurther local feature distribution matching enables fine-grained feature\nalignment. In this paper, we present a method for learning domain-invariant\nlocal feature patterns and jointly aligning holistic and local feature\nstatistics. Comparisons to the state-of-the-art unsupervised domain adaptation\nmethods on two popular benchmark datasets demonstrate the superiority of our\napproach and its effectiveness on alleviating negative transfer.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2018-11-12T23:23:23+00:00",
    "updated": "2018-11-18T20:27:16+00:00",
    "doi": null,
    "comment": "AAAI-2019 accepted",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2011.07435v6": {
    "id": "http://arxiv.org/abs/2011.07435v6",
    "title": "Functorial Manifold Learning",
    "authors": [
      "Dan Shiebler"
    ],
    "abstract": "We adapt previous research on category theory and topological unsupervised\nlearning to develop a functorial perspective on manifold learning, also known\nas nonlinear dimensionality reduction. We first characterize manifold learning\nalgorithms as functors that map pseudometric spaces to optimization objectives\nand that factor through hierarchical clustering functors. We then use this\ncharacterization to prove refinement bounds on manifold learning loss functions\nand construct a hierarchy of manifold learning algorithms based on their\nequivariants. We express several popular manifold learning algorithms as\nfunctors at different levels of this hierarchy, including Metric\nMultidimensional Scaling, IsoMap, and UMAP. Next, we use interleaving distance\nto study the stability of a broad class of manifold learning algorithms. We\npresent bounds on how closely the embeddings these algorithms produce from\nnoisy data approximate the embeddings they would learn from noiseless data.\nFinally, we use our framework to derive a set of novel manifold learning\nalgorithms, which we experimentally demonstrate are competitive with the state\nof the art.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2020-11-15T02:30:23+00:00",
    "updated": "2022-11-03T14:15:41+00:00",
    "doi": "10.4204/EPTCS.372.1",
    "comment": "In Proceedings ACT 2021, arXiv:2211.01102",
    "journal_ref": "EPTCS 372, 2022, pp. 1-13",
    "primary_category": "cs.LG"
  },
  "2309.02233v3": {
    "id": "http://arxiv.org/abs/2309.02233v3",
    "title": "Augmenting Black-box LLMs with Medical Textbooks for Biomedical Question Answering (Published in Findings of EMNLP 2024)",
    "authors": [
      "Yubo Wang",
      "Xueguang Ma",
      "Wenhu Chen"
    ],
    "abstract": "Large-scale language models (LLMs) like ChatGPT have demonstrated impressive\nabilities in generating responses based on human instructions. However, their\nuse in the medical field can be challenging due to their lack of specific,\nin-depth knowledge. In this study, we present a system called LLMs Augmented\nwith Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in\nspecialized domains. LLM-AMT integrates authoritative medical textbooks into\nthe LLMs' framework using plug-and-play modules. These modules include a Query\nAugmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together,\nthey incorporate authoritative medical knowledge. Additionally, an LLM Reader\naids in contextual understanding. Our experimental results on three medical QA\ntasks demonstrate that LLMAMT significantly improves response quality, with\naccuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the\nbase model, LLM-AMT outperforms the specialized Med-PaLM 2 model pre-trained on\na massive amount of medical corpus by 2-3%. We found that despite being 100x\nsmaller in size, medical textbooks as a retrieval corpus is proven to be a more\neffective knowledge database than Wikipedia in the medical domain, boosting\nperformance by 7.8%-13.7%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-09-05T13:39:38+00:00",
    "updated": "2024-10-07T17:21:45+00:00",
    "doi": null,
    "comment": "This version has been accepted and published at EMNLP Findings 2024",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2403.16008v1": {
    "id": "http://arxiv.org/abs/2403.16008v1",
    "title": "CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering",
    "authors": [
      "Hongbin Na"
    ],
    "abstract": "The recent advancements in artificial intelligence highlight the potential of\nlanguage models in psychological health support. While models trained on data\nfrom mental health service platform have achieved preliminary success,\nchallenges persist in areas such as data scarcity, quality, and ensuring a\nsolid foundation in psychological techniques. To address these challenges, this\nstudy introduces a novel approach to enhance the precision and efficacy of\npsychological support through large language models. Specifically, we design a\nspecific prompt derived from principles of Cognitive Behavioral Therapy (CBT)\nand have generated the CBT QA dataset, specifically for Chinese psychological\nhealth Q&A based on CBT structured intervention strategies. Unlike previous\nmethods, our dataset emphasizes professional and structured response. Utilizing\nthis dataset, we fine-tuned the large language model, giving birth to CBT-LLM,\nthe large-scale language model specifically designed for Cognitive Behavioral\nTherapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in\ngenerating structured, professional, and highly relevant responses in\npsychological health support tasks, showcasing its practicality and quality.\nThe model is available on Hugging Face:\nhttps://huggingface.co/Hongbin37/CBT-LLM.",
    "categories": [
      "cs.CL"
    ],
    "published": "2024-03-24T04:34:34+00:00",
    "updated": "2024-03-24T04:34:34+00:00",
    "doi": null,
    "comment": "Accepted at COLING 2024",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2406.01476v2": {
    "id": "http://arxiv.org/abs/2406.01476v2",
    "title": "DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with Video Diffusion Priors",
    "authors": [
      "Tianyu Huang",
      "Haoze Zhang",
      "Yihan Zeng",
      "Zhilu Zhang",
      "Hui Li",
      "Wangmeng Zuo",
      "Rynson W. H. Lau"
    ],
    "abstract": "Dynamic 3D interaction has been attracting a lot of attention recently.\nHowever, creating such 4D content remains challenging. One solution is to\nanimate 3D scenes with physics-based simulation, which requires manually\nassigning precise physical properties to the object or the simulated results\nwould become unnatural. Another solution is to learn the deformation of 3D\nobjects with the distillation of video generative models, which, however, tends\nto produce 3D videos with small and discontinuous motions due to the\ninappropriate extraction and application of physical prior. In this work,\ncombining the strengths and complementing shortcomings of the above two\nsolutions, we propose to learn the physical properties of a material field with\nvideo diffusion priors, and then utilize a physics-based Material-Point-Method\n(MPM) simulator to generate 4D content with realistic motions. In particular,\nwe propose motion distillation sampling to emphasize video motion information\nduring distillation. Moreover, to facilitate the optimization, we further\npropose a KAN-based material field with frame boosting. Experimental results\ndemonstrate that our method enjoys more realistic motion than\nstate-of-the-arts. Codes are released at:\nhttps://github.com/tyhuang0428/DreamPhysics.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-06-03T16:05:25+00:00",
    "updated": "2024-08-30T14:09:36+00:00",
    "doi": null,
    "comment": "Codes are released at: https://github.com/tyhuang0428/DreamPhysics",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2306.08687v3": {
    "id": "http://arxiv.org/abs/2306.08687v3",
    "title": "Norm-guided latent space exploration for text-to-image generation",
    "authors": [
      "Dvir Samuel",
      "Rami Ben-Ari",
      "Nir Darshan",
      "Haggai Maron",
      "Gal Chechik"
    ],
    "abstract": "Text-to-image diffusion models show great potential in synthesizing a large\nvariety of concepts in new compositions and scenarios. However, the latent\nspace of initial seeds is still not well understood and its structure was shown\nto impact the generation of various concepts. Specifically, simple operations\nlike interpolation and finding the centroid of a set of seeds perform poorly\nwhen using standard Euclidean or spherical metrics in the latent space. This\npaper makes the observation that, in current training procedures, diffusion\nmodels observed inputs with a narrow range of norm values. This has strong\nimplications for methods that rely on seed manipulation for image generation,\nwith applications to few-shot and long-tail learning tasks. To address this\nissue, we propose a novel method for interpolating between two seeds and\ndemonstrate that it defines a new non-Euclidean metric that takes into account\na norm-based prior on seeds. We describe a simple yet efficient algorithm for\napproximating this interpolation procedure and use it to further define\ncentroids in the latent seed space. We show that our new interpolation and\ncentroid techniques significantly enhance the generation of rare concept\nimages. This further leads to state-of-the-art performance on few-shot and\nlong-tail benchmarks, improving prior approaches in terms of generation speed,\nimage quality, and semantic content.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-06-14T18:12:15+00:00",
    "updated": "2023-11-05T09:39:07+00:00",
    "doi": null,
    "comment": "Accepted to NeurIPS 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.08136v1": {
    "id": "http://arxiv.org/abs/2312.08136v1",
    "title": "ProNeRF: Learning Efficient Projection-Aware Ray Sampling for Fine-Grained Implicit Neural Radiance Fields",
    "authors": [
      "Juan Luis Gonzalez Bello",
      "Minh-Quan Viet Bui",
      "Munchurl Kim"
    ],
    "abstract": "Recent advances in neural rendering have shown that, albeit slow, implicit\ncompact models can learn a scene's geometries and view-dependent appearances\nfrom multiple views. To maintain such a small memory footprint but achieve\nfaster inference times, recent works have adopted `sampler' networks that\nadaptively sample a small subset of points along each ray in the implicit\nneural radiance fields. Although these methods achieve up to a 10$\\times$\nreduction in rendering time, they still suffer from considerable quality\ndegradation compared to the vanilla NeRF. In contrast, we propose ProNeRF,\nwhich provides an optimal trade-off between memory footprint (similar to NeRF),\nspeed (faster than HyperReel), and quality (better than K-Planes). ProNeRF is\nequipped with a novel projection-aware sampling (PAS) network together with a\nnew training strategy for ray exploration and exploitation, allowing for\nefficient fine-grained particle sampling. Our ProNeRF yields state-of-the-art\nmetrics, being 15-23x faster with 0.65dB higher PSNR than NeRF and yielding\n0.95dB higher PSNR than the best published sampler-based method, HyperReel. Our\nexploration and exploitation training strategy allows ProNeRF to learn the full\nscenes' color and density distributions while also learning efficient ray\nsampling focused on the highest-density regions. We provide extensive\nexperimental results that support the effectiveness of our method on the widely\nadopted forward-facing and 360 datasets, LLFF and Blender, respectively.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "published": "2023-12-13T13:37:32+00:00",
    "updated": "2023-12-13T13:37:32+00:00",
    "doi": null,
    "comment": "Visit our project website at\n  https://kaist-viclab.github.io/pronerf-site/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2106.09681v2": {
    "id": "http://arxiv.org/abs/2106.09681v2",
    "title": "XCiT: Cross-Covariance Image Transformers",
    "authors": [
      "Alaaeldin El-Nouby",
      "Hugo Touvron",
      "Mathilde Caron",
      "Piotr Bojanowski",
      "Matthijs Douze",
      "Armand Joulin",
      "Ivan Laptev",
      "Natalia Neverova",
      "Gabriel Synnaeve",
      "Jakob Verbeek",
      "Herv\u00e9 Jegou"
    ],
    "abstract": "Following their success in natural language processing, transformers have\nrecently shown much promise for computer vision. The self-attention operation\nunderlying transformers yields global interactions between all tokens ,i.e.\nwords or image patches, and enables flexible modelling of image data beyond the\nlocal interactions of convolutions. This flexibility, however, comes with a\nquadratic complexity in time and memory, hindering application to long\nsequences and high-resolution images. We propose a \"transposed\" version of\nself-attention that operates across feature channels rather than tokens, where\nthe interactions are based on the cross-covariance matrix between keys and\nqueries. The resulting cross-covariance attention (XCA) has linear complexity\nin the number of tokens, and allows efficient processing of high-resolution\nimages. Our cross-covariance image transformer (XCiT) is built upon XCA. It\ncombines the accuracy of conventional transformers with the scalability of\nconvolutional architectures. We validate the effectiveness and generality of\nXCiT by reporting excellent results on multiple vision benchmarks, including\nimage classification and self-supervised feature learning on ImageNet-1k,\nobject detection and instance segmentation on COCO, and semantic segmentation\non ADE20k.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2021-06-17T17:33:35+00:00",
    "updated": "2021-06-18T15:33:31+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1911.09723v1": {
    "id": "http://arxiv.org/abs/1911.09723v1",
    "title": "Fast Sparse ConvNets",
    "authors": [
      "Erich Elsen",
      "Marat Dukhan",
      "Trevor Gale",
      "Karen Simonyan"
    ],
    "abstract": "Historically, the pursuit of efficient inference has been one of the driving\nforces behind research into new deep learning architectures and building\nblocks. Some recent examples include: the squeeze-and-excitation module,\ndepthwise separable convolutions in Xception, and the inverted bottleneck in\nMobileNet v2. Notably, in all of these cases, the resulting building blocks\nenabled not only higher efficiency, but also higher accuracy, and found wide\nadoption in the field. In this work, we further expand the arsenal of efficient\nbuilding blocks for neural network architectures; but instead of combining\nstandard primitives (such as convolution), we advocate for the replacement of\nthese dense primitives with their sparse counterparts. While the idea of using\nsparsity to decrease the parameter count is not new, the conventional wisdom is\nthat this reduction in theoretical FLOPs does not translate into real-world\nefficiency gains. We aim to correct this misconception by introducing a family\nof efficient sparse kernels for ARM and WebAssembly, which we open-source for\nthe benefit of the community as part of the XNNPACK library. Equipped with our\nefficient implementation of sparse primitives, we show that sparse versions of\nMobileNet v1, MobileNet v2 and EfficientNet architectures substantially\noutperform strong dense baselines on the efficiency-accuracy curve. On\nSnapdragon 835 our sparse networks outperform their dense equivalents by\n$1.3-2.4\\times$ -- equivalent to approximately one entire generation of\nMobileNet-family improvement. We hope that our findings will facilitate wider\nadoption of sparsity as a tool for creating efficient and accurate deep\nlearning architectures.",
    "categories": [
      "cs.CV"
    ],
    "published": "2019-11-21T19:48:14+00:00",
    "updated": "2019-11-21T19:48:14+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.12469v3": {
    "id": "http://arxiv.org/abs/2308.12469v3",
    "title": "Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion",
    "authors": [
      "Junjiao Tian",
      "Lavisha Aggarwal",
      "Andrea Colaco",
      "Zsolt Kira",
      "Mar Gonzalez-Franco"
    ],
    "abstract": "Producing quality segmentation masks for images is a fundamental problem in\ncomputer vision. Recent research has explored large-scale supervised training\nto enable zero-shot segmentation on virtually any image style and unsupervised\ntraining to enable segmentation without dense annotations. However,\nconstructing a model capable of segmenting anything in a zero-shot manner\nwithout any annotations is still challenging. In this paper, we propose to\nutilize the self-attention layers in stable diffusion models to achieve this\ngoal because the pre-trained stable diffusion model has learned inherent\nconcepts of objects within its attention layers. Specifically, we introduce a\nsimple yet effective iterative merging process based on measuring KL divergence\namong attention maps to merge them into valid segmentation masks. The proposed\nmethod does not require any training or language dependency to extract quality\nsegmentation for any images. On COCO-Stuff-27, our method surpasses the prior\nunsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%\nin mean IoU. The project page is at\n\\url{https://sites.google.com/view/diffseg/home}.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-08-23T23:44:44+00:00",
    "updated": "2024-04-02T17:40:03+00:00",
    "doi": null,
    "comment": "Accepted to CVPR2024",
    "journal_ref": "Conference on Computer Vision and Pattern Recognition, 2024",
    "primary_category": "cs.CV"
  },
  "2212.04048v3": {
    "id": "http://arxiv.org/abs/2212.04048v3",
    "title": "Executing your Commands via Motion Diffusion in Latent Space",
    "authors": [
      "Xin Chen",
      "Biao Jiang",
      "Wen Liu",
      "Zilong Huang",
      "Bin Fu",
      "Tao Chen",
      "Jingyi Yu",
      "Gang Yu"
    ],
    "abstract": "We study a challenging task, conditional human motion generation, which\nproduces plausible human motion sequences according to various conditional\ninputs, such as action classes or textual descriptors. Since human motions are\nhighly diverse and have a property of quite different distribution from\nconditional modalities, such as textual descriptors in natural languages, it is\nhard to learn a probabilistic mapping from the desired conditional modality to\nthe human motion sequences. Besides, the raw motion data from the motion\ncapture system might be redundant in sequences and contain noises; directly\nmodeling the joint distribution over the raw motion sequences and conditional\nmodalities would need a heavy computational overhead and might result in\nartifacts introduced by the captured noises. To learn a better representation\nof the various human motion sequences, we first design a powerful Variational\nAutoEncoder (VAE) and arrive at a representative and low-dimensional latent\ncode for a human motion sequence. Then, instead of using a diffusion model to\nestablish the connections between the raw motion sequences and the conditional\ninputs, we perform a diffusion process on the motion latent space. Our proposed\nMotion Latent-based Diffusion model (MLD) could produce vivid motion sequences\nconforming to the given conditional inputs and substantially reduce the\ncomputational overhead in both the training and inference stages. Extensive\nexperiments on various human motion generation tasks demonstrate that our MLD\nachieves significant improvements over the state-of-the-art methods among\nextensive human motion generation tasks, with two orders of magnitude faster\nthan previous diffusion models on raw motion sequences.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2022-12-08T03:07:00+00:00",
    "updated": "2023-05-19T08:14:04+00:00",
    "doi": null,
    "comment": "18 pages, 11 figures, conference",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2111.11133v11": {
    "id": "http://arxiv.org/abs/2111.11133v11",
    "title": "L-Verse: Bidirectional Generation Between Image and Text",
    "authors": [
      "Taehoon Kim",
      "Gwangmo Song",
      "Sihaeng Lee",
      "Sangyun Kim",
      "Yewon Seo",
      "Soonyoung Lee",
      "Seung Hwan Kim",
      "Honglak Lee",
      "Kyunghoon Bae"
    ],
    "abstract": "Far beyond learning long-range interactions of natural language, transformers\nare becoming the de-facto standard for many vision tasks with their power and\nscalability. Especially with cross-modal tasks between image and text, vector\nquantized variational autoencoders (VQ-VAEs) are widely used to make a raw RGB\nimage into a sequence of feature vectors. To better leverage the correlation\nbetween image and text, we propose L-Verse, a novel architecture consisting of\nfeature-augmented variational autoencoder (AugVAE) and bidirectional\nauto-regressive transformer (BiART) for image-to-text and text-to-image\ngeneration. Our AugVAE shows the state-of-the-art reconstruction performance on\nImageNet1K validation set, along with the robustness to unseen images in the\nwild. Unlike other models, BiART can distinguish between image (or text) as a\nconditional reference and a generation target. L-Verse can be directly used for\nimage-to-text or text-to-image generation without any finetuning or extra\nobject detection framework. In quantitative and qualitative experiments,\nL-Verse shows impressive results against previous methods in both image-to-text\nand text-to-image generation on MS-COCO Captions. We furthermore assess the\nscalability of L-Verse architecture on Conceptual Captions and present the\ninitial result of bidirectional vision-language representation learning on\ngeneral domain.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2021-11-22T11:48:26+00:00",
    "updated": "2022-04-06T09:58:42+00:00",
    "doi": null,
    "comment": "Accepted to CVPR 2022 as Oral Presentation (18 pages, 14 figures, 4\n  tables)",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1909.07483v2": {
    "id": "http://arxiv.org/abs/1909.07483v2",
    "title": "The Animal-AI Environment: Training and Testing Animal-Like Artificial Cognition",
    "authors": [
      "Benjamin Beyret",
      "Jos\u00e9 Hern\u00e1ndez-Orallo",
      "Lucy Cheke",
      "Marta Halina",
      "Murray Shanahan",
      "Matthew Crosby"
    ],
    "abstract": "Recent advances in artificial intelligence have been strongly driven by the\nuse of game environments for training and evaluating agents. Games are often\naccessible and versatile, with well-defined state-transitions and goals\nallowing for intensive training and experimentation. However, agents trained in\na particular environment are usually tested on the same or slightly varied\ndistributions, and solutions do not necessarily imply any understanding. If we\nwant AI systems that can model and understand their environment, we need\nenvironments that explicitly test for this. Inspired by the extensive\nliterature on animal cognition, we present an environment that keeps all the\npositive elements of standard gaming environments, but is explicitly designed\nfor the testing of animal-like artificial cognition.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2019-09-12T10:14:12+00:00",
    "updated": "2019-09-18T09:07:46+00:00",
    "doi": null,
    "comment": "14 pages, 34 figures (update: reduce images size)",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2408.05008v3": {
    "id": "http://arxiv.org/abs/2408.05008v3",
    "title": "FlowDreamer: Exploring High Fidelity Text-to-3D Generation via Rectified Flow",
    "authors": [
      "Hangyu Li",
      "Xiangxiang Chu",
      "Dingyuan Shi",
      "Wang Lin"
    ],
    "abstract": "Recent advances in text-to-3D generation have made significant progress. In\nparticular, with the pretrained diffusion models, existing methods\npredominantly use Score Distillation Sampling (SDS) to train 3D models such as\nNeural RaRecent advances in text-to-3D generation have made significant\nprogress. In particular, with the pretrained diffusion models, existing methods\npredominantly use Score Distillation Sampling (SDS) to train 3D models such as\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS). However, a\nhurdle is that they often encounter difficulties with over-smoothing textures\nand over-saturating colors. The rectified flow model -- which utilizes a simple\nordinary differential equation (ODE) to represent a straight trajectory --\nshows promise as an alternative prior to text-to-3D generation. It learns a\ntime-independent vector field, thereby reducing the ambiguity in 3D model\nupdate gradients that are calculated using time-dependent scores in the SDS\nframework. In light of this, we first develop a mathematical analysis to\nseamlessly integrate SDS with rectified flow model, paving the way for our\ninitial framework known as Vector Field Distillation Sampling (VFDS). However,\nempirical findings indicate that VFDS still results in over-smoothing outcomes.\nTherefore, we analyze the grounding reasons for such a failure from the\nperspective of ODE trajectories. On top, we propose a novel framework, named\nFlowDreamer, which yields high fidelity results with richer textual details and\nfaster convergence. The key insight is to leverage the coupling and reversible\nproperties of the rectified flow model to search for the corresponding noise,\nrather than using randomly sampled noise as in VFDS. Accordingly, we introduce\na novel Unique Couple Matching (UCM) loss, which guides the 3D model to\noptimize along the same trajectory.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-08-09T11:40:20+00:00",
    "updated": "2024-10-09T06:05:53+00:00",
    "doi": null,
    "comment": "Tech Report",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2310.13828v3": {
    "id": "http://arxiv.org/abs/2310.13828v3",
    "title": "Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models",
    "authors": [
      "Shawn Shan",
      "Wenxin Ding",
      "Josephine Passananti",
      "Stanley Wu",
      "Haitao Zheng",
      "Ben Y. Zhao"
    ],
    "abstract": "Data poisoning attacks manipulate training data to introduce unexpected\nbehaviors into machine learning models at training time. For text-to-image\ngenerative models with massive training datasets, current understanding of\npoisoning attacks suggests that a successful attack would require injecting\nmillions of poison samples into their training pipeline. In this paper, we show\nthat poisoning attacks can be successful on generative models. We observe that\ntraining data per concept can be quite limited in these models, making them\nvulnerable to prompt-specific poisoning attacks, which target a model's ability\nto respond to individual prompts.\n  We introduce Nightshade, an optimized prompt-specific poisoning attack where\npoison samples look visually identical to benign images with matching text\nprompts. Nightshade poison samples are also optimized for potency and can\ncorrupt an Stable Diffusion SDXL prompt in <100 poison samples. Nightshade\npoison effects \"bleed through\" to related concepts, and multiple attacks can\ncomposed together in a single prompt. Surprisingly, we show that a moderate\nnumber of Nightshade attacks can destabilize general features in a\ntext-to-image generative model, effectively disabling its ability to generate\nmeaningful images. Finally, we propose the use of Nightshade and similar tools\nas a last defense for content creators against web scrapers that ignore\nopt-out/do-not-crawl directives, and discuss possible implications for model\ntrainers and content creators.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2023-10-20T21:54:10+00:00",
    "updated": "2024-04-29T17:23:59+00:00",
    "doi": null,
    "comment": "IEEE Security and Privacy 2024",
    "journal_ref": null,
    "primary_category": "cs.CR"
  },
  "2010.03934v4": {
    "id": "http://arxiv.org/abs/2010.03934v4",
    "title": "Prioritized Level Replay",
    "authors": [
      "Minqi Jiang",
      "Edward Grefenstette",
      "Tim Rockt\u00e4schel"
    ],
    "abstract": "Environments with procedurally generated content serve as important\nbenchmarks for testing systematic generalization in deep reinforcement\nlearning. In this setting, each level is an algorithmically created environment\ninstance with a unique configuration of its factors of variation. Training on a\nprespecified subset of levels allows for testing generalization to unseen\nlevels. What can be learned from a level depends on the current policy, yet\nprior work defaults to uniform sampling of training levels independently of the\npolicy. We introduce Prioritized Level Replay (PLR), a general framework for\nselectively sampling the next training level by prioritizing those with higher\nestimated learning potential when revisited in the future. We show TD-errors\neffectively estimate a level's future learning potential and, when used to\nguide the sampling procedure, induce an emergent curriculum of increasingly\ndifficult levels. By adapting the sampling of training levels, PLR\nsignificantly improves sample efficiency and generalization on Procgen\nBenchmark--matching the previous state-of-the-art in test return--and readily\ncombines with other methods. Combined with the previous leading method, PLR\nraises the state-of-the-art to over 76% improvement in test return relative to\nstandard RL baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2020-10-08T12:46:57+00:00",
    "updated": "2021-06-12T10:50:10+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2308.10848v3": {
    "id": "http://arxiv.org/abs/2308.10848v3",
    "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors",
    "authors": [
      "Weize Chen",
      "Yusheng Su",
      "Jingwei Zuo",
      "Cheng Yang",
      "Chenfei Yuan",
      "Chi-Min Chan",
      "Heyang Yu",
      "Yaxi Lu",
      "Yi-Hsin Hung",
      "Chen Qian",
      "Yujia Qin",
      "Xin Cong",
      "Ruobing Xie",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Jie Zhou"
    ],
    "abstract": "Autonomous agents empowered by Large Language Models (LLMs) have undergone\nsignificant improvements, enabling them to generalize across a broad spectrum\nof tasks. However, in real-world scenarios, cooperation among individuals is\noften required to enhance the efficiency and effectiveness of task\naccomplishment. Hence, inspired by human group dynamics, we propose a\nmulti-agent framework \\framework that can collaboratively and dynamically\nadjust its composition as a greater-than-the-sum-of-its-parts system. Our\nexperiments demonstrate that \\framework framework can effectively deploy\nmulti-agent groups that outperform a single agent. Furthermore, we delve into\nthe emergence of social behaviors among individual agents within a group during\ncollaborative task accomplishment. In view of these behaviors, we discuss some\npossible strategies to leverage positive ones and mitigate negative ones for\nimproving the collaborative potential of multi-agent groups. Our codes for\n\\framework will soon be released at\n\\url{https://github.com/OpenBMB/AgentVerse}.",
    "categories": [
      "cs.CL"
    ],
    "published": "2023-08-21T16:47:11+00:00",
    "updated": "2023-10-23T05:05:15+00:00",
    "doi": null,
    "comment": "Under review. Code at https://github.com/OpenBMB/AgentVerse/",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2210.04889v1": {
    "id": "http://arxiv.org/abs/2210.04889v1",
    "title": "Turbo Training with Token Dropout",
    "authors": [
      "Tengda Han",
      "Weidi Xie",
      "Andrew Zisserman"
    ],
    "abstract": "The objective of this paper is an efficient training method for video tasks.\nWe make three contributions: (1) We propose Turbo training, a simple and\nversatile training paradigm for Transformers on multiple video tasks. (2) We\nillustrate the advantages of Turbo training on action classification,\nvideo-language representation learning, and long-video activity classification,\nshowing that Turbo training can largely maintain competitive performance while\nachieving almost 4X speed-up and significantly less memory consumption. (3)\nTurbo training enables long-schedule video-language training and end-to-end\nlong-video training, delivering competitive or superior performance than\nprevious works, which were infeasible to train under limited resources.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-10-10T17:59:55+00:00",
    "updated": "2022-10-10T17:59:55+00:00",
    "doi": null,
    "comment": "BMVC2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2002.09797v2": {
    "id": "http://arxiv.org/abs/2002.09797v2",
    "title": "Reliable Fidelity and Diversity Metrics for Generative Models",
    "authors": [
      "Muhammad Ferjad Naeem",
      "Seong Joon Oh",
      "Youngjung Uh",
      "Yunjey Choi",
      "Jaejun Yoo"
    ],
    "abstract": "Devising indicative evaluation metrics for the image generation task remains\nan open problem. The most widely used metric for measuring the similarity\nbetween real and generated images has been the Fr\\'echet Inception Distance\n(FID) score. Because it does not differentiate the fidelity and diversity\naspects of the generated images, recent papers have introduced variants of\nprecision and recall metrics to diagnose those properties separately. In this\npaper, we show that even the latest version of the precision and recall metrics\nare not reliable yet. For example, they fail to detect the match between two\nidentical distributions, they are not robust against outliers, and the\nevaluation hyperparameters are selected arbitrarily. We propose density and\ncoverage metrics that solve the above issues. We analytically and\nexperimentally show that density and coverage provide more interpretable and\nreliable signals for practitioners than the existing metrics. Code:\nhttps://github.com/clovaai/generative-evaluation-prdc.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2020-02-23T00:50:01+00:00",
    "updated": "2020-06-28T20:37:50+00:00",
    "doi": null,
    "comment": "First two authors have contributed equally; ICML 2020 accepted",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.12506v1": {
    "id": "http://arxiv.org/abs/2309.12506v1",
    "title": "License Plate Super-Resolution Using Diffusion Models",
    "authors": [
      "Sawsan AlHalawani",
      "Bilel Benjdira",
      "Adel Ammar",
      "Anis Koubaa",
      "Anas M. Ali"
    ],
    "abstract": "In surveillance, accurately recognizing license plates is hindered by their\noften low quality and small dimensions, compromising recognition precision.\nDespite advancements in AI-based image super-resolution, methods like\nConvolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs)\nstill fall short in enhancing license plate images. This study leverages the\ncutting-edge diffusion model, which has consistently outperformed other deep\nlearning techniques in image restoration. By training this model using a\ncurated dataset of Saudi license plates, both in low and high resolutions, we\ndiscovered the diffusion model's superior efficacy. The method achieves a\n12.55\\% and 37.32% improvement in Peak Signal-to-Noise Ratio (PSNR) over SwinIR\nand ESRGAN, respectively. Moreover, our method surpasses these techniques in\nterms of Structural Similarity Index (SSIM), registering a 4.89% and 17.66%\nimprovement over SwinIR and ESRGAN, respectively. Furthermore, 92% of human\nevaluators preferred our images over those from other algorithms. In essence,\nthis research presents a pioneering solution for license plate\nsuper-resolution, with tangible potential for surveillance systems.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-09-21T22:06:23+00:00",
    "updated": "2023-09-21T22:06:23+00:00",
    "doi": "10.3390/electronics13132670",
    "comment": null,
    "journal_ref": "Electronics, Vol. 13, No. 13, Article 2670, 2024",
    "primary_category": "cs.CV"
  },
  "2109.07100v3": {
    "id": "http://arxiv.org/abs/2109.07100v3",
    "title": "Complementary Feature Enhanced Network with Vision Transformer for Image Dehazing",
    "authors": [
      "Dong Zhao",
      "Jia Li",
      "Hongyu Li",
      "Long Xu"
    ],
    "abstract": "Conventional CNNs-based dehazing models suffer from two essential issues: the\ndehazing framework (limited in interpretability) and the convolution layers\n(content-independent and ineffective to learn long-range dependency\ninformation). In this paper, firstly, we propose a new complementary feature\nenhanced framework, in which the complementary features are learned by several\ncomplementary subtasks and then together serve to boost the performance of the\nprimary task. One of the prominent advantages of the new framework is that the\npurposively chosen complementary tasks can focus on learning weakly dependent\ncomplementary features, avoiding repetitive and ineffective learning of the\nnetworks. We design a new dehazing network based on such a framework.\nSpecifically, we select the intrinsic image decomposition as the complementary\ntasks, where the reflectance and shading prediction subtasks are used to\nextract the color-wise and texture-wise complementary features. To effectively\naggregate these complementary features, we propose a complementary features\nselection module (CFSM) to select the more useful features for image dehazing.\nFurthermore, we introduce a new version of vision transformer block, named\nHybrid Local-Global Vision Transformer (HyLoG-ViT), and incorporate it within\nour dehazing networks. The HyLoG-ViT block consists of the local and the global\nvision transformer paths used to capture local and global dependencies. As a\nresult, the HyLoG-ViT introduces locality in the networks and captures the\nglobal and long-range dependencies. Extensive experiments on homogeneous,\nnon-homogeneous, and nighttime dehazing tasks reveal that the proposed dehazing\nnetwork can achieve comparable or even better performance than CNNs-based\ndehazing models.",
    "categories": [
      "cs.CV",
      "68U10 (Primary) 94A08, 54H30 (Secondary)",
      "I.4.3; I.4.4"
    ],
    "published": "2021-09-15T06:13:22+00:00",
    "updated": "2022-01-05T03:05:45+00:00",
    "doi": null,
    "comment": "12 pages,10 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1706.03762v7": {
    "id": "http://arxiv.org/abs/1706.03762v7",
    "title": "Attention Is All You Need",
    "authors": [
      "Ashish Vaswani",
      "Noam Shazeer",
      "Niki Parmar",
      "Jakob Uszkoreit",
      "Llion Jones",
      "Aidan N. Gomez",
      "Lukasz Kaiser",
      "Illia Polosukhin"
    ],
    "abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2017-06-12T17:57:34+00:00",
    "updated": "2023-08-02T00:41:18+00:00",
    "doi": null,
    "comment": "15 pages, 5 figures",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2406.19589v1": {
    "id": "http://arxiv.org/abs/2406.19589v1",
    "title": "Network Bending of Diffusion Models for Audio-Visual Generation",
    "authors": [
      "Luke Dzwonczyk",
      "Carmine Emanuele Cella",
      "David Ban"
    ],
    "abstract": "In this paper we present the first steps towards the creation of a tool which\nenables artists to create music visualizations using pre-trained, generative,\nmachine learning models. First, we investigate the application of network\nbending, the process of applying transforms within the layers of a generative\nnetwork, to image generation diffusion models by utilizing a range of\npoint-wise, tensor-wise, and morphological operators. We identify a number of\nvisual effects that result from various operators, including some that are not\neasily recreated with standard image editing tools. We find that this process\nallows for continuous, fine-grain control of image generation which can be\nhelpful for creative applications. Next, we generate music-reactive videos\nusing Stable Diffusion by passing audio features as parameters to network\nbending operators. Finally, we comment on certain transforms which radically\nshift the image and the possibilities of learning more about the latent space\nof Stable Diffusion based on these transforms.",
    "categories": [
      "cs.SD",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "published": "2024-06-28T00:39:17+00:00",
    "updated": "2024-06-28T00:39:17+00:00",
    "doi": null,
    "comment": "8 pages, 5 figures, to be published in the proceedings of the 27th\n  International Conference on Digital Audio Effects (DAFx24), for additional\n  image and video examples see https://dzluke.github.io/DAFX2024/",
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "2206.10910v3": {
    "id": "http://arxiv.org/abs/2206.10910v3",
    "title": "SpA-Former: Transformer image shadow detection and removal via spatial attention",
    "authors": [
      "Xiao Feng Zhang",
      "Chao Chen Gu",
      "Shan Ying Zhu"
    ],
    "abstract": "In this paper, we propose an end-to-end SpA-Former to recover a shadow-free\nimage from a single shaded image. Unlike traditional methods that require two\nsteps for shadow detection and then shadow removal, the SpA-Former unifies\nthese steps into one, which is a one-stage network capable of directly learning\nthe mapping function between shadows and no shadows, it does not require a\nseparate shadow detection. Thus, SpA-former is adaptable to real image\nde-shadowing for shadows projected on different semantic regions. SpA-Former\nconsists of transformer layer and a series of joint Fourier transform residual\nblocks and two-wheel joint spatial attention. The network in this paper is able\nto handle the task while achieving a very fast processing efficiency.\n  Our code is relased on\nhttps://github.com/zhangbaijin/SpA-Former-shadow-removal",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2022-06-22T08:30:22+00:00",
    "updated": "2022-10-17T03:27:55+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.05925v4": {
    "id": "http://arxiv.org/abs/2401.05925v4",
    "title": "Learning Segmented 3D Gaussians via Efficient Feature Unprojection for Zero-shot Neural Scene Segmentation",
    "authors": [
      "Bin Dou",
      "Tianyu Zhang",
      "Zhaohui Wang",
      "Yongjia Ma",
      "Zejian Yuan"
    ],
    "abstract": "Zero-shot neural scene segmentation, which reconstructs 3D neural\nsegmentation field without manual annotations, serves as an effective way for\nscene understanding. However, existing models, especially the efficient 3D\nGaussian-based methods, struggle to produce compact segmentation results. This\nissue stems primarily from their redundant learnable attributes assigned on\nindividual Gaussians, leading to a lack of robustness against the\n3D-inconsistencies in zero-shot generated raw labels. To address this problem,\nour work, named Compact Segmented 3D Gaussians (CoSegGaussians), proposes the\nFeature Unprojection and Fusion module as the segmentation field, which\nutilizes a shallow decoder generalizable for all Gaussians based on high-level\nfeatures. Specifically, leveraging the learned Gaussian geometric parameters,\nsemantic-aware image-based features are introduced into the scene via our\nunprojection technique. The lifted features, together with spatial information,\nare fed into the multi-scale aggregation decoder to generate segmentation\nidentities for all Gaussians. Furthermore, we design CoSeg Loss to boost model\nrobustness against 3D-inconsistent noises. Experimental results show that our\nmodel surpasses baselines on zero-shot semantic segmentation task, improving by\n~10% mIoU over the best baseline. Code and more results will be available at\nhttps://David-Dou.github.io/CoSegGaussians.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-01-11T14:05:01+00:00",
    "updated": "2024-07-28T02:40:29+00:00",
    "doi": null,
    "comment": "16 pages, 9 figures, correct writing details",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2201.12787v3": {
    "id": "http://arxiv.org/abs/2201.12787v3",
    "title": "GRPE: Relative Positional Encoding for Graph Transformer",
    "authors": [
      "Wonpyo Park",
      "Woonggi Chang",
      "Donggeon Lee",
      "Juntae Kim",
      "Seung-won Hwang"
    ],
    "abstract": "We propose a novel positional encoding for learning graph on Transformer\narchitecture. Existing approaches either linearize a graph to encode absolute\nposition in the sequence of nodes, or encode relative position with another\nnode using bias terms. The former loses preciseness of relative position from\nlinearization, while the latter loses a tight integration of node-edge and\nnode-topology interaction. To overcome the weakness of the previous approaches,\nour method encodes a graph without linearization and considers both\nnode-topology and node-edge interaction. We name our method Graph Relative\nPositional Encoding dedicated to graph representation learning. Experiments\nconducted on various graph datasets show that the proposed method outperforms\nprevious approaches significantly. Our code is publicly available at\nhttps://github.com/lenscloth/GRPE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2022-01-30T11:10:06+00:00",
    "updated": "2022-10-14T13:52:00+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2006.14090v4": {
    "id": "http://arxiv.org/abs/2006.14090v4",
    "title": "Neural Architecture Design for GPU-Efficient Networks",
    "authors": [
      "Ming Lin",
      "Hesen Chen",
      "Xiuyu Sun",
      "Qi Qian",
      "Hao Li",
      "Rong Jin"
    ],
    "abstract": "Many mission-critical systems are based on GPU for inference. It requires not\nonly high recognition accuracy but also low latency in responding time.\nAlthough many studies are devoted to optimizing the structure of deep models\nfor efficient inference, most of them do not leverage the architecture of\n\\textbf{modern GPU} for fast inference, leading to suboptimal performance. To\naddress this issue, we propose a general principle for designing GPU-efficient\nnetworks based on extensive empirical studies. This design principle enables us\nto search for GPU-efficient network structures effectively by a simple and\nlightweight method as opposed to most Neural Architecture Search (NAS) methods\nthat are complicated and computationally expensive. Based on the proposed\nframework, we design a family of GPU-Efficient Networks, or GENets in short. We\ndid extensive evaluations on multiple GPU platforms and inference engines.\nWhile achieving $\\geq 81.3\\%$ top-1 accuracy on ImageNet, GENet is up to $6.4$\ntimes faster than EfficienNet on GPU. It also outperforms most state-of-the-art\nmodels that are more efficient than EfficientNet in high precision regimes. Our\nsource code and pre-trained models are available from\n\\url{https://github.com/idstcv/GPU-Efficient-Networks}.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-06-24T22:42:18+00:00",
    "updated": "2020-08-11T22:54:26+00:00",
    "doi": null,
    "comment": "update training setting",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.02762v3": {
    "id": "http://arxiv.org/abs/2210.02762v3",
    "title": "Vision Transformer Based Model for Describing a Set of Images as a Story",
    "authors": [
      "Zainy M. Malakan",
      "Ghulam Mubashar Hassan",
      "Ajmal Mian"
    ],
    "abstract": "Visual Story-Telling is the process of forming a multi-sentence story from a\nset of images. Appropriately including visual variation and contextual\ninformation captured inside the input images is one of the most challenging\naspects of visual storytelling. Consequently, stories developed from a set of\nimages often lack cohesiveness, relevance, and semantic relationship. In this\npaper, we propose a novel Vision Transformer Based Model for describing a set\nof images as a story. The proposed method extracts the distinct features of the\ninput images using a Vision Transformer (ViT). Firstly, input images are\ndivided into 16X16 patches and bundled into a linear projection of flattened\npatches. The transformation from a single image to multiple image patches\ncaptures the visual variety of the input visual patterns. These features are\nused as input to a Bidirectional-LSTM which is part of the sequence encoder.\nThis captures the past and future image context of all image patches. Then, an\nattention mechanism is implemented and used to increase the discriminatory\ncapacity of the data fed into the language model, i.e. a Mogrifier-LSTM. The\nperformance of our proposed model is evaluated using the Visual Story-Telling\ndataset (VIST), and the results show that our model outperforms the current\nstate of the art models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2022-10-06T09:01:50+00:00",
    "updated": "2023-07-14T08:42:54+00:00",
    "doi": "10.1007/978-3-031-22695-3_2",
    "comment": "This paper has been accepted at the 35th Australasian Joint\n  Conference on Artificial Intelligence 2022 (Camera-ready version is attached)",
    "journal_ref": "13728, 2022, 15 to 28",
    "primary_category": "cs.CV"
  },
  "2407.05848v2": {
    "id": "http://arxiv.org/abs/2407.05848v2",
    "title": "Wavelet Convolutions for Large Receptive Fields",
    "authors": [
      "Shahaf E. Finder",
      "Roy Amoyal",
      "Eran Treister",
      "Oren Freifeld"
    ],
    "abstract": "In recent years, there have been attempts to increase the kernel size of\nConvolutional Neural Nets (CNNs) to mimic the global receptive field of Vision\nTransformers' (ViTs) self-attention blocks. That approach, however, quickly hit\nan upper bound and saturated way before achieving a global receptive field. In\nthis work, we demonstrate that by leveraging the Wavelet Transform (WT), it is,\nin fact, possible to obtain very large receptive fields without suffering from\nover-parameterization, e.g., for a $k \\times k$ receptive field, the number of\ntrainable parameters in the proposed method grows only logarithmically with\n$k$. The proposed layer, named WTConv, can be used as a drop-in replacement in\nexisting architectures, results in an effective multi-frequency response, and\nscales gracefully with the size of the receptive field. We demonstrate the\neffectiveness of the WTConv layer within ConvNeXt and MobileNetV2 architectures\nfor image classification, as well as backbones for downstream tasks, and show\nit yields additional properties such as robustness to image corruption and an\nincreased response to shapes over textures. Our code is available at\nhttps://github.com/BGU-CS-VIL/WTConv.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-07-08T11:55:10+00:00",
    "updated": "2024-07-15T08:39:57+00:00",
    "doi": null,
    "comment": "Accepted to ECCV 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2306.00863v1": {
    "id": "http://arxiv.org/abs/2306.00863v1",
    "title": "DeepFake-Adapter: Dual-Level Adapter for DeepFake Detection",
    "authors": [
      "Rui Shao",
      "Tianxing Wu",
      "Liqiang Nie",
      "Ziwei Liu"
    ],
    "abstract": "Existing deepfake detection methods fail to generalize well to unseen or\ndegraded samples, which can be attributed to the over-fitting of low-level\nforgery patterns. Here we argue that high-level semantics are also\nindispensable recipes for generalizable forgery detection. Recently, large\npre-trained Vision Transformers (ViTs) have shown promising generalization\ncapability. In this paper, we propose the first parameter-efficient tuning\napproach for deepfake detection, namely DeepFake-Adapter, to effectively and\nefficiently adapt the generalizable high-level semantics from large pre-trained\nViTs to aid deepfake detection. Given large pre-trained models but limited\ndeepfake data, DeepFake-Adapter introduces lightweight yet dedicated dual-level\nadapter modules to a ViT while keeping the model backbone frozen. Specifically,\nto guide the adaptation process to be aware of both global and local forgery\ncues of deepfake data, 1) we not only insert Globally-aware Bottleneck Adapters\nin parallel to MLP layers of ViT, 2) but also actively cross-attend\nLocally-aware Spatial Adapters with features from ViT. Unlike existing deepfake\ndetection methods merely focusing on low-level forgery patterns, the forgery\ndetection process of our model can be regularized by generalizable high-level\nsemantics from a pre-trained ViT and adapted by global and local low-level\nforgeries of deepfake data. Extensive experiments on several standard deepfake\ndetection benchmarks validate the effectiveness of our approach. Notably,\nDeepFake-Adapter demonstrates a convincing advantage under cross-dataset and\ncross-manipulation settings. The source code is released at\nhttps://github.com/rshaojimmy/DeepFake-Adapter",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-06-01T16:23:22+00:00",
    "updated": "2023-06-01T16:23:22+00:00",
    "doi": null,
    "comment": "Github: https://github.com/rshaojimmy/DeepFake-Adapter",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.01647v2": {
    "id": "http://arxiv.org/abs/2401.01647v2",
    "title": "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields",
    "authors": [
      "Jan-Niklas Dihlmann",
      "Andreas Engelhardt",
      "Hendrik Lensch"
    ],
    "abstract": "Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2024-01-03T09:46:43+00:00",
    "updated": "2024-03-27T09:39:41+00:00",
    "doi": null,
    "comment": "Project Page: https://signerf.jdihlmann.com",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1912.01603v3": {
    "id": "http://arxiv.org/abs/1912.01603v3",
    "title": "Dream to Control: Learning Behaviors by Latent Imagination",
    "authors": [
      "Danijar Hafner",
      "Timothy Lillicrap",
      "Jimmy Ba",
      "Mohammad Norouzi"
    ],
    "abstract": "Learned world models summarize an agent's experience to facilitate learning\ncomplex behaviors. While learning world models from high-dimensional sensory\ninputs is becoming feasible through deep learning, there are many potential\nways for deriving behaviors from them. We present Dreamer, a reinforcement\nlearning agent that solves long-horizon tasks from images purely by latent\nimagination. We efficiently learn behaviors by propagating analytic gradients\nof learned state values back through trajectories imagined in the compact state\nspace of a learned world model. On 20 challenging visual control tasks, Dreamer\nexceeds existing approaches in data-efficiency, computation time, and final\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2019-12-03T18:57:16+00:00",
    "updated": "2020-03-17T17:10:58+00:00",
    "doi": null,
    "comment": "9 pages, 12 figures",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2103.14636v2": {
    "id": "http://arxiv.org/abs/2103.14636v2",
    "title": "A Practical Survey on Faster and Lighter Transformers",
    "authors": [
      "Quentin Fournier",
      "Ga\u00e9tan Marceau Caron",
      "Daniel Aloise"
    ],
    "abstract": "Recurrent neural networks are effective models to process sequences. However,\nthey are unable to learn long-term dependencies because of their inherent\nsequential nature. As a solution, Vaswani et al. introduced the Transformer, a\nmodel solely based on the attention mechanism that is able to relate any two\npositions of the input sequence, hence modelling arbitrary long dependencies.\nThe Transformer has improved the state-of-the-art across numerous sequence\nmodelling tasks. However, its effectiveness comes at the expense of a quadratic\ncomputational and memory complexity with respect to the sequence length,\nhindering its adoption. Fortunately, the deep learning community has always\nbeen interested in improving the models' efficiency, leading to a plethora of\nsolutions such as parameter sharing, pruning, mixed-precision, and knowledge\ndistillation. Recently, researchers have directly addressed the Transformer's\nlimitation by designing lower-complexity alternatives such as the Longformer,\nReformer, Linformer, and Performer. However, due to the wide range of\nsolutions, it has become challenging for researchers and practitioners to\ndetermine which methods to apply in practice in order to meet the desired\ntrade-off between capacity, computation, and memory. This survey addresses this\nissue by investigating popular approaches to make Transformers faster and\nlighter and by providing a comprehensive explanation of the methods' strengths,\nlimitations, and underlying assumptions.",
    "categories": [
      "cs.LG"
    ],
    "published": "2021-03-26T17:54:47+00:00",
    "updated": "2023-03-27T15:10:28+00:00",
    "doi": "10.1145/3586074",
    "comment": "ACM Computing Surveys; 40 pages, 18 figures, 4 tables",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1608.04471v3": {
    "id": "http://arxiv.org/abs/1608.04471v3",
    "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm",
    "authors": [
      "Qiang Liu",
      "Dilin Wang"
    ],
    "abstract": "We propose a general purpose variational inference algorithm that forms a\nnatural counterpart of gradient descent for optimization. Our method\niteratively transports a set of particles to match the target distribution, by\napplying a form of functional gradient descent that minimizes the KL\ndivergence. Empirical studies are performed on various real world models and\ndatasets, on which our method is competitive with existing state-of-the-art\nmethods. The derivation of our method is based on a new theoretical result that\nconnects the derivative of KL divergence under smooth transforms with Stein's\nidentity and a recently proposed kernelized Stein discrepancy, which is of\nindependent interest.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2016-08-16T03:24:20+00:00",
    "updated": "2019-09-09T17:31:39+00:00",
    "doi": null,
    "comment": "To appear in NIPS 2016",
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2404.00992v2": {
    "id": "http://arxiv.org/abs/2404.00992v2",
    "title": "SGCNeRF: Few-Shot Neural Rendering via Sparse Geometric Consistency Guidance",
    "authors": [
      "Yuru Xiao",
      "Xianming Liu",
      "Deming Zhai",
      "Kui Jiang",
      "Junjun Jiang",
      "Xiangyang Ji"
    ],
    "abstract": "Neural Radiance Field (NeRF) technology has made significant strides in\ncreating novel viewpoints. However, its effectiveness is hampered when working\nwith sparsely available views, often leading to performance dips due to\noverfitting. FreeNeRF attempts to overcome this limitation by integrating\nimplicit geometry regularization, which incrementally improves both geometry\nand textures. Nonetheless, an initial low positional encoding bandwidth results\nin the exclusion of high-frequency elements. The quest for a holistic approach\nthat simultaneously addresses overfitting and the preservation of\nhigh-frequency details remains ongoing. This study introduces a novel feature\nmatching based sparse geometry regularization module. This module excels in\npinpointing high-frequency keypoints, thereby safeguarding the integrity of\nfine details. Through progressive refinement of geometry and textures across\nNeRF iterations, we unveil an effective few-shot neural rendering architecture,\ndesignated as SGCNeRF, for enhanced novel view synthesis. Our experiments\ndemonstrate that SGCNeRF not only achieves superior geometry-consistent\noutcomes but also surpasses FreeNeRF, with improvements of 0.7 dB and 0.6 dB in\nPSNR on the LLFF and DTU datasets, respectively.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-01T08:37:57+00:00",
    "updated": "2024-06-17T08:16:38+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.17053v4": {
    "id": "http://arxiv.org/abs/2401.17053v4",
    "title": "BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation",
    "authors": [
      "Zhennan Wu",
      "Yang Li",
      "Han Yan",
      "Taizhang Shang",
      "Weixuan Sun",
      "Senbo Wang",
      "Ruikai Cui",
      "Weizhe Liu",
      "Hiroyuki Sato",
      "Hongdong Li",
      "Pan Ji"
    ],
    "abstract": "We present BlockFusion, a diffusion-based model that generates 3D scenes as\nunit blocks and seamlessly incorporates new blocks to extend the scene.\nBlockFusion is trained using datasets of 3D blocks that are randomly cropped\nfrom complete 3D scene meshes. Through per-block fitting, all training blocks\nare converted into the hybrid neural fields: with a tri-plane containing the\ngeometry features, followed by a Multi-layer Perceptron (MLP) for decoding the\nsigned distance values. A variational auto-encoder is employed to compress the\ntri-planes into the latent tri-plane space, on which the denoising diffusion\nprocess is performed. Diffusion applied to the latent representations allows\nfor high-quality and diverse 3D scene generation. To expand a scene during\ngeneration, one needs only to append empty blocks to overlap with the current\nscene and extrapolate existing latent tri-planes to populate new blocks. The\nextrapolation is done by conditioning the generation process with the feature\nsamples from the overlapping tri-planes during the denoising iterations. Latent\ntri-plane extrapolation produces semantically and geometrically meaningful\ntransitions that harmoniously blend with the existing scene. A 2D layout\nconditioning mechanism is used to control the placement and arrangement of\nscene elements. Experimental results indicate that BlockFusion is capable of\ngenerating diverse, geometrically consistent and unbounded large 3D scenes with\nunprecedented high-quality shapes in both indoor and outdoor scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "published": "2024-01-30T14:34:19+00:00",
    "updated": "2024-05-24T03:56:20+00:00",
    "doi": null,
    "comment": "ACM Transactions on Graphics (SIGGRAPH'24). Code:\n  https://yang-l1.github.io/blockfusion",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2302.09311v2": {
    "id": "http://arxiv.org/abs/2302.09311v2",
    "title": "Temporal Interpolation Is All You Need for Dynamic Neural Radiance Fields",
    "authors": [
      "Sungheon Park",
      "Minjung Son",
      "Seokhwan Jang",
      "Young Chun Ahn",
      "Ji-Yeon Kim",
      "Nahyup Kang"
    ],
    "abstract": "Temporal interpolation often plays a crucial role to learn meaningful\nrepresentations in dynamic scenes. In this paper, we propose a novel method to\ntrain spatiotemporal neural radiance fields of dynamic scenes based on temporal\ninterpolation of feature vectors. Two feature interpolation methods are\nsuggested depending on underlying representations, neural networks or grids. In\nthe neural representation, we extract features from space-time inputs via\nmultiple neural network modules and interpolate them based on time frames. The\nproposed multi-level feature interpolation network effectively captures\nfeatures of both short-term and long-term time ranges. In the grid\nrepresentation, space-time features are learned via four-dimensional hash\ngrids, which remarkably reduces training time. The grid representation shows\nmore than 100 times faster training speed than the previous neural-net-based\nmethods while maintaining the rendering quality. Concatenating static and\ndynamic features and adding a simple smoothness term further improve the\nperformance of our proposed models. Despite the simplicity of the model\narchitectures, our method achieved state-of-the-art performance both in\nrendering quality for the neural representation and in training speed for the\ngrid representation.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-02-18T12:01:23+00:00",
    "updated": "2023-03-29T13:31:37+00:00",
    "doi": null,
    "comment": "CVPR 2023. Project page:\n  https://sungheonpark.github.io/tempinterpnerf",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1910.02551v3": {
    "id": "http://arxiv.org/abs/1910.02551v3",
    "title": "Soft-Label Dataset Distillation and Text Dataset Distillation",
    "authors": [
      "Ilia Sucholutsky",
      "Matthias Schonlau"
    ],
    "abstract": "Dataset distillation is a method for reducing dataset sizes by learning a\nsmall number of synthetic samples containing all the information of a large\ndataset. This has several benefits like speeding up model training, reducing\nenergy consumption, and reducing required storage space. Currently, each\nsynthetic sample is assigned a single `hard' label, and also, dataset\ndistillation can currently only be used with image data.\n  We propose to simultaneously distill both images and their labels, thus\nassigning each synthetic sample a `soft' label (a distribution of labels). Our\nalgorithm increases accuracy by 2-4% over the original algorithm for several\nimage classification tasks. Using `soft' labels also enables distilled datasets\nto consist of fewer samples than there are classes as each sample can encode\ninformation for multiple classes. For example, training a LeNet model with 10\ndistilled images (one per class) results in over 96% accuracy on MNIST, and\nalmost 92% accuracy when trained on just 5 distilled images.\n  We also extend the dataset distillation algorithm to distill sequential\ndatasets including texts. We demonstrate that text distillation outperforms\nother methods across multiple datasets. For example, models attain almost their\noriginal accuracy on the IMDB sentiment analysis task using just 20 distilled\nsentences.\n  Our code can be found at\n$\\href{https://github.com/ilia10000/dataset-distillation}{\\text{https://github.com/ilia10000/dataset-distillation}}$.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2019-10-06T23:57:22+00:00",
    "updated": "2020-05-05T04:09:03+00:00",
    "doi": "10.1109/IJCNN52387.2021.9533769",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2109.04282v3": {
    "id": "http://arxiv.org/abs/2109.04282v3",
    "title": "Cartography Active Learning",
    "authors": [
      "Mike Zhang",
      "Barbara Plank"
    ],
    "abstract": "We propose Cartography Active Learning (CAL), a novel Active Learning (AL)\nalgorithm that exploits the behavior of the model on individual instances\nduring training as a proxy to find the most informative instances for labeling.\nCAL is inspired by data maps, which were recently proposed to derive insights\ninto dataset quality (Swayamdipta et al., 2020). We compare our method on\npopular text classification tasks to commonly used AL strategies, which instead\nrely on post-training behavior. We demonstrate that CAL is competitive to other\ncommon AL methods, showing that training dynamics derived from small seed data\ncan be successfully used for AL. We provide insights into our new AL method by\nanalyzing batch-level statistics utilizing the data maps. Our results further\nshow that CAL results in a more data-efficient learning strategy, achieving\ncomparable or better results with considerably less training data.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2021-09-09T14:02:02+00:00",
    "updated": "2022-05-08T10:12:29+00:00",
    "doi": null,
    "comment": "In Findings EMNLP 2021",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1904.12847v6": {
    "id": "http://arxiv.org/abs/1904.12847v6",
    "title": "Optimal Sparse Decision Trees",
    "authors": [
      "Xiyang Hu",
      "Cynthia Rudin",
      "Margo Seltzer"
    ],
    "abstract": "Decision tree algorithms have been among the most popular algorithms for\ninterpretable (transparent) machine learning since the early 1980's. The\nproblem that has plagued decision tree algorithms since their inception is\ntheir lack of optimality, or lack of guarantees of closeness to optimality:\ndecision tree algorithms are often greedy or myopic, and sometimes produce\nunquestionably suboptimal models. Hardness of decision tree optimization is\nboth a theoretical and practical obstacle, and even careful mathematical\nprogramming approaches have not been able to solve these problems efficiently.\nThis work introduces the first practical algorithm for optimal decision trees\nfor binary variables. The algorithm is a co-design of analytical bounds that\nreduce the search space and modern systems techniques, including data\nstructures and a custom bit-vector library. Our experiments highlight\nadvantages in scalability, speed, and proof of optimality. The code is\navailable at https://github.com/xiyanghu/OSDT.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2019-04-29T17:56:34+00:00",
    "updated": "2023-09-26T19:10:54+00:00",
    "doi": null,
    "comment": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2207.01398v2": {
    "id": "http://arxiv.org/abs/2207.01398v2",
    "title": "Large-scale Robustness Analysis of Video Action Recognition Models",
    "authors": [
      "Madeline Chantry Schiappa",
      "Naman Biyani",
      "Prudvi Kamtam",
      "Shruti Vyas",
      "Hamid Palangi",
      "Vibhav Vineet",
      "Yogesh Rawat"
    ],
    "abstract": "We have seen a great progress in video action recognition in recent years.\nThere are several models based on convolutional neural network (CNN) and some\nrecent transformer based approaches which provide top performance on existing\nbenchmarks. In this work, we perform a large-scale robustness analysis of these\nexisting models for video action recognition. We focus on robustness against\nreal-world distribution shift perturbations instead of adversarial\nperturbations. We propose four different benchmark datasets, HMDB51-P,\nUCF101-P, Kinetics400-P, and SSv2-P to perform this analysis. We study\nrobustness of six state-of-the-art action recognition models against 90\ndifferent perturbations. The study reveals some interesting findings, 1)\ntransformer based models are consistently more robust compared to CNN based\nmodels, 2) Pretraining improves robustness for Transformer based models more\nthan CNN based models, and 3) All of the studied models are robust to temporal\nperturbations for all datasets but SSv2; suggesting the importance of temporal\ninformation for action recognition varies based on the dataset and activities.\nNext, we study the role of augmentations in model robustness and present a\nreal-world dataset, UCF101-DS, which contains realistic distribution shifts, to\nfurther validate some of these findings. We believe this study will serve as a\nbenchmark for future research in robust video action recognition.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "published": "2022-07-04T13:29:34+00:00",
    "updated": "2023-04-07T16:40:59+00:00",
    "doi": null,
    "comment": "Accepted in 2023 Conference on Computer Vision and Pattern\n  Recognition (CVPR)",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2304.06939v3": {
    "id": "http://arxiv.org/abs/2304.06939v3",
    "title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text",
    "authors": [
      "Wanrong Zhu",
      "Jack Hessel",
      "Anas Awadalla",
      "Samir Yitzhak Gadre",
      "Jesse Dodge",
      "Alex Fang",
      "Youngjae Yu",
      "Ludwig Schmidt",
      "William Yang Wang",
      "Yejin Choi"
    ],
    "abstract": "In-context vision and language models like Flamingo support arbitrarily\ninterleaved sequences of images and text as input. This format not only enables\nfew-shot learning via interleaving independent supervised (image, text)\nexamples, but also, more complex prompts involving interaction between images,\ne.g., \"What do image A and image B have in common?\" To support this interface,\npretraining occurs over web corpora that similarly contain interleaved\nimages+text. To date, however, large-scale data of this form have not been\npublicly available.\n  We release Multimodal C4, an augmentation of the popular text-only C4 corpus\nwith images interleaved. We use a linear assignment algorithm to place images\ninto longer bodies of text using CLIP features, a process that we show\noutperforms alternatives. Multimodal C4 spans everyday topics like cooking,\ntravel, technology, etc. A manual inspection of a random sample of documents\nshows that a vast majority (88%) of images are topically relevant, and that\nlinear assignment frequently selects individual sentences specifically\nwell-aligned with each image (80%). After filtering NSFW images, ads, etc., the\nresulting corpus consists of 101.2M documents with 571M images interleaved in\n43B English tokens.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "published": "2023-04-14T06:17:46+00:00",
    "updated": "2023-10-28T04:19:41+00:00",
    "doi": null,
    "comment": "NeurIPS D&B 2023. Project homepage: https://github.com/allenai/mmc4",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2004.00053v2": {
    "id": "http://arxiv.org/abs/2004.00053v2",
    "title": "Information Leakage in Embedding Models",
    "authors": [
      "Congzheng Song",
      "Ananth Raghunathan"
    ],
    "abstract": "Embeddings are functions that map raw input data to low-dimensional vector\nrepresentations, while preserving important semantic information about the\ninputs. Pre-training embeddings on a large amount of unlabeled data and\nfine-tuning them for downstream tasks is now a de facto standard in achieving\nstate of the art learning in many domains.\n  We demonstrate that embeddings, in addition to encoding generic semantics,\noften also present a vector that leaks sensitive information about the input\ndata. We develop three classes of attacks to systematically study information\nthat might be leaked by embeddings. First, embedding vectors can be inverted to\npartially recover some of the input data. As an example, we show that our\nattacks on popular sentence embeddings recover between 50\\%--70\\% of the input\nwords (F1 scores of 0.5--0.7). Second, embeddings may reveal sensitive\nattributes inherent in inputs and independent of the underlying semantic task\nat hand. Attributes such as authorship of text can be easily extracted by\ntraining an inference model on just a handful of labeled embedding vectors.\nThird, embedding models leak moderate amount of membership information for\ninfrequent training data inputs. We extensively evaluate our attacks on various\nstate-of-the-art embedding models in the text domain. We also propose and\nevaluate defenses that can prevent the leakage to some extent at a minor cost\nin utility.",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CR",
      "stat.ML"
    ],
    "published": "2020-03-31T18:33:36+00:00",
    "updated": "2020-08-19T19:58:14+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2410.02067v1": {
    "id": "http://arxiv.org/abs/2410.02067v1",
    "title": "DisEnvisioner: Disentangled and Enriched Visual Prompt for Customized Image Generation",
    "authors": [
      "Jing He",
      "Haodong Li",
      "Yongzhe Hu",
      "Guibao Shen",
      "Yingjie Cai",
      "Weichao Qiu",
      "Ying-Cong Chen"
    ],
    "abstract": "In the realm of image generation, creating customized images from visual\nprompt with additional textual instruction emerges as a promising endeavor.\nHowever, existing methods, both tuning-based and tuning-free, struggle with\ninterpreting the subject-essential attributes from the visual prompt. This\nleads to subject-irrelevant attributes infiltrating the generation process,\nultimately compromising the personalization quality in both editability and ID\npreservation. In this paper, we present DisEnvisioner, a novel approach for\neffectively extracting and enriching the subject-essential features while\nfiltering out -irrelevant information, enabling exceptional customization\nperformance, in a tuning-free manner and using only a single image.\nSpecifically, the feature of the subject and other irrelevant components are\neffectively separated into distinctive visual tokens, enabling a much more\naccurate customization. Aiming to further improving the ID consistency, we\nenrich the disentangled features, sculpting them into more granular\nrepresentations. Experiments demonstrate the superiority of our approach over\nexisting methods in instruction response (editability), ID consistency,\ninference speed, and the overall image quality, highlighting the effectiveness\nand efficiency of DisEnvisioner. Project page:\nhttps://disenvisioner.github.io/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-10-02T22:29:14+00:00",
    "updated": "2024-10-02T22:29:14+00:00",
    "doi": null,
    "comment": "The first two authors contributed equally. Project page:\n  https://disenvisioner.github.io/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.09228v3": {
    "id": "http://arxiv.org/abs/2312.09228v3",
    "title": "3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting",
    "authors": [
      "Zhiyin Qian",
      "Shaofei Wang",
      "Marko Mihajlovic",
      "Andreas Geiger",
      "Siyu Tang"
    ],
    "abstract": "We introduce an approach that creates animatable human avatars from monocular\nvideos using 3D Gaussian Splatting (3DGS). Existing methods based on neural\nradiance fields (NeRFs) achieve high-quality novel-view/novel-pose image\nsynthesis but often require days of training, and are extremely slow at\ninference time. Recently, the community has explored fast grid structures for\nefficient training of clothed avatars. Albeit being extremely fast at training,\nthese methods can barely achieve an interactive rendering frame rate with\naround 15 FPS. In this paper, we use 3D Gaussian Splatting and learn a\nnon-rigid deformation network to reconstruct animatable clothed human avatars\nthat can be trained within 30 minutes and rendered at real-time frame rates\n(50+ FPS). Given the explicit nature of our representation, we further\nintroduce as-isometric-as-possible regularizations on both the Gaussian mean\nvectors and the covariance matrices, enhancing the generalization of our model\non highly articulated unseen poses. Experimental results show that our method\nachieves comparable and even better performance compared to state-of-the-art\napproaches on animatable avatar creation from a monocular input, while being\n400x and 250x faster in training and inference, respectively.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-14T18:54:32+00:00",
    "updated": "2024-04-04T15:06:02+00:00",
    "doi": null,
    "comment": "Project page: https://neuralbodies.github.io/3DGS-Avatar",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2104.12229v1": {
    "id": "http://arxiv.org/abs/2104.12229v1",
    "title": "Vector Neurons: A General Framework for SO(3)-Equivariant Networks",
    "authors": [
      "Congyue Deng",
      "Or Litany",
      "Yueqi Duan",
      "Adrien Poulenard",
      "Andrea Tagliasacchi",
      "Leonidas Guibas"
    ],
    "abstract": "Invariance and equivariance to the rotation group have been widely discussed\nin the 3D deep learning community for pointclouds. Yet most proposed methods\neither use complex mathematical tools that may limit their accessibility, or\nare tied to specific input data types and network architectures. In this paper,\nwe introduce a general framework built on top of what we call Vector Neuron\nrepresentations for creating SO(3)-equivariant neural networks for pointcloud\nprocessing. Extending neurons from 1D scalars to 3D vectors, our vector neurons\nenable a simple mapping of SO(3) actions to latent spaces thereby providing a\nframework for building equivariance in common neural operations -- including\nlinear layers, non-linearities, pooling, and normalizations. Due to their\nsimplicity, vector neurons are versatile and, as we demonstrate, can be\nincorporated into diverse network architecture backbones, allowing them to\nprocess geometry inputs in arbitrary poses. Despite its simplicity, our method\nperforms comparably well in accuracy and generalization with other more complex\nand specialized state-of-the-art methods on classification and segmentation\ntasks. We also show for the first time a rotation equivariant reconstruction\nnetwork.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-04-25T18:48:15+00:00",
    "updated": "2021-04-25T18:48:15+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1909.13639v4": {
    "id": "http://arxiv.org/abs/1909.13639v4",
    "title": "NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement Learning",
    "authors": [
      "Ameer Haj-Ali",
      "Nesreen K. Ahmed",
      "Ted Willke",
      "Sophia Shao",
      "Krste Asanovic",
      "Ion Stoica"
    ],
    "abstract": "One of the key challenges arising when compilers vectorize loops for today's\nSIMD-compatible architectures is to decide if vectorization or interleaving is\nbeneficial. Then, the compiler has to determine how many instructions to pack\ntogether and how many loop iterations to interleave. Compilers are designed\ntoday to use fixed-cost models that are based on heuristics to make\nvectorization decisions on loops. However, these models are unable to capture\nthe data dependency, the computation graph, or the organization of\ninstructions. Alternatively, software engineers often hand-write the\nvectorization factors of every loop. This, however, places a huge burden on\nthem, since it requires prior experience and significantly increases the\ndevelopment time. In this work, we explore a novel approach for handling loop\nvectorization and propose an end-to-end solution using deep reinforcement\nlearning (RL). We conjecture that deep RL can capture different instructions,\ndependencies, and data structures to enable learning a sophisticated model that\ncan better predict the actual performance cost and determine the optimal\nvectorization factors. We develop an end-to-end framework, from code to\nvectorization, that integrates deep RL in the LLVM compiler. Our proposed\nframework takes benchmark codes as input and extracts the loop codes. These\nloop codes are then fed to a loop embedding generator that learns an embedding\nfor these loops. Finally, the learned embeddings are used as input to a Deep RL\nagent, which determines the vectorization factors for all the loops. We further\nextend our framework to support multiple supervised learning methods. We\nevaluate our approaches against the currently used LLVM vectorizer and loop\npolyhedral optimization techniques. Our experiments show 1.29X-4.73X\nperformance speedup compared to baseline and only 3% worse than the brute-force\nsearch on a wide range of benchmarks.",
    "categories": [
      "cs.DC",
      "cs.PF",
      "cs.PL"
    ],
    "published": "2019-09-20T12:29:09+00:00",
    "updated": "2020-01-04T09:11:03+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.DC"
  },
  "2205.01381v1": {
    "id": "http://arxiv.org/abs/2205.01381v1",
    "title": "Kompetencer: Fine-grained Skill Classification in Danish Job Postings via Distant Supervision and Transfer Learning",
    "authors": [
      "Mike Zhang",
      "Kristian N\u00f8rgaard Jensen",
      "Barbara Plank"
    ],
    "abstract": "Skill Classification (SC) is the task of classifying job competences from job\npostings. This work is the first in SC applied to Danish job vacancy data. We\nrelease the first Danish job posting dataset: Kompetencer (en: competences),\nannotated for nested spans of competences. To improve upon coarse-grained\nannotations, we make use of The European Skills, Competences, Qualifications\nand Occupations (ESCO; le Vrang et al., 2014) taxonomy API to obtain\nfine-grained labels via distant supervision. We study two setups: The zero-shot\nand few-shot classification setting. We fine-tune English-based models and\nRemBERT (Chung et al., 2020) and compare them to in-language Danish models. Our\nresults show RemBERT significantly outperforms all other models in both the\nzero-shot and the few-shot setting.",
    "categories": [
      "cs.CL"
    ],
    "published": "2022-05-03T09:13:55+00:00",
    "updated": "2022-05-03T09:13:55+00:00",
    "doi": null,
    "comment": "7 pages, accepted to LREC 2022. arXiv admin note: text overlap with\n  arXiv:2204.12811",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1707.03718v1": {
    "id": "http://arxiv.org/abs/1707.03718v1",
    "title": "LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation",
    "authors": [
      "Abhishek Chaurasia",
      "Eugenio Culurciello"
    ],
    "abstract": "Pixel-wise semantic segmentation for visual scene understanding not only\nneeds to be accurate, but also efficient in order to find any use in real-time\napplication. Existing algorithms even though are accurate but they do not focus\non utilizing the parameters of neural network efficiently. As a result they are\nhuge in terms of parameters and number of operations; hence slow too. In this\npaper, we propose a novel deep neural network architecture which allows it to\nlearn without any significant increase in number of parameters. Our network\nuses only 11.5 million parameters and 21.2 GFLOPs for processing an image of\nresolution 3x640x360. It gives state-of-the-art performance on CamVid and\ncomparable results on Cityscapes dataset. We also compare our networks\nprocessing time on NVIDIA GPU and embedded system device with existing\nstate-of-the-art architectures for different image resolutions.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2017-06-14T20:37:17+00:00",
    "updated": "2017-06-14T20:37:17+00:00",
    "doi": "10.1109/VCIP.2017.8305148",
    "comment": "5 pages, 5 figures, GitHub: https://github.com/e-lab/LinkNet",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.14356v2": {
    "id": "http://arxiv.org/abs/2309.14356v2",
    "title": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs",
    "authors": [
      "Tiep Le",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "abstract": "Counterfactual examples have proven to be valuable in the field of natural\nlanguage processing (NLP) for both evaluating and improving the robustness of\nlanguage models to spurious correlations in datasets. Despite their\ndemonstrated utility for NLP, multimodal counterfactual examples have been\nrelatively unexplored due to the difficulty of creating paired image-text data\nwith minimal counterfactual changes. To address this challenge, we introduce a\nscalable framework for automatic generation of counterfactual examples using\ntext-to-image diffusion models. We use our framework to create\nCOCO-Counterfactuals, a multimodal counterfactual dataset of paired image and\ntext captions based on the MS-COCO dataset. We validate the quality of\nCOCO-Counterfactuals through human evaluations and show that existing\nmultimodal models are challenged by our counterfactual image-text pairs.\nAdditionally, we demonstrate the usefulness of COCO-Counterfactuals for\nimproving out-of-domain generalization of multimodal vision-language models via\ntraining data augmentation.",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2023-09-23T00:16:47+00:00",
    "updated": "2023-10-31T15:41:25+00:00",
    "doi": null,
    "comment": "Accepted to NeurIPS 2023 Datasets and Benchmarks Track",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1709.06548v2": {
    "id": "http://arxiv.org/abs/1709.06548v2",
    "title": "Triangle Generative Adversarial Networks",
    "authors": [
      "Zhe Gan",
      "Liqun Chen",
      "Weiyao Wang",
      "Yunchen Pu",
      "Yizhe Zhang",
      "Hao Liu",
      "Chunyuan Li",
      "Lawrence Carin"
    ],
    "abstract": "A Triangle Generative Adversarial Network ($\\Delta$-GAN) is developed for\nsemi-supervised cross-domain joint distribution matching, where the training\ndata consists of samples from each domain, and supervision of domain\ncorrespondence is provided by only a few paired samples. $\\Delta$-GAN consists\nof four neural networks, two generators and two discriminators. The generators\nare designed to learn the two-way conditional distributions between the two\ndomains, while the discriminators implicitly define a ternary discriminative\nfunction, which is trained to distinguish real data pairs and two kinds of fake\ndata pairs. The generators and discriminators are trained together using\nadversarial learning. Under mild assumptions, in theory the joint distributions\ncharacterized by the two generators concentrate to the data distribution. In\nexperiments, three different kinds of domain pairs are considered, image-label,\nimage-image and image-attribute pairs. Experiments on semi-supervised image\nclassification, image-to-image translation and attribute-based image generation\ndemonstrate the superiority of the proposed approach.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2017-09-19T17:50:40+00:00",
    "updated": "2017-11-18T23:41:49+00:00",
    "doi": null,
    "comment": "To appear in NIPS 2017",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2112.00599v1": {
    "id": "http://arxiv.org/abs/2112.00599v1",
    "title": "An implementation of the \"Guess who?\" game using CLIP",
    "authors": [
      "Arnau Mart\u00ed Sarri",
      "Victor Rodriguez-Fernandez"
    ],
    "abstract": "CLIP (Contrastive Language-Image Pretraining) is an efficient method for\nlearning computer vision tasks from natural language supervision that has\npowered a recent breakthrough in deep learning due to its zero-shot transfer\ncapabilities. By training from image-text pairs available on the internet, the\nCLIP model transfers non-trivially to most tasks without the need for any data\nset specific training. In this work, we use CLIP to implement the engine of the\npopular game \"Guess who?\", so that the player interacts with the game using\nnatural language prompts and CLIP automatically decides whether an image in the\ngame board fulfills that prompt or not. We study the performance of this\napproach by benchmarking on different ways of prompting the questions to CLIP,\nand show the limitations of its zero-shot capabilites.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2021-11-30T13:10:52+00:00",
    "updated": "2021-11-30T13:10:52+00:00",
    "doi": "10.1007/978-3-030-91608-4_41",
    "comment": "Code available at https://github.com/ArnauDIMAI/CLIP-GuessWho",
    "journal_ref": "Intelligent Data Engineering and Automated Learning (IDEAL 2021).\n  Lecture Notes in Computer Science, vol 13113",
    "primary_category": "cs.CV"
  },
  "2311.06783v1": {
    "id": "http://arxiv.org/abs/2311.06783v1",
    "title": "Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models",
    "authors": [
      "Haoning Wu",
      "Zicheng Zhang",
      "Erli Zhang",
      "Chaofeng Chen",
      "Liang Liao",
      "Annan Wang",
      "Kaixin Xu",
      "Chunyi Li",
      "Jingwen Hou",
      "Guangtao Zhai",
      "Geng Xue",
      "Wenxiu Sun",
      "Qiong Yan",
      "Weisi Lin"
    ],
    "abstract": "Multi-modality foundation models, as represented by GPT-4V, have brought a\nnew paradigm for low-level visual perception and understanding tasks, that can\nrespond to a broad range of natural human instructions in a model. While\nexisting foundation models have shown exciting potentials on low-level visual\ntasks, their related abilities are still preliminary and need to be improved.\nIn order to enhance these models, we conduct a large-scale subjective\nexperiment collecting a vast number of real human feedbacks on low-level\nvision. Each feedback follows a pathway that starts with a detailed description\non the low-level visual appearance (*e.g. clarity, color, brightness* of an\nimage, and ends with an overall conclusion, with an average length of 45 words.\nThe constructed **Q-Pathway** dataset includes 58K detailed human feedbacks on\n18,973 images with diverse low-level appearance. Moreover, to enable foundation\nmodels to robustly respond to diverse types of questions, we design a\nGPT-participated conversion to process these feedbacks into diverse-format 200K\ninstruction-response pairs. Experimental results indicate that the\n**Q-Instruct** consistently elevates low-level perception and understanding\nabilities across several foundational models. We anticipate that our datasets\ncan pave the way for a future that general intelligence can perceive,\nunderstand low-level visual appearance and evaluate visual quality like a\nhuman. Our dataset, model zoo, and demo is published at:\nhttps://q-future.github.io/Q-Instruct.",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "published": "2023-11-12T09:10:51+00:00",
    "updated": "2023-11-12T09:10:51+00:00",
    "doi": null,
    "comment": "16 pages, 11 figures, page 12-16 as appendix",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2211.11975v1": {
    "id": "http://arxiv.org/abs/2211.11975v1",
    "title": "Pred&Guide: Labeled Target Class Prediction for Guiding Semi-Supervised Domain Adaptation",
    "authors": [
      "Megh Manoj Bhalerao",
      "Anurag Singh",
      "Soma Biswas"
    ],
    "abstract": "Semi-supervised domain adaptation aims to classify data belonging to a target\ndomain by utilizing a related label-rich source domain and very few labeled\nexamples of the target domain. Here, we propose a novel framework, Pred&Guide,\nwhich leverages the inconsistency between the predicted and the actual class\nlabels of the few labeled target examples to effectively guide the domain\nadaptation in a semi-supervised setting. Pred&Guide consists of three stages,\nas follows (1) First, in order to treat all the target samples equally, we\nperform unsupervised domain adaptation coupled with self-training; (2) Second\nis the label prediction stage, where the current model is used to predict the\nlabels of the few labeled target examples, and (3) Finally, the correctness of\nthe label predictions are used to effectively weigh source examples class-wise\nto better guide the domain adaptation process. Extensive experiments show that\nthe proposed Pred&Guide framework achieves state-of-the-art results for two\nlarge-scale benchmark datasets, namely Office-Home and DomainNet.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-11-22T03:21:32+00:00",
    "updated": "2022-11-22T03:21:32+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.06793v2": {
    "id": "http://arxiv.org/abs/2403.06793v2",
    "title": "Boosting Image Restoration via Priors from Pre-trained Models",
    "authors": [
      "Xiaogang Xu",
      "Shu Kong",
      "Tao Hu",
      "Zhe Liu",
      "Hujun Bao"
    ],
    "abstract": "Pre-trained models with large-scale training data, such as CLIP and Stable\nDiffusion, have demonstrated remarkable performance in various high-level\ncomputer vision tasks such as image understanding and generation from language\ndescriptions. Yet, their potential for low-level tasks such as image\nrestoration remains relatively unexplored. In this paper, we explore such\nmodels to enhance image restoration. As off-the-shelf features (OSF) from\npre-trained models do not directly serve image restoration, we propose to learn\nan additional lightweight module called Pre-Train-Guided Refinement Module\n(PTG-RM) to refine restoration results of a target restoration network with\nOSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying\nEnhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention\n(PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations,\nwhile PTG-CSA enhances spatial-channel attention for restoration-related\nlearning. Extensive experiments demonstrate that PTG-RM, with its compact size\n($<$1M parameters), effectively enhances restoration performance of various\nmodels across different tasks, including low-light enhancement, deraining,\ndeblurring, and denoising.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-11T15:11:57+00:00",
    "updated": "2024-03-19T04:46:42+00:00",
    "doi": null,
    "comment": "CVPR2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1602.03483v1": {
    "id": "http://arxiv.org/abs/1602.03483v1",
    "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
    "authors": [
      "Felix Hill",
      "Kyunghyun Cho",
      "Anna Korhonen"
    ],
    "abstract": "Unsupervised methods for learning distributed representations of words are\nubiquitous in today's NLP research, but far less is known about the best ways\nto learn distributed phrase or sentence representations from unlabelled data.\nThis paper is a systematic comparison of models that learn such\nrepresentations. We find that the optimal approach depends critically on the\nintended application. Deeper, more complex models are preferable for\nrepresentations to be used in supervised systems, but shallow log-linear models\nwork best for building representation spaces that can be decoded with simple\nspatial distance metrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to optimise the trade-off between\ntraining time, domain portability and performance.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2016-02-10T18:49:58+00:00",
    "updated": "2016-02-10T18:49:58+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2003.12039v3": {
    "id": "http://arxiv.org/abs/2003.12039v3",
    "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
    "authors": [
      "Zachary Teed",
      "Jia Deng"
    ],
    "abstract": "We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network\narchitecture for optical flow. RAFT extracts per-pixel features, builds\nmulti-scale 4D correlation volumes for all pairs of pixels, and iteratively\nupdates a flow field through a recurrent unit that performs lookups on the\ncorrelation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT\nachieves an F1-all error of 5.10%, a 16% error reduction from the best\npublished result (6.10%). On Sintel (final pass), RAFT obtains an\nend-point-error of 2.855 pixels, a 30% error reduction from the best published\nresult (4.098 pixels). In addition, RAFT has strong cross-dataset\ngeneralization as well as high efficiency in inference time, training speed,\nand parameter count. Code is available at https://github.com/princeton-vl/RAFT.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-03-26T17:12:42+00:00",
    "updated": "2020-08-25T15:49:48+00:00",
    "doi": null,
    "comment": "fixed a formatting issue, Eq 7. no change in content",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2406.10173v2": {
    "id": "http://arxiv.org/abs/2406.10173v2",
    "title": "IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce",
    "authors": [
      "Wenxuan Ding",
      "Weiqi Wang",
      "Sze Heng Douglas Kwok",
      "Minghao Liu",
      "Tianqing Fang",
      "Jiaxin Bai",
      "Xin Liu",
      "Changlong Yu",
      "Zheng Li",
      "Chen Luo",
      "Qingyu Yin",
      "Bing Yin",
      "Junxian He",
      "Yangqiu Song"
    ],
    "abstract": "Enhancing Language Models' (LMs) ability to understand purchase intentions in\nE-commerce scenarios is crucial for their effective assistance in various\ndownstream tasks. However, previous approaches that distill intentions from LMs\noften fail to generate meaningful and human-centric intentions applicable in\nreal-world E-commerce contexts. This raises concerns about the true\ncomprehension and utilization of purchase intentions by LMs. In this paper, we\npresent IntentionQA, a double-task multiple-choice question answering benchmark\nto evaluate LMs' comprehension of purchase intentions in E-commerce.\nSpecifically, LMs are tasked to infer intentions based on purchased products\nand utilize them to predict additional purchases. IntentionQA consists of 4,360\ncarefully curated problems across three difficulty levels, constructed using an\nautomated pipeline to ensure scalability on large E-commerce platforms. Human\nevaluations demonstrate the high quality and low false-negative rate of our\nbenchmark. Extensive experiments across 19 language models show that they still\nstruggle with certain scenarios, such as understanding products and intentions\naccurately, jointly reasoning with products and intentions, and more, in which\nthey fall far behind human performances. Our code and data are publicly\navailable at https://github.com/HKUST-KnowComp/IntentionQA.",
    "categories": [
      "cs.CL"
    ],
    "published": "2024-06-14T16:51:21+00:00",
    "updated": "2024-09-30T03:49:02+00:00",
    "doi": null,
    "comment": "Findings of EMNLP 2024",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2308.07317v2": {
    "id": "http://arxiv.org/abs/2308.07317v2",
    "title": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs",
    "authors": [
      "Ariel N. Lee",
      "Cole J. Hunter",
      "Nataniel Ruiz"
    ],
    "abstract": "We present $\\textbf{Platypus}$, a family of fine-tuned and merged Large\nLanguage Models (LLMs) that achieves the strongest performance and currently\nstands at first place in HuggingFace's Open LLM Leaderboard as of the release\ndate of this work. In this work we describe (1) our curated dataset\n$\\textbf{Open-Platypus}$, that is a subset of other open datasets and which\n$\\textit{we release to the public}$ (2) our process of fine-tuning and merging\nLoRA modules in order to conserve the strong prior of pretrained LLMs, while\nbringing specific domain knowledge to the surface (3) our efforts in checking\nfor test data leaks and contamination in the training data, which can inform\nfuture research. Specifically, the Platypus family achieves strong performance\nin quantitative LLM metrics across model sizes, topping the global Open LLM\nleaderboard while using just a fraction of the fine-tuning data and overall\ncompute that are required for other state-of-the-art fine-tuned LLMs. In\nparticular, a 13B Platypus model can be trained on $\\textit{a single}$ A100 GPU\nusing 25k questions in 5 hours. This is a testament of the quality of our\nOpen-Platypus dataset, and opens opportunities for more improvements in the\nfield. Project page: https://platypus-llm.github.io",
    "categories": [
      "cs.CL"
    ],
    "published": "2023-08-14T17:59:56+00:00",
    "updated": "2024-03-14T20:56:23+00:00",
    "doi": null,
    "comment": "Workshop on Instruction Tuning and Instruction Following at NeurIPS\n  2023",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2211.01324v5": {
    "id": "http://arxiv.org/abs/2211.01324v5",
    "title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers",
    "authors": [
      "Yogesh Balaji",
      "Seungjun Nah",
      "Xun Huang",
      "Arash Vahdat",
      "Jiaming Song",
      "Qinsheng Zhang",
      "Karsten Kreis",
      "Miika Aittala",
      "Timo Aila",
      "Samuli Laine",
      "Bryan Catanzaro",
      "Tero Karras",
      "Ming-Yu Liu"
    ],
    "abstract": "Large-scale diffusion-based generative models have led to breakthroughs in\ntext-conditioned high-resolution image synthesis. Starting from random noise,\nsuch text-to-image diffusion models gradually synthesize images in an iterative\nfashion while conditioning on text prompts. We find that their synthesis\nbehavior qualitatively changes throughout this process: Early in sampling,\ngeneration strongly relies on the text prompt to generate text-aligned content,\nwhile later, the text conditioning is almost entirely ignored. This suggests\nthat sharing model parameters throughout the entire generation process may not\nbe ideal. Therefore, in contrast to existing works, we propose to train an\nensemble of text-to-image diffusion models specialized for different synthesis\nstages. To maintain training efficiency, we initially train a single model,\nwhich is then split into specialized models that are trained for the specific\nstages of the iterative generation process. Our ensemble of diffusion models,\ncalled eDiff-I, results in improved text alignment while maintaining the same\ninference computation cost and preserving high visual quality, outperforming\nprevious large-scale text-to-image diffusion models on the standard benchmark.\nIn addition, we train our model to exploit a variety of embeddings for\nconditioning, including the T5 text, CLIP text, and CLIP image embeddings. We\nshow that these different embeddings lead to different behaviors. Notably, the\nCLIP image embedding allows an intuitive way of transferring the style of a\nreference image to the target text-to-image output. Lastly, we show a technique\nthat enables eDiff-I's \"paint-with-words\" capability. A user can select the\nword in the input text and paint it in a canvas to control the output, which is\nvery handy for crafting the desired image in mind. The project page is\navailable at https://deepimagination.cc/eDiff-I/",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2022-11-02T17:43:04+00:00",
    "updated": "2023-03-14T00:22:14+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2409.10476v1": {
    "id": "http://arxiv.org/abs/2409.10476v1",
    "title": "SimInversion: A Simple Framework for Inversion-Based Text-to-Image Editing",
    "authors": [
      "Qi Qian",
      "Haiyang Xu",
      "Ming Yan",
      "Juhua Hu"
    ],
    "abstract": "Diffusion models demonstrate impressive image generation performance with\ntext guidance. Inspired by the learning process of diffusion, existing images\ncan be edited according to text by DDIM inversion. However, the vanilla DDIM\ninversion is not optimized for classifier-free guidance and the accumulated\nerror will result in the undesired performance. While many algorithms are\ndeveloped to improve the framework of DDIM inversion for editing, in this work,\nwe investigate the approximation error in DDIM inversion and propose to\ndisentangle the guidance scale for the source and target branches to reduce the\nerror while keeping the original framework. Moreover, a better guidance scale\n(i.e., 0.5) than default settings can be derived theoretically. Experiments on\nPIE-Bench show that our proposal can improve the performance of DDIM inversion\ndramatically without sacrificing efficiency.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-09-16T17:10:50+00:00",
    "updated": "2024-09-16T17:10:50+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2402.08939v3": {
    "id": "http://arxiv.org/abs/2402.08939v3",
    "title": "Premise Order Matters in Reasoning with Large Language Models",
    "authors": [
      "Xinyun Chen",
      "Ryan A. Chi",
      "Xuezhi Wang",
      "Denny Zhou"
    ],
    "abstract": "Large language models (LLMs) have accomplished remarkable reasoning\nperformance in various domains. However, in the domain of reasoning tasks, we\ndiscover a frailty: LLMs are surprisingly brittle to the ordering of the\npremises, despite the fact that such ordering does not alter the underlying\ntask. In particular, we observe that LLMs achieve the best performance when the\npremise order aligns with the context required in intermediate reasoning steps.\nFor example, in deductive reasoning tasks, presenting the premises in the same\norder as the ground truth proof in the prompt (as opposed to random ordering)\ndrastically increases the model's accuracy. We first examine the effect of\npremise ordering on deductive reasoning on a variety of LLMs, and our\nevaluation shows that permuting the premise order can cause a performance drop\nof over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to\nexamine the ordering effect for mathematical problem-solving, and we again\nobserve a significant drop in accuracy, relative to the original GSM8K\nbenchmark.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-14T04:50:18+00:00",
    "updated": "2024-05-28T04:32:09+00:00",
    "doi": null,
    "comment": "Published at ICML 2024. Xinyun and Ryan contribute equally",
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2402.07310v2": {
    "id": "http://arxiv.org/abs/2402.07310v2",
    "title": "BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis",
    "authors": [
      "Leandro A. Passos",
      "Douglas Rodrigues",
      "Danilo Jodas",
      "Kelton A. P. Costa",
      "Ahsan Adeel",
      "Jo\u00e3o Paulo Papa"
    ],
    "abstract": "This paper presents BioNeRF, a biologically plausible architecture that\nmodels scenes in a 3D representation and synthesizes new views through radiance\nfields. Since NeRF relies on the network weights to store the scene's\n3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism\nthat fuses inputs from multiple sources into a memory-like structure, improving\nthe storing capacity and extracting more intrinsic and correlated information.\nBioNeRF also mimics a behavior observed in pyramidal cells concerning\ncontextual information, in which the memory is provided as the context and\ncombined with the inputs of two subsequent neural models, one responsible for\nproducing the volumetric densities and the other the colors used to render the\nscene. Experimental results show that BioNeRF outperforms state-of-the-art\nresults concerning a quality measure that encodes human perception in two\ndatasets: real-world images and synthetic data.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-02-11T21:16:42+00:00",
    "updated": "2024-03-25T12:58:45+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.06529v1": {
    "id": "http://arxiv.org/abs/2210.06529v1",
    "title": "Prepended Domain Transformer: Heterogeneous Face Recognition without Bells and Whistles",
    "authors": [
      "Anjith George",
      "Amir Mohammadi",
      "Sebastien Marcel"
    ],
    "abstract": "Heterogeneous Face Recognition (HFR) refers to matching face images captured\nin different domains, such as thermal to visible images (VIS), sketches to\nvisible images, near-infrared to visible, and so on. This is particularly\nuseful in matching visible spectrum images to images captured from other\nmodalities. Though highly useful, HFR is challenging because of the domain gap\nbetween the source and target domain. Often, large-scale paired heterogeneous\nface image datasets are absent, preventing training models specifically for the\nheterogeneous task. In this work, we propose a surprisingly simple, yet, very\neffective method for matching face images across different sensing modalities.\nThe core idea of the proposed approach is to add a novel neural network block\ncalled Prepended Domain Transformer (PDT) in front of a pre-trained face\nrecognition (FR) model to address the domain gap. Retraining this new block\nwith few paired samples in a contrastive learning setup was enough to achieve\nstate-of-the-art performance in many HFR benchmarks. The PDT blocks can be\nretrained for several source-target combinations using the proposed general\nframework. The proposed approach is architecture agnostic, meaning they can be\nadded to any pre-trained FR models. Further, the approach is modular and the\nnew block can be trained with a minimal set of paired samples, making it much\neasier for practical deployment. The source code and protocols will be made\navailable publicly.",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "published": "2022-10-12T18:54:57+00:00",
    "updated": "2022-10-12T18:54:57+00:00",
    "doi": null,
    "comment": "16 pages. Accepted for publication in IEEE TIFS",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1712.09381v4": {
    "id": "http://arxiv.org/abs/1712.09381v4",
    "title": "RLlib: Abstractions for Distributed Reinforcement Learning",
    "authors": [
      "Eric Liang",
      "Richard Liaw",
      "Philipp Moritz",
      "Robert Nishihara",
      "Roy Fox",
      "Ken Goldberg",
      "Joseph E. Gonzalez",
      "Michael I. Jordan",
      "Ion Stoica"
    ],
    "abstract": "Reinforcement learning (RL) algorithms involve the deep nesting of highly\nirregular computation patterns, each of which typically exhibits opportunities\nfor distributed computation. We argue for distributing RL components in a\ncomposable way by adapting algorithms for top-down hierarchical control,\nthereby encapsulating parallelism and resource requirements within\nshort-running compute tasks. We demonstrate the benefits of this principle\nthrough RLlib: a library that provides scalable software primitives for RL.\nThese primitives enable a broad range of algorithms to be implemented with high\nperformance, scalability, and substantial code reuse. RLlib is available at\nhttps://rllib.io/.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "published": "2017-12-26T19:43:59+00:00",
    "updated": "2018-06-29T00:19:24+00:00",
    "doi": null,
    "comment": "Published in the International Conference on Machine Learning (ICML\n  2018), 10 pages",
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2209.15172v1": {
    "id": "http://arxiv.org/abs/2209.15172v1",
    "title": "Understanding Pure CLIP Guidance for Voxel Grid NeRF Models",
    "authors": [
      "Han-Hung Lee",
      "Angel X. Chang"
    ],
    "abstract": "We explore the task of text to 3D object generation using CLIP. Specifically,\nwe use CLIP for guidance without access to any datasets, a setting we refer to\nas pure CLIP guidance. While prior work has adopted this setting, there is no\nsystematic study of mechanics for preventing adversarial generations within\nCLIP. We illustrate how different image-based augmentations prevent the\nadversarial generation problem, and how the generated results are impacted. We\ntest different CLIP model architectures and show that ensembling different\nmodels for guidance can prevent adversarial generations within bigger models\nand generate sharper results. Furthermore, we implement an implicit voxel grid\nmodel to show how neural networks provide an additional layer of\nregularization, resulting in better geometrical structure and coherency of\ngenerated objects. Compared to prior work, we achieve more coherent results\nwith higher memory efficiency and faster training speeds.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2022-09-30T01:47:47+00:00",
    "updated": "2022-09-30T01:47:47+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.16292v2": {
    "id": "http://arxiv.org/abs/2403.16292v2",
    "title": "latentSplat: Autoencoding Variational Gaussians for Fast Generalizable 3D Reconstruction",
    "authors": [
      "Christopher Wewer",
      "Kevin Raj",
      "Eddy Ilg",
      "Bernt Schiele",
      "Jan Eric Lenssen"
    ],
    "abstract": "We present latentSplat, a method to predict semantic Gaussians in a 3D latent\nspace that can be splatted and decoded by a light-weight generative 2D\narchitecture. Existing methods for generalizable 3D reconstruction either do\nnot scale to large scenes and resolutions, or are limited to interpolation of\nclose input views. latentSplat combines the strengths of regression-based and\ngenerative approaches while being trained purely on readily available real\nvideo data. The core of our method are variational 3D Gaussians, a\nrepresentation that efficiently encodes varying uncertainty within a latent\nspace consisting of 3D feature Gaussians. From these Gaussians, specific\ninstances can be sampled and rendered via efficient splatting and a fast,\ngenerative decoder. We show that latentSplat outperforms previous works in\nreconstruction quality and generalization, while being fast and scalable to\nhigh-resolution data.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-24T20:48:36+00:00",
    "updated": "2024-07-30T12:12:00+00:00",
    "doi": null,
    "comment": "Project website: https://geometric-rl.mpi-inf.mpg.de/latentsplat/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.11764v2": {
    "id": "http://arxiv.org/abs/2401.11764v2",
    "title": "Identity-Driven Multimedia Forgery Detection via Reference Assistance",
    "authors": [
      "Junhao Xu",
      "Jingjing Chen",
      "Xue Song",
      "Feng Han",
      "Haijun Shan",
      "Yugang Jiang"
    ],
    "abstract": "Recent advancements in \"deepfake\" techniques have paved the way for\ngenerating various media forgeries. In response to the potential hazards of\nthese media forgeries, many researchers engage in exploring detection methods,\nincreasing the demand for high-quality media forgery datasets. Despite this,\nexisting datasets have certain limitations. Firstly, most datasets focus on\nmanipulating visual modality and usually lack diversity, as only a few forgery\napproaches are considered. Secondly, the quality of media is often inadequate\nin clarity and naturalness. Meanwhile, the size of the dataset is also limited.\nThirdly, it is commonly observed that real-world forgeries are motivated by\nidentity, yet the identity information of the individuals portrayed in these\nforgeries within existing datasets remains under-explored. For detection,\nidentity information could be an essential clue to boost performance. Moreover,\nofficial media concerning relevant identities on the Internet can serve as\nprior knowledge, aiding both the audience and forgery detectors in determining\nthe true identity. Therefore, we propose an identity-driven multimedia forgery\ndataset, IDForge, which contains 249,138 video shots sourced from 324 wild\nvideos of 54 celebrities collected from the Internet. The fake video shots\ninvolve 9 types of manipulation across visual, audio, and textual modalities.\nAdditionally, IDForge provides extra 214,438 real video shots as a reference\nset for the 54 celebrities. Correspondingly, we propose the Reference-assisted\nMultimodal Forgery Detection Network (R-MFDN), aiming at the detection of\ndeepfake videos. Through extensive experiments on the proposed dataset, we\ndemonstrate the effectiveness of R-MFDN on the multimedia detection task.",
    "categories": [
      "cs.MM"
    ],
    "published": "2024-01-22T08:59:09+00:00",
    "updated": "2024-08-07T15:31:39+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.MM"
  },
  "1706.09147v2": {
    "id": "http://arxiv.org/abs/1706.09147v2",
    "title": "Named Entity Disambiguation for Noisy Text",
    "authors": [
      "Yotam Eshel",
      "Noam Cohen",
      "Kira Radinsky",
      "Shaul Markovitch",
      "Ikuya Yamada",
      "Omer Levy"
    ],
    "abstract": "We address the task of Named Entity Disambiguation (NED) for noisy text. We\npresent WikilinksNED, a large-scale NED dataset of text fragments from the web,\nwhich is significantly noisier and more challenging than existing news-based\ndatasets. To capture the limited and noisy local context surrounding each\nmention, we design a neural model and train it with a novel method for sampling\ninformative negative examples. We also describe a new way of initializing word\nand entity embeddings that significantly improves performance. Our model\nsignificantly outperforms existing state-of-the-art methods on WikilinksNED\nwhile achieving comparable performance on a smaller newswire dataset.",
    "categories": [
      "cs.CL"
    ],
    "published": "2017-06-28T07:26:55+00:00",
    "updated": "2017-07-01T22:43:09+00:00",
    "doi": null,
    "comment": "Accepted to CoNLL 2017",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2208.03354v1": {
    "id": "http://arxiv.org/abs/2208.03354v1",
    "title": "A Sketch Is Worth a Thousand Words: Image Retrieval with Text and Sketch",
    "authors": [
      "Patsorn Sangkloy",
      "Wittawat Jitkrittum",
      "Diyi Yang",
      "James Hays"
    ],
    "abstract": "We address the problem of retrieving images with both a sketch and a text\nquery. We present TASK-former (Text And SKetch transformer), an end-to-end\ntrainable model for image retrieval using a text description and a sketch as\ninput. We argue that both input modalities complement each other in a manner\nthat cannot be achieved easily by either one alone. TASK-former follows the\nlate-fusion dual-encoder approach, similar to CLIP, which allows efficient and\nscalable retrieval since the retrieval set can be indexed independently of the\nqueries. We empirically demonstrate that using an input sketch (even a poorly\ndrawn one) in addition to text considerably increases retrieval recall compared\nto traditional text-based image retrieval. To evaluate our approach, we collect\n5,000 hand-drawn sketches for images in the test set of the COCO dataset. The\ncollected sketches are available a https://janesjanes.github.io/tsbir/.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2022-08-05T18:43:37+00:00",
    "updated": "2022-08-05T18:43:37+00:00",
    "doi": null,
    "comment": "ECCV 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2406.01388v2": {
    "id": "http://arxiv.org/abs/2406.01388v2",
    "title": "AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image Generation",
    "authors": [
      "Junhao Cheng",
      "Xi Lu",
      "Hanhui Li",
      "Khun Loun Zai",
      "Baiqiao Yin",
      "Yuhao Cheng",
      "Yiqiang Yan",
      "Xiaodan Liang"
    ],
    "abstract": "As cutting-edge Text-to-Image (T2I) generation models already excel at\nproducing remarkable single images, an even more challenging task, i.e.,\nmulti-turn interactive image generation begins to attract the attention of\nrelated research communities. This task requires models to interact with users\nover multiple turns to generate a coherent sequence of images. However, since\nusers may switch subjects frequently, current efforts struggle to maintain\nsubject consistency while generating diverse images. To address this issue, we\nintroduce a training-free multi-agent framework called AutoStudio. AutoStudio\nemploys three agents based on large language models (LLMs) to handle\ninteractions, along with a stable diffusion (SD) based agent for generating\nhigh-quality images. Specifically, AutoStudio consists of (i) a subject manager\nto interpret interaction dialogues and manage the context of each subject, (ii)\na layout generator to generate fine-grained bounding boxes to control subject\nlocations, (iii) a supervisor to provide suggestions for layout refinements,\nand (iv) a drawer to complete image generation. Furthermore, we introduce a\nParallel-UNet to replace the original UNet in the drawer, which employs two\nparallel cross-attention modules for exploiting subject-aware features. We also\nintroduce a subject-initialized generation method to better preserve small\nsubjects. Our AutoStudio hereby can generate a sequence of multi-subject images\ninteractively and consistently. Extensive experiments on the public CMIGBench\nbenchmark and human evaluations show that AutoStudio maintains multi-subject\nconsistency across multiple turns well, and it also raises the state-of-the-art\nperformance by 13.65% in average Frechet Inception Distance and 2.83% in\naverage character-character similarity.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-06-03T14:51:24+00:00",
    "updated": "2024-06-11T02:29:53+00:00",
    "doi": null,
    "comment": "Multi-turn interactive image generation",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.03977v2": {
    "id": "http://arxiv.org/abs/2308.03977v2",
    "title": "PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning",
    "authors": [
      "Florian Bordes",
      "Shashank Shekhar",
      "Mark Ibrahim",
      "Diane Bouchacourt",
      "Pascal Vincent",
      "Ari S. Morcos"
    ],
    "abstract": "Synthetic image datasets offer unmatched advantages for designing and\nevaluating deep neural networks: they make it possible to (i) render as many\ndata samples as needed, (ii) precisely control each scene and yield granular\nground truth labels (and captions), (iii) precisely control distribution shifts\nbetween training and testing to isolate variables of interest for sound\nexperimentation. Despite such promise, the use of synthetic image data is still\nlimited -- and often played down -- mainly due to their lack of realism. Most\nworks therefore rely on datasets of real images, which have often been scraped\nfrom public images on the internet, and may have issues with regards to\nprivacy, bias, and copyright, while offering little control over how objects\nprecisely appear. In this work, we present a path to democratize the use of\nphotorealistic synthetic data: we develop a new generation of interactive\nenvironments for representation learning research, that offer both\ncontrollability and realism. We use the Unreal Engine, a powerful game engine\nwell known in the entertainment industry, to produce PUG (Photorealistic Unreal\nGraphics) environments and datasets for representation learning. In this paper,\nwe demonstrate the potential of PUG to enable more rigorous evaluations of\nvision models.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-08-08T01:33:13+00:00",
    "updated": "2023-12-13T01:44:58+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2303.14613v4": {
    "id": "http://arxiv.org/abs/2303.14613v4",
    "title": "GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents",
    "authors": [
      "Tenglong Ao",
      "Zeyi Zhang",
      "Libin Liu"
    ],
    "abstract": "The automatic generation of stylized co-speech gestures has recently received\nincreasing attention. Previous systems typically allow style control via\npredefined text labels or example motion clips, which are often not flexible\nenough to convey user intent accurately. In this work, we present\nGestureDiffuCLIP, a neural network framework for synthesizing realistic,\nstylized co-speech gestures with flexible style control. We leverage the power\nof the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and\npresent a novel CLIP-guided mechanism that extracts efficient style\nrepresentations from multiple input modalities, such as a piece of text, an\nexample motion clip, or a video. Our system learns a latent diffusion model to\ngenerate high-quality gestures and infuses the CLIP representations of style\ninto the generator via an adaptive instance normalization (AdaIN) layer. We\nfurther devise a gesture-transcript alignment mechanism that ensures a\nsemantically correct gesture generation based on contrastive learning. Our\nsystem can also be extended to allow fine-grained style control of individual\nbody parts. We demonstrate an extensive set of examples showing the flexibility\nand generalizability of our model to a variety of style descriptions. In a user\nstudy, we show that our system outperforms the state-of-the-art approaches\nregarding human likeness, appropriateness, and style correctness.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2023-03-26T03:35:46+00:00",
    "updated": "2023-10-16T05:37:06+00:00",
    "doi": null,
    "comment": "SIGGRAPH 2023 (Journal Track); Project Page:\n  https://pku-mocca.github.io/GestureDiffuCLIP-Page/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2106.04941v1": {
    "id": "http://arxiv.org/abs/2106.04941v1",
    "title": "Symmetric Spaces for Graph Embeddings: A Finsler-Riemannian Approach",
    "authors": [
      "Federico L\u00f3pez",
      "Beatrice Pozzetti",
      "Steve Trettel",
      "Michael Strube",
      "Anna Wienhard"
    ],
    "abstract": "Learning faithful graph representations as sets of vertex embeddings has\nbecome a fundamental intermediary step in a wide range of machine learning\napplications. We propose the systematic use of symmetric spaces in\nrepresentation learning, a class encompassing many of the previously used\nembedding targets. This enables us to introduce a new method, the use of\nFinsler metrics integrated in a Riemannian optimization scheme, that better\nadapts to dissimilar structures in the graph. We develop a tool to analyze the\nembeddings and infer structural properties of the data sets. For\nimplementation, we choose Siegel spaces, a versatile family of symmetric\nspaces. Our approach outperforms competitive baselines for graph reconstruction\ntasks on various synthetic and real-world datasets. We further demonstrate its\napplicability on two downstream tasks, recommender systems and node\nclassification.",
    "categories": [
      "cs.LG",
      "cs.CG",
      "I.2"
    ],
    "published": "2021-06-09T09:33:33+00:00",
    "updated": "2021-06-09T09:33:33+00:00",
    "doi": null,
    "comment": "28 pages. Accepted at ICML 2021",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2405.14691v1": {
    "id": "http://arxiv.org/abs/2405.14691v1",
    "title": "CityGPT: Towards Urban IoT Learning, Analysis and Interaction with Multi-Agent System",
    "authors": [
      "Qinghua Guan",
      "Jinhui Ouyang",
      "Di Wu",
      "Weiren Yu"
    ],
    "abstract": "The spatiotemporal data generated by massive sensors in the Internet of\nThings (IoT) is extremely dynamic, heterogeneous, large scale and\ntime-dependent. It poses great challenges (e.g. accuracy, reliability, and\nstability) in real-time analysis and decision making for different IoT\napplications. The complexity of IoT data prevents the common people from\ngaining a deeper understanding of it. Agentized systems help address the lack\nof data insight for the common people. We propose a generic framework, namely\nCityGPT, to facilitate the learning and analysis of IoT time series with an\nend-to-end paradigm. CityGPT employs three agents to accomplish the\nspatiotemporal analysis of IoT data. The requirement agent facilitates user\ninputs based on natural language. Then, the analysis tasks are decomposed into\ntemporal and spatial analysis processes, completed by corresponding data\nanalysis agents (temporal and spatial agents). Finally, the spatiotemporal\nfusion agent visualizes the system's analysis results by receiving analysis\nresults from data analysis agents and invoking sub-visualization agents, and\ncan provide corresponding textual descriptions based on user demands. To\nincrease the insight for common people using our framework, we have agnentized\nthe framework, facilitated by a large language model (LLM), to increase the\ndata comprehensibility. Our evaluation results on real-world data with\ndifferent time dependencies show that the CityGPT framework can guarantee\nrobust performance in IoT computing.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2024-05-23T15:27:18+00:00",
    "updated": "2024-05-23T15:27:18+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2403.11194v1": {
    "id": "http://arxiv.org/abs/2403.11194v1",
    "title": "MaskDiffusion: Exploiting Pre-trained Diffusion Models for Semantic Segmentation",
    "authors": [
      "Yasufumi Kawano",
      "Yoshimitsu Aoki"
    ],
    "abstract": "Semantic segmentation is essential in computer vision for various\napplications, yet traditional approaches face significant challenges, including\nthe high cost of annotation and extensive training for supervised learning.\nAdditionally, due to the limited predefined categories in supervised learning,\nmodels typically struggle with infrequent classes and are unable to predict\nnovel classes. To address these limitations, we propose MaskDiffusion, an\ninnovative approach that leverages pretrained frozen Stable Diffusion to\nachieve open-vocabulary semantic segmentation without the need for additional\ntraining or annotation, leading to improved performance compared to similar\nmethods. We also demonstrate the superior performance of MaskDiffusion in\nhandling open vocabularies, including fine-grained and proper noun-based\ncategories, thus expanding the scope of segmentation applications. Overall, our\nMaskDiffusion shows significant qualitative and quantitative improvements in\ncontrast to other comparable unsupervised segmentation methods, i.e. on the\nPotsdam dataset (+10.5 mIoU compared to GEM) and COCO-Stuff (+14.8 mIoU\ncompared to DiffSeg). All code and data will be released at\nhttps://github.com/Valkyrja3607/MaskDiffusion.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-17T12:40:49+00:00",
    "updated": "2024-03-17T12:40:49+00:00",
    "doi": null,
    "comment": "19 pages",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2112.03517v1": {
    "id": "http://arxiv.org/abs/2112.03517v1",
    "title": "CG-NeRF: Conditional Generative Neural Radiance Fields",
    "authors": [
      "Kyungmin Jo",
      "Gyumin Shim",
      "Sanghun Jung",
      "Soyoung Yang",
      "Jaegul Choo"
    ],
    "abstract": "While recent NeRF-based generative models achieve the generation of diverse\n3D-aware images, these approaches have limitations when generating images that\ncontain user-specified characteristics. In this paper, we propose a novel\nmodel, referred to as the conditional generative neural radiance fields\n(CG-NeRF), which can generate multi-view images reflecting extra input\nconditions such as images or texts. While preserving the common characteristics\nof a given input condition, the proposed model generates diverse images in fine\ndetail. We propose: 1) a novel unified architecture which disentangles the\nshape and appearance from a condition given in various forms and 2) the\npose-consistent diversity loss for generating multimodal outputs while\nmaintaining consistency of the view. Experimental results show that the\nproposed method maintains consistent image quality on various condition types\nand achieves superior fidelity and diversity compared to existing NeRF-based\ngenerative models.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2021-12-07T05:57:58+00:00",
    "updated": "2021-12-07T05:57:58+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2011.12438v1": {
    "id": "http://arxiv.org/abs/2011.12438v1",
    "title": "Continuous Surface Embeddings",
    "authors": [
      "Natalia Neverova",
      "David Novotny",
      "Vasil Khalidov",
      "Marc Szafraniec",
      "Patrick Labatut",
      "Andrea Vedaldi"
    ],
    "abstract": "In this work, we focus on the task of learning and representing dense\ncorrespondences in deformable object categories. While this problem has been\nconsidered before, solutions so far have been rather ad-hoc for specific object\ntypes (i.e., humans), often with significant manual work involved. However,\nscaling the geometry understanding to all objects in nature requires more\nautomated approaches that can also express correspondences between related, but\ngeometrically different objects. To this end, we propose a new, learnable\nimage-based representation of dense correspondences. Our model predicts, for\neach pixel in a 2D image, an embedding vector of the corresponding vertex in\nthe object mesh, therefore establishing dense correspondences between image\npixels and 3D object geometry. We demonstrate that the proposed approach\nperforms on par or better than the state-of-the-art methods for dense pose\nestimation for humans, while being conceptually simpler. We also collect a new\nin-the-wild dataset of dense correspondences for animal classes and demonstrate\nthat our framework scales naturally to the new deformable object categories.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-11-24T22:52:15+00:00",
    "updated": "2020-11-24T22:52:15+00:00",
    "doi": null,
    "comment": "NeurIPS, 2020",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1812.09902v2": {
    "id": "http://arxiv.org/abs/1812.09902v2",
    "title": "Invariant and Equivariant Graph Networks",
    "authors": [
      "Haggai Maron",
      "Heli Ben-Hamu",
      "Nadav Shamir",
      "Yaron Lipman"
    ],
    "abstract": "Invariant and equivariant networks have been successfully used for learning\nimages, sets, point clouds, and graphs. A basic challenge in developing such\nnetworks is finding the maximal collection of invariant and equivariant linear\nlayers. Although this question is answered for the first three examples (for\npopular transformations, at-least), a full characterization of invariant and\nequivariant linear layers for graphs is not known.\n  In this paper we provide a characterization of all permutation invariant and\nequivariant linear layers for (hyper-)graph data, and show that their\ndimension, in case of edge-value graph data, is 2 and 15, respectively. More\ngenerally, for graph data defined on k-tuples of nodes, the dimension is the\nk-th and 2k-th Bell numbers. Orthogonal bases for the layers are computed,\nincluding generalization to multi-graph data. The constant number of basis\nelements and their characteristics allow successfully applying the networks to\ndifferent size graphs. From the theoretical point of view, our results\ngeneralize and unify recent advancement in equivariant deep learning. In\nparticular, we show that our model is capable of approximating any message\npassing neural network\n  Applying these new linear layers in a simple deep neural network framework is\nshown to achieve comparable results to state-of-the-art and to have better\nexpressivity than previous invariant and equivariant bases.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2018-12-24T11:52:27+00:00",
    "updated": "2019-04-30T06:01:53+00:00",
    "doi": null,
    "comment": "ICLR 2019",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2408.04803v1": {
    "id": "http://arxiv.org/abs/2408.04803v1",
    "title": "FewShotNeRF: Meta-Learning-based Novel View Synthesis for Rapid Scene-Specific Adaptation",
    "authors": [
      "Piraveen Sivakumar",
      "Paul Janson",
      "Jathushan Rajasegaran",
      "Thanuja Ambegoda"
    ],
    "abstract": "In this paper, we address the challenge of generating novel views of\nreal-world objects with limited multi-view images through our proposed\napproach, FewShotNeRF. Our method utilizes meta-learning to acquire optimal\ninitialization, facilitating rapid adaptation of a Neural Radiance Field (NeRF)\nto specific scenes. The focus of our meta-learning process is on capturing\nshared geometry and textures within a category, embedded in the weight\ninitialization. This approach expedites the learning process of NeRFs and\nleverages recent advancements in positional encodings to reduce the time\nrequired for fitting a NeRF to a scene, thereby accelerating the inner loop\noptimization of meta-learning. Notably, our method enables meta-learning on a\nlarge number of 3D scenes to establish a robust 3D prior for various\ncategories. Through extensive evaluations on the Common Objects in 3D open\nsource dataset, we empirically demonstrate the efficacy and potential of\nmeta-learning in generating high-quality novel views of objects.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-08-09T01:13:14+00:00",
    "updated": "2024-08-09T01:13:14+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1810.03944v3": {
    "id": "http://arxiv.org/abs/1810.03944v3",
    "title": "Transfer Metric Learning: Algorithms, Applications and Outlooks",
    "authors": [
      "Yong Luo",
      "Yonggang Wen",
      "Ling-Yu Duan",
      "Dacheng Tao"
    ],
    "abstract": "Distance metric learning (DML) aims to find an appropriate way to reveal the\nunderlying data relationship. It is critical in many machine learning, pattern\nrecognition and data mining algorithms, and usually require large amount of\nlabel information (such as class labels or pair/triplet constraints) to achieve\nsatisfactory performance. However, the label information may be insufficient in\nreal-world applications due to the high-labeling cost, and DML may fail in this\ncase. Transfer metric learning (TML) is able to mitigate this issue for DML in\nthe domain of interest (target domain) by leveraging knowledge/information from\nother related domains (source domains). Although achieved a certain level of\ndevelopment, TML has limited success in various aspects such as selective\ntransfer, theoretical understanding, handling complex data, big data and\nextreme cases. In this survey, we present a systematic review of the TML\nliterature. In particular, we group TML into different categories according to\ndifferent settings and metric transfer strategies, such as direct metric\napproximation, subspace approximation, distance approximation, and distribution\napproximation. A summarization and insightful discussion of the various TML\napproaches and their applications will be presented. Finally, we indicate some\nchallenges and provide possible future directions.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2018-10-09T12:46:20+00:00",
    "updated": "2018-11-12T06:53:11+00:00",
    "doi": null,
    "comment": "14 pages, 5 figures",
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2304.12015v2": {
    "id": "http://arxiv.org/abs/2304.12015v2",
    "title": "ITER: Iterative Neural Repair for Multi-Location Patches",
    "authors": [
      "He Ye",
      "Martin Monperrus"
    ],
    "abstract": "Automated program repair (APR) has achieved promising results, especially\nusing neural networks. Yet, the overwhelming majority of patches produced by\nAPR tools are confined to one single location. When looking at the patches\nproduced with neural repair, most of them fail to compile, while a few\nuncompilable ones go in the right direction. In both cases, the fundamental\nproblem is to ignore the potential of partial patches. In this paper, we\npropose an iterative program repair paradigm called ITER founded on the concept\nof improving partial patches until they become plausible and correct. First,\nITER iteratively improves partial single-location patches by fixing compilation\nerrors and further refining the previously generated code. Second, ITER\niteratively improves partial patches to construct multi-location patches, with\nfault localization re-execution. ITER is implemented for Java based on\nbattle-proven deep neural networks and code representation. ITER is evaluated\non 476 bugs from 10 open-source projects in Defects4J 2.0. ITER succeeds in\nrepairing 15.5% of them, including 9 uniquely repaired multi-location bugs.",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2023-04-24T11:32:02+00:00",
    "updated": "2024-04-23T19:49:42+00:00",
    "doi": "10.1145/3597503.3623337",
    "comment": null,
    "journal_ref": "Proceedings of International Conference on Software Engineering,\n  2024",
    "primary_category": "cs.SE"
  },
  "2005.07683v2": {
    "id": "http://arxiv.org/abs/2005.07683v2",
    "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning",
    "authors": [
      "Victor Sanh",
      "Thomas Wolf",
      "Alexander M. Rush"
    ],
    "abstract": "Magnitude pruning is a widely used strategy for reducing model size in pure\nsupervised learning; however, it is less effective in the transfer learning\nregime that has become standard for state-of-the-art natural language\nprocessing applications. We propose the use of movement pruning, a simple,\ndeterministic first-order weight pruning method that is more adaptive to\npretrained model fine-tuning. We give mathematical foundations to the method\nand compare it to existing zeroth- and first-order pruning methods. Experiments\nshow that when pruning large pretrained language models, movement pruning shows\nsignificant improvements in high-sparsity regimes. When combined with\ndistillation, the approach achieves minimal accuracy loss with down to only 3%\nof the model parameters.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2020-05-15T17:54:15+00:00",
    "updated": "2020-10-23T16:14:58+00:00",
    "doi": null,
    "comment": "14 pages, 6 figures, 3 tables. Published at NeurIPS2020. Code:\n  \\url{huggingface.co/mvp}",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2012.07810v1": {
    "id": "http://arxiv.org/abs/2012.07810v1",
    "title": "Real-Time High-Resolution Background Matting",
    "authors": [
      "Shanchuan Lin",
      "Andrey Ryabtsev",
      "Soumyadip Sengupta",
      "Brian Curless",
      "Steve Seitz",
      "Ira Kemelmacher-Shlizerman"
    ],
    "abstract": "We introduce a real-time, high-resolution background replacement technique\nwhich operates at 30fps in 4K resolution, and 60fps for HD on a modern GPU. Our\ntechnique is based on background matting, where an additional frame of the\nbackground is captured and used in recovering the alpha matte and the\nforeground layer. The main challenge is to compute a high-quality alpha matte,\npreserving strand-level hair details, while processing high-resolution images\nin real-time. To achieve this goal, we employ two neural networks; a base\nnetwork computes a low-resolution result which is refined by a second network\noperating at high-resolution on selective patches. We introduce two largescale\nvideo and image matting datasets: VideoMatte240K and PhotoMatte13K/85. Our\napproach yields higher quality results compared to the previous\nstate-of-the-art in background matting, while simultaneously yielding a\ndramatic boost in both speed and resolution.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-12-14T18:43:32+00:00",
    "updated": "2020-12-14T18:43:32+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2212.03597v3": {
    "id": "http://arxiv.org/abs/2212.03597v3",
    "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing",
    "authors": [
      "Conglong Li",
      "Zhewei Yao",
      "Xiaoxia Wu",
      "Minjia Zhang",
      "Connor Holmes",
      "Cheng Li",
      "Yuxiong He"
    ],
    "abstract": "Recent advances on deep learning models come at the price of formidable\ntraining cost. The increasing model size is one of the root causes, but another\nless-emphasized fact is that data scale is actually increasing at a similar\nspeed as model scale, and the training cost is proportional to both of them.\nCompared to the rapidly evolving model architecture, how to efficiently use the\ntraining data (especially for the expensive foundation model pretraining) is\nboth less explored and difficult to realize due to the lack of a convenient\nframework that focuses on data efficiency capabilities. To this end, we present\nDeepSpeed Data Efficiency, a framework that makes better use of data, increases\ntraining efficiency, and improves model quality. Specifically, we propose and\ncombine two data efficiency techniques: efficient data sampling via a general\ncurriculum learning library, and efficient data routing via a novel random\nlayerwise token dropping technique. For GPT-3 1.3B language model pretraining,\nour work achieves 12.5x less data/time/cost (\\$3.7K if rent on Azure), while\nstill maintaining 95% of model quality compared to baseline with full data and\ncost (\\$46.3K). For GPT-3 1.3B and BERT-large pretraining, our work can also\nachieve the same model quality with up to 2x less data/time/cost, or achieve\nbetter model quality under same data/time/cost. DeepSpeed Data Efficiency is\neasy to use and tune, enabling us to easily apply it and verify its benefit on\nadditional tasks including GPT-3 MoE model pretraining and small-scale\nGPT-2/ViT finetuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2022-12-07T12:27:28+00:00",
    "updated": "2024-01-14T22:14:26+00:00",
    "doi": null,
    "comment": "Published in AAAI 2024 Main Technical Track. Equal contribution by\n  the first 3 authors. Code has been released as a part of\n  https://github.com/microsoft/DeepSpeed. Part of this paper is from our\n  previous arxiv report (arXiv:2211.11586)",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2210.13311v1": {
    "id": "http://arxiv.org/abs/2210.13311v1",
    "title": "Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Delta Tuning",
    "authors": [
      "Jing Yi",
      "Weize Chen",
      "Yujia Qin",
      "Yankai Lin",
      "Ning Ding",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun",
      "Jie Zhou"
    ],
    "abstract": "Delta tuning (DET, also known as parameter-efficient tuning) is deemed as the\nnew paradigm for using pre-trained language models (PLMs). Up to now, various\nDETs with distinct design elements have been proposed, achieving performance on\npar with fine-tuning. However, the mechanisms behind the above success are\nstill under-explored, especially the connections among various DETs. To fathom\nthe mystery, we hypothesize that the adaptations of different DETs could all be\nreparameterized as low-dimensional optimizations in a unified optimization\nsubspace, which could be found by jointly decomposing independent solutions of\ndifferent DETs. Then we explore the connections among different DETs by\nconducting optimization within the subspace. In experiments, we find that, for\na certain DET, conducting optimization simply in the subspace could achieve\ncomparable performance to its original space, and the found solution in the\nsubspace could be transferred to another DET and achieve non-trivial\nperformance. We also visualize the performance landscape of the subspace and\nfind that there exists a substantial region where different DETs all perform\nwell. Finally, we extend our analysis and show the strong connections between\nfine-tuning and DETs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2022-10-24T14:57:35+00:00",
    "updated": "2022-10-24T14:57:35+00:00",
    "doi": null,
    "comment": "Findings of EMNLP 2022",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2305.17333v3": {
    "id": "http://arxiv.org/abs/2305.17333v3",
    "title": "Fine-Tuning Language Models with Just Forward Passes",
    "authors": [
      "Sadhika Malladi",
      "Tianyu Gao",
      "Eshaan Nichani",
      "Alex Damian",
      "Jason D. Lee",
      "Danqi Chen",
      "Sanjeev Arora"
    ],
    "abstract": "Fine-tuning language models (LMs) has yielded success on diverse downstream\ntasks, but as LMs grow in size, backpropagation requires a prohibitively large\namount of memory. Zeroth-order (ZO) methods can in principle estimate gradients\nusing only two forward passes but are theorized to be catastrophically slow for\noptimizing large models. In this work, we propose a memory-efficient\nzerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate\nin-place, thereby fine-tuning LMs with the same memory footprint as inference.\nFor example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter\nmodel, whereas fine-tuning with backpropagation can train only a 2.7B LM with\nthe same budget. We conduct comprehensive experiments across model types\n(masked and autoregressive LMs), model scales (up to 66B), and downstream tasks\n(classification, multiple-choice, and generation). Our results demonstrate that\n(1) MeZO significantly outperforms in-context learning and linear probing; (2)\nMeZO achieves comparable performance to fine-tuning with backpropagation across\nmultiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction\nin our implementation; (3) MeZO is compatible with both full-parameter and\nparameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO\ncan effectively optimize non-differentiable objectives (e.g., maximizing\naccuracy or F1). We support our empirical findings with theoretical insights,\nhighlighting how adequate pre-training and task prompts enable MeZO to\nfine-tune huge models, despite classical ZO analyses suggesting otherwise.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2023-05-27T02:28:10+00:00",
    "updated": "2024-01-11T13:56:52+00:00",
    "doi": null,
    "comment": "Accepted by NeurIPS 2023 (oral). Code available at\n  https://github.com/princeton-nlp/MeZO",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2305.18047v1": {
    "id": "http://arxiv.org/abs/2305.18047v1",
    "title": "InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions",
    "authors": [
      "Qian Wang",
      "Biao Zhang",
      "Michael Birsak",
      "Peter Wonka"
    ],
    "abstract": "Recent works have explored text-guided image editing using diffusion models\nand generated edited images based on text prompts. However, the models struggle\nto accurately locate the regions to be edited and faithfully perform precise\nedits. In this work, we propose a framework termed InstructEdit that can do\nfine-grained editing based on user instructions. Our proposed framework has\nthree components: language processor, segmenter, and image editor. The first\ncomponent, the language processor, processes the user instruction using a large\nlanguage model. The goal of this processing is to parse the user instruction\nand output prompts for the segmenter and captions for the image editor. We\nadopt ChatGPT and optionally BLIP2 for this step. The second component, the\nsegmenter, uses the segmentation prompt provided by the language processor. We\nemploy a state-of-the-art segmentation framework Grounded Segment Anything to\nautomatically generate a high-quality mask based on the segmentation prompt.\nThe third component, the image editor, uses the captions from the language\nprocessor and the masks from the segmenter to compute the edited image. We\nadopt Stable Diffusion and the mask-guided generation from DiffEdit for this\npurpose. Experiments show that our method outperforms previous editing methods\nin fine-grained editing applications where the input image contains a complex\nobject or multiple objects. We improve the mask quality over DiffEdit and thus\nimprove the quality of edited images. We also show that our framework can\naccept multiple forms of user instructions as input. We provide the code at\nhttps://github.com/QianWangX/InstructEdit.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-05-29T12:24:58+00:00",
    "updated": "2023-05-29T12:24:58+00:00",
    "doi": null,
    "comment": "Project page: https://qianwangx.github.io/InstructEdit/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2103.03874v2": {
    "id": "http://arxiv.org/abs/2103.03874v2",
    "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
    "authors": [
      "Dan Hendrycks",
      "Collin Burns",
      "Saurav Kadavath",
      "Akul Arora",
      "Steven Basart",
      "Eric Tang",
      "Dawn Song",
      "Jacob Steinhardt"
    ],
    "abstract": "Many intellectual endeavors require mathematical problem solving, but this\nskill remains beyond the capabilities of computers. To measure this ability in\nmachine learning models, we introduce MATH, a new dataset of 12,500 challenging\ncompetition mathematics problems. Each problem in MATH has a full step-by-step\nsolution which can be used to teach models to generate answer derivations and\nexplanations. To facilitate future research and increase accuracy on MATH, we\nalso contribute a large auxiliary pretraining dataset which helps teach models\nthe fundamentals of mathematics. Even though we are able to increase accuracy\non MATH, our results show that accuracy remains relatively low, even with\nenormous Transformer models. Moreover, we find that simply increasing budgets\nand model parameter counts will be impractical for achieving strong\nmathematical reasoning if scaling trends continue. While scaling Transformers\nis automatically solving most other text-based tasks, scaling is not currently\nsolving MATH. To have more traction on mathematical problem solving we will\nlikely need new algorithmic advancements from the broader research community.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2021-03-05T18:59:39+00:00",
    "updated": "2021-11-08T21:30:18+00:00",
    "doi": null,
    "comment": "NeurIPS 2021. Code and the MATH dataset is available at\n  https://github.com/hendrycks/math/",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2302.00083v3": {
    "id": "http://arxiv.org/abs/2302.00083v3",
    "title": "In-Context Retrieval-Augmented Language Models",
    "authors": [
      "Ori Ram",
      "Yoav Levine",
      "Itay Dalmedigos",
      "Dor Muhlgay",
      "Amnon Shashua",
      "Kevin Leyton-Brown",
      "Yoav Shoham"
    ],
    "abstract": "Retrieval-Augmented Language Modeling (RALM) methods, which condition a\nlanguage model (LM) on relevant documents from a grounding corpus during\ngeneration, were shown to significantly improve language modeling performance.\nIn addition, they can mitigate the problem of factually inaccurate text\ngeneration and provide natural source attribution mechanism. Existing RALM\napproaches focus on modifying the LM architecture in order to facilitate the\nincorporation of external information, significantly complicating deployment.\nThis paper considers a simple alternative, which we dub In-Context RALM:\nleaving the LM architecture unchanged and prepending grounding documents to the\ninput, without any further training of the LM. We show that In-Context RALM\nthat builds on off-the-shelf general purpose retrievers provides surprisingly\nlarge LM gains across model sizes and diverse corpora. We also demonstrate that\nthe document retrieval and ranking mechanism can be specialized to the RALM\nsetting to further boost performance. We conclude that In-Context RALM has\nconsiderable potential to increase the prevalence of LM grounding, particularly\nin settings where a pretrained LM must be used without modification or even via\nAPI access.",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "published": "2023-01-31T20:26:16+00:00",
    "updated": "2023-08-01T12:10:15+00:00",
    "doi": null,
    "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL). pre-MIT Press publication version",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2309.05153v4": {
    "id": "http://arxiv.org/abs/2309.05153v4",
    "title": "Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood",
    "authors": [
      "Yaxuan Zhu",
      "Jianwen Xie",
      "Yingnian Wu",
      "Ruiqi Gao"
    ],
    "abstract": "Training energy-based models (EBMs) on high-dimensional data can be both\nchallenging and time-consuming, and there exists a noticeable gap in sample\nquality between EBMs and other generative frameworks like GANs and diffusion\nmodels. To close this gap, inspired by the recent efforts of learning EBMs by\nmaximizing diffusion recovery likelihood (DRL), we propose cooperative\ndiffusion recovery likelihood (CDRL), an effective approach to tractably learn\nand sample from a series of EBMs defined on increasingly noisy versions of a\ndataset, paired with an initializer model for each EBM. At each noise level,\nthe two models are jointly estimated within a cooperative training framework:\nsamples from the initializer serve as starting points that are refined by a few\nMCMC sampling steps from the EBM. The EBM is then optimized by maximizing\nrecovery likelihood, while the initializer model is optimized by learning from\nthe difference between the refined samples and the initial samples. In\naddition, we made several practical designs for EBM training to further improve\nthe sample quality. Combining these advances, our approach significantly boost\nthe generation performance compared to existing EBM methods on CIFAR-10 and\nImageNet datasets. We also demonstrate the effectiveness of our models for\nseveral downstream tasks, including classifier-free guided generation,\ncompositional generation, image inpainting and out-of-distribution detection.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2023-09-10T22:05:24+00:00",
    "updated": "2024-04-18T04:02:03+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2211.11427v1": {
    "id": "http://arxiv.org/abs/2211.11427v1",
    "title": "Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations",
    "authors": [
      "Peng Jin",
      "Jinfa Huang",
      "Fenglin Liu",
      "Xian Wu",
      "Shen Ge",
      "Guoli Song",
      "David A. Clifton",
      "Jie Chen"
    ],
    "abstract": "Most video-and-language representation learning approaches employ contrastive\nlearning, e.g., CLIP, to project the video and text features into a common\nlatent space according to the semantic similarities of text-video pairs.\nHowever, such learned shared latent spaces are not often optimal, and the\nmodality gap between visual and textual representation can not be fully\neliminated. In this paper, we propose Expectation-Maximization Contrastive\nLearning (EMCL) to learn compact video-and-language representations.\nSpecifically, we use the Expectation-Maximization algorithm to find a compact\nset of bases for the latent space, where the features could be concisely\nrepresented as the linear combinations of these bases. Such feature\ndecomposition of video-and-language representations reduces the rank of the\nlatent space, resulting in increased representing power for the semantics.\nExtensive experiments on three benchmark text-video retrieval datasets prove\nthat our EMCL can learn more discriminative video-and-language representations\nthan previous methods, and significantly outperform previous state-of-the-art\nmethods across all metrics. More encouragingly, the proposed method can be\napplied to boost the performance of existing approaches either as a jointly\ntraining layer or an out-of-the-box inference module with no extra training,\nmaking it easy to be incorporated into any existing methods.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-11-21T13:12:44+00:00",
    "updated": "2022-11-21T13:12:44+00:00",
    "doi": null,
    "comment": "Accepted to NeurIPS 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1705.04932v1": {
    "id": "http://arxiv.org/abs/1705.04932v1",
    "title": "GeneGAN: Learning Object Transfiguration and Attribute Subspace from Unpaired Data",
    "authors": [
      "Shuchang Zhou",
      "Taihong Xiao",
      "Yi Yang",
      "Dieqiao Feng",
      "Qinyao He",
      "Weiran He"
    ],
    "abstract": "Object Transfiguration replaces an object in an image with another object\nfrom a second image. For example it can perform tasks like \"putting exactly\nthose eyeglasses from image A on the nose of the person in image B\". Usage of\nexemplar images allows more precise specification of desired modifications and\nimproves the diversity of conditional image generation. However, previous\nmethods that rely on feature space operations, require paired data and/or\nappearance models for training or disentangling objects from background. In\nthis work, we propose a model that can learn object transfiguration from two\nunpaired sets of images: one set containing images that \"have\" that kind of\nobject, and the other set being the opposite, with the mild constraint that the\nobjects be located approximately at the same place. For example, the training\ndata can be one set of reference face images that have eyeglasses, and another\nset of images that have not, both of which spatially aligned by face landmarks.\nDespite the weak 0/1 labels, our model can learn an \"eyeglasses\" subspace that\ncontain multiple representatives of different types of glasses. Consequently,\nwe can perform fine-grained control of generated images, like swapping the\nglasses in two images by swapping the projected components in the \"eyeglasses\"\nsubspace, to create novel images of people wearing eyeglasses.\n  Overall, our deterministic generative model learns disentangled attribute\nsubspaces from weakly labeled data by adversarial training. Experiments on\nCelebA and Multi-PIE datasets validate the effectiveness of the proposed model\non real world data, in generating images with specified eyeglasses, smiling,\nhair styles, and lighting conditions etc. The code is available online.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2017-05-14T08:59:36+00:00",
    "updated": "2017-05-14T08:59:36+00:00",
    "doi": null,
    "comment": "Github: https://github.com/Prinsphield/GeneGAN",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.07084v1": {
    "id": "http://arxiv.org/abs/2404.07084v1",
    "title": "Dynamic Generation of Personalities with Large Language Models",
    "authors": [
      "Jianzhi Liu",
      "Hexiang Gu",
      "Tianyu Zheng",
      "Liuyu Xiang",
      "Huijia Wu",
      "Jie Fu",
      "Zhaofeng He"
    ],
    "abstract": "In the realm of mimicking human deliberation, large language models (LLMs)\nshow promising performance, thereby amplifying the importance of this research\narea. Deliberation is influenced by both logic and personality. However,\nprevious studies predominantly focused on the logic of LLMs, neglecting the\nexploration of personality aspects. In this work, we introduce Dynamic\nPersonality Generation (DPG), a dynamic personality generation method based on\nHypernetworks. Initially, we embed the Big Five personality theory into GPT-4\nto form a personality assessment machine, enabling it to evaluate characters'\npersonality traits from dialogues automatically. We propose a new metric to\nassess personality generation capability based on this evaluation method. Then,\nwe use this personality assessment machine to evaluate dialogues in script\ndata, resulting in a personality-dialogue dataset. Finally, we fine-tune DPG on\nthe personality-dialogue dataset. Experiments prove that DPG's personality\ngeneration capability is stronger after fine-tuning on this dataset than\ntraditional fine-tuning methods, surpassing prompt-based GPT-4.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-10T15:17:17+00:00",
    "updated": "2024-04-10T15:17:17+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1804.06826v1": {
    "id": "http://arxiv.org/abs/1804.06826v1",
    "title": "Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking",
    "authors": [
      "Zhe Jia",
      "Marco Maggioni",
      "Benjamin Staiger",
      "Daniele P. Scarpazza"
    ],
    "abstract": "Every year, novel NVIDIA GPU designs are introduced. This rapid architectural\nand technological progression, coupled with a reluctance by manufacturers to\ndisclose low-level details, makes it difficult for even the most proficient GPU\nsoftware designers to remain up-to-date with the technological advances at a\nmicroarchitectural level. To address this dearth of public,\nmicroarchitectural-level information on the novel NVIDIA GPUs, independent\nresearchers have resorted to microbenchmarks-based dissection and discovery.\nThis has led to a prolific line of publications that shed light on instruction\nencoding, and memory hierarchy's geometry and features at each level. Namely,\nresearch that describes the performance and behavior of the Kepler, Maxwell and\nPascal architectures. In this technical report, we continue this line of\nresearch by presenting the microarchitectural details of the NVIDIA Volta\narchitecture, discovered through microbenchmarks and instruction set\ndisassembly. Additionally, we compare quantitatively our Volta findings against\nits predecessors, Kepler, Maxwell and Pascal.",
    "categories": [
      "cs.DC",
      "cs.PF"
    ],
    "published": "2018-04-18T17:25:13+00:00",
    "updated": "2018-04-18T17:25:13+00:00",
    "doi": null,
    "comment": "Technical report. First Edition. April 18th, 2018. 66 pages",
    "journal_ref": null,
    "primary_category": "cs.DC"
  },
  "2404.06109v1": {
    "id": "http://arxiv.org/abs/2404.06109v1",
    "title": "Revising Densification in Gaussian Splatting",
    "authors": [
      "Samuel Rota Bul\u00f2",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ],
    "abstract": "In this paper, we address the limitations of Adaptive Density Control (ADC)\nin 3D Gaussian Splatting (3DGS), a scene representation method achieving\nhigh-quality, photorealistic results for novel view synthesis. ADC has been\nintroduced for automatic 3D point primitive management, controlling\ndensification and pruning, however, with certain limitations in the\ndensification logic. Our main contribution is a more principled, pixel-error\ndriven formulation for density control in 3DGS, leveraging an auxiliary,\nper-pixel error function as the criterion for densification. We further\nintroduce a mechanism to control the total number of primitives generated per\nscene and correct a bias in the current opacity handling strategy of ADC during\ncloning operations. Our approach leads to consistent quality improvements\nacross a variety of benchmark scenes, without sacrificing the method's\nefficiency.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-09T08:20:37+00:00",
    "updated": "2024-04-09T08:20:37+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.07234v2": {
    "id": "http://arxiv.org/abs/2403.07234v2",
    "title": "It's All About Your Sketch: Democratising Sketch Control in Diffusion Models",
    "authors": [
      "Subhadeep Koley",
      "Ayan Kumar Bhunia",
      "Deeptanshu Sekhri",
      "Aneeshan Sain",
      "Pinaki Nath Chowdhury",
      "Tao Xiang",
      "Yi-Zhe Song"
    ],
    "abstract": "This paper unravels the potential of sketches for diffusion models,\naddressing the deceptive promise of direct sketch control in generative AI. We\nimportantly democratise the process, enabling amateur sketches to generate\nprecise images, living up to the commitment of \"what you sketch is what you\nget\". A pilot study underscores the necessity, revealing that deformities in\nexisting models stem from spatial-conditioning. To rectify this, we propose an\nabstraction-aware framework, utilising a sketch adapter, adaptive time-step\nsampling, and discriminative guidance from a pre-trained fine-grained\nsketch-based image retrieval model, working synergistically to reinforce\nfine-grained sketch-photo association. Our approach operates seamlessly during\ninference without the need for textual prompts; a simple, rough sketch akin to\nwhat you and I can create suffices! We welcome everyone to examine results\npresented in the paper and its supplementary. Contributions include\ndemocratising sketch control, introducing an abstraction-aware framework, and\nleveraging discriminative guidance, validated through extensive experiments.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-12T01:05:25+00:00",
    "updated": "2024-03-20T19:23:17+00:00",
    "doi": null,
    "comment": "Accepted in CVPR 2024. Project page available at\n  https://subhadeepkoley.github.io/StableSketching",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2207.10448v1": {
    "id": "http://arxiv.org/abs/2207.10448v1",
    "title": "An Efficient Spatio-Temporal Pyramid Transformer for Action Detection",
    "authors": [
      "Yuetian Weng",
      "Zizheng Pan",
      "Mingfei Han",
      "Xiaojun Chang",
      "Bohan Zhuang"
    ],
    "abstract": "The task of action detection aims at deducing both the action category and\nlocalization of the start and end moment for each action instance in a long,\nuntrimmed video. While vision Transformers have driven the recent advances in\nvideo understanding, it is non-trivial to design an efficient architecture for\naction detection due to the prohibitively expensive self-attentions over a long\nsequence of video clips. To this end, we present an efficient hierarchical\nSpatio-Temporal Pyramid Transformer (STPT) for action detection, building upon\nthe fact that the early self-attention layers in Transformers still focus on\nlocal patterns. Specifically, we propose to use local window attention to\nencode rich local spatio-temporal representations in the early stages while\napplying global attention modules to capture long-term space-time dependencies\nin the later stages. In this way, our STPT can encode both locality and\ndependency with largely reduced redundancy, delivering a promising trade-off\nbetween accuracy and efficiency. For example, with only RGB input, the proposed\nSTPT achieves 53.6% mAP on THUMOS14, surpassing I3D+AFSD RGB model by over 10%\nand performing favorably against state-of-the-art AFSD that uses additional\nflow features with 31% fewer GFLOPs, which serves as an effective and efficient\nend-to-end Transformer-based framework for action detection.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-07-21T12:38:05+00:00",
    "updated": "2022-07-21T12:38:05+00:00",
    "doi": null,
    "comment": "Accepted to ECCV 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1906.09023v2": {
    "id": "http://arxiv.org/abs/1906.09023v2",
    "title": "Backpropagation-Friendly Eigendecomposition",
    "authors": [
      "Wei Wang",
      "Zheng Dang",
      "Yinlin Hu",
      "Pascal Fua",
      "Mathieu Salzmann"
    ],
    "abstract": "Eigendecomposition (ED) is widely used in deep networks. However, the\nbackpropagation of its results tends to be numerically unstable, whether using\nED directly or approximating it with the Power Iteration method, particularly\nwhen dealing with large matrices. While this can be mitigated by partitioning\nthe data in small and arbitrary groups, doing so has no theoretical basis and\nmakes its impossible to exploit the power of ED to the full. In this paper, we\nintroduce a numerically stable and differentiable approach to leveraging\neigenvectors in deep networks. It can handle large matrices without requiring\nto split them. We demonstrate the better robustness of our approach over\nstandard ED and PI for ZCA whitening, an alternative to batch normalization,\nand for PCA denoising, which we introduce as a new normalization strategy for\ndeep networks, aiming to further denoise the network's features.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "published": "2019-06-21T09:17:14+00:00",
    "updated": "2019-06-27T07:42:43+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2405.00630v3": {
    "id": "http://arxiv.org/abs/2405.00630v3",
    "title": "Depth Priors in Removal Neural Radiance Fields",
    "authors": [
      "Zhihao Guo",
      "Peng Wang"
    ],
    "abstract": "Neural Radiance Fields (NeRF) have achieved impressive results in 3D\nreconstruction and novel view generation. A significant challenge within NeRF\ninvolves editing reconstructed 3D scenes, such as object removal, which demands\nconsistency across multiple views and the synthesis of high-quality\nperspectives. Previous studies have integrated depth priors, typically sourced\nfrom LiDAR or sparse depth estimates from COLMAP, to enhance NeRF's performance\nin object removal. However, these methods are either expensive or\ntime-consuming. This paper proposes a new pipeline that leverages SpinNeRF and\nmonocular depth estimation models like ZoeDepth to enhance NeRF's performance\nin complex object removal with improved efficiency. A thorough evaluation of\nCOLMAP's dense depth reconstruction on the KITTI dataset is conducted to\ndemonstrate that COLMAP can be viewed as a cost-effective and scalable\nalternative for acquiring depth ground truth compared to traditional methods\nlike LiDAR. This serves as the basis for evaluating the performance of\nmonocular depth estimation models to determine the best one for generating\ndepth priors for SpinNeRF. The new pipeline is tested in various scenarios\ninvolving 3D reconstruction and object removal, and the results indicate that\nour pipeline significantly reduces the time required for the acquisition of\ndepth priors for object removal and enhances the fidelity of the synthesized\nviews, suggesting substantial potential for building high-fidelity digital twin\nsystems with increased efficiency in the future.",
    "categories": [
      "cs.CV",
      "68T40, 68T07, 68T45",
      "I.4.5"
    ],
    "published": "2024-05-01T16:55:08+00:00",
    "updated": "2024-07-03T15:23:00+00:00",
    "doi": null,
    "comment": "17 pages",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.05481v2": {
    "id": "http://arxiv.org/abs/2308.05481v2",
    "title": "LLM As DBA",
    "authors": [
      "Xuanhe Zhou",
      "Guoliang Li",
      "Zhiyuan Liu"
    ],
    "abstract": "Database administrators (DBAs) play a crucial role in managing, maintaining\nand optimizing a database system to ensure data availability, performance, and\nreliability. However, it is hard and tedious for DBAs to manage a large number\nof database instances (e.g., millions of instances on the cloud databases).\nRecently large language models (LLMs) have shown great potential to understand\nvaluable documents and accordingly generate reasonable answers. Thus, we\npropose D-Bot, a LLM-based database administrator that can continuously acquire\ndatabase maintenance experience from textual sources, and provide reasonable,\nwell-founded, in-time diagnosis and optimization advice for target databases.\nThis paper presents a revolutionary LLM-centric framework for database\nmaintenance, including (i) database maintenance knowledge detection from\ndocuments and tools, (ii) tree of thought reasoning for root cause analysis,\nand (iii) collaborative diagnosis among multiple LLMs. Our preliminary\nexperimental results that D-Bot can efficiently and effectively diagnose the\nroot causes and our code is available at\ngithub.com/TsinghuaDatabaseGroup/DB-GPT.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-08-10T10:12:43+00:00",
    "updated": "2023-08-11T07:55:19+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.DB"
  },
  "2407.11784v1": {
    "id": "http://arxiv.org/abs/2407.11784v1",
    "title": "Data-Juicer Sandbox: A Comprehensive Suite for Multimodal Data-Model Co-development",
    "authors": [
      "Daoyuan Chen",
      "Haibin Wang",
      "Yilun Huang",
      "Ce Ge",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "abstract": "The emergence of large-scale multi-modal generative models has drastically\nadvanced artificial intelligence, introducing unprecedented levels of\nperformance and functionality. However, optimizing these models remains\nchallenging due to historically isolated paths of model-centric and\ndata-centric developments, leading to suboptimal outcomes and inefficient\nresource utilization. In response, we present a novel sandbox suite tailored\nfor integrated data-model co-development. This sandbox provides a comprehensive\nexperimental platform, enabling rapid iteration and insight-driven refinement\nof both data and models. Our proposed \"Probe-Analyze-Refine\" workflow,\nvalidated through applications on state-of-the-art LLaVA-like and DiT based\nmodels, yields significant performance boosts, such as topping the VBench\nleaderboard. We also uncover fruitful insights gleaned from exhaustive\nbenchmarks, shedding light on the critical interplay between data quality,\ndiversity, and model behavior. With the hope of fostering deeper understanding\nand future progress in multi-modal data and generative modeling, our codes,\ndatasets, and models are maintained and accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/Sandbox.md.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-07-16T14:40:07+00:00",
    "updated": "2024-07-16T14:40:07+00:00",
    "doi": null,
    "comment": "26 pages, 9 figures, 5 tables",
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2408.17424v1": {
    "id": "http://arxiv.org/abs/2408.17424v1",
    "title": "CinePreGen: Camera Controllable Video Previsualization via Engine-powered Diffusion",
    "authors": [
      "Yiran Chen",
      "Anyi Rao",
      "Xuekun Jiang",
      "Shishi Xiao",
      "Ruiqing Ma",
      "Zeyu Wang",
      "Hui Xiong",
      "Bo Dai"
    ],
    "abstract": "With advancements in video generative AI models (e.g., SORA), creators are\nincreasingly using these techniques to enhance video previsualization. However,\nthey face challenges with incomplete and mismatched AI workflows. Existing\nmethods mainly rely on text descriptions and struggle with camera placement, a\nkey component of previsualization. To address these issues, we introduce\nCinePreGen, a visual previsualization system enhanced with engine-powered\ndiffusion. It features a novel camera and storyboard interface that offers\ndynamic control, from global to local camera adjustments. This is combined with\na user-friendly AI rendering workflow, which aims to achieve consistent results\nthrough multi-masked IP-Adapter and engine simulation guidelines. In our\ncomprehensive evaluation study, we demonstrate that our system reduces\ndevelopment viscosity (i.e., the complexity and challenges in the development\nprocess), meets users' needs for extensive control and iteration in the design\nprocess, and outperforms other AI video production workflows in cinematic\ncamera movement, as shown by our experiments and a within-subjects user study.\nWith its intuitive camera controls and realistic rendering of camera motion,\nCinePreGen shows great potential for improving video production for both\nindividual creators and industry professionals.",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "published": "2024-08-30T17:16:18+00:00",
    "updated": "2024-08-30T17:16:18+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1712.05884v2": {
    "id": "http://arxiv.org/abs/1712.05884v2",
    "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions",
    "authors": [
      "Jonathan Shen",
      "Ruoming Pang",
      "Ron J. Weiss",
      "Mike Schuster",
      "Navdeep Jaitly",
      "Zongheng Yang",
      "Zhifeng Chen",
      "Yu Zhang",
      "Yuxuan Wang",
      "RJ Skerry-Ryan",
      "Rif A. Saurous",
      "Yannis Agiomyrgiannakis",
      "Yonghui Wu"
    ],
    "abstract": "This paper describes Tacotron 2, a neural network architecture for speech\nsynthesis directly from text. The system is composed of a recurrent\nsequence-to-sequence feature prediction network that maps character embeddings\nto mel-scale spectrograms, followed by a modified WaveNet model acting as a\nvocoder to synthesize timedomain waveforms from those spectrograms. Our model\nachieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for\nprofessionally recorded speech. To validate our design choices, we present\nablation studies of key components of our system and evaluate the impact of\nusing mel spectrograms as the input to WaveNet instead of linguistic, duration,\nand $F_0$ features. We further demonstrate that using a compact acoustic\nintermediate representation enables significant simplification of the WaveNet\narchitecture.",
    "categories": [
      "cs.CL"
    ],
    "published": "2017-12-16T00:51:40+00:00",
    "updated": "2018-02-16T01:28:23+00:00",
    "doi": null,
    "comment": "Accepted to ICASSP 2018",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2007.06299v1": {
    "id": "http://arxiv.org/abs/2007.06299v1",
    "title": "Monitoring and explainability of models in production",
    "authors": [
      "Janis Klaise",
      "Arnaud Van Looveren",
      "Clive Cox",
      "Giovanni Vacanti",
      "Alexandru Coca"
    ],
    "abstract": "The machine learning lifecycle extends beyond the deployment stage.\nMonitoring deployed models is crucial for continued provision of high quality\nmachine learning enabled services. Key areas include model performance and data\nmonitoring, detecting outliers and data drift using statistical techniques, and\nproviding explanations of historic predictions. We discuss the challenges to\nsuccessful implementation of solutions in each of these areas with some recent\nexamples of production ready solutions using open source tools.",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2020-07-13T10:37:05+00:00",
    "updated": "2020-07-13T10:37:05+00:00",
    "doi": null,
    "comment": "Workshop on Challenges in Deploying and Monitoring Machine Learning\n  Systems (ICML 2020)",
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "2403.06269v1": {
    "id": "http://arxiv.org/abs/2403.06269v1",
    "title": "FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing",
    "authors": [
      "Youyuan Zhang",
      "Xuan Ju",
      "James J. Clark"
    ],
    "abstract": "Diffusion models have demonstrated remarkable capabilities in text-to-image\nand text-to-video generation, opening up possibilities for video editing based\non textual input. However, the computational cost associated with sequential\nsampling in diffusion models poses challenges for efficient video editing.\nExisting approaches relying on image generation models for video editing suffer\nfrom time-consuming one-shot fine-tuning, additional condition extraction, or\nDDIM inversion, making real-time applications impractical. In this work, we\npropose FastVideoEdit, an efficient zero-shot video editing approach inspired\nby Consistency Models (CMs). By leveraging the self-consistency property of\nCMs, we eliminate the need for time-consuming inversion or additional condition\nextraction, reducing editing time. Our method enables direct mapping from\nsource video to target video with strong preservation ability utilizing a\nspecial variance schedule. This results in improved speed advantages, as fewer\nsampling steps can be used while maintaining comparable generation quality.\nExperimental results validate the state-of-the-art performance and speed\nadvantages of FastVideoEdit across evaluation metrics encompassing editing\nspeed, temporal consistency, and text-video alignment.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-03-10T17:12:01+00:00",
    "updated": "2024-03-10T17:12:01+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2203.04099v2": {
    "id": "http://arxiv.org/abs/2203.04099v2",
    "title": "VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer",
    "authors": [
      "Juan F. Montesinos",
      "Venkatesh S. Kadandale",
      "Gloria Haro"
    ],
    "abstract": "This paper presents an audio-visual approach for voice separation which\nproduces state-of-the-art results at a low latency in two scenarios: speech and\nsinging voice. The model is based on a two-stage network. Motion cues are\nobtained with a lightweight graph convolutional network that processes face\nlandmarks. Then, both audio and motion features are fed to an audio-visual\ntransformer which produces a fairly good estimation of the isolated target\nsource. In a second stage, the predominant voice is enhanced with an audio-only\nnetwork. We present different ablation studies and comparison to\nstate-of-the-art methods. Finally, we explore the transferability of models\ntrained for speech separation in the task of singing voice separation. The\ndemos, code, and weights are available in https://ipcv.github.io/VoViT/",
    "categories": [
      "cs.SD",
      "cs.CV",
      "cs.LG",
      "eess.AS"
    ],
    "published": "2022-03-08T14:08:47+00:00",
    "updated": "2022-07-19T16:54:03+00:00",
    "doi": null,
    "comment": "Accepted to ECCV 2022",
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "1505.05770v6": {
    "id": "http://arxiv.org/abs/1505.05770v6",
    "title": "Variational Inference with Normalizing Flows",
    "authors": [
      "Danilo Jimenez Rezende",
      "Shakir Mohamed"
    ],
    "abstract": "The choice of approximate posterior distribution is one of the core problems\nin variational inference. Most applications of variational inference employ\nsimple families of posterior approximations in order to allow for efficient\ninference, focusing on mean-field or other simple structured approximations.\nThis restriction has a significant impact on the quality of inferences made\nusing variational methods. We introduce a new approach for specifying flexible,\narbitrarily complex and scalable approximate posterior distributions. Our\napproximations are distributions constructed through a normalizing flow,\nwhereby a simple initial density is transformed into a more complex one by\napplying a sequence of invertible transformations until a desired level of\ncomplexity is attained. We use this view of normalizing flows to develop\ncategories of finite and infinitesimal flows and provide a unified view of\napproaches for constructing rich posterior approximations. We demonstrate that\nthe theoretical advantages of having posteriors that better match the true\nposterior, combined with the scalability of amortized variational approaches,\nprovides a clear improvement in performance and applicability of variational\ninference.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.CO",
      "stat.ME"
    ],
    "published": "2015-05-21T15:36:37+00:00",
    "updated": "2016-06-14T09:01:36+00:00",
    "doi": null,
    "comment": "Proceedings of the 32nd International Conference on Machine Learning",
    "journal_ref": null,
    "primary_category": "stat.ML"
  },
  "1903.03698v4": {
    "id": "http://arxiv.org/abs/1903.03698v4",
    "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning",
    "authors": [
      "Vitchyr H. Pong",
      "Murtaza Dalal",
      "Steven Lin",
      "Ashvin Nair",
      "Shikhar Bahl",
      "Sergey Levine"
    ],
    "abstract": "Autonomous agents that must exhibit flexible and broad capabilities will need\nto be equipped with large repertoires of skills. Defining each skill with a\nmanually-designed reward function limits this repertoire and imposes a manual\nengineering burden. Self-supervised agents that set their own goals can\nautomate this process, but designing appropriate goal setting objectives can be\ndifficult, and often involves heuristic design decisions. In this paper, we\npropose a formal exploration objective for goal-reaching policies that\nmaximizes state coverage. We show that this objective is equivalent to\nmaximizing goal reaching performance together with the entropy of the goal\ndistribution, where goals correspond to full state observations. To instantiate\nthis principle, we present an algorithm called Skew-Fit for learning a\nmaximum-entropy goal distributions. We prove that, under regularity conditions,\nSkew-Fit converges to a uniform distribution over the set of valid states, even\nwhen we do not know this set beforehand. Our experiments show that combining\nSkew-Fit for learning goal distributions with existing goal-reaching methods\noutperforms a variety of prior methods on open-sourced visual goal-reaching\ntasks. Moreover, we demonstrate that Skew-Fit enables a real-world robot to\nlearn to open a door, entirely from scratch, from pixels, and without any\nmanually-designed reward function.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "published": "2019-03-08T23:32:17+00:00",
    "updated": "2020-08-04T04:07:27+00:00",
    "doi": null,
    "comment": "ICML 2020. 8 pages, 8 figures; 9 pages appendix (6 additional\n  figures)",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2204.01273v1": {
    "id": "http://arxiv.org/abs/2204.01273v1",
    "title": "FedSynth: Gradient Compression via Synthetic Data in Federated Learning",
    "authors": [
      "Shengyuan Hu",
      "Jack Goetz",
      "Kshitiz Malik",
      "Hongyuan Zhan",
      "Zhe Liu",
      "Yue Liu"
    ],
    "abstract": "Model compression is important in federated learning (FL) with large models\nto reduce communication cost. Prior works have been focusing on sparsification\nbased compression that could desparately affect the global model accuracy. In\nthis work, we propose a new scheme for upstream communication where instead of\ntransmitting the model update, each client learns and transmits a light-weight\nsynthetic dataset such that using it as the training data, the model performs\nsimilarly well on the real training data. The server will recover the local\nmodel update via the synthetic data and apply standard aggregation. We then\nprovide a new algorithm FedSynth to learn the synthetic data locally.\nEmpirically, we find our method is comparable/better than random masking\nbaselines in all three common federated learning benchmark datasets.",
    "categories": [
      "cs.LG"
    ],
    "published": "2022-04-04T06:47:20+00:00",
    "updated": "2022-04-04T06:47:20+00:00",
    "doi": null,
    "comment": "8 pages",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2205.15723v2": {
    "id": "http://arxiv.org/abs/2205.15723v2",
    "title": "DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes",
    "authors": [
      "Jia-Wei Liu",
      "Yan-Pei Cao",
      "Weijia Mao",
      "Wenqiao Zhang",
      "David Junhao Zhang",
      "Jussi Keppo",
      "Ying Shan",
      "Xiaohu Qie",
      "Mike Zheng Shou"
    ],
    "abstract": "Modeling dynamic scenes is important for many applications such as virtual\nreality and telepresence. Despite achieving unprecedented fidelity for novel\nview synthesis in dynamic scenes, existing methods based on Neural Radiance\nFields (NeRF) suffer from slow convergence (i.e., model training time measured\nin days). In this paper, we present DeVRF, a novel representation to accelerate\nlearning dynamic radiance fields. The core of DeVRF is to model both the 3D\ncanonical space and 4D deformation field of a dynamic, non-rigid scene with\nexplicit and discrete voxel-based representations. However, it is quite\nchallenging to train such a representation which has a large number of model\nparameters, often resulting in overfitting issues. To overcome this challenge,\nwe devise a novel static-to-dynamic learning paradigm together with a new data\ncapture setup that is convenient to deploy in practice. This paradigm unlocks\nefficient learning of deformable radiance fields via utilizing the 3D\nvolumetric canonical space learnt from multi-view static images to ease the\nlearning of 4D voxel deformation field with only few-view dynamic sequences. To\nfurther improve the efficiency of our DeVRF and its synthesized novel view's\nquality, we conduct thorough explorations and identify a set of strategies. We\nevaluate DeVRF on both synthetic and real-world dynamic scenes with different\ntypes of deformation. Experiments demonstrate that DeVRF achieves two orders of\nmagnitude speedup (100x faster) with on-par high-fidelity results compared to\nthe previous state-of-the-art approaches. The code and dataset will be released\nin https://github.com/showlab/DeVRF.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-05-31T12:13:54+00:00",
    "updated": "2022-06-04T06:33:34+00:00",
    "doi": null,
    "comment": "Project page: https://jia-wei-liu.github.io/DeVRF/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1911.11206v1": {
    "id": "http://arxiv.org/abs/1911.11206v1",
    "title": "Oops! Predicting Unintentional Action in Video",
    "authors": [
      "Dave Epstein",
      "Boyuan Chen",
      "Carl Vondrick"
    ],
    "abstract": "From just a short glance at a video, we can often tell whether a person's\naction is intentional or not. Can we train a model to recognize this? We\nintroduce a dataset of in-the-wild videos of unintentional action, as well as a\nsuite of tasks for recognizing, localizing, and anticipating its onset. We\ntrain a supervised neural network as a baseline and analyze its performance\ncompared to human consistency on the tasks. We also investigate self-supervised\nrepresentations that leverage natural signals in our dataset, and show the\neffectiveness of an approach that uses the intrinsic speed of video to perform\ncompetitively with highly-supervised pretraining. However, a significant gap\nbetween machine and human performance remains. The project website is available\nat https://oops.cs.columbia.edu",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "published": "2019-11-25T20:15:31+00:00",
    "updated": "2019-11-25T20:15:31+00:00",
    "doi": null,
    "comment": "11 pages, 9 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2403.19887v2": {
    "id": "http://arxiv.org/abs/2403.19887v2",
    "title": "Jamba: A Hybrid Transformer-Mamba Language Model",
    "authors": [
      "Opher Lieber",
      "Barak Lenz",
      "Hofit Bata",
      "Gal Cohen",
      "Jhonathan Osin",
      "Itay Dalmedigos",
      "Erez Safahi",
      "Shaked Meirom",
      "Yonatan Belinkov",
      "Shai Shalev-Shwartz",
      "Omri Abend",
      "Raz Alon",
      "Tomer Asida",
      "Amir Bergman",
      "Roman Glozman",
      "Michael Gokhman",
      "Avashalom Manevich",
      "Nir Ratner",
      "Noam Rozen",
      "Erez Shwartz",
      "Mor Zusman",
      "Yoav Shoham"
    ],
    "abstract": "We present Jamba, a new base large language model based on a novel hybrid\nTransformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\ninterleaves blocks of Transformer and Mamba layers, enjoying the benefits of\nboth model families. MoE is added in some of these layers to increase model\ncapacity while keeping active parameter usage manageable. This flexible\narchitecture allows resource- and objective-specific configurations. In the\nparticular configuration we have implemented, we end up with a powerful model\nthat fits in a single 80GB GPU. Built at large scale, Jamba provides high\nthroughput and small memory footprint compared to vanilla Transformers, and at\nthe same time state-of-the-art performance on standard language model\nbenchmarks and long-context evaluations. Remarkably, the model presents strong\nresults for up to 256K tokens context length. We study various architectural\ndecisions, such as how to combine Transformer and Mamba layers, and how to mix\nexperts, and show that some of them are crucial in large scale modeling. We\nalso describe several interesting properties of these architectures which the\ntraining and evaluation of Jamba have revealed, and plan to release checkpoints\nfrom various ablation runs, to encourage further exploration of this novel\narchitecture. We make the weights of our implementation of Jamba publicly\navailable under a permissive license.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-03-28T23:55:06+00:00",
    "updated": "2024-07-03T14:30:33+00:00",
    "doi": null,
    "comment": "Webpage: https://www.ai21.com/jamba",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2203.09135v2": {
    "id": "http://arxiv.org/abs/2203.09135v2",
    "title": "Co-visual pattern augmented generative transformer learning for automobile geo-localization",
    "authors": [
      "Jianwei Zhao",
      "Qiang Zhai",
      "Pengbo Zhao",
      "Rui Huang",
      "Hong Cheng"
    ],
    "abstract": "Geolocation is a fundamental component of route planning and navigation for\nunmanned vehicles, but GNSS-based geolocation fails under denial-of-service\nconditions. Cross-view geo-localization (CVGL), which aims to estimate the\ngeographical location of the ground-level camera by matching against enormous\ngeo-tagged aerial (\\emph{e.g.}, satellite) images, has received lots of\nattention but remains extremely challenging due to the drastic appearance\ndifferences across aerial-ground views. In existing methods, global\nrepresentations of different views are extracted primarily using Siamese-like\narchitectures, but their interactive benefits are seldom taken into account. In\nthis paper, we present a novel approach using cross-view knowledge generative\ntechniques in combination with transformers, namely mutual generative\ntransformer learning (MGTL), for CVGL. Specifically, by taking the initial\nrepresentations produced by the backbone network, MGTL develops two separate\ngenerative sub-modules -- one for aerial-aware knowledge generation from\nground-view semantics and vice versa -- and fully exploits the entirely mutual\nbenefits through the attention mechanism. Moreover, to better capture the\nco-visual relationships between aerial and ground views, we introduce a\ncascaded attention masking algorithm to further boost accuracy. Extensive\nexperiments on challenging public benchmarks, \\emph{i.e.}, {CVACT} and {CVUSA},\ndemonstrate the effectiveness of the proposed method which sets new records\ncompared with the existing state-of-the-art models.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-03-17T07:29:02+00:00",
    "updated": "2023-04-20T12:49:27+00:00",
    "doi": null,
    "comment": "21pages",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2402.11271v2": {
    "id": "http://arxiv.org/abs/2402.11271v2",
    "title": "MONAL: Model Autophagy Analysis for Modeling Human-AI Interactions",
    "authors": [
      "Shu Yang",
      "Muhammad Asif Ali",
      "Lu Yu",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "The increasing significance of large models and their multi-modal variants in\nsocietal information processing has ignited debates on social safety and\nethics. However, there exists a paucity of comprehensive analysis for: (i) the\ninteractions between human and artificial intelligence systems, and (ii)\nunderstanding and addressing the associated limitations. To bridge this gap, we\npropose Model Autophagy Analysis (MONAL) for large models' self-consumption\nexplanation. MONAL employs two distinct autophagous loops (referred to as\n``self-consumption loops'') to elucidate the suppression of human-generated\ninformation in the exchange between human and AI systems. Through comprehensive\nexperiments on diverse datasets, we evaluate the capacities of generated models\nas both creators and disseminators of information. Our key findings reveal (i)\nA progressive prevalence of model-generated synthetic information over time\nwithin training datasets compared to human-generated information; (ii) The\ndiscernible tendency of large models, when acting as information transmitters\nacross multiple iterations, to selectively modify or prioritize specific\ncontents; and (iii) The potential for a reduction in the diversity of socially\nor human-generated information, leading to bottlenecks in the performance\nenhancement of large models and confining them to local optima.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2024-02-17T13:02:54+00:00",
    "updated": "2024-03-30T22:05:59+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2311.11255v4": {
    "id": "http://arxiv.org/abs/2311.11255v4",
    "title": "M$^{2}$UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models",
    "authors": [
      "Shansong Liu",
      "Atin Sakkeer Hussain",
      "Chenshuo Sun",
      "Ying Shan"
    ],
    "abstract": "The current landscape of research leveraging large language models (LLMs) is\nexperiencing a surge. Many works harness the powerful reasoning capabilities of\nthese models to comprehend various modalities, such as text, speech, images,\nvideos, etc. They also utilize LLMs to understand human intention and generate\ndesired outputs like images, videos, and music. However, research that combines\nboth understanding and generation using LLMs is still limited and in its\nnascent stage. To address this gap, we introduce a Multi-modal Music\nUnderstanding and Generation (M$^{2}$UGen) framework that integrates LLM's\nabilities to comprehend and generate music for different modalities. The\nM$^{2}$UGen framework is purpose-built to unlock creative potential from\ndiverse sources of inspiration, encompassing music, image, and video through\nthe use of pretrained MERT, ViT, and ViViT models, respectively. To enable\nmusic generation, we explore the use of AudioLDM 2 and MusicGen. Bridging\nmulti-modal understanding and music generation is accomplished through the\nintegration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA\nmodel to generate extensive datasets that support text/image/video-to-music\ngeneration, facilitating the training of our M$^{2}$UGen framework. We conduct\na thorough evaluation of our proposed framework. The experimental results\ndemonstrate that our model achieves or surpasses the performance of the current\nstate-of-the-art models.",
    "categories": [
      "cs.SD",
      "cs.MM",
      "eess.AS"
    ],
    "published": "2023-11-19T06:50:52+00:00",
    "updated": "2024-03-05T03:12:03+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "2402.05369v2": {
    "id": "http://arxiv.org/abs/2402.05369v2",
    "title": "Noise Contrastive Alignment of Language Models with Explicit Rewards",
    "authors": [
      "Huayu Chen",
      "Guande He",
      "Lifan Yuan",
      "Ganqu Cui",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract": "User intentions are typically formalized as evaluation rewards to be\nmaximized when fine-tuning language models (LMs). Existing alignment methods,\nsuch as Direct Preference Optimization (DPO), are mainly tailored for pairwise\npreference data where rewards are implicitly defined rather than explicitly\ngiven. In this paper, we introduce a general framework for LM alignment,\nleveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling\nreward datasets explicitly annotated with scalar evaluations. Our framework\ncomprises two parallel algorithms, NCA and InfoNCA, both enabling the direct\nextraction of an LM policy from reward data as well as preference data.\nNotably, we show that the DPO loss is a special case of our proposed InfoNCA\nobjective under pairwise preference settings, thereby integrating and extending\ncurrent alignment theories. By comparing NCA and InfoNCA, we demonstrate that\nthe well-observed decreasing-likelihood trend of DPO/InfoNCA is caused by their\nfocus on adjusting relative likelihood across different responses. In contrast,\nNCA optimizes the absolute likelihood for each response, thereby effectively\npreventing the chosen likelihood from decreasing. We evaluate our methods in\nboth reward and preference settings with Mistral-8*7B and 7B models.\nExperiments suggest that InfoNCA/NCA surpasses various preference baselines\nwhen reward datasets are available. We also find NCA significantly outperforms\nDPO in complex reasoning tasks like math and coding.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-02-08T02:58:47+00:00",
    "updated": "2024-07-03T13:53:06+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2311.09571v1": {
    "id": "http://arxiv.org/abs/2311.09571v1",
    "title": "3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation",
    "authors": [
      "Dale Decatur",
      "Itai Lang",
      "Kfir Aberman",
      "Rana Hanocka"
    ],
    "abstract": "In this work we develop 3D Paintbrush, a technique for automatically\ntexturing local semantic regions on meshes via text descriptions. Our method is\ndesigned to operate directly on meshes, producing texture maps which seamlessly\nintegrate into standard graphics pipelines. We opt to simultaneously produce a\nlocalization map (to specify the edit region) and a texture map which conforms\nto it. This synergistic approach improves the quality of both the localization\nand the stylization. To enhance the details and resolution of the textured\narea, we leverage multiple stages of a cascaded diffusion model to supervise\nour local editing technique with generative priors learned from images at\ndifferent resolutions. Our technique, referred to as Cascaded Score\nDistillation (CSD), simultaneously distills scores at multiple resolutions in a\ncascaded fashion, enabling control over both the granularity and global\nunderstanding of the supervision. We demonstrate the effectiveness of 3D\nPaintbrush to locally texture a variety of shapes within different semantic\nregions. Project page: https://threedle.github.io/3d-paintbrush",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "published": "2023-11-16T05:13:44+00:00",
    "updated": "2023-11-16T05:13:44+00:00",
    "doi": null,
    "comment": "Project page: https://threedle.github.io/3d-paintbrush",
    "journal_ref": null,
    "primary_category": "cs.GR"
  },
  "2307.11335v1": {
    "id": "http://arxiv.org/abs/2307.11335v1",
    "title": "Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields",
    "authors": [
      "Wenbo Hu",
      "Yuling Wang",
      "Lin Ma",
      "Bangbang Yang",
      "Lin Gao",
      "Xiao Liu",
      "Yuewen Ma"
    ],
    "abstract": "Despite the tremendous progress in neural radiance fields (NeRF), we still\nface a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF\npresents fine-detailed and anti-aliased renderings but takes days for training,\nwhile Instant-ngp can accomplish the reconstruction in a few minutes but\nsuffers from blurring or aliasing when rendering at various distances or\nresolutions due to ignoring the sampling area. To this end, we propose a novel\nTri-Mip encoding that enables both instant reconstruction and anti-aliased\nhigh-fidelity rendering for neural radiance fields. The key is to factorize the\npre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can\nefficiently perform 3D area sampling by taking advantage of 2D pre-filtered\nfeature maps, which significantly elevates the rendering quality without\nsacrificing efficiency. To cope with the novel Tri-Mip representation, we\npropose a cone-casting rendering technique to efficiently sample anti-aliased\n3D features with the Tri-Mip encoding considering both pixel imaging and\nobserving distance. Extensive experiments on both synthetic and real-world\ndatasets demonstrate our method achieves state-of-the-art rendering quality and\nreconstruction speed while maintaining a compact representation that reduces\n25% model size compared against Instant-ngp.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "published": "2023-07-21T03:47:28+00:00",
    "updated": "2023-07-21T03:47:28+00:00",
    "doi": null,
    "comment": "Accepted to ICCV 2023 Project page:\n  https://wbhu.github.io/projects/Tri-MipRF",
    "journal_ref": "ICCV 2023",
    "primary_category": "cs.CV"
  },
  "2109.10057v3": {
    "id": "http://arxiv.org/abs/2109.10057v3",
    "title": "LOTR: Face Landmark Localization Using Localization Transformer",
    "authors": [
      "Ukrit Watchareeruetai",
      "Benjaphan Sommana",
      "Sanjana Jain",
      "Pavit Noinongyao",
      "Ankush Ganguly",
      "Aubin Samacoits",
      "Samuel W. F. Earp",
      "Nakarin Sritrakool"
    ],
    "abstract": "This paper presents a novel Transformer-based facial landmark localization\nnetwork named Localization Transformer (LOTR). The proposed framework is a\ndirect coordinate regression approach leveraging a Transformer network to\nbetter utilize the spatial information in the feature map. An LOTR model\nconsists of three main modules: 1) a visual backbone that converts an input\nimage into a feature map, 2) a Transformer module that improves the feature\nrepresentation from the visual backbone, and 3) a landmark prediction head that\ndirectly predicts the landmark coordinates from the Transformer's\nrepresentation. Given cropped-and-aligned face images, the proposed LOTR can be\ntrained end-to-end without requiring any post-processing steps. This paper also\nintroduces the smooth-Wing loss function, which addresses the gradient\ndiscontinuity of the Wing loss, leading to better convergence than standard\nloss functions such as L1, L2, and Wing loss. Experimental results on the JD\nlandmark dataset provided by the First Grand Challenge of 106-Point Facial\nLandmark Localization indicate the superiority of LOTR over the existing\nmethods on the leaderboard and two recent heatmap-based approaches. On the WFLW\ndataset, the proposed LOTR framework demonstrates promising results compared\nwith several state-of-the-art methods. Additionally, we report the improvement\nin state-of-the-art face recognition performance when using our proposed LOTRs\nfor face alignment.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2021-09-21T09:54:27+00:00",
    "updated": "2022-02-22T07:53:32+00:00",
    "doi": "10.1109/ACCESS.2022.3149380",
    "comment": "Accepted for publication in IEEE Access",
    "journal_ref": "IEEE Access 2022",
    "primary_category": "cs.CV"
  },
  "2312.08882v2": {
    "id": "http://arxiv.org/abs/2312.08882v2",
    "title": "Neural Video Fields Editing",
    "authors": [
      "Shuzhou Yang",
      "Chong Mou",
      "Jiwen Yu",
      "Yuhan Wang",
      "Xiandong Meng",
      "Jian Zhang"
    ],
    "abstract": "Diffusion models have revolutionized text-driven video editing. However,\napplying these methods to real-world editing encounters two significant\nchallenges: (1) the rapid increase in GPU memory demand as the number of frames\ngrows, and (2) the inter-frame inconsistency in edited videos. To this end, we\npropose NVEdit, a novel text-driven video editing framework designed to\nmitigate memory overhead and improve consistent editing for real-world long\nvideos. Specifically, we construct a neural video field, powered by tri-plane\nand sparse grid, to enable encoding long videos with hundreds of frames in a\nmemory-efficient manner. Next, we update the video field through off-the-shelf\nText-to-Image (T2I) models to impart text-driven editing effects. A progressive\noptimization strategy is developed to preserve original temporal priors.\nImportantly, both the neural video field and T2I model are adaptable and\nreplaceable, thus inspiring future research. Experiments demonstrate the\nability of our approach to edit hundreds of frames with impressive inter-frame\nconsistency. Our project is available at: https://nvedit.github.io/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-12T14:48:48+00:00",
    "updated": "2024-03-09T06:45:58+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2010.15980v2": {
    "id": "http://arxiv.org/abs/2010.15980v2",
    "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
    "authors": [
      "Taylor Shin",
      "Yasaman Razeghi",
      "Robert L. Logan IV",
      "Eric Wallace",
      "Sameer Singh"
    ],
    "abstract": "The remarkable success of pretrained language models has motivated the study\nof what kinds of knowledge these models learn during pretraining. Reformulating\ntasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach\nfor gauging such knowledge, however, its usage is limited by the manual effort\nand guesswork required to write suitable prompts. To address this, we develop\nAutoPrompt, an automated method to create prompts for a diverse set of tasks,\nbased on a gradient-guided search. Using AutoPrompt, we show that masked\nlanguage models (MLMs) have an inherent capability to perform sentiment\nanalysis and natural language inference without additional parameters or\nfinetuning, sometimes achieving performance on par with recent state-of-the-art\nsupervised models. We also show that our prompts elicit more accurate factual\nknowledge from MLMs than the manually created prompts on the LAMA benchmark,\nand that MLMs can be used as relation extractors more effectively than\nsupervised relation extraction models. These results demonstrate that\nautomatically generated prompts are a viable parameter-free alternative to\nexisting probing methods, and as pretrained LMs become more sophisticated and\ncapable, potentially a replacement for finetuning.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2020-10-29T22:54:00+00:00",
    "updated": "2020-11-07T05:33:35+00:00",
    "doi": null,
    "comment": "v2: Fixed error in Figure 2",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2301.07464v2": {
    "id": "http://arxiv.org/abs/2301.07464v2",
    "title": "CLIPTER: Looking at the Bigger Picture in Scene Text Recognition",
    "authors": [
      "Aviad Aberdam",
      "David Bensa\u00efd",
      "Alona Golts",
      "Roy Ganz",
      "Oren Nuriel",
      "Royee Tichauer",
      "Shai Mazor",
      "Ron Litman"
    ],
    "abstract": "Reading text in real-world scenarios often requires understanding the context\nsurrounding it, especially when dealing with poor-quality text. However,\ncurrent scene text recognizers are unaware of the bigger picture as they\noperate on cropped text images. In this study, we harness the representative\ncapabilities of modern vision-language models, such as CLIP, to provide\nscene-level information to the crop-based recognizer. We achieve this by fusing\na rich representation of the entire image, obtained from the vision-language\nmodel, with the recognizer word-level features via a gated cross-attention\nmechanism. This component gradually shifts to the context-enhanced\nrepresentation, allowing for stable fine-tuning of a pretrained recognizer. We\ndemonstrate the effectiveness of our model-agnostic framework, CLIPTER (CLIP\nTExt Recognition), on leading text recognition architectures and achieve\nstate-of-the-art results across multiple benchmarks. Furthermore, our analysis\nhighlights improved robustness to out-of-vocabulary words and enhanced\ngeneralization in low-data regimes.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-01-18T12:16:19+00:00",
    "updated": "2023-07-23T13:51:34+00:00",
    "doi": null,
    "comment": "Accepted for publication by ICCV 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.00640v2": {
    "id": "http://arxiv.org/abs/2210.00640v2",
    "title": "Wide Attention Is The Way Forward For Transformers?",
    "authors": [
      "Jason Ross Brown",
      "Yiren Zhao",
      "Ilia Shumailov",
      "Robert D Mullins"
    ],
    "abstract": "The Transformer is an extremely powerful and prominent deep learning\narchitecture. In this work, we challenge the commonly held belief in deep\nlearning that going deeper is better, and show an alternative design approach\nthat is building wider attention Transformers. We demonstrate that wide single\nlayer Transformer models can compete with or outperform deeper ones in a\nvariety of Natural Language Processing (NLP) tasks when both are trained from\nscratch. The impact of changing the model aspect ratio on Transformers is then\nstudied systematically. This ratio balances the number of layers and the number\nof attention heads per layer while keeping the total number of attention heads\nand all other hyperparameters constant. On average, across 4 NLP tasks and 10\nattention types, single layer wide models perform 0.3% better than their deep\ncounterparts. We show an in-depth evaluation and demonstrate how wide models\nrequire a far smaller memory footprint and can run faster on commodity\nhardware, in addition, these wider models are also more interpretable. For\nexample, a single layer Transformer on the IMDb byte level text classification\nhas 3.1x faster inference latency on a CPU than its equally accurate deeper\ncounterpart, and is half the size. We therefore put forward wider and shallower\nmodels as a viable and desirable alternative for small models on NLP tasks, and\nas an important area of research for domains beyond this.",
    "categories": [
      "cs.LG",
      "I.2.7"
    ],
    "published": "2022-10-02T21:49:54+00:00",
    "updated": "2022-11-08T22:24:09+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2103.16746v1": {
    "id": "http://arxiv.org/abs/2103.16746v1",
    "title": "Towards More Flexible and Accurate Object Tracking with Natural Language: Algorithms and Benchmark",
    "authors": [
      "Xiao Wang",
      "Xiujun Shu",
      "Zhipeng Zhang",
      "Bo Jiang",
      "Yaowei Wang",
      "Yonghong Tian",
      "Feng Wu"
    ],
    "abstract": "Tracking by natural language specification is a new rising research topic\nthat aims at locating the target object in the video sequence based on its\nlanguage description. Compared with traditional bounding box (BBox) based\ntracking, this setting guides object tracking with high-level semantic\ninformation, addresses the ambiguity of BBox, and links local and global search\norganically together. Those benefits may bring more flexible, robust and\naccurate tracking performance in practical scenarios. However, existing natural\nlanguage initialized trackers are developed and compared on benchmark datasets\nproposed for tracking-by-BBox, which can't reflect the true power of\ntracking-by-language. In this work, we propose a new benchmark specifically\ndedicated to the tracking-by-language, including a large scale dataset, strong\nand diverse baseline methods. Specifically, we collect 2k video sequences\n(contains a total of 1,244,340 frames, 663 words) and split 1300/700 for the\ntrain/testing respectively. We densely annotate one sentence in English and\ncorresponding bounding boxes of the target object for each video. We also\nintroduce two new challenges into TNL2K for the object tracking task, i.e.,\nadversarial samples and modality switch. A strong baseline method based on an\nadaptive local-global-search scheme is proposed for future works to compare. We\nbelieve this benchmark will greatly boost related researches on natural\nlanguage guided tracking.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2021-03-31T00:57:32+00:00",
    "updated": "2021-03-31T00:57:32+00:00",
    "doi": null,
    "comment": "Accepted by CVPR 2021",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2407.01102v1": {
    "id": "http://arxiv.org/abs/2407.01102v1",
    "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
    "authors": [
      "David Rau",
      "Herv\u00e9 D\u00e9jean",
      "Nadezhda Chirkova",
      "Thibault Formal",
      "Shuai Wang",
      "Vassilina Nikoulina",
      "St\u00e9phane Clinchant"
    ],
    "abstract": "Retrieval-Augmented Generation allows to enhance Large Language Models with\nexternal knowledge. In response to the recent popularity of generative LLMs,\nmany RAG approaches have been proposed, which involve an intricate number of\ndifferent configurations such as evaluation datasets, collections, metrics,\nretrievers, and LLMs. Inconsistent benchmarking poses a major challenge in\ncomparing approaches and understanding the impact of each component in the\npipeline. In this work, we study best practices that lay the groundwork for a\nsystematic evaluation of RAG and present BERGEN, an end-to-end library for\nreproducible research standardizing RAG experiments. In an extensive study\nfocusing on QA, we benchmark different state-of-the-art retrievers, rerankers,\nand LLMs. Additionally, we analyze existing RAG metrics and datasets. Our\nopen-source library BERGEN is available under\n\\url{https://github.com/naver/bergen}.",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "published": "2024-07-01T09:09:27+00:00",
    "updated": "2024-07-01T09:09:27+00:00",
    "doi": null,
    "comment": "29 pages",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2212.03741v4": {
    "id": "http://arxiv.org/abs/2212.03741v4",
    "title": "FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance Generation",
    "authors": [
      "Ronghui Li",
      "Junfan Zhao",
      "Yachao Zhang",
      "Mingyang Su",
      "Zeping Ren",
      "Han Zhang",
      "Yansong Tang",
      "Xiu Li"
    ],
    "abstract": "Generating full-body and multi-genre dance sequences from given music is a\nchallenging task, due to the limitations of existing datasets and the inherent\ncomplexity of the fine-grained hand motion and dance genres. To address these\nproblems, we propose FineDance, which contains 14.6 hours of music-dance paired\ndata, with fine-grained hand motions, fine-grained genres (22 dance genres),\nand accurate posture. To the best of our knowledge, FineDance is the largest\nmusic-dance paired dataset with the most dance genres. Additionally, to address\nmonotonous and unnatural hand movements existing in previous methods, we\npropose a full-body dance generation network, which utilizes the diverse\ngeneration capabilities of the diffusion model to solve monotonous problems,\nand use expert nets to solve unreal problems. To further enhance the\ngenre-matching and long-term stability of generated dances, we propose a\nGenre&Coherent aware Retrieval Module. Besides, we propose a novel metric named\nGenre Matching Score to evaluate the genre-matching degree between dance and\nmusic. Quantitative and qualitative experiments demonstrate the quality of\nFineDance, and the state-of-the-art performance of FineNet. The FineDance\nDataset and more qualitative samples can be found at our website.",
    "categories": [
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2022-12-07T16:10:08+00:00",
    "updated": "2023-08-30T04:18:50+00:00",
    "doi": null,
    "comment": "Accepted by ICCV 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2212.06909v2": {
    "id": "http://arxiv.org/abs/2212.06909v2",
    "title": "Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting",
    "authors": [
      "Su Wang",
      "Chitwan Saharia",
      "Ceslee Montgomery",
      "Jordi Pont-Tuset",
      "Shai Noy",
      "Stefano Pellegrini",
      "Yasumasa Onoe",
      "Sarah Laszlo",
      "David J. Fleet",
      "Radu Soricut",
      "Jason Baldridge",
      "Mohammad Norouzi",
      "Peter Anderson",
      "William Chan"
    ],
    "abstract": "Text-guided image editing can have a transformative impact in supporting\ncreative applications. A key challenge is to generate edits that are faithful\nto input text prompts, while consistent with input images. We present Imagen\nEditor, a cascaded diffusion model built, by fine-tuning Imagen on text-guided\nimage inpainting. Imagen Editor's edits are faithful to the text prompts, which\nis accomplished by using object detectors to propose inpainting masks during\ntraining. In addition, Imagen Editor captures fine details in the input image\nby conditioning the cascaded pipeline on the original high resolution image. To\nimprove qualitative and quantitative evaluation, we introduce EditBench, a\nsystematic benchmark for text-guided image inpainting. EditBench evaluates\ninpainting edits on natural and generated images exploring objects, attributes,\nand scenes. Through extensive human evaluation on EditBench, we find that\nobject-masking during training leads to across-the-board improvements in\ntext-image alignment -- such that Imagen Editor is preferred over DALL-E 2 and\nStable Diffusion -- and, as a cohort, these models are better at\nobject-rendering than text-rendering, and handle material/color/size attributes\nbetter than count/shape attributes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2022-12-13T21:25:11+00:00",
    "updated": "2023-04-12T22:42:08+00:00",
    "doi": null,
    "comment": "CVPR 2023 Camera Ready",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.02414v2": {
    "id": "http://arxiv.org/abs/2210.02414v2",
    "title": "GLM-130B: An Open Bilingual Pre-trained Model",
    "authors": [
      "Aohan Zeng",
      "Xiao Liu",
      "Zhengxiao Du",
      "Zihan Wang",
      "Hanyu Lai",
      "Ming Ding",
      "Zhuoyi Yang",
      "Yifan Xu",
      "Wendi Zheng",
      "Xiao Xia",
      "Weng Lam Tam",
      "Zixuan Ma",
      "Yufei Xue",
      "Jidong Zhai",
      "Wenguang Chen",
      "Peng Zhang",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language\nmodel with 130 billion parameters. It is an attempt to open-source a 100B-scale\nmodel at least as good as GPT-3 (davinci) and unveil how models of such a scale\ncan be successfully pre-trained. Over the course of this effort, we face\nnumerous unexpected technical and engineering challenges, particularly on loss\nspikes and divergence. In this paper, we introduce the training process of\nGLM-130B including its design choices, training strategies for both efficiency\nand stability, and engineering efforts. The resultant GLM-130B model offers\nsignificant outperformance over GPT-3 175B (davinci) on a wide range of popular\nEnglish benchmarks while the performance advantage is not observed in OPT-175B\nand BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN\n3.0 260B -- the largest Chinese language model -- across related benchmarks.\nFinally, we leverage a unique scaling property of GLM-130B to reach INT4\nquantization without post training, with almost no performance loss, making it\nthe first among 100B-scale models and more importantly, allowing its effective\ninference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the\nmost affordable GPUs required for using 100B-scale models. The GLM-130B model\nweights are publicly accessible and its code, training logs, related toolkit,\nand lessons learned are open-sourced at\n\\url{https://github.com/THUDM/GLM-130B/}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2022-10-05T17:34:44+00:00",
    "updated": "2023-10-25T05:22:43+00:00",
    "doi": null,
    "comment": "Accepted to ICLR 2023",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1803.10892v1": {
    "id": "http://arxiv.org/abs/1803.10892v1",
    "title": "Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks",
    "authors": [
      "Agrim Gupta",
      "Justin Johnson",
      "Li Fei-Fei",
      "Silvio Savarese",
      "Alexandre Alahi"
    ],
    "abstract": "Understanding human motion behavior is critical for autonomous moving\nplatforms (like self-driving cars and social robots) if they are to navigate\nhuman-centric environments. This is challenging because human motion is\ninherently multimodal: given a history of human motion paths, there are many\nsocially plausible ways that people could move in the future. We tackle this\nproblem by combining tools from sequence prediction and generative adversarial\nnetworks: a recurrent sequence-to-sequence model observes motion histories and\npredicts future behavior, using a novel pooling mechanism to aggregate\ninformation across people. We predict socially plausible futures by training\nadversarially against a recurrent discriminator, and encourage diverse\npredictions with a novel variety loss. Through experiments on several datasets\nwe demonstrate that our approach outperforms prior work in terms of accuracy,\nvariety, collision avoidance, and computational complexity.",
    "categories": [
      "cs.CV"
    ],
    "published": "2018-03-29T01:24:02+00:00",
    "updated": "2018-03-29T01:24:02+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1712.05474v4": {
    "id": "http://arxiv.org/abs/1712.05474v4",
    "title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
    "authors": [
      "Eric Kolve",
      "Roozbeh Mottaghi",
      "Winson Han",
      "Eli VanderBilt",
      "Luca Weihs",
      "Alvaro Herrasti",
      "Matt Deitke",
      "Kiana Ehsani",
      "Daniel Gordon",
      "Yuke Zhu",
      "Aniruddha Kembhavi",
      "Abhinav Gupta",
      "Ali Farhadi"
    ],
    "abstract": "We introduce The House Of inteRactions (THOR), a framework for visual AI\nresearch, available at http://ai2thor.allenai.org. AI2-THOR consists of near\nphoto-realistic 3D indoor scenes, where AI agents can navigate in the scenes\nand interact with objects to perform tasks. AI2-THOR enables research in many\ndifferent domains including but not limited to deep reinforcement learning,\nimitation learning, learning by interaction, planning, visual question\nanswering, unsupervised representation learning, object detection and\nsegmentation, and learning models of cognition. The goal of AI2-THOR is to\nfacilitate building visually intelligent models and push the research forward\nin this domain.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2017-12-14T23:17:24+00:00",
    "updated": "2022-08-26T17:12:17+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2406.04551v2": {
    "id": "http://arxiv.org/abs/2406.04551v2",
    "title": "Improving Geo-diversity of Generated Images with Contextualized Vendi Score Guidance",
    "authors": [
      "Reyhane Askari Hemmat",
      "Melissa Hall",
      "Alicia Sun",
      "Candace Ross",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ],
    "abstract": "With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-06T23:35:51+00:00",
    "updated": "2024-08-02T16:09:49+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2303.08120v1": {
    "id": "http://arxiv.org/abs/2303.08120v1",
    "title": "Blind Video Deflickering by Neural Filtering with a Flawed Atlas",
    "authors": [
      "Chenyang Lei",
      "Xuanchi Ren",
      "Zhaoxiang Zhang",
      "Qifeng Chen"
    ],
    "abstract": "Many videos contain flickering artifacts. Common causes of flicker include\nvideo processing algorithms, video generation algorithms, and capturing videos\nunder specific situations. Prior work usually requires specific guidance such\nas the flickering frequency, manual annotations, or extra consistent videos to\nremove the flicker. In this work, we propose a general flicker removal\nframework that only receives a single flickering video as input without\nadditional guidance. Since it is blind to a specific flickering type or\nguidance, we name this \"blind deflickering.\" The core of our approach is\nutilizing the neural atlas in cooperation with a neural filtering strategy. The\nneural atlas is a unified representation for all frames in a video that\nprovides temporal consistency guidance but is flawed in many cases. To this\nend, a neural network is trained to mimic a filter to learn the consistent\nfeatures (e.g., color, brightness) and avoid introducing the artifacts in the\natlas. To validate our method, we construct a dataset that contains diverse\nreal-world flickering videos. Extensive experiments show that our method\nachieves satisfying deflickering performance and even outperforms baselines\nthat use extra guidance on a public benchmark.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-03-14T17:52:29+00:00",
    "updated": "2023-03-14T17:52:29+00:00",
    "doi": null,
    "comment": "To appear in CVPR2023. Code:\n  github.com/ChenyangLEI/All-In-One-Deflicker Website:\n  chenyanglei.github.io/deflicker",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2109.13821v2": {
    "id": "http://arxiv.org/abs/2109.13821v2",
    "title": "Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme",
    "authors": [
      "Vadim Popov",
      "Ivan Vovk",
      "Vladimir Gogoryan",
      "Tasnima Sadekova",
      "Mikhail Kudinov",
      "Jiansheng Wei"
    ],
    "abstract": "Voice conversion is a common speech synthesis task which can be solved in\ndifferent ways depending on a particular real-world scenario. The most\nchallenging one often referred to as one-shot many-to-many voice conversion\nconsists in copying the target voice from only one reference utterance in the\nmost general case when both source and target speakers do not belong to the\ntraining dataset. We present a scalable high-quality solution based on\ndiffusion probabilistic modeling and demonstrate its superior quality compared\nto state-of-the-art one-shot voice conversion approaches. Moreover, focusing on\nreal-time applications, we investigate general principles which can make\ndiffusion models faster while keeping synthesis quality at a high level. As a\nresult, we develop a novel Stochastic Differential Equations solver suitable\nfor various diffusion model types and generative tasks as shown through\nempirical studies and justify it by theoretical analysis.",
    "categories": [
      "cs.SD",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2021-09-28T15:48:22+00:00",
    "updated": "2022-08-04T10:25:40+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "1707.01219v2": {
    "id": "http://arxiv.org/abs/1707.01219v2",
    "title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer",
    "authors": [
      "Zehao Huang",
      "Naiyan Wang"
    ],
    "abstract": "Despite deep neural networks have demonstrated extraordinary power in various\napplications, their superior performances are at expense of high storage and\ncomputational costs. Consequently, the acceleration and compression of neural\nnetworks have attracted much attention recently. Knowledge Transfer (KT), which\naims at training a smaller student network by transferring knowledge from a\nlarger teacher model, is one of the popular solutions. In this paper, we\npropose a novel knowledge transfer method by treating it as a distribution\nmatching problem. Particularly, we match the distributions of neuron\nselectivity patterns between teacher and student networks. To achieve this\ngoal, we devise a new KT loss function by minimizing the Maximum Mean\nDiscrepancy (MMD) metric between these distributions. Combined with the\noriginal loss function, our method can significantly improve the performance of\nstudent networks. We validate the effectiveness of our method across several\ndatasets, and further combine it with other KT methods to explore the best\npossible results. Last but not least, we fine-tune the model to other tasks\nsuch as object detection. The results are also encouraging, which confirm the\ntransferability of the learned features.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2017-07-05T05:44:02+00:00",
    "updated": "2017-12-18T22:35:22+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2301.03410v2": {
    "id": "http://arxiv.org/abs/2301.03410v2",
    "title": "In Defense of Structural Symbolic Representation for Video Event-Relation Prediction",
    "authors": [
      "Andrew Lu",
      "Xudong Lin",
      "Yulei Niu",
      "Shih-Fu Chang"
    ],
    "abstract": "Understanding event relationships in videos requires a model to understand\nthe underlying structures of events (i.e. the event type, the associated\nargument roles, and corresponding entities) and factual knowledge for\nreasoning. Structural symbolic representation (SSR) based methods directly take\nevent types and associated argument roles/entities as inputs to perform\nreasoning. However, the state-of-the-art video event-relation prediction system\nshows the necessity of using continuous feature vectors from input videos;\nexisting methods based solely on SSR inputs fail completely, even when given\noracle event types and argument roles. In this paper, we conduct an extensive\nempirical analysis to answer the following questions: 1) why SSR-based method\nfailed; 2) how to understand the evaluation setting of video event relation\nprediction properly; 3) how to uncover the potential of SSR-based methods. We\nfirst identify suboptimal training settings as causing the failure of previous\nSSR-based video event prediction models. Then through qualitative and\nquantitative analysis, we show how evaluation that takes only video as inputs\nis currently unfeasible, as well as the reliance on oracle event information to\nobtain an accurate evaluation. Based on these findings, we propose to further\ncontextualize the SSR-based model to an Event-Sequence Model and equip it with\nmore factual knowledge through a simple yet effective way of reformulating\nexternal visual commonsense knowledge bases into an event-relation prediction\npretraining dataset. The resultant new state-of-the-art model eventually\nestablishes a 25% Macro-accuracy performance boost.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-01-06T14:43:19+00:00",
    "updated": "2023-04-12T15:19:16+00:00",
    "doi": null,
    "comment": "CVPRW 23, Learning with Limited Labelled Data",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.10003v1": {
    "id": "http://arxiv.org/abs/2312.10003v1",
    "title": "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent",
    "authors": [
      "Renat Aksitov",
      "Sobhan Miryoosefi",
      "Zonglin Li",
      "Daliang Li",
      "Sheila Babayan",
      "Kavya Kopparapu",
      "Zachary Fisher",
      "Ruiqi Guo",
      "Sushant Prakash",
      "Pranesh Srinivasan",
      "Manzil Zaheer",
      "Felix Yu",
      "Sanjiv Kumar"
    ],
    "abstract": "Answering complex natural language questions often necessitates multi-step\nreasoning and integrating external information. Several systems have combined\nknowledge retrieval with a large language model (LLM) to answer such questions.\nThese systems, however, suffer from various failure cases, and we cannot\ndirectly train them end-to-end to fix such failures, as interaction with\nexternal knowledge is non-differentiable. To address these deficiencies, we\ndefine a ReAct-style LLM agent with the ability to reason and act upon external\nknowledge. We further refine the agent through a ReST-like method that\niteratively trains on previous trajectories, employing growing-batch\nreinforcement learning with AI feedback for continuous self-improvement and\nself-distillation. Starting from a prompted large model and after just two\niterations of the algorithm, we can produce a fine-tuned small model that\nachieves comparable performance on challenging compositional question-answering\nbenchmarks with two orders of magnitude fewer parameters.",
    "categories": [
      "cs.CL"
    ],
    "published": "2023-12-15T18:20:15+00:00",
    "updated": "2023-12-15T18:20:15+00:00",
    "doi": null,
    "comment": "19 pages, 4 figures, 4 tables, 8 listings",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2308.06873v2": {
    "id": "http://arxiv.org/abs/2308.06873v2",
    "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
    "authors": [
      "Xiaofei Wang",
      "Manthan Thakker",
      "Zhuo Chen",
      "Naoyuki Kanda",
      "Sefik Emre Eskimez",
      "Sanyuan Chen",
      "Min Tang",
      "Shujie Liu",
      "Jinyu Li",
      "Takuya Yoshioka"
    ],
    "abstract": "Recent advancements in generative speech models based on audio-text prompts\nhave enabled remarkable innovations like high-quality zero-shot text-to-speech.\nHowever, existing models still face limitations in handling diverse audio-text\nspeech generation tasks involving transforming input speech and processing\naudio captured in adverse acoustic conditions. This paper introduces SpeechX, a\nversatile speech generation model capable of zero-shot TTS and various speech\ntransformation tasks, dealing with both clean and noisy signals. SpeechX\ncombines neural codec language modeling with multi-task learning using\ntask-dependent prompting, enabling unified and extensible modeling and\nproviding a consistent way for leveraging textual input in speech enhancement\nand transformation tasks. Experimental results show SpeechX's efficacy in\nvarious tasks, including zero-shot TTS, noise suppression, target speaker\nextraction, speech removal, and speech editing with or without background\nnoise, achieving comparable or superior performance to specialized models\nacross tasks. See https://aka.ms/speechx for demo samples.",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "published": "2023-08-14T01:01:19+00:00",
    "updated": "2024-06-25T18:38:28+00:00",
    "doi": null,
    "comment": "To appear in TASLP. See https://aka.ms/speechx for demo samples",
    "journal_ref": null,
    "primary_category": "eess.AS"
  },
  "2303.17591v1": {
    "id": "http://arxiv.org/abs/2303.17591v1",
    "title": "Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models",
    "authors": [
      "Eric Zhang",
      "Kai Wang",
      "Xingqian Xu",
      "Zhangyang Wang",
      "Humphrey Shi"
    ],
    "abstract": "The unlearning problem of deep learning models, once primarily an academic\nconcern, has become a prevalent issue in the industry. The significant advances\nin text-to-image generation techniques have prompted global discussions on\nprivacy, copyright, and safety, as numerous unauthorized personal IDs, content,\nartistic creations, and potentially harmful materials have been learned by\nthese models and later utilized to generate and distribute uncontrolled\ncontent. To address this challenge, we propose \\textbf{Forget-Me-Not}, an\nefficient and low-cost solution designed to safely remove specified IDs,\nobjects, or styles from a well-configured text-to-image model in as little as\n30 seconds, without impairing its ability to generate other content. Alongside\nour method, we introduce the \\textbf{Memorization Score (M-Score)} and\n\\textbf{ConceptBench} to measure the models' capacity to generate general\nconcepts, grouped into three primary categories: ID, object, and style. Using\nM-Score and ConceptBench, we demonstrate that Forget-Me-Not can effectively\neliminate targeted concepts while maintaining the model's performance on other\nconcepts. Furthermore, Forget-Me-Not offers two practical extensions: a)\nremoval of potentially harmful or NSFW content, and b) enhancement of model\naccuracy, inclusion and diversity through \\textbf{concept correction and\ndisentanglement}. It can also be adapted as a lightweight model patch for\nStable Diffusion, allowing for concept manipulation and convenient\ndistribution. To encourage future research in this critical area and promote\nthe development of safe and inclusive generative models, we will open-source\nour code and ConceptBench at\n\\href{https://github.com/SHI-Labs/Forget-Me-Not}{https://github.com/SHI-Labs/Forget-Me-Not}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-03-30T17:58:11+00:00",
    "updated": "2023-03-30T17:58:11+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2202.09367v3": {
    "id": "http://arxiv.org/abs/2202.09367v3",
    "title": "Snowflake Point Deconvolution for Point Cloud Completion and Generation with Skip-Transformer",
    "authors": [
      "Peng Xiang",
      "Xin Wen",
      "Yu-Shen Liu",
      "Yan-Pei Cao",
      "Pengfei Wan",
      "Wen Zheng",
      "Zhizhong Han"
    ],
    "abstract": "Most existing point cloud completion methods suffer from the discrete nature\nof point clouds and the unstructured prediction of points in local regions,\nwhich makes it difficult to reveal fine local geometric details. To resolve\nthis issue, we propose SnowflakeNet with snowflake point deconvolution (SPD) to\ngenerate complete point clouds. SPD models the generation of point clouds as\nthe snowflake-like growth of points, where child points are generated\nprogressively by splitting their parent points after each SPD. Our insight into\nthe detailed geometry is to introduce a skip-transformer in the SPD to learn\nthe point splitting patterns that can best fit the local regions. The\nskip-transformer leverages attention mechanism to summarize the splitting\npatterns used in the previous SPD layer to produce the splitting in the current\nlayer. The locally compact and structured point clouds generated by SPD\nprecisely reveal the structural characteristics of the 3D shape in local\npatches, which enables us to predict highly detailed geometries. Moreover,\nsince SPD is a general operation that is not limited to completion, we explore\nits applications in other generative tasks, including point cloud\nauto-encoding, generation, single image reconstruction, and upsampling. Our\nexperimental results outperform state-of-the-art methods under widely used\nbenchmarks.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-02-18T17:09:49+00:00",
    "updated": "2022-10-28T06:36:31+00:00",
    "doi": null,
    "comment": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI), 2022. This work is a journal extension of our ICCV 2021 paper\n  arXiv:2108.04444 . The first two authors contributed equally",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2105.07175v1": {
    "id": "http://arxiv.org/abs/2105.07175v1",
    "title": "Cross-Modal Progressive Comprehension for Referring Segmentation",
    "authors": [
      "Si Liu",
      "Tianrui Hui",
      "Shaofei Huang",
      "Yunchao Wei",
      "Bo Li",
      "Guanbin Li"
    ],
    "abstract": "Given a natural language expression and an image/video, the goal of referring\nsegmentation is to produce the pixel-level masks of the entities described by\nthe subject of the expression. Previous approaches tackle this problem by\nimplicit feature interaction and fusion between visual and linguistic\nmodalities in a one-stage manner. However, human tends to solve the referring\nproblem in a progressive manner based on informative words in the expression,\ni.e., first roughly locating candidate entities and then distinguishing the\ntarget one. In this paper, we propose a Cross-Modal Progressive Comprehension\n(CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I\n(Image) module and a CMPC-V (Video) module to improve referring image and video\nsegmentation models. For image data, our CMPC-I module first employs entity and\nattribute words to perceive all the related entities that might be considered\nby the expression. Then, the relational words are adopted to highlight the\ntarget entity as well as suppress other irrelevant ones by spatial graph\nreasoning. For video data, our CMPC-V module further exploits action words\nbased on CMPC-I to highlight the correct entity matched with the action cues by\ntemporal graph reasoning. In addition to the CMPC, we also introduce a simple\nyet effective Text-Guided Feature Exchange (TGFE) module to integrate the\nreasoned multimodal features corresponding to different levels in the visual\nbackbone under the guidance of textual information. In this way, multi-level\nfeatures can communicate with each other and be mutually refined based on the\ntextual context. Combining CMPC-I or CMPC-V with TGFE can form our image or\nvideo version referring segmentation frameworks and our frameworks achieve new\nstate-of-the-art performances on four referring image segmentation benchmarks\nand three referring video segmentation benchmarks respectively.",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "published": "2021-05-15T08:55:51+00:00",
    "updated": "2021-05-15T08:55:51+00:00",
    "doi": null,
    "comment": "Accepted by TPAMI 2021",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1906.05199v1": {
    "id": "http://arxiv.org/abs/1906.05199v1",
    "title": "Tackling Partial Domain Adaptation with Self-Supervision",
    "authors": [
      "Silvia Bucci",
      "Antonio D'Innocente",
      "Tatiana Tommasi"
    ],
    "abstract": "Domain adaptation approaches have shown promising results in reducing the\nmarginal distribution difference among visual domains. They allow to train\nreliable models that work over datasets of different nature (photos, paintings\netc), but they still struggle when the domains do not share an identical label\nspace. In the partial domain adaptation setting, where the target covers only a\nsubset of the source classes, it is challenging to reduce the domain gap\nwithout incurring in negative transfer. Many solutions just keep the standard\ndomain adaptation techniques by adding heuristic sample weighting strategies.\nIn this work we show how the self-supervisory signal obtained from the spatial\nco-location of patches can be used to define a side task that supports\nadaptation regardless of the exact label sharing condition across domains. We\nbuild over a recent work that introduced a jigsaw puzzle task for domain\ngeneralization: we describe how to reformulate this approach for partial domain\nadaptation and we show how it boosts existing adaptive solutions when combined\nwith them. The obtained experimental results on three datasets supports the\neffectiveness of our approach.",
    "categories": [
      "cs.CV"
    ],
    "published": "2019-06-12T15:16:31+00:00",
    "updated": "2019-06-12T15:16:31+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1611.04135v3": {
    "id": "http://arxiv.org/abs/1611.04135v3",
    "title": "Responses to Critiques on Machine Learning of Criminality Perceptions (Addendum of arXiv:1611.04135)",
    "authors": [
      "Xiaolin Wu",
      "Xi Zhang"
    ],
    "abstract": "In November 2016 we submitted to arXiv our paper \"Automated Inference on\nCriminality Using Face Images\". It generated a great deal of discussions in the\nInternet and some media outlets. Our work is only intended for pure academic\ndiscussions; how it has become a media consumption is a total surprise to us.\nAlthough in agreement with our critics on the need and importance of policing\nAI research for the general good of the society, we are deeply baffled by the\nways some of them mispresented our work, in particular the motive and objective\nof our research.",
    "categories": [
      "cs.CV"
    ],
    "published": "2016-11-13T13:32:11+00:00",
    "updated": "2017-05-26T07:48:10+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1710.10370v5": {
    "id": "http://arxiv.org/abs/1710.10370v5",
    "title": "Topology Adaptive Graph Convolutional Networks",
    "authors": [
      "Jian Du",
      "Shanghang Zhang",
      "Guanhang Wu",
      "Jose M. F. Moura",
      "Soummya Kar"
    ],
    "abstract": "Spectral graph convolutional neural networks (CNNs) require approximation to\nthe convolution to alleviate the computational complexity, resulting in\nperformance loss. This paper proposes the topology adaptive graph convolutional\nnetwork (TAGCN), a novel graph convolutional network defined in the vertex\ndomain. We provide a systematic way to design a set of fixed-size learnable\nfilters to perform convolutions on graphs. The topologies of these filters are\nadaptive to the topology of the graph when they scan the graph to perform\nconvolution. The TAGCN not only inherits the properties of convolutions in CNN\nfor grid-structured data, but it is also consistent with convolution as defined\nin graph signal processing. Since no approximation to the convolution is\nneeded, TAGCN exhibits better performance than existing spectral CNNs on a\nnumber of data sets and is also computationally simpler than other recent\nmethods.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2017-10-28T02:12:51+00:00",
    "updated": "2018-02-11T20:53:09+00:00",
    "doi": null,
    "comment": "13 pages",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1905.04899v2": {
    "id": "http://arxiv.org/abs/1905.04899v2",
    "title": "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features",
    "authors": [
      "Sangdoo Yun",
      "Dongyoon Han",
      "Seong Joon Oh",
      "Sanghyuk Chun",
      "Junsuk Choe",
      "Youngjoon Yoo"
    ],
    "abstract": "Regional dropout strategies have been proposed to enhance the performance of\nconvolutional neural network classifiers. They have proved to be effective for\nguiding the model to attend on less discriminative parts of objects (e.g. leg\nas opposed to head of a person), thereby letting the network generalize better\nand have better object localization capabilities. On the other hand, current\nmethods for regional dropout remove informative pixels on training images by\noverlaying a patch of either black pixels or random noise. Such removal is not\ndesirable because it leads to information loss and inefficiency during\ntraining. We therefore propose the CutMix augmentation strategy: patches are\ncut and pasted among training images where the ground truth labels are also\nmixed proportionally to the area of the patches. By making efficient use of\ntraining pixels and retaining the regularization effect of regional dropout,\nCutMix consistently outperforms the state-of-the-art augmentation strategies on\nCIFAR and ImageNet classification tasks, as well as on the ImageNet\nweakly-supervised localization task. Moreover, unlike previous augmentation\nmethods, our CutMix-trained ImageNet classifier, when used as a pretrained\nmodel, results in consistent performance gains in Pascal detection and MS-COCO\nimage captioning benchmarks. We also show that CutMix improves the model\nrobustness against input corruptions and its out-of-distribution detection\nperformances. Source code and pretrained models are available at\nhttps://github.com/clovaai/CutMix-PyTorch .",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2019-05-13T08:10:22+00:00",
    "updated": "2019-08-07T07:15:29+00:00",
    "doi": null,
    "comment": "Accepted at ICCV 2019 (oral talk). 14 pages, 5 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.15698v4": {
    "id": "http://arxiv.org/abs/2312.15698v4",
    "title": "RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair",
    "authors": [
      "Andr\u00e9 Silva",
      "Sen Fang",
      "Martin Monperrus"
    ],
    "abstract": "Automated Program Repair (APR) has evolved significantly with the advent of\nLarge Language Models (LLMs). Fine-tuning LLMs for program repair is a recent\navenue of research, with many dimensions which have not been explored. Existing\nwork mostly fine-tune LLMs with naive code representations and does not scale\nto frontier models. To address this problem, we propose RepairLLaMA, a novel\nprogram repair approach that 1) identifies optimal code representations for APR\nwith fine-tuned models, and 2) pioneers state-of-the-art parameter-efficient\nfine-tuning technique (PEFT) for program repair. This results in RepairLLaMA\nproducing a highly effective `program repair adapter' for fixing bugs with AI.\nOur experiments demonstrate the validity of both concepts. First, fine-tuning\nadapters with program repair specific code representations enables the model to\nuse meaningful repair signals and produce better patches. Second,\nparameter-efficient fine-tuning helps fine-tuning to converge and clearly\ncontributes to the effectiveness of RepairLLaMA in fixing bugs outside the\nfine-tuning data distribution. Overall, RepairLLaMA correctly fixes 144\nDefects4J v2 and 109 HumanEval-Java bugs, outperforming all baselines.",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2023-12-25T11:39:46+00:00",
    "updated": "2024-06-07T13:21:26+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "2204.07441v2": {
    "id": "http://arxiv.org/abs/2204.07441v2",
    "title": "COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval",
    "authors": [
      "Haoyu Lu",
      "Nanyi Fei",
      "Yuqi Huo",
      "Yizhao Gao",
      "Zhiwu Lu",
      "Ji-Rong Wen"
    ],
    "abstract": "Large-scale single-stream pre-training has shown dramatic performance in\nimage-text retrieval. Regrettably, it faces low inference efficiency due to\nheavy attention layers. Recently, two-stream methods like CLIP and ALIGN with\nhigh inference efficiency have also shown promising performance, however, they\nonly consider instance-level alignment between the two streams (thus there is\nstill room for improvement). To overcome these limitations, we propose a novel\nCOllaborative Two-Stream vision-language pretraining model termed COTS for\nimage-text retrieval by enhancing cross-modal interaction. In addition to\ninstance level alignment via momentum contrastive learning, we leverage two\nextra levels of cross-modal interactions in our COTS: (1) Token-level\ninteraction - a masked visionlanguage modeling (MVLM) learning objective is\ndevised without using a cross-stream network module, where variational\nautoencoder is imposed on the visual encoder to generate visual tokens for each\nimage. (2) Task-level interaction - a KL-alignment learning objective is\ndevised between text-to-image and image-to-text retrieval tasks, where the\nprobability distribution per task is computed with the negative queues in\nmomentum contrastive learning. Under a fair comparison setting, our COTS\nachieves the highest performance among all two-stream methods and comparable\nperformance (but with 10,800X faster in inference) w.r.t. the latest\nsingle-stream methods. Importantly, our COTS is also applicable to\ntext-to-video retrieval, yielding new state-ofthe-art on the widely-used\nMSR-VTT dataset.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2022-04-15T12:34:47+00:00",
    "updated": "2022-05-20T13:23:30+00:00",
    "doi": null,
    "comment": "Accepted by CVPR2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2004.06427v1": {
    "id": "http://arxiv.org/abs/2004.06427v1",
    "title": "Jointly Modeling Aspect and Sentiment with Dynamic Heterogeneous Graph Neural Networks",
    "authors": [
      "Shu Liu",
      "Wei Li",
      "Yunfang Wu",
      "Qi Su",
      "Xu Sun"
    ],
    "abstract": "Target-Based Sentiment Analysis aims to detect the opinion aspects (aspect\nextraction) and the sentiment polarities (sentiment detection) towards them.\nBoth the previous pipeline and integrated methods fail to precisely model the\ninnate connection between these two objectives. In this paper, we propose a\nnovel dynamic heterogeneous graph to jointly model the two objectives in an\nexplicit way. Both the ordinary words and sentiment labels are treated as nodes\nin the heterogeneous graph, so that the aspect words can interact with the\nsentiment information. The graph is initialized with multiple types of\ndependencies, and dynamically modified during real-time prediction. Experiments\non the benchmark datasets show that our model outperforms the state-of-the-art\nmodels. Further analysis demonstrates that our model obtains significant\nperformance gain on the challenging instances under multiple-opinion aspects\nand no-opinion aspect situations.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2020-04-14T11:27:30+00:00",
    "updated": "2020-04-14T11:27:30+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2109.09881v1": {
    "id": "http://arxiv.org/abs/2109.09881v1",
    "title": "Estimating and Exploiting the Aleatoric Uncertainty in Surface Normal Estimation",
    "authors": [
      "Gwangbin Bae",
      "Ignas Budvytis",
      "Roberto Cipolla"
    ],
    "abstract": "Surface normal estimation from a single image is an important task in 3D\nscene understanding. In this paper, we address two limitations shared by the\nexisting methods: the inability to estimate the aleatoric uncertainty and lack\nof detail in the prediction. The proposed network estimates the per-pixel\nsurface normal probability distribution. We introduce a new parameterization\nfor the distribution, such that its negative log-likelihood is the angular loss\nwith learned attenuation. The expected value of the angular error is then used\nas a measure of the aleatoric uncertainty. We also present a novel decoder\nframework where pixel-wise multi-layer perceptrons are trained on a subset of\npixels sampled based on the estimated uncertainty. The proposed\nuncertainty-guided sampling prevents the bias in training towards large planar\nsurfaces and improves the quality of prediction, especially near object\nboundaries and on small structures. Experimental results show that the proposed\nmethod outperforms the state-of-the-art in ScanNet and NYUv2, and that the\nestimated uncertainty correlates well with the prediction error. Code is\navailable at https://github.com/baegwangbin/surface_normal_uncertainty.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-09-20T23:30:04+00:00",
    "updated": "2021-09-20T23:30:04+00:00",
    "doi": null,
    "comment": "ICCV 2021 (oral)",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2301.11100v1": {
    "id": "http://arxiv.org/abs/2301.11100v1",
    "title": "Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities",
    "authors": [
      "Melissa Hall",
      "Laura Gustafson",
      "Aaron Adcock",
      "Ishan Misra",
      "Candace Ross"
    ],
    "abstract": "We explore the extent to which zero-shot vision-language models exhibit\ngender bias for different vision tasks. Vision models traditionally required\ntask-specific labels for representing concepts, as well as finetuning;\nzero-shot models like CLIP instead perform tasks with an open-vocabulary,\nmeaning they do not need a fixed set of labels, by using text embeddings to\nrepresent concepts. With these capabilities in mind, we ask: Do vision-language\nmodels exhibit gender bias when performing zero-shot image classification,\nobject detection and semantic segmentation? We evaluate different\nvision-language models with multiple datasets across a set of concepts and find\n(i) all models evaluated show distinct performance differences based on the\nperceived gender of the person co-occurring with a given concept in the image\nand that aggregating analyses over all concepts can mask these concerns; (ii)\nmodel calibration (i.e. the relationship between accuracy and confidence) also\ndiffers distinctly by perceived gender, even when evaluating on similar\nrepresentations of concepts; and (iii) these observed disparities align with\nexisting gender biases in word embeddings from language models. These findings\nsuggest that, while language greatly expands the capability of vision tasks, it\ncan also contribute to social biases in zero-shot vision settings. Furthermore,\nbiases can further propagate when foundational models like CLIP are used by\nother models to enable zero-shot capabilities.",
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2023-01-26T13:44:31+00:00",
    "updated": "2023-01-26T13:44:31+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2205.00434v1": {
    "id": "http://arxiv.org/abs/2205.00434v1",
    "title": "Reinforced Swin-Convs Transformer for Underwater Image Enhancement",
    "authors": [
      "Tingdi Ren",
      "Haiyong Xu",
      "Gangyi Jiang",
      "Mei Yu",
      "Ting Luo"
    ],
    "abstract": "Underwater Image Enhancement (UIE) technology aims to tackle the challenge of\nrestoring the degraded underwater images due to light absorption and\nscattering. To address problems, a novel U-Net based Reinforced Swin-Convs\nTransformer for the Underwater Image Enhancement method (URSCT-UIE) is\nproposed. Specifically, with the deficiency of U-Net based on pure\nconvolutions, we embedded the Swin Transformer into U-Net for improving the\nability to capture the global dependency. Then, given the inadequacy of the\nSwin Transformer capturing the local attention, the reintroduction of\nconvolutions may capture more local attention. Thus, we provide an ingenious\nmanner for the fusion of convolutions and the core attention mechanism to build\na Reinforced Swin-Convs Transformer Block (RSCTB) for capturing more local\nattention, which is reinforced in the channel and the spatial attention of the\nSwin Transformer. Finally, the experimental results on available datasets\ndemonstrate that the proposed URSCT-UIE achieves state-of-the-art performance\ncompared with other methods in terms of both subjective and objective\nevaluations. The code will be released on GitHub after acceptance.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "published": "2022-05-01T09:46:33+00:00",
    "updated": "2022-05-01T09:46:33+00:00",
    "doi": null,
    "comment": "Submitted by NeurIPS 2022",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2309.03445v1": {
    "id": "http://arxiv.org/abs/2309.03445v1",
    "title": "Underwater Image Enhancement by Transformer-based Diffusion Model with Non-uniform Sampling for Skip Strategy",
    "authors": [
      "Yi Tang",
      "Takafumi Iwaguchi",
      "Hiroshi Kawasaki"
    ],
    "abstract": "In this paper, we present an approach to image enhancement with diffusion\nmodel in underwater scenes. Our method adapts conditional denoising diffusion\nprobabilistic models to generate the corresponding enhanced images by using the\nunderwater images and the Gaussian noise as the inputs. Additionally, in order\nto improve the efficiency of the reverse process in the diffusion model, we\nadopt two different ways. We firstly propose a lightweight transformer-based\ndenoising network, which can effectively promote the time of network forward\nper iteration. On the other hand, we introduce a skip sampling strategy to\nreduce the number of iterations. Besides, based on the skip sampling strategy,\nwe propose two different non-uniform sampling methods for the sequence of the\ntime step, namely piecewise sampling and searching with the evolutionary\nalgorithm. Both of them are effective and can further improve performance by\nusing the same steps against the previous uniform sampling. In the end, we\nconduct a relative evaluation of the widely used underwater enhancement\ndatasets between the recent state-of-the-art methods and the proposed approach.\nThe experimental results prove that our approach can achieve both competitive\nperformance and high efficiency. Our code is available at\n\\href{mailto:https://github.com/piggy2009/DM_underwater}{\\color{blue}{https://github.com/piggy2009/DM\\_underwater}}.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-09-07T01:58:06+00:00",
    "updated": "2023-09-07T01:58:06+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2011.07205v1": {
    "id": "http://arxiv.org/abs/2011.07205v1",
    "title": "Bi-Dimensional Feature Alignment for Cross-Domain Object Detection",
    "authors": [
      "Zhen Zhao",
      "Yuhong Guo",
      "Jieping Ye"
    ],
    "abstract": "Recently the problem of cross-domain object detection has started drawing\nattention in the computer vision community. In this paper, we propose a novel\nunsupervised cross-domain detection model that exploits the annotated data in a\nsource domain to train an object detector for a different target domain. The\nproposed model mitigates the cross-domain representation divergence for object\ndetection by performing cross-domain feature alignment in two dimensions, the\ndepth dimension and the spatial dimension. In the depth dimension of channel\nlayers, it uses inter-channel information to bridge the domain divergence with\nrespect to image style alignment. In the dimension of spatial layers, it\ndeploys spatial attention modules to enhance detection relevant regions and\nsuppress irrelevant regions with respect to cross-domain feature alignment.\nExperiments are conducted on a number of benchmark cross-domain detection\ndatasets. The empirical results show the proposed method outperforms the\nstate-of-the-art comparison methods.",
    "categories": [
      "cs.CV"
    ],
    "published": "2020-11-14T03:03:11+00:00",
    "updated": "2020-11-14T03:03:11+00:00",
    "doi": null,
    "comment": "ECCV20 TASK-CV Workshop",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1805.03162v1": {
    "id": "http://arxiv.org/abs/1805.03162v1",
    "title": "Polite Dialogue Generation Without Parallel Data",
    "authors": [
      "Tong Niu",
      "Mohit Bansal"
    ],
    "abstract": "Stylistic dialogue response generation, with valuable applications in\npersonality-based conversational agents, is a challenging task because the\nresponse needs to be fluent, contextually-relevant, as well as\nparalinguistically accurate. Moreover, parallel datasets for\nregular-to-stylistic pairs are usually unavailable. We present three\nweakly-supervised models that can generate diverse polite (or rude) dialogue\nresponses without parallel data. Our late fusion model (Fusion) merges the\ndecoder of an encoder-attention-decoder dialogue model with a language model\ntrained on stand-alone polite utterances. Our label-fine-tuning (LFT) model\nprepends to each source sequence a politeness-score scaled label (predicted by\nour state-of-the-art politeness classifier) during training, and at test time\nis able to generate polite, neutral, and rude responses by simply scaling the\nlabel embedding by the corresponding score. Our reinforcement learning model\n(Polite-RL) encourages politeness generation by assigning rewards proportional\nto the politeness classifier score of the sampled response. We also present two\nretrieval-based polite dialogue model baselines. Human evaluation validates\nthat while the Fusion and the retrieval-based models achieve politeness with\npoorer context-relevance, the LFT and Polite-RL models can produce\nsignificantly more polite responses without sacrificing dialogue quality.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2018-05-08T16:56:15+00:00",
    "updated": "2018-05-08T16:56:15+00:00",
    "doi": null,
    "comment": "To Appear in TACL Journal (16 pages) (first submission cycle: Oct1\n  2017)",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2211.10194v2": {
    "id": "http://arxiv.org/abs/2211.10194v2",
    "title": "Self-Remixing: Unsupervised Speech Separation via Separation and Remixing",
    "authors": [
      "Kohei Saijo",
      "Tetsuji Ogawa"
    ],
    "abstract": "We present Self-Remixing, a novel self-supervised speech separation method,\nwhich refines a pre-trained separation model in an unsupervised manner. The\nproposed method consists of a shuffler module and a solver module, and they\ngrow together through separation and remixing processes. Specifically, the\nshuffler first separates observed mixtures and makes pseudo-mixtures by\nshuffling and remixing the separated signals. The solver then separates the\npseudo-mixtures and remixes the separated signals back to the observed\nmixtures. The solver is trained using the observed mixtures as supervision,\nwhile the shuffler's weights are updated by taking the moving average with the\nsolver's, generating the pseudo-mixtures with fewer distortions. Our\nexperiments demonstrate that Self-Remixing gives better performance over\nexisting remixing-based self-supervised methods with the same or less training\ncosts under unsupervised setup. Self-Remixing also outperforms baselines in\nsemi-supervised domain adaptation, showing effectiveness in multiple setups.",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "published": "2022-11-18T12:37:32+00:00",
    "updated": "2023-09-01T10:13:08+00:00",
    "doi": null,
    "comment": "Accepted by ICASSP2023, 5pages, 2figures, 2tables",
    "journal_ref": null,
    "primary_category": "eess.AS"
  },
  "2406.17256v1": {
    "id": "http://arxiv.org/abs/2406.17256v1",
    "title": "Disentangled Motion Modeling for Video Frame Interpolation",
    "authors": [
      "Jaihyun Lew",
      "Jooyoung Choi",
      "Chaehun Shin",
      "Dahuin Jung",
      "Sungroh Yoon"
    ],
    "abstract": "Video frame interpolation (VFI) aims to synthesize intermediate frames in\nbetween existing frames to enhance visual smoothness and quality. Beyond the\nconventional methods based on the reconstruction loss, recent works employ the\nhigh quality generative models for perceptual quality. However, they require\ncomplex training and large computational cost for modeling on the pixel space.\nIn this paper, we introduce disentangled Motion Modeling (MoMo), a\ndiffusion-based approach for VFI that enhances visual quality by focusing on\nintermediate motion modeling. We propose disentangled two-stage training\nprocess, initially training a frame synthesis model to generate frames from\ninput pairs and their optical flows. Subsequently, we propose a motion\ndiffusion model, equipped with our novel diffusion U-Net architecture designed\nfor optical flow, to produce bi-directional flows between frames. This method,\nby leveraging the simpler low-frequency representation of motions, achieves\nsuperior perceptual quality with reduced computational demands compared to\ngenerative modeling methods on the pixel space. Our method surpasses\nstate-of-the-art methods in perceptual metrics across various benchmarks,\ndemonstrating its efficacy and efficiency in VFI. Our code is available at:\nhttps://github.com/JHLew/MoMo",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-06-25T03:50:20+00:00",
    "updated": "2024-06-25T03:50:20+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1510.04342v4": {
    "id": "http://arxiv.org/abs/1510.04342v4",
    "title": "Estimation and Inference of Heterogeneous Treatment Effects using Random Forests",
    "authors": [
      "Stefan Wager",
      "Susan Athey"
    ],
    "abstract": "Many scientific and engineering challenges -- ranging from personalized\nmedicine to customized marketing recommendations -- require an understanding of\ntreatment effect heterogeneity. In this paper, we develop a non-parametric\ncausal forest for estimating heterogeneous treatment effects that extends\nBreiman's widely used random forest algorithm. In the potential outcomes\nframework with unconfoundedness, we show that causal forests are pointwise\nconsistent for the true treatment effect, and have an asymptotically Gaussian\nand centered sampling distribution. We also discuss a practical method for\nconstructing asymptotic confidence intervals for the true treatment effect that\nare centered at the causal forest estimates. Our theoretical results rely on a\ngeneric Gaussian theory for a large family of random forest algorithms. To our\nknowledge, this is the first set of results that allows any type of random\nforest, including classification and regression forests, to be used for\nprovably valid statistical inference. In experiments, we find causal forests to\nbe substantially more powerful than classical methods based on nearest-neighbor\nmatching, especially in the presence of irrelevant covariates.",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "published": "2015-10-14T22:54:59+00:00",
    "updated": "2017-07-10T01:15:47+00:00",
    "doi": null,
    "comment": "To appear in the Journal of the American Statistical Association.\n  Part of the results developed in this paper were made available as an earlier\n  technical report \"Asymptotic Theory for Random Forests\", available at\n  (arXiv:1405.0352)",
    "journal_ref": null,
    "primary_category": "stat.ME"
  },
  "2401.06066v1": {
    "id": "http://arxiv.org/abs/2401.06066v1",
    "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
    "authors": [
      "Damai Dai",
      "Chengqi Deng",
      "Chenggang Zhao",
      "R. X. Xu",
      "Huazuo Gao",
      "Deli Chen",
      "Jiashi Li",
      "Wangding Zeng",
      "Xingkai Yu",
      "Y. Wu",
      "Zhenda Xie",
      "Y. K. Li",
      "Panpan Huang",
      "Fuli Luo",
      "Chong Ruan",
      "Zhifang Sui",
      "Wenfeng Liang"
    ],
    "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising\narchitecture for managing computational costs when scaling up model parameters.\nHowever, conventional MoE architectures like GShard, which activate the top-$K$\nout of $N$ experts, face challenges in ensuring expert specialization, i.e.\neach expert acquires non-overlapping and focused knowledge. In response, we\npropose the DeepSeekMoE architecture towards ultimate expert specialization. It\ninvolves two principal strategies: (1) finely segmenting the experts into $mN$\nones and activating $mK$ from them, allowing for a more flexible combination of\nactivated experts; (2) isolating $K_s$ experts as shared ones, aiming at\ncapturing common knowledge and mitigating redundancy in routed experts.\nStarting from a modest scale with 2B parameters, we demonstrate that\nDeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5\ntimes the expert parameters and computation. In addition, DeepSeekMoE 2B nearly\napproaches the performance of its dense counterpart with the same number of\ntotal parameters, which set the upper bound of MoE models. Subsequently, we\nscale up DeepSeekMoE to 16B parameters and show that it achieves comparable\nperformance with LLaMA2 7B, with only about 40% of computations. Further, our\npreliminary efforts to scale up DeepSeekMoE to 145B parameters consistently\nvalidate its substantial advantages over the GShard architecture, and show its\nperformance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%)\nof computations.",
    "categories": [
      "cs.CL"
    ],
    "published": "2024-01-11T17:31:42+00:00",
    "updated": "2024-01-11T17:31:42+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1603.09246v3": {
    "id": "http://arxiv.org/abs/1603.09246v3",
    "title": "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles",
    "authors": [
      "Mehdi Noroozi",
      "Paolo Favaro"
    ],
    "abstract": "In this paper we study the problem of image representation learning without\nhuman annotation. By following the principles of self-supervision, we build a\nconvolutional neural network (CNN) that can be trained to solve Jigsaw puzzles\nas a pretext task, which requires no manual labeling, and then later repurposed\nto solve object classification and detection. To maintain the compatibility\nacross tasks we introduce the context-free network (CFN), a siamese-ennead CNN.\nThe CFN takes image tiles as input and explicitly limits the receptive field\n(or context) of its early processing units to one tile at a time. We show that\nthe CFN includes fewer parameters than AlexNet while preserving the same\nsemantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we\nlearn both a feature mapping of object parts as well as their correct spatial\narrangement. Our experimental evaluations show that the learned features\ncapture semantically relevant content. Our proposed method for learning visual\nrepresentations outperforms state of the art methods in several transfer\nlearning benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "published": "2016-03-30T15:27:37+00:00",
    "updated": "2017-08-22T17:32:19+00:00",
    "doi": null,
    "comment": "ECCV 2016",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2111.11165v2": {
    "id": "http://arxiv.org/abs/2111.11165v2",
    "title": "Graph-Based Similarity of Neural Network Representations",
    "authors": [
      "Zuohui Chen",
      "Yao Lu",
      "Jinxuan Hu",
      "Wen Yang",
      "Qi Xuan",
      "Zhen Wang",
      "Xiaoniu Yang"
    ],
    "abstract": "Understanding the black-box representations in Deep Neural Networks (DNN) is\nan essential problem in deep learning. In this work, we propose Graph-Based\nSimilarity (GBS) to measure the similarity of layer features. Contrary to\nprevious works that compute the similarity directly on the feature maps, GBS\nmeasures the correlation based on the graph constructed with hidden layer\noutputs. By treating each input sample as a node and the corresponding layer\noutput similarity as edges, we construct the graph of DNN representations for\neach layer. The similarity between graphs of layers identifies the\ncorrespondences between representations of models trained in different datasets\nand initializations. We demonstrate and prove the invariance property of GBS,\nincluding invariance to orthogonal transformation and invariance to isotropic\nscaling, and compare GBS with CKA. GBS shows state-of-the-art performance in\nreflecting the similarity and provides insights on explaining the adversarial\nsample behavior on the hidden layer space.",
    "categories": [
      "cs.LG"
    ],
    "published": "2021-11-22T13:04:19+00:00",
    "updated": "2022-05-25T08:55:54+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2309.15606v1": {
    "id": "http://arxiv.org/abs/2309.15606v1",
    "title": "From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining",
    "authors": [
      "Xiaoxue Ren",
      "Xinyuan Ye",
      "Dehai Zhao",
      "Zhenchang Xing",
      "Xiaohu Yang"
    ],
    "abstract": "Large Language Models (LLMs) have shown promising results in automatic code\ngeneration by improving coding efficiency to a certain extent. However,\ngenerating high-quality and reliable code remains a formidable task because of\nLLMs' lack of good programming practice, especially in exception handling. In\nthis paper, we first conduct an empirical study and summarise three crucial\nchallenges of LLMs in exception handling, i.e., incomplete exception handling,\nincorrect exception handling and abuse of try-catch. We then try prompts with\ndifferent granularities to address such challenges, finding fine-grained\nknowledge-driven prompts works best. Based on our empirical study, we propose a\nnovel Knowledge-driven Prompt Chaining-based code generation approach, name\nKPC, which decomposes code generation into an AI chain with iterative\ncheck-rewrite steps and chains fine-grained knowledge-driven prompts to assist\nLLMs in considering exception-handling specifications. We evaluate our\nKPC-based approach with 3,079 code generation tasks extracted from the Java\nofficial API documentation. Extensive experimental results demonstrate that the\nKPC-based approach has considerable potential to ameliorate the quality of code\ngenerated by LLMs. It achieves this through proficiently managing exceptions\nand obtaining remarkable enhancements of 109.86% and 578.57% with static\nevaluation methods, as well as a reduction of 18 runtime bugs in the sampled\ndataset with dynamic validation.",
    "categories": [
      "cs.SE"
    ],
    "published": "2023-09-27T12:09:07+00:00",
    "updated": "2023-09-27T12:09:07+00:00",
    "doi": null,
    "comment": "Accepted by 38th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2023)",
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "1703.09387v1": {
    "id": "http://arxiv.org/abs/1703.09387v1",
    "title": "Adversarial Transformation Networks: Learning to Generate Adversarial Examples",
    "authors": [
      "Shumeet Baluja",
      "Ian Fischer"
    ],
    "abstract": "Multiple different approaches of generating adversarial examples have been\nproposed to attack deep neural networks. These approaches involve either\ndirectly computing gradients with respect to the image pixels, or directly\nsolving an optimization on the image pixels. In this work, we present a\nfundamentally new method for generating adversarial examples that is fast to\nexecute and provides exceptional diversity of output. We efficiently train\nfeed-forward neural networks in a self-supervised manner to generate\nadversarial examples against a target network or set of networks. We call such\na network an Adversarial Transformation Network (ATN). ATNs are trained to\ngenerate adversarial examples that minimally modify the classifier's outputs\ngiven the original input, while constraining the new classification to match an\nadversarial target class. We present methods to train ATNs and analyze their\neffectiveness targeting a variety of MNIST classifiers as well as the latest\nstate-of-the-art ImageNet classifier Inception ResNet v2.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2017-03-28T03:24:33+00:00",
    "updated": "2017-03-28T03:24:33+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.NE"
  },
  "2211.14558v1": {
    "id": "http://arxiv.org/abs/2211.14558v1",
    "title": "Toward Universal Text-to-Music Retrieval",
    "authors": [
      "SeungHeon Doh",
      "Minz Won",
      "Keunwoo Choi",
      "Juhan Nam"
    ],
    "abstract": "This paper introduces effective design choices for text-to-music retrieval\nsystems. An ideal text-based retrieval system would support various input\nqueries such as pre-defined tags, unseen tags, and sentence-level descriptions.\nIn reality, most previous works mainly focused on a single query type (tag or\nsentence) which may not generalize to another input type. Hence, we review\nrecent text-based music retrieval systems using our proposed benchmark in two\nmain aspects: input text representation and training objectives. Our findings\nenable a universal text-to-music retrieval system that achieves comparable\nretrieval performances in both tag- and sentence-level inputs. Furthermore, the\nproposed multimodal representation generalizes to 9 different downstream music\nclassification tasks. We present the code and demo online.",
    "categories": [
      "cs.IR",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2022-11-26T13:07:26+00:00",
    "updated": "2022-11-26T13:07:26+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.IR"
  },
  "1804.00090v1": {
    "id": "http://arxiv.org/abs/1804.00090v1",
    "title": "FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans",
    "authors": [
      "Chen Liu",
      "Jiaye Wu",
      "Yasutaka Furukawa"
    ],
    "abstract": "The ultimate goal of this indoor mapping research is to automatically\nreconstruct a floorplan simply by walking through a house with a smartphone in\na pocket. This paper tackles this problem by proposing FloorNet, a novel deep\nneural architecture. The challenge lies in the processing of RGBD streams\nspanning a large 3D space. FloorNet effectively processes the data through\nthree neural network branches: 1) PointNet with 3D points, exploiting the 3D\ninformation; 2) CNN with a 2D point density image in a top-down view, enhancing\nthe local spatial reasoning; and 3) CNN with RGB images, utilizing the full\nimage information. FloorNet exchanges intermediate features across the branches\nto exploit the best of all the architectures. We have created a benchmark for\nfloorplan reconstruction by acquiring RGBD video streams for 155 residential\nhouses or apartments with Google Tango phones and annotating complete floorplan\ninformation. Our qualitative and quantitative evaluations demonstrate that the\nfusion of three branches effectively improves the reconstruction quality. We\nhope that the paper together with the benchmark will be an important step\ntowards solving a challenging vector-graphics reconstruction problem. Code and\ndata are available at https://github.com/art-programmer/FloorNet.",
    "categories": [
      "cs.CV"
    ],
    "published": "2018-03-31T00:22:27+00:00",
    "updated": "2018-03-31T00:22:27+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2302.07739v1": {
    "id": "http://arxiv.org/abs/2302.07739v1",
    "title": "Meta-Learning Triplet Network with Adaptive Margins for Few-Shot Named Entity Recognition",
    "authors": [
      "Chengcheng Han",
      "Renyu Zhu",
      "Jun Kuang",
      "FengJiao Chen",
      "Xiang Li",
      "Ming Gao",
      "Xuezhi Cao",
      "Wei Wu"
    ],
    "abstract": "Meta-learning methods have been widely used in few-shot named entity\nrecognition (NER), especially prototype-based methods. However, the Other(O)\nclass is difficult to be represented by a prototype vector because there are\ngenerally a large number of samples in the class that have miscellaneous\nsemantics. To solve the problem, we propose MeTNet, which generates prototype\nvectors for entity types only but not O-class. We design an improved triplet\nnetwork to map samples and prototype vectors into a low-dimensional space that\nis easier to be classified and propose an adaptive margin for each entity type.\nThe margin plays as a radius and controls a region with adaptive size in the\nlow-dimensional space. Based on the regions, we propose a new inference\nprocedure to predict the label of a query instance. We conduct extensive\nexperiments in both in-domain and cross-domain settings to show the superiority\nof MeTNet over other state-of-the-art methods. In particular, we release a\nChinese few-shot NER dataset FEW-COMM extracted from a well-known e-commerce\nplatform. To the best of our knowledge, this is the first Chinese few-shot NER\ndataset. All the datasets and codes are provided at\nhttps://github.com/hccngu/MeTNet.",
    "categories": [
      "cs.CL"
    ],
    "published": "2023-02-14T15:10:26+00:00",
    "updated": "2023-02-14T15:10:26+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2405.04356v1": {
    "id": "http://arxiv.org/abs/2405.04356v1",
    "title": "Diffusion-driven GAN Inversion for Multi-Modal Face Image Generation",
    "authors": [
      "Jihyun Kim",
      "Changjae Oh",
      "Hoseok Do",
      "Soohyun Kim",
      "Kwanghoon Sohn"
    ],
    "abstract": "We present a new multi-modal face image generation method that converts a\ntext prompt and a visual input, such as a semantic mask or scribble map, into a\nphoto-realistic face image. To do this, we combine the strengths of Generative\nAdversarial networks (GANs) and diffusion models (DMs) by employing the\nmulti-modal features in the DM into the latent space of the pre-trained GANs.\nWe present a simple mapping and a style modulation network to link two models\nand convert meaningful representations in feature maps and attention maps into\nlatent codes. With GAN inversion, the estimated latent codes can be used to\ngenerate 2D or 3D-aware facial images. We further present a multi-step training\nstrategy that reflects textual and structural representations into the\ngenerated image. Our proposed network produces realistic 2D, multi-view, and\nstylized face images, which align well with inputs. We validate our method by\nusing pre-trained 2D and 3D GANs, and our results outperform existing methods.\nOur project page is available at\nhttps://github.com/1211sh/Diffusion-driven_GAN-Inversion/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-05-07T14:33:40+00:00",
    "updated": "2024-05-07T14:33:40+00:00",
    "doi": null,
    "comment": "Accepted by CVPR 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2301.11325v1": {
    "id": "http://arxiv.org/abs/2301.11325v1",
    "title": "MusicLM: Generating Music From Text",
    "authors": [
      "Andrea Agostinelli",
      "Timo I. Denk",
      "Zal\u00e1n Borsos",
      "Jesse Engel",
      "Mauro Verzetti",
      "Antoine Caillon",
      "Qingqing Huang",
      "Aren Jansen",
      "Adam Roberts",
      "Marco Tagliasacchi",
      "Matt Sharifi",
      "Neil Zeghidour",
      "Christian Frank"
    ],
    "abstract": "We introduce MusicLM, a model generating high-fidelity music from text\ndescriptions such as \"a calming violin melody backed by a distorted guitar\nriff\". MusicLM casts the process of conditional music generation as a\nhierarchical sequence-to-sequence modeling task, and it generates music at 24\nkHz that remains consistent over several minutes. Our experiments show that\nMusicLM outperforms previous systems both in audio quality and adherence to the\ntext description. Moreover, we demonstrate that MusicLM can be conditioned on\nboth text and a melody in that it can transform whistled and hummed melodies\naccording to the style described in a text caption. To support future research,\nwe publicly release MusicCaps, a dataset composed of 5.5k music-text pairs,\nwith rich text descriptions provided by human experts.",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "published": "2023-01-26T18:58:53+00:00",
    "updated": "2023-01-26T18:58:53+00:00",
    "doi": null,
    "comment": "Supplementary material at\n  https://google-research.github.io/seanet/musiclm/examples and\n  https://kaggle.com/datasets/googleai/musiccaps",
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "2404.09735v1": {
    "id": "http://arxiv.org/abs/2404.09735v1",
    "title": "Equipping Diffusion Models with Differentiable Spatial Entropy for Low-Light Image Enhancement",
    "authors": [
      "Wenyi Lian",
      "Wenjing Lian",
      "Ziwei Luo"
    ],
    "abstract": "Image restoration, which aims to recover high-quality images from their\ncorrupted counterparts, often faces the challenge of being an ill-posed problem\nthat allows multiple solutions for a single input. However, most deep learning\nbased works simply employ l1 loss to train their network in a deterministic\nway, resulting in over-smoothed predictions with inferior perceptual quality.\nIn this work, we propose a novel method that shifts the focus from a\ndeterministic pixel-by-pixel comparison to a statistical perspective,\nemphasizing the learning of distributions rather than individual pixel values.\nThe core idea is to introduce spatial entropy into the loss function to measure\nthe distribution difference between predictions and targets. To make this\nspatial entropy differentiable, we employ kernel density estimation (KDE) to\napproximate the probabilities for specific intensity values of each pixel with\ntheir neighbor areas. Specifically, we equip the entropy with diffusion models\nand aim for superior accuracy and enhanced perceptual quality over l1 based\nnoise matching loss. In the experiments, we evaluate the proposed method for\nlow light enhancement on two datasets and the NTIRE challenge 2024. All these\nresults illustrate the effectiveness of our statistic-based entropy loss. Code\nis available at https://github.com/shermanlian/spatial-entropy-loss.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-15T12:35:10+00:00",
    "updated": "2024-04-15T12:35:10+00:00",
    "doi": null,
    "comment": "CVPRW 2024, best LPIPS in the NTIRE low light enhancement challenge\n  2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2101.02477v2": {
    "id": "http://arxiv.org/abs/2101.02477v2",
    "title": "GAN-Control: Explicitly Controllable GANs",
    "authors": [
      "Alon Shoshan",
      "Nadav Bhonker",
      "Igor Kviatkovsky",
      "Gerard Medioni"
    ],
    "abstract": "We present a framework for training GANs with explicit control over generated\nimages. We are able to control the generated image by settings exact attributes\nsuch as age, pose, expression, etc. Most approaches for editing GAN-generated\nimages achieve partial control by leveraging the latent space disentanglement\nproperties, obtained implicitly after standard GAN training. Such methods are\nable to change the relative intensity of certain attributes, but not explicitly\nset their values. Recently proposed methods, designed for explicit control over\nhuman faces, harness morphable 3D face models to allow fine-grained control\ncapabilities in GANs. Unlike these methods, our control is not constrained to\nmorphable 3D face model parameters and is extendable beyond the domain of human\nfaces. Using contrastive learning, we obtain GANs with an explicitly\ndisentangled latent space. This disentanglement is utilized to train\ncontrol-encoders mapping human-interpretable inputs to suitable latent vectors,\nthus allowing explicit control. In the domain of human faces we demonstrate\ncontrol over identity, age, pose, expression, hair color and illumination. We\nalso demonstrate control capabilities of our framework in the domains of\npainted portraits and dog image generation. We demonstrate that our approach\nachieves state-of-the-art performance both qualitatively and quantitatively.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-01-07T10:54:17+00:00",
    "updated": "2021-10-03T10:09:39+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2206.15331v2": {
    "id": "http://arxiv.org/abs/2206.15331v2",
    "title": "GitHub Copilot AI pair programmer: Asset or Liability?",
    "authors": [
      "Arghavan Moradi Dakhel",
      "Vahid Majdinasab",
      "Amin Nikanjam",
      "Foutse Khomh",
      "Michel C. Desmarais",
      "Zhen Ming",
      "Jiang"
    ],
    "abstract": "Automatic program synthesis is a long-lasting dream in software engineering.\nRecently, a promising Deep Learning (DL) based solution, called Copilot, has\nbeen proposed by OpenAI and Microsoft as an industrial product. Although some\nstudies evaluate the correctness of Copilot solutions and report its issues,\nmore empirical evaluations are necessary to understand how developers can\nbenefit from it effectively. In this paper, we study the capabilities of\nCopilot in two different programming tasks: (i) generating (and reproducing)\ncorrect and efficient solutions for fundamental algorithmic problems, and (ii)\ncomparing Copilot's proposed solutions with those of human programmers on a set\nof programming tasks. For the former, we assess the performance and\nfunctionality of Copilot in solving selected fundamental problems in computer\nscience, like sorting and implementing data structures. In the latter, a\ndataset of programming problems with human-provided solutions is used. The\nresults show that Copilot is capable of providing solutions for almost all\nfundamental algorithmic problems, however, some solutions are buggy and\nnon-reproducible. Moreover, Copilot has some difficulties in combining multiple\nmethods to generate a solution. Comparing Copilot to humans, our results show\nthat the correct ratio of humans' solutions is greater than Copilot's\nsuggestions, while the buggy solutions generated by Copilot require less effort\nto be repaired.",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2022-06-30T15:00:03+00:00",
    "updated": "2023-04-14T20:52:00+00:00",
    "doi": null,
    "comment": "27 pages, 8 figures",
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "2010.03509v3": {
    "id": "http://arxiv.org/abs/2010.03509v3",
    "title": "Automatic Backward Filtering Forward Guiding for Markov processes and graphical models",
    "authors": [
      "Frank van der Meulen",
      "Moritz Schauer"
    ],
    "abstract": "We incorporate discrete and continuous time Markov processes as building\nblocks into probabilistic graphical models with latent and observed variables.\nWe introduce the automatic Backward Filtering Forward Guiding (BFFG) paradigm\n(Mider et al., 2021) for programmable inference on latent states and model\nparameters. Our starting point is a generative model, a forward description of\nthe probabilistic process dynamics. We backpropagate the information provided\nby observations through the model to transform the generative (forward) model\ninto a pre-conditional model guided by the data. It approximates the actual\nconditional model with known likelihood-ratio between the two. The backward\nfilter and the forward change of measure are suitable to be incorporated into a\nprobabilistic programming context because they can be formulated as a set of\ntransformation rules.\n  The guided generative model can be incorporated in different approaches to\nefficiently sample latent states and parameters conditional on observations. We\nshow applicability in a variety of settings, including Markov chains with\ndiscrete state space, interacting particle systems, state space models,\nbranching diffusions and Gamma processes.",
    "categories": [
      "stat.CO",
      "stat.ME",
      "Primary 62H22, 62M20, secondary 60J05, 60J25"
    ],
    "published": "2020-10-07T16:30:13+00:00",
    "updated": "2022-10-31T18:49:41+00:00",
    "doi": null,
    "comment": "43 pages",
    "journal_ref": null,
    "primary_category": "stat.CO"
  },
  "1804.03619v2": {
    "id": "http://arxiv.org/abs/1804.03619v2",
    "title": "Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation",
    "authors": [
      "Ariel Ephrat",
      "Inbar Mosseri",
      "Oran Lang",
      "Tali Dekel",
      "Kevin Wilson",
      "Avinatan Hassidim",
      "William T. Freeman",
      "Michael Rubinstein"
    ],
    "abstract": "We present a joint audio-visual model for isolating a single speech signal\nfrom a mixture of sounds such as other speakers and background noise. Solving\nthis task using only audio as input is extremely challenging and does not\nprovide an association of the separated speech signals with speakers in the\nvideo. In this paper, we present a deep network-based model that incorporates\nboth visual and auditory signals to solve this task. The visual features are\nused to \"focus\" the audio on desired speakers in a scene and to improve the\nspeech separation quality. To train our joint audio-visual model, we introduce\nAVSpeech, a new dataset comprised of thousands of hours of video segments from\nthe Web. We demonstrate the applicability of our method to classic speech\nseparation tasks, as well as real-world scenarios involving heated interviews,\nnoisy bars, and screaming children, only requiring the user to specify the face\nof the person in the video whose speech they want to isolate. Our method shows\nclear advantage over state-of-the-art audio-only speech separation in cases of\nmixed speech. In addition, our model, which is speaker-independent (trained\nonce, applicable to any speaker), produces better results than recent\naudio-visual speech separation methods that are speaker-dependent (require\ntraining a separate model for each speaker of interest).",
    "categories": [
      "cs.SD",
      "cs.CV",
      "eess.AS"
    ],
    "published": "2018-04-10T16:28:59+00:00",
    "updated": "2018-08-09T21:22:37+00:00",
    "doi": "10.1145/3197517.3201357",
    "comment": "Accepted to SIGGRAPH 2018. Project webpage:\n  https://looking-to-listen.github.io",
    "journal_ref": "ACM Trans. Graph. 37(4): 112:1-112:11 (2018)",
    "primary_category": "cs.SD"
  },
  "2402.18078v2": {
    "id": "http://arxiv.org/abs/2402.18078v2",
    "title": "Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis",
    "authors": [
      "Yanzuo Lu",
      "Manlin Zhang",
      "Andy J Ma",
      "Xiaohua Xie",
      "Jian-Huang Lai"
    ],
    "abstract": "Diffusion model is a promising approach to image generation and has been\nemployed for Pose-Guided Person Image Synthesis (PGPIS) with competitive\nperformance. While existing methods simply align the person appearance to the\ntarget pose, they are prone to overfitting due to the lack of a high-level\nsemantic understanding on the source person image. In this paper, we propose a\nnovel Coarse-to-Fine Latent Diffusion (CFLD) method for PGPIS. In the absence\nof image-caption pairs and textual prompts, we develop a novel training\nparadigm purely based on images to control the generation process of a\npre-trained text-to-image diffusion model. A perception-refined decoder is\ndesigned to progressively refine a set of learnable queries and extract\nsemantic understanding of person images as a coarse-grained prompt. This allows\nfor the decoupling of fine-grained appearance and pose information controls at\ndifferent stages, and thus circumventing the potential overfitting problem. To\ngenerate more realistic texture details, a hybrid-granularity attention module\nis proposed to encode multi-scale fine-grained appearance features as bias\nterms to augment the coarse-grained prompt. Both quantitative and qualitative\nexperimental results on the DeepFashion benchmark demonstrate the superiority\nof our method over the state of the arts for PGPIS. Code is available at\nhttps://github.com/YanzuoLu/CFLD.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-02-28T06:07:07+00:00",
    "updated": "2024-04-09T14:12:02+00:00",
    "doi": null,
    "comment": "Accepted by CVPR 2024 (Highlight)",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.03653v2": {
    "id": "http://arxiv.org/abs/2404.03653v2",
    "title": "CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching",
    "authors": [
      "Dongzhi Jiang",
      "Guanglu Song",
      "Xiaoshi Wu",
      "Renrui Zhang",
      "Dazhong Shen",
      "Zhuofan Zong",
      "Yu Liu",
      "Hongsheng Li"
    ],
    "abstract": "Diffusion models have demonstrated great success in the field of\ntext-to-image generation. However, alleviating the misalignment between the\ntext prompts and images is still challenging. The root reason behind the\nmisalignment has not been extensively investigated. We observe that the\nmisalignment is caused by inadequate token attention activation. We further\nattribute this phenomenon to the diffusion model's insufficient condition\nutilization, which is caused by its training paradigm. To address the issue, we\npropose CoMat, an end-to-end diffusion model fine-tuning strategy with an\nimage-to-text concept matching mechanism. We leverage an image captioning model\nto measure image-to-text alignment and guide the diffusion model to revisit\nignored tokens. A novel attribute concentration module is also proposed to\naddress the attribute binding problem. Without any image or human preference\ndata, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.\nExtensive experiments show that CoMat-SDXL significantly outperforms the\nbaseline model SDXL in two text-to-image alignment benchmarks and achieves\nstart-of-the-art performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-04T17:59:46+00:00",
    "updated": "2024-06-03T06:02:34+00:00",
    "doi": null,
    "comment": "Project Page: https://caraj7.github.io/comat",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1212.0402v1": {
    "id": "http://arxiv.org/abs/1212.0402v1",
    "title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild",
    "authors": [
      "Khurram Soomro",
      "Amir Roshan Zamir",
      "Mubarak Shah"
    ],
    "abstract": "We introduce UCF101 which is currently the largest dataset of human actions.\nIt consists of 101 action classes, over 13k clips and 27 hours of video data.\nThe database consists of realistic user uploaded videos containing camera\nmotion and cluttered background. Additionally, we provide baseline action\nrecognition results on this new dataset using standard bag of words approach\nwith overall performance of 44.5%. To the best of our knowledge, UCF101 is\ncurrently the most challenging dataset of actions due to its large number of\nclasses, large number of clips and also unconstrained nature of such clips.",
    "categories": [
      "cs.CV"
    ],
    "published": "2012-12-03T14:45:31+00:00",
    "updated": "2012-12-03T14:45:31+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2407.12781v2": {
    "id": "http://arxiv.org/abs/2407.12781v2",
    "title": "VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control",
    "authors": [
      "Sherwin Bahmani",
      "Ivan Skorokhodov",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Guocheng Qian",
      "Michael Vasilkovsky",
      "Hsin-Ying Lee",
      "Chaoyang Wang",
      "Jiaxu Zou",
      "Andrea Tagliasacchi",
      "David B. Lindell",
      "Sergey Tulyakov"
    ],
    "abstract": "Modern text-to-video synthesis models demonstrate coherent, photorealistic\ngeneration of complex videos from a text description. However, most existing\nmodels lack fine-grained control over camera movement, which is critical for\ndownstream applications related to content creation, visual effects, and 3D\nvision. Recently, new methods demonstrate the ability to generate videos with\ncontrollable camera poses these techniques leverage pre-trained U-Net-based\ndiffusion models that explicitly disentangle spatial and temporal generation.\nStill, no existing approach enables camera control for new, transformer-based\nvideo diffusion models that process spatial and temporal information jointly.\nHere, we propose to tame video transformers for 3D camera control using a\nControlNet-like conditioning mechanism that incorporates spatiotemporal camera\nembeddings based on Plucker coordinates. The approach demonstrates\nstate-of-the-art performance for controllable video generation after\nfine-tuning on the RealEstate10K dataset. To the best of our knowledge, our\nwork is the first to enable camera control for transformer-based video\ndiffusion models.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-07-17T17:59:05+00:00",
    "updated": "2024-07-20T19:43:10+00:00",
    "doi": null,
    "comment": "Project Page: https://snap-research.github.io/vd3d/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1611.06624v3": {
    "id": "http://arxiv.org/abs/1611.06624v3",
    "title": "Temporal Generative Adversarial Nets with Singular Value Clipping",
    "authors": [
      "Masaki Saito",
      "Eiichi Matsumoto",
      "Shunta Saito"
    ],
    "abstract": "In this paper, we propose a generative model, Temporal Generative Adversarial\nNets (TGAN), which can learn a semantic representation of unlabeled videos, and\nis capable of generating videos. Unlike existing Generative Adversarial Nets\n(GAN)-based methods that generate videos with a single generator consisting of\n3D deconvolutional layers, our model exploits two different types of\ngenerators: a temporal generator and an image generator. The temporal generator\ntakes a single latent variable as input and outputs a set of latent variables,\neach of which corresponds to an image frame in a video. The image generator\ntransforms a set of such latent variables into a video. To deal with\ninstability in training of GAN with such advanced networks, we adopt a recently\nproposed model, Wasserstein GAN, and propose a novel method to train it stably\nin an end-to-end manner. The experimental results demonstrate the effectiveness\nof our methods.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2016-11-21T01:10:50+00:00",
    "updated": "2017-08-18T02:32:16+00:00",
    "doi": null,
    "comment": "to appear in ICCV 2017",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2204.02148v2": {
    "id": "http://arxiv.org/abs/2204.02148v2",
    "title": "Dual-AI: Dual-path Actor Interaction Learning for Group Activity Recognition",
    "authors": [
      "Mingfei Han",
      "David Junhao Zhang",
      "Yali Wang",
      "Rui Yan",
      "Lina Yao",
      "Xiaojun Chang",
      "Yu Qiao"
    ],
    "abstract": "Learning spatial-temporal relation among multiple actors is crucial for group\nactivity recognition. Different group activities often show the diversified\ninteractions between actors in the video. Hence, it is often difficult to model\ncomplex group activities from a single view of spatial-temporal actor\nevolution. To tackle this problem, we propose a distinct Dual-path Actor\nInteraction (DualAI) framework, which flexibly arranges spatial and temporal\ntransformers in two complementary orders, enhancing actor relations by\nintegrating merits from different spatiotemporal paths. Moreover, we introduce\na novel Multi-scale Actor Contrastive Loss (MAC-Loss) between two interactive\npaths of Dual-AI. Via self-supervised actor consistency in both frame and video\nlevels, MAC-Loss can effectively distinguish individual actor representations\nto reduce action confusion among different actors. Consequently, our Dual-AI\ncan boost group activity recognition by fusing such discriminative features of\ndifferent actors. To evaluate the proposed approach, we conduct extensive\nexperiments on the widely used benchmarks, including Volleyball, Collective\nActivity, and NBA datasets. The proposed Dual-AI achieves state-of-the-art\nperformance on all these datasets. It is worth noting the proposed Dual-AI with\n50% training data outperforms a number of recent approaches with 100% training\ndata. This confirms the generalization power of Dual-AI for group activity\nrecognition, even under the challenging scenarios of limited supervision.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-04-05T12:17:40+00:00",
    "updated": "2022-04-06T12:34:28+00:00",
    "doi": null,
    "comment": "CVPR 2022 Oral presentation",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2308.07665v2": {
    "id": "http://arxiv.org/abs/2308.07665v2",
    "title": "Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training",
    "authors": [
      "Ximing Xing",
      "Chuang Wang",
      "Haitao Zhou",
      "Zhihao Hu",
      "Chongxuan Li",
      "Dong Xu",
      "Qian Yu"
    ],
    "abstract": "Exemplar-based sketch-to-photo synthesis allows users to generate\nphoto-realistic images based on sketches. Recently, diffusion-based methods\nhave achieved impressive performance on image generation tasks, enabling\nhighly-flexible control through text-driven generation or energy functions.\nHowever, generating photo-realistic images with color and texture from sketch\nimages remains challenging for diffusion models. Sketches typically consist of\nonly a few strokes, with most regions left blank, making it difficult for\ndiffusion-based methods to produce photo-realistic images. In this work, we\npropose a two-stage method named ``Inversion-by-Inversion\" for exemplar-based\nsketch-to-photo synthesis. This approach includes shape-enhancing inversion and\nfull-control inversion. During the shape-enhancing inversion process, an\nuncolored photo is generated with the guidance of a shape-energy function. This\nstep is essential to ensure control over the shape of the generated photo. In\nthe full-control inversion process, we propose an appearance-energy function to\ncontrol the color and texture of the final generated photo.Importantly, our\nInversion-by-Inversion pipeline is training-free and can accept different types\nof exemplars for color and texture control. We conducted extensive experiments\nto evaluate our proposed method, and the results demonstrate its effectiveness.\nThe code and project can be found at\nhttps://ximinng.github.io/inversion-by-inversion-project/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-08-15T09:27:57+00:00",
    "updated": "2024-01-03T14:36:11+00:00",
    "doi": null,
    "comment": "15 pages",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.04650v1": {
    "id": "http://arxiv.org/abs/2404.04650v1",
    "title": "InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization",
    "authors": [
      "Xiefan Guo",
      "Jinlin Liu",
      "Miaomiao Cui",
      "Jiankai Li",
      "Hongyu Yang",
      "Di Huang"
    ],
    "abstract": "Recent strides in the development of diffusion models, exemplified by\nadvancements such as Stable Diffusion, have underscored their remarkable\nprowess in generating visually compelling images. However, the imperative of\nachieving a seamless alignment between the generated image and the provided\nprompt persists as a formidable challenge. This paper traces the root of these\ndifficulties to invalid initial noise, and proposes a solution in the form of\nInitial Noise Optimization (InitNO), a paradigm that refines this noise.\nConsidering text prompts, not all random noises are effective in synthesizing\nsemantically-faithful images. We design the cross-attention response score and\nthe self-attention conflict score to evaluate the initial noise, bifurcating\nthe initial latent space into valid and invalid sectors. A strategically\ncrafted noise optimization pipeline is developed to guide the initial noise\ntowards valid regions. Our method, validated through rigorous experimentation,\nshows a commendable proficiency in generating images in strict accordance with\ntext prompts. Our code is available at https://github.com/xiefan-guo/initno.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-06T14:56:59+00:00",
    "updated": "2024-04-06T14:56:59+00:00",
    "doi": null,
    "comment": "Accepted by CVPR 2024",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2301.13688v2": {
    "id": "http://arxiv.org/abs/2301.13688v2",
    "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
    "authors": [
      "Shayne Longpre",
      "Le Hou",
      "Tu Vu",
      "Albert Webson",
      "Hyung Won Chung",
      "Yi Tay",
      "Denny Zhou",
      "Quoc V. Le",
      "Barret Zoph",
      "Jason Wei",
      "Adam Roberts"
    ],
    "abstract": "We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-01-31T15:03:44+00:00",
    "updated": "2023-02-14T16:33:33+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.AI"
  },
  "2402.00575v1": {
    "id": "http://arxiv.org/abs/2402.00575v1",
    "title": "Diffusion-based Light Field Synthesis",
    "authors": [
      "Ruisheng Gao",
      "Yutong Liu",
      "Zeyu Xiao",
      "Zhiwei Xiong"
    ],
    "abstract": "Light fields (LFs), conducive to comprehensive scene radiance recorded across\nangular dimensions, find wide applications in 3D reconstruction, virtual\nreality, and computational photography.However, the LF acquisition is\ninevitably time-consuming and resource-intensive due to the mainstream\nacquisition strategy involving manual capture or laborious software\nsynthesis.Given such a challenge, we introduce LFdiff, a straightforward yet\neffective diffusion-based generative framework tailored for LF synthesis, which\nadopts only a single RGB image as input.LFdiff leverages disparity estimated by\na monocular depth estimation network and incorporates two distinctive\ncomponents: a novel condition scheme and a noise estimation network tailored\nfor LF data.Specifically, we design a position-aware warping condition scheme,\nenhancing inter-view geometry learning via a robust conditional signal.We then\npropose DistgUnet, a disentanglement-based noise estimation network, to harness\ncomprehensive LF representations.Extensive experiments demonstrate that LFdiff\nexcels in synthesizing visually pleasing and disparity-controllable light\nfields with enhanced generalization capability.Additionally, comprehensive\nresults affirm the broad applicability of the generated LF data, spanning\napplications like LF super-resolution and refocusing.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-02-01T13:13:16+00:00",
    "updated": "2024-02-01T13:13:16+00:00",
    "doi": null,
    "comment": "11 pages,9 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1904.09664v2": {
    "id": "http://arxiv.org/abs/1904.09664v2",
    "title": "Deep Hough Voting for 3D Object Detection in Point Clouds",
    "authors": [
      "Charles R. Qi",
      "Or Litany",
      "Kaiming He",
      "Leonidas J. Guibas"
    ],
    "abstract": "Current 3D object detection methods are heavily influenced by 2D detectors.\nIn order to leverage architectures in 2D detectors, they often convert 3D point\nclouds to regular grids (i.e., to voxel grids or to bird's eye view images), or\nrely on detection in 2D images to propose 3D boxes. Few works have attempted to\ndirectly detect objects in point clouds. In this work, we return to first\nprinciples to construct a 3D detection pipeline for point cloud data and as\ngeneric as possible. However, due to the sparse nature of the data -- samples\nfrom 2D manifolds in 3D space -- we face a major challenge when directly\npredicting bounding box parameters from scene points: a 3D object centroid can\nbe far from any surface point thus hard to regress accurately in one step. To\naddress the challenge, we propose VoteNet, an end-to-end 3D object detection\nnetwork based on a synergy of deep point set networks and Hough voting. Our\nmodel achieves state-of-the-art 3D detection on two large datasets of real 3D\nscans, ScanNet and SUN RGB-D with a simple design, compact model size and high\nefficiency. Remarkably, VoteNet outperforms previous methods by using purely\ngeometric information without relying on color images.",
    "categories": [
      "cs.CV"
    ],
    "published": "2019-04-21T21:36:36+00:00",
    "updated": "2019-08-22T20:54:40+00:00",
    "doi": null,
    "comment": "ICCV 2019",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2103.13630v3": {
    "id": "http://arxiv.org/abs/2103.13630v3",
    "title": "A Survey of Quantization Methods for Efficient Neural Network Inference",
    "authors": [
      "Amir Gholami",
      "Sehoon Kim",
      "Zhen Dong",
      "Zhewei Yao",
      "Michael W. Mahoney",
      "Kurt Keutzer"
    ],
    "abstract": "As soon as abstract mathematical computations were adapted to computation on\ndigital computers, the problem of efficient representation, manipulation, and\ncommunication of the numerical values in those computations arose. Strongly\nrelated to the problem of numerical representation is the problem of\nquantization: in what manner should a set of continuous real-valued numbers be\ndistributed over a fixed discrete set of numbers to minimize the number of bits\nrequired and also to maximize the accuracy of the attendant computations? This\nperennial problem of quantization is particularly relevant whenever memory\nand/or computational resources are severely restricted, and it has come to the\nforefront in recent years due to the remarkable performance of Neural Network\nmodels in computer vision, natural language processing, and related areas.\nMoving from floating-point representations to low-precision fixed integer\nvalues represented in four bits or less holds the potential to reduce the\nmemory footprint and latency by a factor of 16x; and, in fact, reductions of 4x\nto 8x are often realized in practice in these applications. Thus, it is not\nsurprising that quantization has emerged recently as an important and very\nactive sub-area of research in the efficient implementation of computations\nassociated with Neural Networks. In this article, we survey approaches to the\nproblem of quantizing the numerical values in deep Neural Network computations,\ncovering the advantages/disadvantages of current methods. With this survey and\nits organization, we hope to have presented a useful snapshot of the current\nresearch in quantization for Neural Networks and to have given an intelligent\norganization to ease the evaluation of future research in this area.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-03-25T06:57:11+00:00",
    "updated": "2021-06-21T21:01:12+00:00",
    "doi": null,
    "comment": "Book Chapter: Low-Power Computer Vision: Improving the Efficiency of\n  Artificial Intelligence",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.13789v2": {
    "id": "http://arxiv.org/abs/2312.13789v2",
    "title": "TinySAM: Pushing the Envelope for Efficient Segment Anything Model",
    "authors": [
      "Han Shu",
      "Wenshuo Li",
      "Yehui Tang",
      "Yiman Zhang",
      "Yihao Chen",
      "Houqiang Li",
      "Yunhe Wang",
      "Xinghao Chen"
    ],
    "abstract": "Recently segment anything model (SAM) has shown powerful segmentation\ncapability and has drawn great attention in computer vision fields. Massive\nfollowing works have developed various applications based on the pretrained SAM\nand achieved impressive performance on downstream vision tasks.\n  However, SAM consists of heavy architectures and requires massive\ncomputational capacity, which hinders the further application of SAM on\ncomputation constrained edge devices. To this end, in this paper we propose a\nframework to obtain a tiny segment anything model (TinySAM) while maintaining\nthe strong zero-shot performance. We first propose a full-stage knowledge\ndistillation method with hard prompt sampling and hard mask weighting strategy\nto distill a lightweight student model. We also adapt the post-training\nquantization to the promptable segmentation task and further reduce the\ncomputational cost. Moreover, a hierarchical segmenting everything strategy is\nproposed to accelerate the everything inference by $2\\times$ with almost no\nperformance degradation. With all these proposed methods, our TinySAM leads to\norders of magnitude computational reduction and pushes the envelope for\nefficient segment anything task. Extensive experiments on various zero-shot\ntransfer tasks demonstrate the significantly advantageous performance of our\nTinySAM against counterpart methods. Pre-trained models and codes are available\nat https://github.com/xinghaochen/TinySAM and\nhttps://gitee.com/mindspore/models/tree/master/research/cv/TinySAM.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-21T12:26:11+00:00",
    "updated": "2024-03-09T08:31:47+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2301.03598v1": {
    "id": "http://arxiv.org/abs/2301.03598v1",
    "title": "Stream-K: Work-centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU",
    "authors": [
      "Muhammad Osama",
      "Duane Merrill",
      "Cris Cecka",
      "Michael Garland",
      "John D. Owens"
    ],
    "abstract": "We introduce Stream-K, a work-centric parallelization of matrix\nmultiplication (GEMM) and related computations in dense linear algebra. Whereas\ncontemporary decompositions are primarily tile-based, our method operates by\npartitioning an even share of the aggregate inner loop iterations among\nphysical processing elements. This provides a near-perfect utilization of\ncomputing resources, regardless of how efficiently the output tiling for any\ngiven problem quantizes across the underlying processing elements.\n  On GPU processors, our Stream-K parallelization of GEMM produces a peak\nspeedup of up to 14$\\times$ and 6.7$\\times$, and an average performance\nresponse that is both higher and more consistent across 32,824 GEMM problem\ngeometries than state-of-the-art math libraries such as CUTLASS and cuBLAS.\nFurthermore, we achieve this performance from a single tile size configuration\nper floating-point precision, whereas today's math libraries employ complex\nkernel-selection heuristics to select from a large ensemble of kernel variants.",
    "categories": [
      "cs.DS",
      "cs.DC"
    ],
    "published": "2023-01-09T18:22:11+00:00",
    "updated": "2023-01-09T18:22:11+00:00",
    "doi": null,
    "comment": "This work previously appeared in the author's PhD dissertation,\n  available at arXiv:2212.08964",
    "journal_ref": null,
    "primary_category": "cs.DS"
  },
  "1905.02249v2": {
    "id": "http://arxiv.org/abs/1905.02249v2",
    "title": "MixMatch: A Holistic Approach to Semi-Supervised Learning",
    "authors": [
      "David Berthelot",
      "Nicholas Carlini",
      "Ian Goodfellow",
      "Nicolas Papernot",
      "Avital Oliver",
      "Colin Raffel"
    ],
    "abstract": "Semi-supervised learning has proven to be a powerful paradigm for leveraging\nunlabeled data to mitigate the reliance on large labeled datasets. In this\nwork, we unify the current dominant approaches for semi-supervised learning to\nproduce a new algorithm, MixMatch, that works by guessing low-entropy labels\nfor data-augmented unlabeled examples and mixing labeled and unlabeled data\nusing MixUp. We show that MixMatch obtains state-of-the-art results by a large\nmargin across many datasets and labeled data amounts. For example, on CIFAR-10\nwith 250 labels, we reduce error rate by a factor of 4 (from 38% to 11%) and by\na factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a\ndramatically better accuracy-privacy trade-off for differential privacy.\nFinally, we perform an ablation study to tease apart which components of\nMixMatch are most important for its success.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "published": "2019-05-06T19:56:03+00:00",
    "updated": "2019-10-23T18:47:34+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2204.09273v4": {
    "id": "http://arxiv.org/abs/2204.09273v4",
    "title": "Sound-Guided Semantic Video Generation",
    "authors": [
      "Seung Hyun Lee",
      "Gyeongrok Oh",
      "Wonmin Byeon",
      "Chanyoung Kim",
      "Won Jeong Ryoo",
      "Sang Ho Yoon",
      "Hyunjun Cho",
      "Jihyun Bae",
      "Jinkyu Kim",
      "Sangpil Kim"
    ],
    "abstract": "The recent success in StyleGAN demonstrates that pre-trained StyleGAN latent\nspace is useful for realistic video generation. However, the generated motion\nin the video is usually not semantically meaningful due to the difficulty of\ndetermining the direction and magnitude in the StyleGAN latent space. In this\npaper, we propose a framework to generate realistic videos by leveraging\nmultimodal (sound-image-text) embedding space. As sound provides the temporal\ncontexts of the scene, our framework learns to generate a video that is\nsemantically consistent with sound. First, our sound inversion module maps the\naudio directly into the StyleGAN latent space. We then incorporate the\nCLIP-based multimodal embedding space to further provide the audio-visual\nrelationships. Finally, the proposed frame generator learns to find the\ntrajectory in the latent space which is coherent with the corresponding sound\nand generates a video in a hierarchical manner. We provide the new\nhigh-resolution landscape video dataset (audio-visual pair) for the\nsound-guided video generation task. The experiments show that our model\noutperforms the state-of-the-art methods in terms of video quality. We further\nshow several applications including image and video editing to verify the\neffectiveness of our method.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2022-04-20T07:33:10+00:00",
    "updated": "2022-10-21T06:10:08+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2311.16099v1": {
    "id": "http://arxiv.org/abs/2311.16099v1",
    "title": "GART: Gaussian Articulated Template Models",
    "authors": [
      "Jiahui Lei",
      "Yufu Wang",
      "Georgios Pavlakos",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ],
    "abstract": "We introduce Gaussian Articulated Template Model GART, an explicit,\nefficient, and expressive representation for non-rigid articulated subject\ncapturing and rendering from monocular videos. GART utilizes a mixture of\nmoving 3D Gaussians to explicitly approximate a deformable subject's geometry\nand appearance. It takes advantage of a categorical template model prior (SMPL,\nSMAL, etc.) with learnable forward skinning while further generalizing to more\ncomplex non-rigid deformations with novel latent bones. GART can be\nreconstructed via differentiable rendering from monocular videos in seconds or\nminutes and rendered in novel poses faster than 150fps.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2023-11-27T18:59:30+00:00",
    "updated": "2023-11-27T18:59:30+00:00",
    "doi": null,
    "comment": "13 pages, code available at\n  https://www.cis.upenn.edu/~leijh/projects/gart/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1309.6392v2": {
    "id": "http://arxiv.org/abs/1309.6392v2",
    "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation",
    "authors": [
      "Alex Goldstein",
      "Adam Kapelner",
      "Justin Bleich",
      "Emil Pitkin"
    ],
    "abstract": "This article presents Individual Conditional Expectation (ICE) plots, a tool\nfor visualizing the model estimated by any supervised learning algorithm.\nClassical partial dependence plots (PDPs) help visualize the average partial\nrelationship between the predicted response and one or more features. In the\npresence of substantial interaction effects, the partial response relationship\ncan be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate\nthe complexity of the modeled relationship. Accordingly, ICE plots refine the\npartial dependence plot by graphing the functional relationship between the\npredicted response and the feature for individual observations. Specifically,\nICE plots highlight the variation in the fitted values across the range of a\ncovariate, suggesting where and to what extent heterogeneities might exist. In\naddition to providing a plotting suite for exploratory analysis, we include a\nvisual test for additive structure in the data generating model. Through\nsimulated examples and real data sets, we demonstrate how ICE plots can shed\nlight on estimated models in ways PDPs cannot. Procedures outlined are\navailable in the R package ICEbox.",
    "categories": [
      "stat.AP"
    ],
    "published": "2013-09-25T03:34:37+00:00",
    "updated": "2014-03-20T01:12:11+00:00",
    "doi": null,
    "comment": "22 pages, 14 figures, 2 algorithms",
    "journal_ref": null,
    "primary_category": "stat.AP"
  },
  "2310.03174v1": {
    "id": "http://arxiv.org/abs/2310.03174v1",
    "title": "Test Case Recommendations with Distributed Representation of Code Syntactic Features",
    "authors": [
      "Mosab Rezaei",
      "Hamed Alhoori",
      "Mona Rahimi"
    ],
    "abstract": "Frequent modifications of unit test cases are inevitable due to software's\ncontinuous underlying changes in source code, design, and requirements. Since\nmanually maintaining software test suites is tedious, timely, and costly,\nautomating the process of generation and maintenance of test units will\nsignificantly impact the effectiveness and efficiency of software testing\nprocesses.\n  To this end, we propose an automated approach which exploits both structural\nand semantic properties of source code methods and test cases to recommend the\nmost relevant and useful unit tests to the developers. The proposed approach\ninitially trains a neural network to transform method-level source code, as\nwell as unit tests, into distributed representations (embedded vectors) while\npreserving the importance of the structure in the code. Retrieving the semantic\nand structural properties of a given method, the approach computes cosine\nsimilarity between the method's embedding and the previously-embedded training\ninstances. Further, according to the similarity scores between the embedding\nvectors, the model identifies the closest methods of embedding and the\nassociated unit tests as the most similar recommendations.\n  The results on the Methods2Test dataset showed that, while there is no\nguarantee to have similar relevant test cases for the group of similar methods,\nthe proposed approach extracts the most similar existing test cases for a given\nmethod in the dataset, and evaluations show that recommended test cases\ndecrease the developers' effort to generating expected test cases.",
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "published": "2023-10-04T21:42:01+00:00",
    "updated": "2023-10-04T21:42:01+00:00",
    "doi": null,
    "comment": "8 pages, 4 figures, 14th Workshop on Automating Test Case Design,\n  Selection and Evaluation (A-TEST 2023) co-located with 38th IEEE/ACM\n  International Conference on ASE 2023 conference",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1612.08468v2": {
    "id": "http://arxiv.org/abs/1612.08468v2",
    "title": "Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models",
    "authors": [
      "Daniel W. Apley",
      "Jingyu Zhu"
    ],
    "abstract": "When fitting black box supervised learning models (e.g., complex trees,\nneural networks, boosted trees, random forests, nearest neighbors, local\nkernel-weighted methods, etc.), visualizing the main effects of the individual\npredictor variables and their low-order interaction effects is often important,\nand partial dependence (PD) plots are the most popular approach for\naccomplishing this. However, PD plots involve a serious pitfall if the\npredictor variables are far from independent, which is quite common with large\nobservational data sets. Namely, PD plots require extrapolation of the response\nat predictor values that are far outside the multivariate envelope of the\ntraining data, which can render the PD plots unreliable. Although marginal\nplots (M plots) do not require such extrapolation, they produce substantially\nbiased and misleading results when the predictors are dependent, analogous to\nthe omitted variable bias in regression. We present a new visualization\napproach that we term accumulated local effects (ALE) plots, which inherits the\ndesirable characteristics of PD and M plots, without inheriting their preceding\nshortcomings. Like M plots, ALE plots do not require extrapolation; and like PD\nplots, they are not biased by the omitted variable phenomenon. Moreover, ALE\nplots are far less computationally expensive than PD plots.",
    "categories": [
      "stat.ME"
    ],
    "published": "2016-12-27T01:08:55+00:00",
    "updated": "2019-08-19T20:27:57+00:00",
    "doi": null,
    "comment": "The R package ALEPlot is available on CRAN. The new version contains\n  refined definitions of ALE effects, a new illustrative example, theorems and\n  proofs of asymptotic properties of ALE effects and estimators, and extra\n  implementation details",
    "journal_ref": null,
    "primary_category": "stat.ME"
  },
  "2012.06460v1": {
    "id": "http://arxiv.org/abs/2012.06460v1",
    "title": "Orthogonal Language and Task Adapters in Zero-Shot Cross-Lingual Transfer",
    "authors": [
      "Marko Vidoni",
      "Ivan Vuli\u0107",
      "Goran Glava\u0161"
    ],
    "abstract": "Adapter modules, additional trainable parameters that enable efficient\nfine-tuning of pretrained transformers, have recently been used for language\nspecialization of multilingual transformers, improving downstream zero-shot\ncross-lingual transfer. In this work, we propose orthogonal language and task\nadapters (dubbed orthoadapters) for cross-lingual transfer. They are trained to\nencode language- and task-specific information that is complementary (i.e.,\northogonal) to the knowledge already stored in the pretrained transformer's\nparameters. Our zero-shot cross-lingual transfer experiments, involving three\ntasks (POS-tagging, NER, NLI) and a set of 10 diverse languages, 1) point to\nthe usefulness of orthoadapters in cross-lingual transfer, especially for the\nmost complex NLI task, but also 2) indicate that the optimal adapter\nconfiguration highly depends on the task and the target language. We hope that\nour work will motivate a wider investigation of usefulness of orthogonality\nconstraints in language- and task-specific fine-tuning of pretrained\ntransformers.",
    "categories": [
      "cs.CL"
    ],
    "published": "2020-12-11T16:32:41+00:00",
    "updated": "2020-12-11T16:32:41+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2304.08244v2": {
    "id": "http://arxiv.org/abs/2304.08244v2",
    "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
    "authors": [
      "Minghao Li",
      "Yingxiu Zhao",
      "Bowen Yu",
      "Feifan Song",
      "Hangyu Li",
      "Haiyang Yu",
      "Zhoujun Li",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "Recent research has demonstrated that Large Language Models (LLMs) can\nenhance their capabilities by utilizing external tools. However, three pivotal\nquestions remain unanswered: (1) How effective are current LLMs in utilizing\ntools? (2) How can we enhance LLMs' ability to utilize tools? (3) What\nobstacles need to be overcome to leverage tools? To address these questions, we\nintroduce API-Bank, a groundbreaking benchmark, specifically designed for\ntool-augmented LLMs. For the first question, we develop a runnable evaluation\nsystem consisting of 73 API tools. We annotate 314 tool-use dialogues with 753\nAPI calls to assess the existing LLMs' capabilities in planning, retrieving,\nand calling APIs. For the second question, we construct a comprehensive\ntraining set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000\ndistinct domains. Using this dataset, we train Lynx, a tool-augmented LLM\ninitialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits\nimproved tool utilization compared to GPT-3, while GPT-4 excels in planning.\nHowever, there is still significant potential for further improvement.\nMoreover, Lynx surpasses Alpaca's tool utilization performance by more than 26\npts and approaches the effectiveness of GPT-3.5. Through error analysis, we\nhighlight the key challenges for future research in this field to answer the\nthird question.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-04-14T14:05:32+00:00",
    "updated": "2023-10-25T06:54:12+00:00",
    "doi": null,
    "comment": "EMNLP 2023",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1611.03641v2": {
    "id": "http://arxiv.org/abs/1611.03641v2",
    "title": "Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure",
    "authors": [
      "Oded Avraham",
      "Yoav Goldberg"
    ],
    "abstract": "We suggest a new method for creating and using gold-standard datasets for\nword similarity evaluation. Our goal is to improve the reliability of the\nevaluation, and we do this by redesigning the annotation task to achieve higher\ninter-rater agreement, and by defining a performance measure which takes the\nreliability of each annotation decision in the dataset into account.",
    "categories": [
      "cs.CL"
    ],
    "published": "2016-11-11T10:06:29+00:00",
    "updated": "2017-02-27T18:38:56+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2305.16653v1": {
    "id": "http://arxiv.org/abs/2305.16653v1",
    "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
    "authors": [
      "Haotian Sun",
      "Yuchen Zhuang",
      "Lingkai Kong",
      "Bo Dai",
      "Chao Zhang"
    ],
    "abstract": "Large language models (LLMs) have recently demonstrated the potential in\nacting as autonomous agents for sequential decision-making tasks. However, most\nexisting methods either take actions greedily without planning or rely on\nstatic plans that are not adaptable to environmental feedback. Consequently,\nthe sequential decision-making performance of LLM agents degenerates with\nproblem complexity and plan horizons increase. We propose a closed-loop\napproach, AdaPlanner, which allows the LLM agent to refine its self-generated\nplan adaptively in response to environmental feedback. In AdaPlanner, the LLM\nagent adaptively refines its plan from feedback with both in-plan and\nout-of-plan refinement strategies. To mitigate hallucination, we develop a\ncode-style LLM prompt structure that facilitates plan generation across a\nvariety of tasks, environments, and agent capabilities. Furthermore, we propose\na skill discovery mechanism that leverages successful plans as few-shot\nexemplars, enabling the agent to plan and refine with fewer task\ndemonstrations. Our experiments in the ALFWorld and MiniWoB++ environments\ndemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and\n4.11% while utilizing 2x and 600x fewer samples, respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-05-26T05:52:27+00:00",
    "updated": "2023-05-26T05:52:27+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2105.01928v3": {
    "id": "http://arxiv.org/abs/2105.01928v3",
    "title": "Instances as Queries",
    "authors": [
      "Yuxin Fang",
      "Shusheng Yang",
      "Xinggang Wang",
      "Yu Li",
      "Chen Fang",
      "Ying Shan",
      "Bin Feng",
      "Wenyu Liu"
    ],
    "abstract": "Recently, query based object detection frameworks achieve comparable\nperformance with previous state-of-the-art object detectors. However, how to\nfully leverage such frameworks to perform instance segmentation remains an open\nproblem. In this paper, we present QueryInst (Instances as Queries), a query\nbased instance segmentation method driven by parallel supervision on dynamic\nmask heads. The key insight of QueryInst is to leverage the intrinsic\none-to-one correspondence in object queries across different stages, as well as\none-to-one correspondence between mask RoI features and object queries in the\nsame stage. This approach eliminates the explicit multi-stage mask head\nconnection and the proposal distribution inconsistency issues inherent in\nnon-query based multi-stage instance segmentation methods. We conduct extensive\nexperiments on three challenging benchmarks, i.e., COCO, CityScapes, and\nYouTube-VIS to evaluate the effectiveness of QueryInst in instance segmentation\nand video instance segmentation (VIS) task. Specifically, using ResNet-101-FPN\nbackbone, QueryInst obtains 48.1 box AP and 42.8 mask AP on COCO test-dev,\nwhich is 2 points higher than HTC in terms of both box AP and mask AP, while\nruns 2.4 times faster. For video instance segmentation, QueryInst achieves the\nbest performance among all online VIS approaches and strikes a decent\nspeed-accuracy trade-off. Code is available at\n\\url{https://github.com/hustvl/QueryInst}.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-05-05T08:38:25+00:00",
    "updated": "2021-05-23T16:36:54+00:00",
    "doi": null,
    "comment": "14 pages, 8 figures, including the Appendix",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2405.15227v1": {
    "id": "http://arxiv.org/abs/2405.15227v1",
    "title": "Neural Elevation Models for Terrain Mapping and Path Planning",
    "authors": [
      "Adam Dai",
      "Shubh Gupta",
      "Grace Gao"
    ],
    "abstract": "This work introduces Neural Elevations Models (NEMos), which adapt Neural\nRadiance Fields to a 2.5D continuous and differentiable terrain model. In\ncontrast to traditional terrain representations such as digital elevation\nmodels, NEMos can be readily generated from imagery, a low-cost data source,\nand provide a lightweight representation of terrain through an implicit\ncontinuous and differentiable height field. We propose a novel method for\njointly training a height field and radiance field within a NeRF framework,\nleveraging quantile regression. Additionally, we introduce a path planning\nalgorithm that performs gradient-based optimization of a continuous cost\nfunction for minimizing distance, slope changes, and control effort, enabled by\ndifferentiability of the height field. We perform experiments on simulated and\nreal-world terrain imagery, demonstrating NEMos ability to generate\nhigh-quality reconstructions and produce smoother paths compared to discrete\npath planning methods. Future work will explore the incorporation of features\nand semantics into the height field, creating a generalized terrain model.",
    "categories": [
      "cs.RO"
    ],
    "published": "2024-05-24T05:39:00+00:00",
    "updated": "2024-05-24T05:39:00+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.RO"
  },
  "2010.09774v1": {
    "id": "http://arxiv.org/abs/2010.09774v1",
    "title": "GAMesh: Guided and Augmented Meshing for Deep Point Networks",
    "authors": [
      "Nitin Agarwal",
      "M Gopi"
    ],
    "abstract": "We present a new meshing algorithm called guided and augmented meshing,\nGAMesh, which uses a mesh prior to generate a surface for the output points of\na point network. By projecting the output points onto this prior and\nsimplifying the resulting mesh, GAMesh ensures a surface with the same topology\nas the mesh prior but whose geometric fidelity is controlled by the point\nnetwork. This makes GAMesh independent of both the density and distribution of\nthe output points, a common artifact in traditional surface reconstruction\nalgorithms. We show that such a separation of geometry from topology can have\nseveral advantages especially in single-view shape prediction, fair evaluation\nof point networks and reconstructing surfaces for networks which output sparse\npoint clouds. We further show that by training point networks with GAMesh, we\ncan directly optimize the vertex positions to generate adaptive meshes with\narbitrary topologies.",
    "categories": [
      "cs.CV",
      "cs.CG",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2020-10-19T18:23:53+00:00",
    "updated": "2020-10-19T18:23:53+00:00",
    "doi": null,
    "comment": "Accepted to 3DV 2020",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.16310v3": {
    "id": "http://arxiv.org/abs/2401.16310v3",
    "title": "An Insight into Security Code Review with LLMs: Capabilities, Obstacles and Influential Factors",
    "authors": [
      "Jiaxin Yu",
      "Peng Liang",
      "Yujia Fu",
      "Amjed Tahir",
      "Mojtaba Shahin",
      "Chong Wang",
      "Yangxiao Cai"
    ],
    "abstract": "Security code review is a time-consuming and labor-intensive process\ntypically requiring integration with automated security defect detection tools.\nHowever, existing security analysis tools struggle with poor generalization,\nhigh false positive rates, and coarse detection granularity. Large Language\nModels (LLMs) have been considered promising candidates for addressing those\nchallenges. In this study, we conducted an empirical study to explore the\npotential of LLMs in detecting security defects during code review.\nSpecifically, we evaluated the performance of six LLMs under five different\nprompts and compared them with state-of-theart static analysis tools. We also\nperformed linguistic and regression analyses for the best-performing LLM to\nidentify quality problems in its responses and factors influencing its\nperformance. Our findings show that: (1) existing pre-trained LLMs have limited\ncapability in security code review but? significantly outperform the\nstate-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs\nwhen provided with a CWE list for reference. (3) GPT-4 frequently generates\nresponses that are verbose or not compliant with the task requirements given in\nthe prompts. (4) GPT-4 is more adept at identifying security defects in code\nfiles with fewer tokens, containing functional logic, or written by developers\nwith less involvement in the project.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-01-29T17:13:44+00:00",
    "updated": "2024-10-04T18:02:55+00:00",
    "doi": null,
    "comment": "26 pages, 5 images, 7 tables, Manuscript submitted to a journal\n  (2024)",
    "journal_ref": null,
    "primary_category": "cs.SE"
  },
  "2207.12396v2": {
    "id": "http://arxiv.org/abs/2207.12396v2",
    "title": "Exploring CLIP for Assessing the Look and Feel of Images",
    "authors": [
      "Jianyi Wang",
      "Kelvin C. K. Chan",
      "Chen Change Loy"
    ],
    "abstract": "Measuring the perception of visual content is a long-standing problem in\ncomputer vision. Many mathematical models have been developed to evaluate the\nlook or quality of an image. Despite the effectiveness of such tools in\nquantifying degradations such as noise and blurriness levels, such\nquantification is loosely coupled with human language. When it comes to more\nabstract perception about the feel of visual content, existing methods can only\nrely on supervised models that are explicitly trained with labeled data\ncollected via laborious user study. In this paper, we go beyond the\nconventional paradigms by exploring the rich visual language prior encapsulated\nin Contrastive Language-Image Pre-training (CLIP) models for assessing both the\nquality perception (look) and abstract perception (feel) of images in a\nzero-shot manner. In particular, we discuss effective prompt designs and show\nan effective prompt pairing strategy to harness the prior. We also provide\nextensive experiments on controlled datasets and Image Quality Assessment (IQA)\nbenchmarks. Our results show that CLIP captures meaningful priors that\ngeneralize well to different perceptual assessments. Code is avaliable at\nhttps://github.com/IceClear/CLIP-IQA.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-07-25T17:58:16+00:00",
    "updated": "2022-11-23T13:17:33+00:00",
    "doi": null,
    "comment": "Accepted by AAAI2023. Code: https://github.com/IceClear/CLIP-IQA",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2010.02803v3": {
    "id": "http://arxiv.org/abs/2010.02803v3",
    "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning",
    "authors": [
      "George Zerveas",
      "Srideepika Jayaraman",
      "Dhaval Patel",
      "Anuradha Bhamidipaty",
      "Carsten Eickhoff"
    ],
    "abstract": "In this work we propose for the first time a transformer-based framework for\nunsupervised representation learning of multivariate time series. Pre-trained\nmodels can be potentially used for downstream tasks such as regression and\nclassification, forecasting and missing value imputation. By evaluating our\nmodels on several benchmark datasets for multivariate time series regression\nand classification, we show that not only does our modeling approach represent\nthe most successful method employing unsupervised learning of multivariate time\nseries presented to date, but also that it exceeds the current state-of-the-art\nperformance of supervised methods; it does so even when the number of training\nsamples is very limited, while offering computational efficiency. Finally, we\ndemonstrate that unsupervised pre-training of our transformer models offers a\nsubstantial performance benefit over fully supervised learning, even without\nleveraging additional unlabeled data, i.e., by reusing the same data samples\nthrough the unsupervised objective.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2020-10-06T15:14:46+00:00",
    "updated": "2020-12-08T21:57:10+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2310.08442v3": {
    "id": "http://arxiv.org/abs/2310.08442v3",
    "title": "Unmasking Bias in Diffusion Model Training",
    "authors": [
      "Hu Yu",
      "Li Shen",
      "Jie Huang",
      "Hongsheng Li",
      "Feng Zhao"
    ],
    "abstract": "Denoising diffusion models have emerged as a dominant approach for image\ngeneration, however they still suffer from slow convergence in training and\ncolor shift issues in sampling. In this paper, we identify that these obstacles\ncan be largely attributed to bias and suboptimality inherent in the default\ntraining paradigm of diffusion models. Specifically, we offer theoretical\ninsights that the prevailing constant loss weight strategy in\n$\\epsilon$-prediction of diffusion models leads to biased estimation during the\ntraining phase, hindering accurate estimations of original images. To address\nthe issue, we propose a simple but effective weighting strategy derived from\nthe unlocked biased part. Furthermore, we conduct a comprehensive and\nsystematic exploration, unraveling the inherent bias problem in terms of its\nexistence, impact and underlying reasons. These analyses contribute to\nadvancing the understanding of diffusion models. Empirical results demonstrate\nthat our method remarkably elevates sample quality and displays improved\nefficiency in both training and sampling processes, by only adjusting loss\nweighting strategy. The code is released publicly at\n\\url{https://github.com/yuhuUSTC/Debias}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-10-12T16:04:41+00:00",
    "updated": "2024-08-03T06:05:38+00:00",
    "doi": null,
    "comment": "Camera ready version of ECCV2024. Codes are available",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2209.13948v1": {
    "id": "http://arxiv.org/abs/2209.13948v1",
    "title": "Obj2Seq: Formatting Objects as Sequences with Class Prompt for Visual Tasks",
    "authors": [
      "Zhiyang Chen",
      "Yousong Zhu",
      "Zhaowen Li",
      "Fan Yang",
      "Wei Li",
      "Haixin Wang",
      "Chaoyang Zhao",
      "Liwei Wu",
      "Rui Zhao",
      "Jinqiao Wang",
      "Ming Tang"
    ],
    "abstract": "Visual tasks vary a lot in their output formats and concerned contents,\ntherefore it is hard to process them with an identical structure. One main\nobstacle lies in the high-dimensional outputs in object-level visual tasks. In\nthis paper, we propose an object-centric vision framework, Obj2Seq. Obj2Seq\ntakes objects as basic units, and regards most object-level visual tasks as\nsequence generation problems of objects. Therefore, these visual tasks can be\ndecoupled into two steps. First recognize objects of given categories, and then\ngenerate a sequence for each of these objects. The definition of the output\nsequences varies for different tasks, and the model is supervised by matching\nthese sequences with ground-truth targets. Obj2Seq is able to flexibly\ndetermine input categories to satisfy customized requirements, and be easily\nextended to different visual tasks. When experimenting on MS COCO, Obj2Seq\nachieves 45.7% AP on object detection, 89.0% AP on multi-label classification\nand 65.0% AP on human pose estimation. These results demonstrate its potential\nto be generally applied to different visual tasks. Code has been made available\nat: https://github.com/CASIA-IVA-Lab/Obj2Seq.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-09-28T09:24:04+00:00",
    "updated": "2022-09-28T09:24:04+00:00",
    "doi": null,
    "comment": "Accepted by NeurIPS 2022. Code available at\n  https://github.com/CASIA-IVA-Lab/Obj2Seq",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2109.13925v2": {
    "id": "http://arxiv.org/abs/2109.13925v2",
    "title": "Fine-tuning Vision Transformers for the Prediction of State Variables in Ising Models",
    "authors": [
      "Onur Kara",
      "Arijit Sehanobish",
      "Hector H Corzo"
    ],
    "abstract": "Transformers are state-of-the-art deep learning models that are composed of\nstacked attention and point-wise, fully connected layers designed for handling\nsequential data. Transformers are not only ubiquitous throughout Natural\nLanguage Processing (NLP), but, recently, they have inspired a new wave of\nComputer Vision (CV) applications research. In this work, a Vision Transformer\n(ViT) is applied to predict the state variables of 2-dimensional Ising model\nsimulations. Our experiments show that ViT outperform state-of-the-art\nConvolutional Neural Networks (CNN) when using a small number of microstate\nimages from the Ising model corresponding to various boundary conditions and\ntemperatures. This work opens the possibility of applying ViT to other\nsimulations, and raises interesting research directions on how attention maps\ncan learn about the underlying physics governing different phenomena.",
    "categories": [
      "cs.CV",
      "cond-mat.stat-mech",
      "cs.LG",
      "physics.comp-ph"
    ],
    "published": "2021-09-28T00:23:31+00:00",
    "updated": "2021-11-30T04:27:14+00:00",
    "doi": null,
    "comment": "Accepted at Ml4Physical Sciences Workshop at Neurips 2021",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2104.02615v1": {
    "id": "http://arxiv.org/abs/2104.02615v1",
    "title": "Optical Flow Dataset Synthesis from Unpaired Images",
    "authors": [
      "Adrian W\u00e4lchli",
      "Paolo Favaro"
    ],
    "abstract": "The estimation of optical flow is an ambiguous task due to the lack of\ncorrespondence at occlusions, shadows, reflections, lack of texture and changes\nin illumination over time. Thus, unsupervised methods face major challenges as\nthey need to tune complex cost functions with several terms designed to handle\neach of these sources of ambiguity. In contrast, supervised methods avoid these\nchallenges altogether by relying on explicit ground truth optical flow obtained\ndirectly from synthetic or real data. In the case of synthetic data, the ground\ntruth provides an exact and explicit description of what optical flow to assign\nto a given scene. However, the domain gap between synthetic data and real data\noften limits the ability of a trained network to generalize. In the case of\nreal data, the ground truth is obtained through multiple sensors and additional\ndata processing, which might introduce persistent errors and contaminate it. As\na solution to these issues, we introduce a novel method to build a training set\nof pseudo-real images that can be used to train optical flow in a supervised\nmanner. Our dataset uses two unpaired frames from real data and creates pairs\nof frames by simulating random warps, occlusions with super-pixels, shadows and\nillumination changes, and associates them to their corresponding exact optical\nflow. We thus obtain the benefit of directly training on real data while having\naccess to an exact ground truth. Training with our datasets on the Sintel and\nKITTI benchmarks is straightforward and yields models on par or with state of\nthe art performance compared to much more sophisticated training approaches.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-04-02T22:19:47+00:00",
    "updated": "2021-04-02T22:19:47+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.15424v2": {
    "id": "http://arxiv.org/abs/2210.15424v2",
    "title": "What Language Model to Train if You Have One Million GPU Hours?",
    "authors": [
      "Teven Le Scao",
      "Thomas Wang",
      "Daniel Hesslow",
      "Lucile Saulnier",
      "Stas Bekman",
      "M Saiful Bari",
      "Stella Biderman",
      "Hady Elsahar",
      "Niklas Muennighoff",
      "Jason Phang",
      "Ofir Press",
      "Colin Raffel",
      "Victor Sanh",
      "Sheng Shen",
      "Lintang Sutawika",
      "Jaesung Tae",
      "Zheng Xin Yong",
      "Julien Launay",
      "Iz Beltagy"
    ],
    "abstract": "The crystallization of modeling methods around the Transformer architecture\nhas been a boon for practitioners. Simple, well-motivated architectural\nvariations can transfer across tasks and scale, increasing the impact of\nmodeling research. However, with the emergence of state-of-the-art 100B+\nparameters models, large language models are increasingly expensive to\naccurately design and train. Notably, it can be difficult to evaluate how\nmodeling decisions may impact emergent capabilities, given that these\ncapabilities arise mainly from sheer scale alone. In the process of building\nBLOOM--the Big Science Large Open-science Open-access Multilingual language\nmodel--our goal is to identify an architecture and training setup that makes\nthe best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform\nan ablation study at the billion-parameter scale comparing different modeling\npractices and their impact on zero-shot generalization. In addition, we study\nthe impact of various popular pre-training corpora on zero-shot generalization.\nWe also study the performance of a multilingual model and how it compares to\nthe English-only one. Finally, we consider the scaling behaviour of\nTransformers to choose the target model size, shape, and training setup. All\nour models and code are open-sourced at https://huggingface.co/bigscience .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2022-10-27T13:43:27+00:00",
    "updated": "2022-11-08T04:56:12+00:00",
    "doi": null,
    "comment": "Findings of EMNLP 2022",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2402.12326v2": {
    "id": "http://arxiv.org/abs/2402.12326v2",
    "title": "PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents",
    "authors": [
      "Qisen Yang",
      "Zekun Wang",
      "Honghui Chen",
      "Shenzhi Wang",
      "Yifan Pu",
      "Xin Gao",
      "Wenhao Huang",
      "Shiji Song",
      "Gao Huang"
    ],
    "abstract": "Psychological measurement is essential for mental health, self-understanding,\nand personal development. Traditional methods, such as self-report scales and\npsychologist interviews, often face challenges with engagement and\naccessibility. While game-based and LLM-based tools have been explored to\nimprove user interest and automate assessment, they struggle to balance\nengagement with generalizability. In this work, we propose PsychoGAT\n(Psychological Game AgenTs) to achieve a generic gamification of psychological\nassessment. The main insight is that powerful LLMs can function both as adept\npsychologists and innovative game designers. By incorporating LLM agents into\ndesignated roles and carefully managing their interactions, PsychoGAT can\ntransform any standardized scales into personalized and engaging interactive\nfiction games. To validate the proposed method, we conduct psychometric\nevaluations to assess its effectiveness and employ human evaluators to examine\nthe generated content across various psychological constructs, including\ndepression, cognitive distortions, and personality traits. Results demonstrate\nthat PsychoGAT serves as an effective assessment tool, achieving statistically\nsignificant excellence in psychometric metrics such as reliability, convergent\nvalidity, and discriminant validity. Moreover, human evaluations confirm\nPsychoGAT's enhancements in content coherence, interactivity, interest,\nimmersion, and satisfaction.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2024-02-19T18:00:30+00:00",
    "updated": "2024-08-29T08:27:27+00:00",
    "doi": null,
    "comment": "ACL 2024",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2306.13643v1": {
    "id": "http://arxiv.org/abs/2306.13643v1",
    "title": "LightGlue: Local Feature Matching at Light Speed",
    "authors": [
      "Philipp Lindenberger",
      "Paul-Edouard Sarlin",
      "Marc Pollefeys"
    ],
    "abstract": "We introduce LightGlue, a deep neural network that learns to match local\nfeatures across images. We revisit multiple design decisions of SuperGlue, the\nstate of the art in sparse matching, and derive simple but effective\nimprovements. Cumulatively, they make LightGlue more efficient - in terms of\nboth memory and computation, more accurate, and much easier to train. One key\nproperty is that LightGlue is adaptive to the difficulty of the problem: the\ninference is much faster on image pairs that are intuitively easy to match, for\nexample because of a larger visual overlap or limited appearance change. This\nopens up exciting prospects for deploying deep matchers in latency-sensitive\napplications like 3D reconstruction. The code and trained models are publicly\navailable at https://github.com/cvg/LightGlue.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-06-23T17:52:54+00:00",
    "updated": "2023-06-23T17:52:54+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.01723v1": {
    "id": "http://arxiv.org/abs/2210.01723v1",
    "title": "Dense Prediction Transformer for Scale Estimation in Monocular Visual Odometry",
    "authors": [
      "Andr\u00e9 O. Fran\u00e7ani",
      "Marcos R. O. A. Maximo"
    ],
    "abstract": "Monocular visual odometry consists of the estimation of the position of an\nagent through images of a single camera, and it is applied in autonomous\nvehicles, medical robots, and augmented reality. However, monocular systems\nsuffer from the scale ambiguity problem due to the lack of depth information in\n2D frames. This paper contributes by showing an application of the dense\nprediction transformer model for scale estimation in monocular visual odometry\nsystems. Experimental results show that the scale drift problem of monocular\nsystems can be reduced through the accurate estimation of the depth map by this\nmodel, achieving competitive state-of-the-art performance on a visual odometry\nbenchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2022-10-04T16:29:21+00:00",
    "updated": "2022-10-04T16:29:21+00:00",
    "doi": "10.1109/LARS/SBR/WRE56824.2022.9995735",
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1709.10489v3": {
    "id": "http://arxiv.org/abs/1709.10489v3",
    "title": "Self-supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation",
    "authors": [
      "Gregory Kahn",
      "Adam Villaflor",
      "Bosen Ding",
      "Pieter Abbeel",
      "Sergey Levine"
    ],
    "abstract": "Enabling robots to autonomously navigate complex environments is essential\nfor real-world deployment. Prior methods approach this problem by having the\nrobot maintain an internal map of the world, and then use a localization and\nplanning method to navigate through the internal map. However, these approaches\noften include a variety of assumptions, are computationally intensive, and do\nnot learn from failures. In contrast, learning-based methods improve as the\nrobot acts in the environment, but are difficult to deploy in the real-world\ndue to their high sample complexity. To address the need to learn complex\npolicies with few samples, we propose a generalized computation graph that\nsubsumes value-based model-free methods and model-based methods, with specific\ninstantiations interpolating between model-free and model-based. We then\ninstantiate this graph to form a navigation model that learns from raw images\nand is sample efficient. Our simulated car experiments explore the design\ndecisions of our navigation model, and show our approach outperforms\nsingle-step and $N$-step double Q-learning. We also evaluate our approach on a\nreal-world RC car and show it can learn to navigate through a complex indoor\nenvironment with a few hours of fully autonomous, self-supervised training.\nVideos of the experiments and code can be found at github.com/gkahn13/gcg",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2017-09-29T16:47:14+00:00",
    "updated": "2018-05-17T22:32:25+00:00",
    "doi": null,
    "comment": "ICRA 2018",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2312.17115v2": {
    "id": "http://arxiv.org/abs/2312.17115v2",
    "title": "How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability of Human Behavior Simulation",
    "authors": [
      "Yang Xiao",
      "Yi Cheng",
      "Jinlan Fu",
      "Jiashuo Wang",
      "Wenjie Li",
      "Pengfei Liu"
    ],
    "abstract": "In recent years, AI has demonstrated remarkable capabilities in simulating\nhuman behaviors, particularly those implemented with large language models\n(LLMs). However, due to the lack of systematic evaluation of LLMs' simulated\nbehaviors, the believability of LLMs among humans remains ambiguous, i.e., it\nis unclear which behaviors of LLMs are convincingly human-like and which need\nfurther improvements. In this work, we design SimulateBench to evaluate the\nbelievability of LLMs when simulating human behaviors. In specific, we evaluate\nthe believability of LLMs based on two critical dimensions: 1) consistency: the\nextent to which LLMs can behave consistently with the given information of a\nhuman to simulate; and 2) robustness: the ability of LLMs' simulated behaviors\nto remain robust when faced with perturbations. SimulateBench includes 65\ncharacter profiles and a total of 8,400 questions to examine LLMs' simulated\nbehaviors. Based on SimulateBench, we evaluate the performances of 10 widely\nused LLMs when simulating characters. The experimental results reveal that\ncurrent LLMs struggle to align their behaviors with assigned characters and are\nvulnerable to perturbations in certain factors.",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "published": "2023-12-28T16:51:11+00:00",
    "updated": "2024-06-15T14:08:30+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2311.17967v1": {
    "id": "http://arxiv.org/abs/2311.17967v1",
    "title": "Discovering Galaxy Features via Dataset Distillation",
    "authors": [
      "Haowen Guan",
      "Xuan Zhao",
      "Zishi Wang",
      "Zhiyang Li",
      "Julia Kempe"
    ],
    "abstract": "In many applications, Neural Nets (NNs) have classification performance on\npar or even exceeding human capacity. Moreover, it is likely that NNs leverage\nunderlying features that might differ from those humans perceive to classify.\nCan we \"reverse-engineer\" pertinent features to enhance our scientific\nunderstanding? Here, we apply this idea to the notoriously difficult task of\ngalaxy classification: NNs have reached high performance for this task, but\nwhat does a neural net (NN) \"see\" when it classifies galaxies? Are there\nmorphological features that the human eye might overlook that could help with\nthe task and provide new insights? Can we visualize tracers of early evolution,\nor additionally incorporated spectral data? We present a novel way to summarize\nand visualize galaxy morphology through the lens of neural networks, leveraging\nDataset Distillation, a recent deep-learning methodology with the primary\nobjective to distill knowledge from a large dataset and condense it into a\ncompact synthetic dataset, such that a model trained on this synthetic dataset\nachieves performance comparable to a model trained on the full dataset. We\ncurate a class-balanced, medium-size high-confidence version of the Galaxy Zoo\n2 dataset, and proceed with dataset distillation from our accurate\nNN-classifier to create synthesized prototypical images of galaxy morphological\nfeatures, demonstrating its effectiveness. Of independent interest, we\nintroduce a self-adaptive version of the state-of-the-art Matching Trajectory\nalgorithm to automate the distillation process, and show enhanced performance\non computer vision benchmarks.",
    "categories": [
      "cs.CV",
      "astro-ph.IM",
      "cs.LG"
    ],
    "published": "2023-11-29T12:39:31+00:00",
    "updated": "2023-11-29T12:39:31+00:00",
    "doi": null,
    "comment": "Accepted to NeurIPS Workshop on Machine Learning and the Physical\n  Sciences, 2023",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2204.08790v6": {
    "id": "http://arxiv.org/abs/2204.08790v6",
    "title": "ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models",
    "authors": [
      "Chunyuan Li",
      "Haotian Liu",
      "Liunian Harold Li",
      "Pengchuan Zhang",
      "Jyoti Aneja",
      "Jianwei Yang",
      "Ping Jin",
      "Houdong Hu",
      "Zicheng Liu",
      "Yong Jae Lee",
      "Jianfeng Gao"
    ],
    "abstract": "Learning visual representations from natural language supervision has\nrecently shown great promise in a number of pioneering works. In general, these\nlanguage-augmented visual models demonstrate strong transferability to a\nvariety of datasets and tasks. However, it remains challenging to evaluate the\ntransferablity of these models due to the lack of easy-to-use evaluation\ntoolkits and public benchmarks. To tackle this, we build ELEVATER (Evaluation\nof Language-augmented Visual Task-level Transfer), the first benchmark and\ntoolkit for evaluating(pre-trained) language-augmented visual models. ELEVATER\nis composed of three components. (i) Datasets. As downstream evaluation suites,\nit consists of 20 image classification datasets and 35 object detection\ndatasets, each of which is augmented with external knowledge. (ii) Toolkit. An\nautomatic hyper-parameter tuning toolkit is developed to facilitate model\nevaluation on downstream tasks. (iii) Metrics. A variety of evaluation metrics\nare used to measure sample-efficiency (zero-shot and few-shot) and\nparameter-efficiency (linear probing and full model fine-tuning). ELEVATER is a\nplatform for Computer Vision in the Wild (CVinW), and is publicly released at\nat https://computer-vision-in-the-wild.github.io/ELEVATER/",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2022-04-19T10:23:42+00:00",
    "updated": "2022-10-13T04:32:12+00:00",
    "doi": null,
    "comment": "NeurIPS 2022 (Datasets and Benchmarks Track). The first two authors\n  contribute equally. Benchmark page:\n  https://computer-vision-in-the-wild.github.io/ELEVATER/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2307.04738v1": {
    "id": "http://arxiv.org/abs/2307.04738v1",
    "title": "RoCo: Dialectic Multi-Robot Collaboration with Large Language Models",
    "authors": [
      "Zhao Mandi",
      "Shreeya Jain",
      "Shuran Song"
    ],
    "abstract": "We propose a novel approach to multi-robot collaboration that harnesses the\npower of pre-trained large language models (LLMs) for both high-level\ncommunication and low-level path planning. Robots are equipped with LLMs to\ndiscuss and collectively reason task strategies. They then generate sub-task\nplans and task space waypoint paths, which are used by a multi-arm motion\nplanner to accelerate trajectory planning. We also provide feedback from the\nenvironment, such as collision checking, and prompt the LLM agents to improve\ntheir plan and waypoints in-context. For evaluation, we introduce RoCoBench, a\n6-task benchmark covering a wide range of multi-robot collaboration scenarios,\naccompanied by a text-only dataset for agent representation and reasoning. We\nexperimentally demonstrate the effectiveness of our approach -- it achieves\nhigh success rates across all tasks in RoCoBench and adapts to variations in\ntask semantics. Our dialog setup offers high interpretability and flexibility\n-- in real world experiments, we show RoCo easily incorporates\nhuman-in-the-loop, where a user can communicate and collaborate with a robot\nagent to complete tasks together. See project website\nhttps://project-roco.github.io for videos and code.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-07-10T17:52:01+00:00",
    "updated": "2023-07-10T17:52:01+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.RO"
  },
  "2406.12459v1": {
    "id": "http://arxiv.org/abs/2406.12459v1",
    "title": "HumanSplat: Generalizable Single-Image Human Gaussian Splatting with Structure Priors",
    "authors": [
      "Panwang Pan",
      "Zhuo Su",
      "Chenguo Lin",
      "Zhen Fan",
      "Yongjie Zhang",
      "Zeming Li",
      "Tingting Shen",
      "Yadong Mu",
      "Yebin Liu"
    ],
    "abstract": "Despite recent advancements in high-fidelity human reconstruction techniques,\nthe requirements for densely captured images or time-consuming per-instance\noptimization significantly hinder their applications in broader scenarios. To\ntackle these issues, we present HumanSplat which predicts the 3D Gaussian\nSplatting properties of any human from a single input image in a generalizable\nmanner. In particular, HumanSplat comprises a 2D multi-view diffusion model and\na latent reconstruction transformer with human structure priors that adeptly\nintegrate geometric priors and semantic features within a unified framework. A\nhierarchical loss that incorporates human semantic information is further\ndesigned to achieve high-fidelity texture modeling and better constrain the\nestimated multiple views. Comprehensive experiments on standard benchmarks and\nin-the-wild images demonstrate that HumanSplat surpasses existing\nstate-of-the-art methods in achieving photorealistic novel-view synthesis.",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-06-18T10:05:33+00:00",
    "updated": "2024-06-18T10:05:33+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2206.11892v3": {
    "id": "http://arxiv.org/abs/2206.11892v3",
    "title": "DDPM-CD: Denoising Diffusion Probabilistic Models as Feature Extractors for Change Detection",
    "authors": [
      "Wele Gedara Chaminda Bandara",
      "Nithin Gopalakrishnan Nair",
      "Vishal M. Patel"
    ],
    "abstract": "Remote sensing change detection is crucial for understanding the dynamics of\nour planet's surface, facilitating the monitoring of environmental changes,\nevaluating human impact, predicting future trends, and supporting\ndecision-making. In this work, we introduce a novel approach for change\ndetection that can leverage off-the-shelf, unlabeled remote sensing images in\nthe training process by pre-training a Denoising Diffusion Probabilistic Model\n(DDPM) - a class of generative models used in image synthesis. DDPMs learn the\ntraining data distribution by gradually converting training images into a\nGaussian distribution using a Markov chain. During inference (i.e., sampling),\nthey can generate a diverse set of samples closer to the training distribution,\nstarting from Gaussian noise, achieving state-of-the-art image synthesis\nresults. However, in this work, our focus is not on image synthesis but on\nutilizing it as a pre-trained feature extractor for the downstream application\nof change detection. Specifically, we fine-tune a lightweight change classifier\nutilizing the feature representations produced by the pre-trained DDPM\nalongside change labels. Experiments conducted on the LEVIR-CD, WHU-CD,\nDSIFN-CD, and CDD datasets demonstrate that the proposed DDPM-CD method\nsignificantly outperforms the existing state-of-the-art change detection\nmethods in terms of F1 score, IoU, and overall accuracy, highlighting the\npivotal role of pre-trained DDPM as a feature extractor for downstream\napplications. We have made both the code and pre-trained models available at\nhttps://github.com/wgcban/ddpm-cd",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2022-06-23T17:58:29+00:00",
    "updated": "2024-01-12T14:37:36+00:00",
    "doi": null,
    "comment": "Code available at: https://github.com/wgcban/ddpm-cd",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2408.12588v1": {
    "id": "http://arxiv.org/abs/2408.12588v1",
    "title": "Real-Time Video Generation with Pyramid Attention Broadcast",
    "authors": [
      "Xuanlei Zhao",
      "Xiaolong Jin",
      "Kai Wang",
      "Yang You"
    ],
    "abstract": "We present Pyramid Attention Broadcast (PAB), a real-time, high quality and\ntraining-free approach for DiT-based video generation. Our method is founded on\nthe observation that attention difference in the diffusion process exhibits a\nU-shaped pattern, indicating significant redundancy. We mitigate this by\nbroadcasting attention outputs to subsequent steps in a pyramid style. It\napplies different broadcast strategies to each attention based on their\nvariance for best efficiency. We further introduce broadcast sequence parallel\nfor more efficient distributed inference. PAB demonstrates superior results\nacross three models compared to baselines, achieving real-time generation for\nup to 720p videos. We anticipate that our simple yet effective method will\nserve as a robust baseline and facilitate future research and application for\nvideo generation.",
    "categories": [
      "cs.CV",
      "cs.DC"
    ],
    "published": "2024-08-22T17:54:21+00:00",
    "updated": "2024-08-22T17:54:21+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2408.13858v1": {
    "id": "http://arxiv.org/abs/2408.13858v1",
    "title": "Draw Like an Artist: Complex Scene Generation with Diffusion Model via Composition, Painting, and Retouching",
    "authors": [
      "Minghao Liu",
      "Le Zhang",
      "Yingjie Tian",
      "Xiaochao Qu",
      "Luoqi Liu",
      "Ting Liu"
    ],
    "abstract": "Recent advances in text-to-image diffusion models have demonstrated\nimpressive capabilities in image quality. However, complex scene generation\nremains relatively unexplored, and even the definition of `complex scene'\nitself remains unclear. In this paper, we address this gap by providing a\nprecise definition of complex scenes and introducing a set of Complex\nDecomposition Criteria (CDC) based on this definition. Inspired by the artists\npainting process, we propose a training-free diffusion framework called Complex\nDiffusion (CxD), which divides the process into three stages: composition,\npainting, and retouching. Our method leverages the powerful chain-of-thought\ncapabilities of large language models (LLMs) to decompose complex prompts based\non CDC and to manage composition and layout. We then develop an attention\nmodulation method that guides simple prompts to specific regions to complete\nthe complex scene painting. Finally, we inject the detailed output of the LLM\ninto a retouching model to enhance the image details, thus implementing the\nretouching stage. Extensive experiments demonstrate that our method outperforms\nprevious SOTA approaches, significantly improving the generation of\nhigh-quality, semantically consistent, and visually diverse images for complex\nscenes, even with intricate prompts.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-08-25T15:05:32+00:00",
    "updated": "2024-08-25T15:05:32+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2303.15649v2": {
    "id": "http://arxiv.org/abs/2303.15649v2",
    "title": "StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing",
    "authors": [
      "Senmao Li",
      "Joost van de Weijer",
      "Taihang Hu",
      "Fahad Shahbaz Khan",
      "Qibin Hou",
      "Yaxing Wang",
      "Jian Yang"
    ],
    "abstract": "A significant research effort is focused on exploiting the amazing capacities\nof pretrained diffusion models for the editing of images. They either finetune\nthe model, or invert the image in the latent space of the pretrained model.\nHowever, they suffer from two problems: (1) Unsatisfying results for selected\nregions, and unexpected changes in nonselected regions. (2) They require\ncareful text prompt editing where the prompt should include all visual objects\nin the input image. To address this, we propose two improvements: (1) Only\noptimizing the input of the value linear network in the cross-attention layers,\nis sufficiently powerful to reconstruct a real image. (2) We propose attention\nregularization to preserve the object-like attention maps after editing,\nenabling us to obtain accurate style editing without invoking significant\nstructural changes. We further improve the editing technique which is used for\nthe unconditional branch of classifier-free guidance, as well as the\nconditional one as used by P2P. Extensive experimental prompt-editing results\non a variety of images, demonstrate qualitatively and quantitatively that our\nmethod has superior editing capabilities than existing and concurrent works.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-03-28T00:16:45+00:00",
    "updated": "2023-08-20T11:58:44+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1710.09553v2": {
    "id": "http://arxiv.org/abs/1710.09553v2",
    "title": "Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior",
    "authors": [
      "Charles H. Martin",
      "Michael W. Mahoney"
    ],
    "abstract": "We describe an approach to understand the peculiar and counterintuitive\ngeneralization properties of deep neural networks. The approach involves going\nbeyond worst-case theoretical capacity control frameworks that have been\npopular in machine learning in recent years to revisit old ideas in the\nstatistical mechanics of neural networks. Within this approach, we present a\nprototypical Very Simple Deep Learning (VSDL) model, whose behavior is\ncontrolled by two control parameters, one describing an effective amount of\ndata, or load, on the network (that decreases when noise is added to the\ninput), and one with an effective temperature interpretation (that increases\nwhen algorithms are early stopped). Using this model, we describe how a very\nsimple application of ideas from the statistical mechanics theory of\ngeneralization provides a strong qualitative description of recently-observed\nempirical results regarding the inability of deep neural networks not to\noverfit training data, discontinuous learning and sharp transitions in the\ngeneralization properties of learning algorithms, etc.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2017-10-26T06:08:39+00:00",
    "updated": "2019-02-17T05:57:09+00:00",
    "doi": null,
    "comment": "31 pages; added brief discussion of recent papers that use/extend\n  these ideas",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2307.10802v1": {
    "id": "http://arxiv.org/abs/2307.10802v1",
    "title": "Meta-Transformer: A Unified Framework for Multimodal Learning",
    "authors": [
      "Yiyuan Zhang",
      "Kaixiong Gong",
      "Kaipeng Zhang",
      "Hongsheng Li",
      "Yu Qiao",
      "Wanli Ouyang",
      "Xiangyu Yue"
    ],
    "abstract": "Multimodal learning aims to build models that can process and relate\ninformation from multiple modalities. Despite years of development in this\nfield, it still remains challenging to design a unified network for processing\nvarious modalities ($\\textit{e.g.}$ natural language, 2D images, 3D point\nclouds, audio, video, time series, tabular data) due to the inherent gaps among\nthem. In this work, we propose a framework, named Meta-Transformer, that\nleverages a $\\textbf{frozen}$ encoder to perform multimodal perception without\nany paired multimodal training data. In Meta-Transformer, the raw input data\nfrom various modalities are mapped into a shared token space, allowing a\nsubsequent encoder with frozen parameters to extract high-level semantic\nfeatures of the input data. Composed of three main components: a unified data\ntokenizer, a modality-shared encoder, and task-specific heads for downstream\ntasks, Meta-Transformer is the first framework to perform unified learning\nacross 12 modalities with unpaired data. Experiments on different benchmarks\nreveal that Meta-Transformer can handle a wide range of tasks including\nfundamental perception (text, image, point cloud, audio, video), practical\napplication (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,\ntabular, and time-series). Meta-Transformer indicates a promising future for\ndeveloping unified multimodal intelligence with transformers. Code will be\navailable at https://github.com/invictus717/MetaTransformer",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2023-07-20T12:10:29+00:00",
    "updated": "2023-07-20T12:10:29+00:00",
    "doi": null,
    "comment": "Project website: https://kxgong.github.io/meta_transformer/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2312.13271v3": {
    "id": "http://arxiv.org/abs/2312.13271v3",
    "title": "Repaint123: Fast and High-quality One Image to 3D Generation with Progressive Controllable 2D Repainting",
    "authors": [
      "Junwu Zhang",
      "Zhenyu Tang",
      "Yatian Pang",
      "Xinhua Cheng",
      "Peng Jin",
      "Yida Wei",
      "Munan Ning",
      "Li Yuan"
    ],
    "abstract": "Recent one image to 3D generation methods commonly adopt Score Distillation\nSampling (SDS). Despite the impressive results, there are multiple deficiencies\nincluding multi-view inconsistency, over-saturated and over-smoothed textures,\nas well as the slow generation speed. To address these deficiencies, we present\nRepaint123 to alleviate multi-view bias as well as texture degradation and\nspeed up the generation process. The core idea is to combine the powerful image\ngeneration capability of the 2D diffusion model and the texture alignment\nability of the repainting strategy for generating high-quality multi-view\nimages with consistency. We further propose visibility-aware adaptive\nrepainting strength for overlap regions to enhance the generated image quality\nin the repainting process. The generated high-quality and multi-view consistent\nimages enable the use of simple Mean Square Error (MSE) loss for fast 3D\ncontent generation. We conduct extensive experiments and show that our method\nhas a superior ability to generate high-quality 3D content with multi-view\nconsistency and fine textures in 2 minutes from scratch. Our project page is\navailable at https://pku-yuangroup.github.io/repaint123/.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-12-20T18:51:02+00:00",
    "updated": "2023-12-27T10:51:27+00:00",
    "doi": null,
    "comment": "Project page: https://pku-yuangroup.github.io/repaint123/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2401.14726v2": {
    "id": "http://arxiv.org/abs/2401.14726v2",
    "title": "3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field",
    "authors": [
      "Zhenyu Bao",
      "Guibiao Liao",
      "Zhongyuan Zhao",
      "Kanglin Liu",
      "Qing Li",
      "Guoping Qiu"
    ],
    "abstract": "Simultaneously achieving 3D reconstruction and new view synthesis for indoor\nenvironments has widespread applications but is technically very challenging.\nState-of-the-art methods based on implicit neural functions can achieve\nexcellent 3D reconstruction results, but their performances on new view\nsynthesis can be unsatisfactory. The exciting development of neural radiance\nfield (NeRF) has revolutionized new view synthesis, however, NeRF-based models\ncan fail to reconstruct clean geometric surfaces. We have developed a dual\nneural radiance field (Du-NeRF) to simultaneously achieve high-quality geometry\nreconstruction and view rendering. Du-NeRF contains two geometric fields, one\nderived from the SDF field to facilitate geometric reconstruction and the other\nderived from the density field to boost new view synthesis. One of the\ninnovative features of Du-NeRF is that it decouples a view-independent\ncomponent from the density field and uses it as a label to supervise the\nlearning process of the SDF field. This reduces shape-radiance ambiguity and\nenables geometry and color to benefit from each other during the learning\nprocess. Extensive experiments demonstrate that Du-NeRF can significantly\nimprove the performance of novel view synthesis and 3D reconstruction for\nindoor environments and it is particularly effective in constructing areas\ncontaining fine geometries that do not obey multi-view color consistency.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2024-01-26T09:21:46+00:00",
    "updated": "2024-07-19T09:35:18+00:00",
    "doi": null,
    "comment": "20 pages, 8 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2210.08573v1": {
    "id": "http://arxiv.org/abs/2210.08573v1",
    "title": "DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models",
    "authors": [
      "Yueqin Yin",
      "Lianghua Huang",
      "Yu Liu",
      "Kaiqi Huang"
    ],
    "abstract": "Recent generative models show impressive results in photo-realistic image\ngeneration. However, artifacts often inevitably appear in the generated\nresults, leading to downgraded user experience and reduced performance in\ndownstream tasks. This work aims to develop a plugin post-processing module for\ndiverse generative models, which can faithfully restore images from diverse\ngenerative artifacts. This is challenging because: (1) Unlike traditional\ndegradation patterns, generative artifacts are non-linear and the\ntransformation function is highly complex. (2) There are no readily available\nartifact-image pairs. (3) Different from model-specific anti-artifact methods,\na model-agnostic framework views the generator as a black-box machine and has\nno access to the architecture details. In this work, we first design a group of\nmechanisms to simulate generative artifacts of popular generators (i.e., GANs,\nautoregressive models, and diffusion models), given real images. Second, we\nimplement the model-agnostic anti-artifact framework as an image-to-image\ndiffusion model, due to its advantage in generation quality and capacity.\nFinally, we design a conditioning scheme for the diffusion model to enable both\nblind and non-blind image restoration. A guidance parameter is also introduced\nto allow for a trade-off between restoration accuracy and image quality.\nExtensive experiments show that our method significantly outperforms previous\napproaches on the proposed datasets and real-world artifact images.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-10-16T16:08:47+00:00",
    "updated": "2022-10-16T16:08:47+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2106.06801v3": {
    "id": "http://arxiv.org/abs/2106.06801v3",
    "title": "Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation",
    "authors": [
      "Prashant Pandey",
      "Ajey Pai",
      "Nisarg Bhatt",
      "Prasenjit Das",
      "Govind Makharia",
      "Prathosh AP",
      "Mausam"
    ],
    "abstract": "Contrastive Learning (CL) is a recent representation learning approach, which\nencourages inter-class separability and intra-class compactness in learned\nimage representations. Since medical images often contain multiple semantic\nclasses in an image, using CL to learn representations of local features (as\nopposed to global) is important. In this work, we present a novel\nsemi-supervised 2D medical segmentation solution that applies CL on image\npatches, instead of full images. These patches are meaningfully constructed\nusing the semantic information of different classes obtained via pseudo\nlabeling. We also propose a novel consistency regularization (CR) scheme, which\nworks in synergy with CL. It addresses the problem of confirmation bias, and\nencourages better clustering in the feature space. We evaluate our method on\nfour public medical segmentation datasets and a novel histopathology dataset\nthat we introduce. Our method obtains consistent improvements over\nstate-of-the-art semi-supervised segmentation approaches for all datasets.",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-06-12T15:43:24+00:00",
    "updated": "2021-08-06T16:33:11+00:00",
    "doi": null,
    "comment": "The paper is withdrawn due to a bug in experimental protocol that\n  renders its experimental results and observations invalid. All expts were\n  conducted by the student authors. The roles of senior authors (Prasenjit Das,\n  Govind Makharia, Prathosh, and Mausam) were in defining the problem\n  statement, discussions of potential solutions and framing of the paper and\n  not in performing experiments",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2310.03708v4": {
    "id": "http://arxiv.org/abs/2310.03708v4",
    "title": "Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization",
    "authors": [
      "Zhanhui Zhou",
      "Jie Liu",
      "Jing Shao",
      "Xiangyu Yue",
      "Chao Yang",
      "Wanli Ouyang",
      "Yu Qiao"
    ],
    "abstract": "A single language model, even when aligned with labelers through\nreinforcement learning from human feedback (RLHF), may not suit all human\npreferences. Recent approaches therefore prefer customization, gathering\nmulti-dimensional feedback, and creating distinct reward models for each\ndimension. Different language models are then optimized for various preferences\nusing multi-objective RLHF (MORLHF) with varying reward weights. However, RL\nfine-tuning is unstable and resource-heavy, especially with diverse and usually\nconflicting objectives. In this paper, we present Multi-Objective Direct\nPreference Optimization (MODPO), an RL-free extension of Direct Preference\nOptimization (DPO) for multiple alignment objectives. Essentially, MODPO folds\nlanguage modeling directly into reward modeling, training language models as\nimplicit collective reward models that combine all objectives with specific\nweights. MODPO theoretically yields the same optimal solutions as MORLHF but is\npractically more stable and efficient. Empirical results in safety alignment\nand long-form question answering show that MODPO matches or outperforms\nexisting methods, producing a Pareto front of language models catering to\ndiverse preferences with three times less computational resources compared to\nMORLHF. Code is available at https://github.com/ZHZisZZ/modpo.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-10-05T17:35:26+00:00",
    "updated": "2024-08-17T13:39:13+00:00",
    "doi": null,
    "comment": "Findings of ACL 2024",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2311.16054v3": {
    "id": "http://arxiv.org/abs/2311.16054v3",
    "title": "Metric Space Magnitude for Evaluating the Diversity of Latent Representations",
    "authors": [
      "Katharina Limbeck",
      "Rayna Andreeva",
      "Rik Sarkar",
      "Bastian Rieck"
    ],
    "abstract": "The magnitude of a metric space is a novel invariant that provides a measure\nof the 'effective size' of a space across multiple scales, while also capturing\nnumerous geometrical properties, such as curvature, density, or entropy. We\ndevelop a family of magnitude-based measures of the intrinsic diversity of\nlatent representations, formalising a novel notion of dissimilarity between\nmagnitude functions of finite metric spaces. Our measures are provably stable\nunder perturbations of the data, can be efficiently calculated, and enable a\nrigorous multi-scale characterisation and comparison of latent representations.\nWe show their utility and superior performance across different domains and\ntasks, including (i) the automated estimation of diversity, (ii) the detection\nof mode collapse, and (iii) the evaluation of generative models for text,\nimage, and graph data.",
    "categories": [
      "cs.LG",
      "math.GT",
      "stat.ML"
    ],
    "published": "2023-11-27T18:19:07+00:00",
    "updated": "2024-06-21T06:25:49+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "1812.03443v3": {
    "id": "http://arxiv.org/abs/1812.03443v3",
    "title": "FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search",
    "authors": [
      "Bichen Wu",
      "Xiaoliang Dai",
      "Peizhao Zhang",
      "Yanghan Wang",
      "Fei Sun",
      "Yiming Wu",
      "Yuandong Tian",
      "Peter Vajda",
      "Yangqing Jia",
      "Kurt Keutzer"
    ],
    "abstract": "Designing accurate and efficient ConvNets for mobile devices is challenging\nbecause the design space is combinatorially large. Due to this, previous neural\narchitecture search (NAS) methods are computationally expensive. ConvNet\narchitecture optimality depends on factors such as input resolution and target\ndevices. However, existing approaches are too expensive for case-by-case\nredesigns. Also, previous work focuses primarily on reducing FLOPs, but FLOP\ncount does not always reflect actual latency. To address these, we propose a\ndifferentiable neural architecture search (DNAS) framework that uses\ngradient-based methods to optimize ConvNet architectures, avoiding enumerating\nand training individual architectures separately as in previous methods.\nFBNets, a family of models discovered by DNAS surpass state-of-the-art models\nboth designed manually and generated automatically. FBNet-B achieves 74.1%\ntop-1 accuracy on ImageNet with 295M FLOPs and 23.1 ms latency on a Samsung S8\nphone, 2.4x smaller and 1.5x faster than MobileNetV2-1.3 with similar accuracy.\nDespite higher accuracy and lower latency than MnasNet, we estimate FBNet-B's\nsearch cost is 420x smaller than MnasNet's, at only 216 GPU-hours. Searched for\ndifferent resolutions and channel sizes, FBNets achieve 1.5% to 6.4% higher\naccuracy than MobileNetV2. The smallest FBNet achieves 50.2% accuracy and 2.9\nms latency (345 frames per second) on a Samsung S8. Over a Samsung-optimized\nFBNet, the iPhone-X-optimized model achieves a 1.4x speedup on an iPhone X.",
    "categories": [
      "cs.CV"
    ],
    "published": "2018-12-09T08:24:50+00:00",
    "updated": "2019-05-24T05:47:40+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.04026v1": {
    "id": "http://arxiv.org/abs/2404.04026v1",
    "title": "MM-Gaussian: 3D Gaussian-based Multi-modal Fusion for Localization and Reconstruction in Unbounded Scenes",
    "authors": [
      "Chenyang Wu",
      "Yifan Duan",
      "Xinran Zhang",
      "Yu Sheng",
      "Jianmin Ji",
      "Yanyong Zhang"
    ],
    "abstract": "Localization and mapping are critical tasks for various applications such as\nautonomous vehicles and robotics. The challenges posed by outdoor environments\npresent particular complexities due to their unbounded characteristics. In this\nwork, we present MM-Gaussian, a LiDAR-camera multi-modal fusion system for\nlocalization and mapping in unbounded scenes. Our approach is inspired by the\nrecently developed 3D Gaussians, which demonstrate remarkable capabilities in\nachieving high rendering quality and fast rendering speed. Specifically, our\nsystem fully utilizes the geometric structure information provided by\nsolid-state LiDAR to address the problem of inaccurate depth encountered when\nrelying solely on visual solutions in unbounded, outdoor scenarios.\nAdditionally, we utilize 3D Gaussian point clouds, with the assistance of\npixel-level gradient descent, to fully exploit the color information in photos,\nthereby achieving realistic rendering effects. To further bolster the\nrobustness of our system, we designed a relocalization module, which assists in\nreturning to the correct trajectory in the event of a localization failure.\nExperiments conducted in multiple scenarios demonstrate the effectiveness of\nour method.",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2024-04-05T11:14:19+00:00",
    "updated": "2024-04-05T11:14:19+00:00",
    "doi": null,
    "comment": "7 pages, 5 figures",
    "journal_ref": null,
    "primary_category": "cs.RO"
  },
  "2212.01381v2": {
    "id": "http://arxiv.org/abs/2212.01381v2",
    "title": "LatentSwap3D: Semantic Edits on 3D Image GANs",
    "authors": [
      "Enis Simsar",
      "Alessio Tonioni",
      "Evin P\u0131nar \u00d6rnek",
      "Federico Tombari"
    ],
    "abstract": "3D GANs have the ability to generate latent codes for entire 3D volumes\nrather than only 2D images. These models offer desirable features like\nhigh-quality geometry and multi-view consistency, but, unlike their 2D\ncounterparts, complex semantic image editing tasks for 3D GANs have only been\npartially explored. To address this problem, we propose LatentSwap3D, a\nsemantic edit approach based on latent space discovery that can be used with\nany off-the-shelf 3D or 2D GAN model and on any dataset. LatentSwap3D relies on\nidentifying the latent code dimensions corresponding to specific attributes by\nfeature ranking using a random forest classifier. It then performs the edit by\nswapping the selected dimensions of the image being edited with the ones from\nan automatically selected reference image. Compared to other latent space\ncontrol-based edit methods, which were mainly designed for 2D GANs, our method\non 3D GANs provides remarkably consistent semantic edits in a disentangled\nmanner and outperforms others both qualitatively and quantitatively. We show\nresults on seven 3D GANs (pi-GAN, GIRAFFE, StyleSDF, MVCGAN, EG3D, StyleNeRF,\nand VolumeGAN) and on five datasets (FFHQ, AFHQ, Cats, MetFaces, and CompCars).",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-12-02T18:59:51+00:00",
    "updated": "2023-09-04T19:12:46+00:00",
    "doi": null,
    "comment": "The paper has been accepted by ICCV'23 AI3DCC",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2006.12971v2": {
    "id": "http://arxiv.org/abs/2006.12971v2",
    "title": "Gaining Insight into SARS-CoV-2 Infection and COVID-19 Severity Using Self-supervised Edge Features and Graph Neural Networks",
    "authors": [
      "Arijit Sehanobish",
      "Neal G. Ravindra",
      "David van Dijk"
    ],
    "abstract": "A molecular and cellular understanding of how SARS-CoV-2 variably infects and\ncauses severe COVID-19 remains a bottleneck in developing interventions to end\nthe pandemic. We sought to use deep learning to study the biology of SARS-CoV-2\ninfection and COVID-19 severity by identifying transcriptomic patterns and cell\ntypes associated with SARS-CoV-2 infection and COVID-19 severity. To do this,\nwe developed a new approach to generating self-supervised edge features. We\npropose a model that builds on Graph Attention Networks (GAT), creates edge\nfeatures using self-supervised learning, and ingests these edge features via a\nSet Transformer. This model achieves significant improvements in predicting the\ndisease state of individual cells, given their transcriptome. We apply our\nmodel to single-cell RNA sequencing datasets of SARS-CoV-2 infected lung\norganoids and bronchoalveolar lavage fluid samples of patients with COVID-19,\nachieving state-of-the-art performance on both datasets with our model. We then\nborrow from the field of explainable AI (XAI) to identify the features (genes)\nand cell types that discriminate bystander vs. infected cells across time and\nmoderate vs. severe COVID-19 disease. To the best of our knowledge, this\nrepresents the first application of deep learning to identifying the molecular\nand cellular determinants of SARS-CoV-2 infection and COVID-19 severity using\nsingle-cell omics data.",
    "categories": [
      "cs.LG",
      "q-bio.GN",
      "stat.ML"
    ],
    "published": "2020-06-23T13:22:16+00:00",
    "updated": "2020-12-15T17:05:18+00:00",
    "doi": null,
    "comment": "To appear at AAAI'21. Previous version (v2) accepted as a spotlight\n  talk at ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+)\n  and recipient of best paper award for Covid-19 applications. Significant\n  improvements over v2",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2208.00361v3": {
    "id": "http://arxiv.org/abs/2208.00361v3",
    "title": "One for All: One-stage Referring Expression Comprehension with Dynamic Reasoning",
    "authors": [
      "Zhipeng Zhang",
      "Zhimin Wei",
      "Zhongzhen Huang",
      "Rui Niu",
      "Peng Wang"
    ],
    "abstract": "Referring Expression Comprehension (REC) is one of the most important tasks\nin visual reasoning that requires a model to detect the target object referred\nby a natural language expression. Among the proposed pipelines, the one-stage\nReferring Expression Comprehension (OSREC) has become the dominant trend since\nit merges the region proposal and selection stages. Many state-of-the-art OSREC\nmodels adopt a multi-hop reasoning strategy because a sequence of objects is\nfrequently mentioned in a single expression which needs multi-hop reasoning to\nanalyze the semantic relation. However, one unsolved issue of these models is\nthat the number of reasoning steps needs to be pre-defined and fixed before\ninference, ignoring the varying complexity of expressions. In this paper, we\npropose a Dynamic Multi-step Reasoning Network, which allows the reasoning\nsteps to be dynamically adjusted based on the reasoning state and expression\ncomplexity. Specifically, we adopt a Transformer module to memorize & process\nthe reasoning state and a Reinforcement Learning strategy to dynamically infer\nthe reasoning steps. The work achieves the state-of-the-art performance or\nsignificant improvements on several REC datasets, ranging from RefCOCO (+, g)\nwith short expressions, to Ref-Reasoning, a dataset with long and complex\ncompositional expressions.",
    "categories": [
      "cs.CV"
    ],
    "published": "2022-07-31T04:51:27+00:00",
    "updated": "2022-10-27T11:30:23+00:00",
    "doi": null,
    "comment": "27 pages, 6 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1312.6204v2": {
    "id": "http://arxiv.org/abs/1312.6204v2",
    "title": "One-Shot Adaptation of Supervised Deep Convolutional Models",
    "authors": [
      "Judy Hoffman",
      "Eric Tzeng",
      "Jeff Donahue",
      "Yangqing Jia",
      "Kate Saenko",
      "Trevor Darrell"
    ],
    "abstract": "Dataset bias remains a significant barrier towards solving real world\ncomputer vision tasks. Though deep convolutional networks have proven to be a\ncompetitive approach for image classification, a question remains: have these\nmodels have solved the dataset bias problem? In general, training or\nfine-tuning a state-of-the-art deep model on a new domain requires a\nsignificant amount of data, which for many applications is simply not\navailable. Transfer of models directly to new domains without adaptation has\nhistorically led to poor recognition performance. In this paper, we pose the\nfollowing question: is a single image dataset, much larger than previously\nexplored for adaptation, comprehensive enough to learn general deep models that\nmay be effectively applied to new image domains? In other words, are deep CNNs\ntrained on large amounts of labeled data as susceptible to dataset bias as\nprevious methods have been shown to be? We show that a generic supervised deep\nCNN model trained on a large dataset reduces, but does not remove, dataset\nbias. Furthermore, we propose several methods for adaptation with deep models\nthat are able to operate with little (one example per category) or no labeled\ndomain specific data. Our experiments show that adaptation of deep models on\nbenchmark visual domain adaptation datasets can provide a significant\nperformance boost.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2013-12-21T04:32:51+00:00",
    "updated": "2014-02-18T02:57:42+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": "ICLR Workshop 2014",
    "primary_category": "cs.CV"
  },
  "1712.03904v2": {
    "id": "http://arxiv.org/abs/1712.03904v2",
    "title": "Feature Mapping for Learning Fast and Accurate 3D Pose Inference from Synthetic Images",
    "authors": [
      "Mahdi Rad",
      "Markus Oberweger",
      "Vincent Lepetit"
    ],
    "abstract": "We propose a simple and efficient method for exploiting synthetic images when\ntraining a Deep Network to predict a 3D pose from an image. The ability of\nusing synthetic images for training a Deep Network is extremely valuable as it\nis easy to create a virtually infinite training set made of such images, while\ncapturing and annotating real images can be very cumbersome. However, synthetic\nimages do not resemble real images exactly, and using them for training can\nresult in suboptimal performance. It was recently shown that for exemplar-based\napproaches, it is possible to learn a mapping from the exemplar representations\nof real images to the exemplar representations of synthetic images. In this\npaper, we show that this approach is more general, and that a network can also\nbe applied after the mapping to infer a 3D pose: At run time, given a real\nimage of the target object, we first compute the features for the image, map\nthem to the feature space of synthetic images, and finally use the resulting\nfeatures as input to another network which predicts the 3D pose. Since this\nnetwork can be trained very effectively by using synthetic images, it performs\nvery well in practice, and inference is faster and more accurate than with an\nexemplar-based approach. We demonstrate our approach on the LINEMOD dataset for\n3D object pose estimation from color images, and the NYU dataset for 3D hand\npose estimation from depth maps. We show that it allows us to outperform the\nstate-of-the-art on both datasets.",
    "categories": [
      "cs.CV"
    ],
    "published": "2017-12-11T17:27:49+00:00",
    "updated": "2018-03-26T15:56:40+00:00",
    "doi": null,
    "comment": "CVPR 2018",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2203.06345v1": {
    "id": "http://arxiv.org/abs/2203.06345v1",
    "title": "The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy",
    "authors": [
      "Tianlong Chen",
      "Zhenyu Zhang",
      "Yu Cheng",
      "Ahmed Awadallah",
      "Zhangyang Wang"
    ],
    "abstract": "Vision transformers (ViTs) have gained increasing popularity as they are\ncommonly believed to own higher modeling capacity and representation\nflexibility, than traditional convolutional networks. However, it is\nquestionable whether such potential has been fully unleashed in practice, as\nthe learned ViTs often suffer from over-smoothening, yielding likely redundant\nmodels. Recent works made preliminary attempts to identify and alleviate such\nredundancy, e.g., via regularizing embedding similarity or re-injecting\nconvolution-like structures. However, a \"head-to-toe assessment\" regarding the\nextent of redundancy in ViTs, and how much we could gain by thoroughly\nmitigating such, has been absent for this field. This paper, for the first\ntime, systematically studies the ubiquitous existence of redundancy at all\nthree levels: patch embedding, attention map, and weight space. In view of\nthem, we advocate a principle of diversity for training ViTs, by presenting\ncorresponding regularizers that encourage the representation diversity and\ncoverage at each of those levels, that enabling capturing more discriminative\ninformation. Extensive experiments on ImageNet with a number of ViT backbones\nvalidate the effectiveness of our proposals, largely eliminating the observed\nViT redundancy and significantly boosting the model generalization. For\nexample, our diversified DeiT obtains 0.70%~1.76% accuracy boosts on ImageNet\nwith highly reduced similarity. Our codes are fully available in\nhttps://github.com/VITA-Group/Diverse-ViT.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2022-03-12T04:48:12+00:00",
    "updated": "2022-03-12T04:48:12+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2003.12687v2": {
    "id": "http://arxiv.org/abs/2003.12687v2",
    "title": "Serialized Output Training for End-to-End Overlapped Speech Recognition",
    "authors": [
      "Naoyuki Kanda",
      "Yashesh Gaur",
      "Xiaofei Wang",
      "Zhong Meng",
      "Takuya Yoshioka"
    ],
    "abstract": "This paper proposes serialized output training (SOT), a novel framework for\nmulti-speaker overlapped speech recognition based on an attention-based\nencoder-decoder approach. Instead of having multiple output layers as with the\npermutation invariant training (PIT), SOT uses a model with only one output\nlayer that generates the transcriptions of multiple speakers one after another.\nThe attention and decoder modules take care of producing multiple\ntranscriptions from overlapped speech. SOT has two advantages over PIT: (1) no\nlimitation in the maximum number of speakers, and (2) an ability to model the\ndependencies among outputs for different speakers. We also propose a simple\ntrick that allows SOT to be executed in $O(S)$, where $S$ is the number of the\nspeakers in the training sample, by using the start times of the constituent\nsource utterances. Experimental results on LibriSpeech corpus show that the SOT\nmodels can transcribe overlapped speech with variable numbers of speakers\nsignificantly better than PIT-based models. We also show that the SOT models\ncan accurately count the number of speakers in the input audio.",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2020-03-28T02:37:09+00:00",
    "updated": "2020-08-08T20:08:37+00:00",
    "doi": null,
    "comment": "Accepted to INTERSPEECH 2020",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2303.17003v1": {
    "id": "http://arxiv.org/abs/2303.17003v1",
    "title": "Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams",
    "authors": [
      "Desnes Nunes",
      "Ricardo Primi",
      "Ramon Pires",
      "Roberto Lotufo",
      "Rodrigo Nogueira"
    ],
    "abstract": "The present study aims to explore the capabilities of Language Models (LMs)\nin tackling high-stakes multiple-choice tests, represented here by the Exame\nNacional do Ensino M\\'edio (ENEM), a multidisciplinary entrance examination\nwidely adopted by Brazilian universities. This exam poses challenging tasks for\nLMs, since its questions may span into multiple fields of knowledge, requiring\nunderstanding of information from diverse domains. For instance, a question may\nrequire comprehension of both statistics and biology to be solved. This work\nanalyzed responses generated by GPT-3.5 and GPT-4 models for questions\npresented in the 2009-2017 exams, as well as for questions of the 2022 exam,\nwhich were made public after the training of the models was completed.\nFurthermore, different prompt strategies were tested, including the use of\nChain-of-Thought (CoT) prompts to generate explanations for answers. On the\n2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy\nof 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on\nexperiments are available at https://github.com/piresramon/gpt-4-enem.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-03-29T20:10:13+00:00",
    "updated": "2023-03-29T20:10:13+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2107.11817v3": {
    "id": "http://arxiv.org/abs/2107.11817v3",
    "title": "Go Wider Instead of Deeper",
    "authors": [
      "Fuzhao Xue",
      "Ziji Shi",
      "Futao Wei",
      "Yuxuan Lou",
      "Yong Liu",
      "Yang You"
    ],
    "abstract": "More transformer blocks with residual connections have recently achieved\nimpressive results on various tasks. To achieve better performance with fewer\ntrainable parameters, recent methods are proposed to go shallower by parameter\nsharing or model compressing along with the depth. However, weak modeling\ncapacity limits their performance. Contrastively, going wider by inducing more\ntrainable matrixes and parameters would produce a huge model requiring advanced\nparallelism to train and inference.\n  In this paper, we propose a parameter-efficient framework, going wider\ninstead of deeper. Specially, following existing works, we adapt parameter\nsharing to compress along depth. But, such deployment would limit the\nperformance. To maximize modeling capacity, we scale along model width by\nreplacing feed-forward network (FFN) with mixture-of-experts (MoE). Across\ntransformer blocks, instead of sharing normalization layers, we propose to use\nindividual layernorms to transform various semantic representations in a more\nparameter-efficient way. To evaluate our plug-and-run framework, we design\nWideNet and conduct comprehensive experiments on popular computer vision and\nnatural language processing benchmarks. On ImageNet-1K, our best model\noutperforms Vision Transformer (ViT) by $1.5\\%$ with $0.72 \\times$ trainable\nparameters. Using $0.46 \\times$ and $0.13 \\times$ parameters, our WideNet can\nstill surpass ViT and ViT-MoE by $0.8\\%$ and $2.1\\%$, respectively. On four\nnatural language processing datasets, WideNet outperforms ALBERT by $1.8\\%$ on\naverage and surpass BERT using factorized embedding parameterization by $0.8\\%$\nwith fewer parameters.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2021-07-25T14:44:24+00:00",
    "updated": "2021-09-07T11:58:00+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2403.12596v1": {
    "id": "http://arxiv.org/abs/2403.12596v1",
    "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs",
    "authors": [
      "Victor Carbune",
      "Hassan Mansoor",
      "Fangyu Liu",
      "Rahul Aralikatte",
      "Gilles Baechler",
      "Jindong Chen",
      "Abhanshu Sharma"
    ],
    "abstract": "Vision-language models (VLMs) are achieving increasingly strong performance\non multimodal tasks. However, reasoning capabilities remain limited\nparticularly for smaller VLMs, while those of large-language models (LLMs) have\nseen numerous improvements. We propose a technique to transfer capabilities\nfrom LLMs to VLMs. On the recently introduced ChartQA, our method obtains\nstate-of-the-art performance when applied on the PaLI3-5B VLM by\n\\citet{chen2023pali3}, while also enabling much better performance on PlotQA\nand FigureQA.\n  We first improve the chart representation by continuing the pre-training\nstage using an improved version of the chart-to-table translation task by\n\\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than\nthe original training set. To improve general reasoning capabilities and\nimprove numerical operations, we synthesize reasoning traces using the table\nrepresentation of charts. Lastly, our model is fine-tuned using the multitask\nloss introduced by \\citet{hsieh2023distilling}.\n  Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B\nwithout using an upstream OCR system, while keeping inference time constant\ncompared to the PaLI3-5B baseline. When rationales are further refined with a\nsimple program-of-thought prompt \\cite{chen2023program}, our model outperforms\nthe recently introduced Gemini Ultra and GPT-4V.",
    "categories": [
      "cs.CL"
    ],
    "published": "2024-03-19T10:03:07+00:00",
    "updated": "2024-03-19T10:03:07+00:00",
    "doi": null,
    "comment": "Findings of NAACL 2024",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1812.02900v3": {
    "id": "http://arxiv.org/abs/1812.02900v3",
    "title": "Off-Policy Deep Reinforcement Learning without Exploration",
    "authors": [
      "Scott Fujimoto",
      "David Meger",
      "Doina Precup"
    ],
    "abstract": "Many practical applications of reinforcement learning constrain agents to\nlearn from a fixed batch of data which has already been gathered, without\noffering further possibility for data collection. In this paper, we demonstrate\nthat due to errors introduced by extrapolation, standard off-policy deep\nreinforcement learning algorithms, such as DQN and DDPG, are incapable of\nlearning with data uncorrelated to the distribution under the current policy,\nmaking them ineffective for this fixed batch setting. We introduce a novel\nclass of off-policy algorithms, batch-constrained reinforcement learning, which\nrestricts the action space in order to force the agent towards behaving close\nto on-policy with respect to a subset of the given data. We present the first\ncontinuous control deep reinforcement learning algorithm which can learn\neffectively from arbitrary, fixed batch data, and empirically demonstrate the\nquality of its behavior in several tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2018-12-07T04:03:25+00:00",
    "updated": "2019-08-10T03:36:31+00:00",
    "doi": null,
    "comment": "ICML 2019",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2307.07487v1": {
    "id": "http://arxiv.org/abs/2307.07487v1",
    "title": "DreamTeacher: Pretraining Image Backbones with Deep Generative Models",
    "authors": [
      "Daiqing Li",
      "Huan Ling",
      "Amlan Kar",
      "David Acuna",
      "Seung Wook Kim",
      "Karsten Kreis",
      "Antonio Torralba",
      "Sanja Fidler"
    ],
    "abstract": "In this work, we introduce a self-supervised feature representation learning\nframework DreamTeacher that utilizes generative networks for pre-training\ndownstream image backbones. We propose to distill knowledge from a trained\ngenerative model into standard image backbones that have been well engineered\nfor specific perception tasks. We investigate two types of knowledge\ndistillation: 1) distilling learned generative features onto target image\nbackbones as an alternative to pretraining these backbones on large labeled\ndatasets such as ImageNet, and 2) distilling labels obtained from generative\nnetworks with task heads onto logits of target backbones. We perform extensive\nanalyses on multiple generative models, dense prediction benchmarks, and\nseveral pre-training regimes. We empirically find that our DreamTeacher\nsignificantly outperforms existing self-supervised representation learning\napproaches across the board. Unsupervised ImageNet pre-training with\nDreamTeacher leads to significant improvements over ImageNet classification\npre-training on downstream datasets, showcasing generative models, and\ndiffusion generative models specifically, as a promising approach to\nrepresentation learning on large, diverse datasets without requiring manual\nannotation.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-07-14T17:17:17+00:00",
    "updated": "2023-07-14T17:17:17+00:00",
    "doi": null,
    "comment": "Project page:\n  https://research.nvidia.com/labs/toronto-ai/DreamTeacher/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2404.10625v2": {
    "id": "http://arxiv.org/abs/2404.10625v2",
    "title": "Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks",
    "authors": [
      "Florian Barthel",
      "Arian Beckmann",
      "Wieland Morgenstern",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or\nGIRAFFE have shown very high rendering quality under large representational\nvariety. However, rendering with Neural Radiance Fields poses challenges for 3D\napplications: First, the significant computational demands of NeRF rendering\npreclude its use on low-power devices, such as mobiles and VR/AR headsets.\nSecond, implicit representations based on neural networks are difficult to\nincorporate into explicit 3D scenes, such as VR environments or video games. 3D\nGaussian Splatting (3DGS) overcomes these limitations by providing an explicit\n3D representation that can be rendered efficiently at high frame rates. In this\nwork, we present a novel approach that combines the high rendering quality of\nNeRF-based 3D-aware GANs with the flexibility and computational advantages of\n3DGS. By training a decoder that maps implicit NeRF representations to explicit\n3D Gaussian Splatting attributes, we can integrate the representational\ndiversity and quality of 3D GANs into the ecosystem of 3D Gaussian Splatting\nfor the first time. Additionally, our approach allows for a high resolution GAN\ninversion and real-time GAN editing with 3D Gaussian Splatting scenes. Project\npage: florian-barthel.github.io/gaussian_decoder",
    "categories": [
      "cs.CV"
    ],
    "published": "2024-04-16T14:48:40+00:00",
    "updated": "2024-06-17T18:19:07+00:00",
    "doi": null,
    "comment": "CVPRW",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2112.12130v2": {
    "id": "http://arxiv.org/abs/2112.12130v2",
    "title": "NICE-SLAM: Neural Implicit Scalable Encoding for SLAM",
    "authors": [
      "Zihan Zhu",
      "Songyou Peng",
      "Viktor Larsson",
      "Weiwei Xu",
      "Hujun Bao",
      "Zhaopeng Cui",
      "Martin R. Oswald",
      "Marc Pollefeys"
    ],
    "abstract": "Neural implicit representations have recently shown encouraging results in\nvarious domains, including promising progress in simultaneous localization and\nmapping (SLAM). Nevertheless, existing methods produce over-smoothed scene\nreconstructions and have difficulty scaling up to large scenes. These\nlimitations are mainly due to their simple fully-connected network architecture\nthat does not incorporate local information in the observations. In this paper,\nwe present NICE-SLAM, a dense SLAM system that incorporates multi-level local\ninformation by introducing a hierarchical scene representation. Optimizing this\nrepresentation with pre-trained geometric priors enables detailed\nreconstruction on large indoor scenes. Compared to recent neural implicit SLAM\nsystems, our approach is more scalable, efficient, and robust. Experiments on\nfive challenging datasets demonstrate competitive results of NICE-SLAM in both\nmapping and tracking quality. Project page:\nhttps://pengsongyou.github.io/nice-slam",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-12-22T18:45:44+00:00",
    "updated": "2022-04-21T17:33:59+00:00",
    "doi": null,
    "comment": "CVPR 2022, first two authors contributed equally. Project page:\n  https://pengsongyou.github.io/nice-slam",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2207.05688v1": {
    "id": "http://arxiv.org/abs/2207.05688v1",
    "title": "ReLyMe: Improving Lyric-to-Melody Generation by Incorporating Lyric-Melody Relationships",
    "authors": [
      "Chen Zhang",
      "Luchin Chang",
      "Songruoyao Wu",
      "Xu Tan",
      "Tao Qin",
      "Tie-Yan Liu",
      "Kejun Zhang"
    ],
    "abstract": "Lyric-to-melody generation, which generates melody according to given lyrics,\nis one of the most important automatic music composition tasks. With the rapid\ndevelopment of deep learning, previous works address this task with end-to-end\nneural network models. However, deep learning models cannot well capture the\nstrict but subtle relationships between lyrics and melodies, which compromises\nthe harmony between lyrics and generated melodies. In this paper, we propose\nReLyMe, a method that incorporates Relationships between Lyrics and Melodies\nfrom music theory to ensure the harmony between lyrics and melodies.\nSpecifically, we first introduce several principles that lyrics and melodies\nshould follow in terms of tone, rhythm, and structure relationships. These\nprinciples are then integrated into neural network lyric-to-melody models by\nadding corresponding constraints during the decoding process to improve the\nharmony between lyrics and melodies. We use a series of objective and\nsubjective metrics to evaluate the generated melodies. Experiments on both\nEnglish and Chinese song datasets show the effectiveness of ReLyMe,\ndemonstrating the superiority of incorporating lyric-melody relationships from\nthe music domain into neural lyric-to-melody generation.",
    "categories": [
      "cs.SD",
      "cs.MM",
      "eess.AS"
    ],
    "published": "2022-07-12T17:09:44+00:00",
    "updated": "2022-07-12T17:09:44+00:00",
    "doi": "10.1145/3503161.3548357",
    "comment": "Accepted by ACMMM 2022, oral",
    "journal_ref": null,
    "primary_category": "cs.SD"
  },
  "2207.10049v1": {
    "id": "http://arxiv.org/abs/2207.10049v1",
    "title": "Pretraining a Neural Network before Knowing Its Architecture",
    "authors": [
      "Boris Knyazev"
    ],
    "abstract": "Training large neural networks is possible by training a smaller hypernetwork\nthat predicts parameters for the large ones. A recently released Graph\nHyperNetwork (GHN) trained this way on one million smaller ImageNet\narchitectures is able to predict parameters for large unseen networks such as\nResNet-50. While networks with predicted parameters lose performance on the\nsource task, the predicted parameters have been found useful for fine-tuning on\nother tasks. We study if fine-tuning based on the same GHN is still useful on\nnovel strong architectures that were published after the GHN had been trained.\nWe found that for recent architectures such as ConvNeXt, GHN initialization\nbecomes less useful than for ResNet-50. One potential reason is the increased\ndistribution shift of novel architectures from those used to train the GHN. We\nalso found that the predicted parameters lack the diversity necessary to\nsuccessfully fine-tune parameters with gradient descent. We alleviate this\nlimitation by applying simple post-processing techniques to predicted\nparameters before fine-tuning them on a target task and improve fine-tuning of\nResNet-50 and ConvNeXt.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2022-07-20T17:27:50+00:00",
    "updated": "2022-07-20T17:27:50+00:00",
    "doi": null,
    "comment": "Accepted at ICML 2022 Workshop on Pre-training: Perspectives,\n  Pitfalls, and Paths Forward, source code is available at\n  https://github.com/facebookresearch/ppuda",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2406.11683v1": {
    "id": "http://arxiv.org/abs/2406.11683v1",
    "title": "HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing",
    "authors": [
      "Jing Chen",
      "Xinyu Zhu",
      "Cheng Yang",
      "Chufan Shi",
      "Yadong Xi",
      "Yuxiang Zhang",
      "Junjie Wang",
      "Jiashu Pu",
      "Rongsheng Zhang",
      "Yujiu Yang",
      "Tian Feng"
    ],
    "abstract": "Generative AI has demonstrated unprecedented creativity in the field of\ncomputer vision, yet such phenomena have not been observed in natural language\nprocessing. In particular, large language models (LLMs) can hardly produce\nwritten works at the level of human experts due to the extremely high\ncomplexity of literature writing. In this paper, we present HoLLMwood, an\nautomated framework for unleashing the creativity of LLMs and exploring their\npotential in screenwriting, which is a highly demanding task. Mimicking the\nhuman creative process, we assign LLMs to different roles involved in the\nreal-world scenario. In addition to the common practice of treating LLMs as\n${Writer}$, we also apply LLMs as ${Editor}$, who is responsible for providing\nfeedback and revision advice to ${Writer}$. Besides, to enrich the characters\nand deepen the plots, we introduce a role-playing mechanism and adopt LLMs as\n${Actors}$ that can communicate and interact with each other. Evaluations on\nautomatically generated screenplays show that HoLLMwood substantially\noutperforms strong baselines in terms of coherence, relevance, interestingness\nand overall quality.",
    "categories": [
      "cs.CL"
    ],
    "published": "2024-06-17T16:01:33+00:00",
    "updated": "2024-06-17T16:01:33+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2402.11187v1": {
    "id": "http://arxiv.org/abs/2402.11187v1",
    "title": "LaCo: Large Language Model Pruning via Layer Collapse",
    "authors": [
      "Yifei Yang",
      "Zouying Cao",
      "Hai Zhao"
    ],
    "abstract": "Large language models (LLMs) based on transformer are witnessing a notable\ntrend of size expansion, which brings considerable costs to both model training\nand inference. However, existing methods such as model quantization, knowledge\ndistillation, and model pruning are constrained by various issues, including\nhardware support limitations, the need for extensive training, and alterations\nto the internal structure of the model. In this paper, we propose a concise\nlayer-wise pruning method called \\textit{Layer Collapse (LaCo)}, in which rear\nmodel layers collapse into a prior layer, enabling a rapid reduction in model\nsize while preserving the model structure. Comprehensive experiments show that\nour method maintains an average task performance of over 80\\% at pruning ratios\nof 25-30\\%, significantly outperforming existing state-of-the-art structured\npruning methods. We also conduct post-training experiments to confirm that the\nproposed pruning method effectively inherits the parameters of the original\nmodel. Finally, we discuss our motivation from the perspective of layer-wise\nsimilarity and evaluate the performance of the pruned LLMs across various\npruning ratios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-17T04:16:30+00:00",
    "updated": "2024-02-17T04:16:30+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "2303.06555v2": {
    "id": "http://arxiv.org/abs/2303.06555v2",
    "title": "One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale",
    "authors": [
      "Fan Bao",
      "Shen Nie",
      "Kaiwen Xue",
      "Chongxuan Li",
      "Shi Pu",
      "Yaole Wang",
      "Gang Yue",
      "Yue Cao",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract": "This paper proposes a unified diffusion framework (dubbed UniDiffuser) to fit\nall distributions relevant to a set of multi-modal data in one model. Our key\ninsight is -- learning diffusion models for marginal, conditional, and joint\ndistributions can be unified as predicting the noise in the perturbed data,\nwhere the perturbation levels (i.e. timesteps) can be different for different\nmodalities. Inspired by the unified view, UniDiffuser learns all distributions\nsimultaneously with a minimal modification to the original diffusion model --\nperturbs data in all modalities instead of a single modality, inputs individual\ntimesteps in different modalities, and predicts the noise of all modalities\ninstead of a single modality. UniDiffuser is parameterized by a transformer for\ndiffusion models to handle input types of different modalities. Implemented on\nlarge-scale paired image-text data, UniDiffuser is able to perform image, text,\ntext-to-image, image-to-text, and image-text pair generation by setting proper\ntimesteps without additional overhead. In particular, UniDiffuser is able to\nproduce perceptually realistic samples in all tasks and its quantitative\nresults (e.g., the FID and CLIP score) are not only superior to existing\ngeneral-purpose models but also comparable to the bespoken models (e.g., Stable\nDiffusion and DALL-E 2) in representative tasks (e.g., text-to-image\ngeneration).",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2023-03-12T03:38:39+00:00",
    "updated": "2023-05-30T17:42:56+00:00",
    "doi": null,
    "comment": "Accepted to ICML2023",
    "journal_ref": null,
    "primary_category": "cs.LG"
  },
  "2403.13802v2": {
    "id": "http://arxiv.org/abs/2403.13802v2",
    "title": "ZigMa: A DiT-style Zigzag Mamba Diffusion Model",
    "authors": [
      "Vincent Tao Hu",
      "Stefan Andreas Baumann",
      "Ming Gui",
      "Olga Grebenkova",
      "Pingchuan Ma",
      "Johannes Fischer",
      "Bj\u00f6rn Ommer"
    ],
    "abstract": "The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$ . Code will be released at https://taohu.me/zigma/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-03-20T17:59:14+00:00",
    "updated": "2024-04-01T17:58:02+00:00",
    "doi": null,
    "comment": "Project Page: https://taohu.me/zigma/",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "1603.06549v4": {
    "id": "http://arxiv.org/abs/1603.06549v4",
    "title": "Consistently faster and smaller compressed bitmaps with Roaring",
    "authors": [
      "Daniel Lemire",
      "Gregory Ssi-Yan-Kai",
      "Owen Kaser"
    ],
    "abstract": "Compressed bitmap indexes are used in databases and search engines. Many\nbitmap compression techniques have been proposed, almost all relying primarily\non run-length encoding (RLE). However, on unsorted data, we can get superior\nperformance with a hybrid compression technique that uses both uncompressed\nbitmaps and packed arrays inside a two-level tree. An instance of this\ntechnique, Roaring, has recently been proposed. Due to its good performance, it\nhas been adopted by several production platforms (e.g., Apache Lucene, Apache\nSpark, Apache Kylin and Druid).\n  Yet there are cases where run-length encoded bitmaps are smaller than the\noriginal Roaring bitmaps---typically when the data is sorted so that the\nbitmaps contain long compressible runs. To better handle these cases, we build\na new Roaring hybrid that combines uncompressed bitmaps, packed arrays and RLE\ncompressed segments. The result is a new Roaring format that compresses better.\n  Overall, our new implementation of Roaring can be several times faster (up to\ntwo orders of magnitude) than the implementations of traditional RLE-based\nalternatives (WAH, Concise, EWAH) while compressing better. We review the\ndesign choices and optimizations that make these good results possible.",
    "categories": [
      "cs.DB"
    ],
    "published": "2016-03-21T19:30:53+00:00",
    "updated": "2018-03-02T18:35:46+00:00",
    "doi": "10.1002/spe.2402",
    "comment": null,
    "journal_ref": "Software: Practice and Experience Volume 46, Issue 11, pages\n  1547-1569, November 2016",
    "primary_category": "cs.DB"
  },
  "2112.12494v2": {
    "id": "http://arxiv.org/abs/2112.12494v2",
    "title": "LaTr: Layout-Aware Transformer for Scene-Text VQA",
    "authors": [
      "Ali Furkan Biten",
      "Ron Litman",
      "Yusheng Xie",
      "Srikar Appalaraju",
      "R. Manmatha"
    ],
    "abstract": "We propose a novel multimodal architecture for Scene Text Visual Question\nAnswering (STVQA), named Layout-Aware Transformer (LaTr). The task of STVQA\nrequires models to reason over different modalities. Thus, we first investigate\nthe impact of each modality, and reveal the importance of the language module,\nespecially when enriched with layout information. Accounting for this, we\npropose a single objective pre-training scheme that requires only text and\nspatial cues. We show that applying this pre-training scheme on scanned\ndocuments has certain advantages over using natural images, despite the domain\ngap. Scanned documents are easy to procure, text-dense and have a variety of\nlayouts, helping the model learn various spatial cues (e.g. left-of, below\netc.) by tying together language and layout information. Compared to existing\napproaches, our method performs vocabulary-free decoding and, as shown,\ngeneralizes well beyond the training vocabulary. We further demonstrate that\nLaTr improves robustness towards OCR errors, a common reason for failure cases\nin STVQA. In addition, by leveraging a vision transformer, we eliminate the\nneed for an external object detector. LaTr outperforms state-of-the-art STVQA\nmethods on multiple datasets. In particular, +7.6% on TextVQA, +10.8% on ST-VQA\nand +4.0% on OCR-VQA (all absolute accuracy numbers).",
    "categories": [
      "cs.CV"
    ],
    "published": "2021-12-23T12:41:26+00:00",
    "updated": "2021-12-24T11:06:59+00:00",
    "doi": null,
    "comment": null,
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2302.04850v2": {
    "id": "http://arxiv.org/abs/2302.04850v2",
    "title": "Robot Synesthesia: A Sound and Emotion Guided AI Painter",
    "authors": [
      "Vihaan Misra",
      "Peter Schaldenbrand",
      "Jean Oh"
    ],
    "abstract": "If a picture paints a thousand words, sound may voice a million. While recent\nrobotic painting and image synthesis methods have achieved progress in\ngenerating visuals from text inputs, the translation of sound into images is\nvastly unexplored. Generally, sound-based interfaces and sonic interactions\nhave the potential to expand accessibility and control for the user and provide\na means to convey complex emotions and the dynamic aspects of the real world.\nIn this paper, we propose an approach for using sound and speech to guide a\nrobotic painting process, known here as robot synesthesia. For general sound,\nwe encode the simulated paintings and input sounds into the same latent space.\nFor speech, we decouple speech into its transcribed text and the tone of the\nspeech. Whereas we use the text to control the content, we estimate the\nemotions from the tone to guide the mood of the painting. Our approach has been\nfully integrated with FRIDA, a robotic painting framework, adding sound and\nspeech to FRIDA's existing input modalities, such as text and style. In two\nsurveys, participants were able to correctly guess the emotion or natural sound\nused to generate a given painting more than twice as likely as random chance.\nOn our sound-guided image manipulation and music-guided paintings, we discuss\nthe results qualitatively.",
    "categories": [
      "cs.CV"
    ],
    "published": "2023-02-09T18:53:44+00:00",
    "updated": "2024-05-23T21:33:49+00:00",
    "doi": null,
    "comment": "9 pages, 10 figures",
    "journal_ref": null,
    "primary_category": "cs.CV"
  },
  "2112.07868v2": {
    "id": "http://arxiv.org/abs/2112.07868v2",
    "title": "Few-shot Instruction Prompts for Pretrained Language Models to Detect Social Biases",
    "authors": [
      "Shrimai Prabhumoye",
      "Rafal Kocielnik",
      "Mohammad Shoeybi",
      "Anima Anandkumar",
      "Bryan Catanzaro"
    ],
    "abstract": "Detecting social bias in text is challenging due to nuance, subjectivity, and\ndifficulty in obtaining good quality labeled datasets at scale, especially\ngiven the evolving nature of social biases and society. To address these\nchallenges, we propose a few-shot instruction-based method for prompting\npre-trained language models (LMs). We select a few class-balanced exemplars\nfrom a small support repository that are closest to the query to be labeled in\nthe embedding space. We then provide the LM with instruction that consists of\nthis subset of labeled exemplars, the query text to be classified, a definition\nof bias, and prompt it to make a decision. We demonstrate that large LMs used\nin a few-shot context can detect different types of fine-grained biases with\nsimilar and sometimes superior accuracy to fine-tuned models. We observe that\nthe largest 530B parameter model is significantly more effective in detecting\nsocial bias compared to smaller models (achieving at least 13% improvement in\nAUC metric compared to other models). It also maintains a high AUC (dropping\nless than 2%) when the labeled repository is reduced to as few as $100$\nsamples. Large pretrained language models thus make it easier and quicker to\nbuild new bias detectors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2021-12-15T04:19:52+00:00",
    "updated": "2022-04-15T23:31:53+00:00",
    "doi": null,
    "comment": "Submission revised with new results",
    "journal_ref": null,
    "primary_category": "cs.CL"
  },
  "1807.00734v3": {
    "id": "http://arxiv.org/abs/1807.00734v3",
    "title": "The relativistic discriminator: a key element missing from standard GAN",
    "authors": [
      "Alexia Jolicoeur-Martineau"
    ],
    "abstract": "In standard generative adversarial network (SGAN), the discriminator\nestimates the probability that the input data is real. The generator is trained\nto increase the probability that fake data is real. We argue that it should\nalso simultaneously decrease the probability that real data is real because 1)\nthis would account for a priori knowledge that half of the data in the\nmini-batch is fake, 2) this would be observed with divergence minimization, and\n3) in optimal settings, SGAN would be equivalent to integral probability metric\n(IPM) GANs.\n  We show that this property can be induced by using a relativistic\ndiscriminator which estimate the probability that the given real data is more\nrealistic than a randomly sampled fake data. We also present a variant in which\nthe discriminator estimate the probability that the given real data is more\nrealistic than fake data, on average. We generalize both approaches to\nnon-standard GAN loss functions and we refer to them respectively as\nRelativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that\nIPM-based GANs are a subset of RGANs which use the identity function.\n  Empirically, we observe that 1) RGANs and RaGANs are significantly more\nstable and generate higher quality data samples than their non-relativistic\ncounterparts, 2) Standard RaGAN with gradient penalty generate data of better\nquality than WGAN-GP while only requiring a single discriminator update per\ngenerator update (reducing the time taken for reaching the state-of-the-art by\n400%), and 3) RaGANs are able to generate plausible high resolutions images\n(256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these\nimages are of significantly better quality than the ones generated by WGAN-GP\nand SGAN with spectral normalization.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "stat.ML"
    ],
    "published": "2018-07-02T15:11:23+00:00",
    "updated": "2018-09-10T17:11:59+00:00",
    "doi": null,
    "comment": "https://github.com/AlexiaJM/RelativisticGAN",
    "journal_ref": null,
    "primary_category": "cs.LG"
  }
}