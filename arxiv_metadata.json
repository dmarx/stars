{
  "2303.11436": {
    "id": "http://arxiv.org/abs/2303.11436v2",
    "title": "Mind meets machine: Unravelling GPT-4's cognitive psychology",
    "authors": [
      "Sifatkaur Dhingra",
      "Manmeet Singh",
      "Vaisakh SB",
      "Neetiraj Malviya",
      "Sukhpal Singh Gill"
    ],
    "abstract": "  Cognitive psychology delves on understanding perception, attention, memory,\nlanguage, problem-solving, decision-making, and reasoning. Large language\nmodels (LLMs) are emerging as potent tools increasingly capable of performing\nhuman-level tasks. The recent development in the form of GPT-4 and its\ndemonstrated success in tasks complex to humans exam and complex problems has\nled to an increased confidence in the LLMs to become perfect instruments of\nintelligence. Although GPT-4 report has shown performance on some cognitive\npsychology tasks, a comprehensive assessment of GPT-4, via the existing\nwell-established datasets is required. In this study, we focus on the\nevaluation of GPT-4's performance on a set of cognitive psychology datasets\nsuch as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how\nGPT-4 processes and integrates cognitive psychology with contextual\ninformation, providing insight into the underlying cognitive processes that\nenable its ability to generate the responses. We show that GPT-4 exhibits a\nhigh level of accuracy in cognitive psychology tasks relative to the prior\nstate-of-the-art models. Our results strengthen the already available\nassessments and confidence on GPT-4's cognitive psychology abilities. It has\nsignificant potential to revolutionize the field of AI, by enabling machines to\nbridge the gap between human and machine reasoning.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-20T20:28:26Z",
    "updated": "2023-04-12T15:46:20Z",
    "doi": null
  },
  "2310.00874": {
    "id": "http://arxiv.org/abs/2310.00874v1",
    "title": "PC-NeRF: Parent-Child Neural Radiance Fields under Partial Sensor Data\n  Loss in Autonomous Driving Environments",
    "authors": [
      "Xiuzhong Hu",
      "Guangming Xiong",
      "Zheng Zang",
      "Peng Jia",
      "Yuxuan Han",
      "Junyi Ma"
    ],
    "abstract": "  Reconstructing large-scale 3D scenes is essential for autonomous vehicles,\nespecially when partial sensor data is lost. Although the recently developed\nneural radiance fields (NeRF) have shown compelling results in implicit\nrepresentations, the large-scale 3D scene reconstruction using partially lost\nLiDAR point cloud data still needs to be explored. To bridge this gap, we\npropose a novel 3D scene reconstruction framework called parent-child neural\nradiance field (PC-NeRF). The framework comprises two modules, the parent NeRF\nand the child NeRF, to simultaneously optimize scene-level, segment-level, and\npoint-level scene representations. Sensor data can be utilized more efficiently\nby leveraging the segment-level representation capabilities of child NeRFs, and\nan approximate volumetric representation of the scene can be quickly obtained\neven with limited observations. With extensive experiments, our proposed\nPC-NeRF is proven to achieve high-precision 3D reconstruction in large-scale\nscenes. Moreover, PC-NeRF can effectively tackle situations where partial\nsensor data is lost and has high deployment efficiency with limited training\ntime. Our approach implementation and the pre-trained models will be available\nat https://github.com/biter0088/pc-nerf.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-02T03:32:35Z",
    "updated": "2023-10-02T03:32:35Z",
    "doi": null
  },
  "2308.11606": {
    "id": "http://arxiv.org/abs/2308.11606v2",
    "title": "StoryBench: A Multifaceted Benchmark for Continuous Story Visualization",
    "authors": [
      "Emanuele Bugliarello",
      "Hernan Moraldo",
      "Ruben Villegas",
      "Mohammad Babaeizadeh",
      "Mohammad Taghi Saffar",
      "Han Zhang",
      "Dumitru Erhan",
      "Vittorio Ferrari",
      "Pieter-Jan Kindermans",
      "Paul Voigtlaender"
    ],
    "abstract": "  Generating video stories from text prompts is a complex task. In addition to\nhaving high visual quality, videos need to realistically adhere to a sequence\nof text prompts whilst being consistent throughout the frames. Creating a\nbenchmark for video generation requires data annotated over time, which\ncontrasts with the single caption used often in video datasets. To fill this\ngap, we collect comprehensive human annotations on three existing datasets, and\nintroduce StoryBench: a new, challenging multi-task benchmark to reliably\nevaluate forthcoming text-to-video models. Our benchmark includes three video\ngeneration tasks of increasing difficulty: action execution, where the next\naction must be generated starting from a conditioning video; story\ncontinuation, where a sequence of actions must be executed starting from a\nconditioning video; and story generation, where a video must be generated from\nonly text prompts. We evaluate small yet strong text-to-video baselines, and\nshow the benefits of training on story-like data algorithmically generated from\nexisting video captions. Finally, we establish guidelines for human evaluation\nof video stories, and reaffirm the need of better automatic metrics for video\ngeneration. StoryBench aims at encouraging future research efforts in this\nexciting new area.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-22T17:53:55Z",
    "updated": "2023-10-12T17:50:38Z",
    "doi": null
  },
  "2304.02008": {
    "id": "http://arxiv.org/abs/2304.02008v3",
    "title": "GlueStick: Robust Image Matching by Sticking Points and Lines Together",
    "authors": [
      "R\u00e9mi Pautrat",
      "Iago Su\u00e1rez",
      "Yifan Yu",
      "Marc Pollefeys",
      "Viktor Larsson"
    ],
    "abstract": "  Line segments are powerful features complementary to points. They offer\nstructural cues, robust to drastic viewpoint and illumination changes, and can\nbe present even in texture-less areas. However, describing and matching them is\nmore challenging compared to points due to partial occlusions, lack of texture,\nor repetitiveness. This paper introduces a new matching paradigm, where points,\nlines, and their descriptors are unified into a single wireframe structure. We\npropose GlueStick, a deep matching Graph Neural Network (GNN) that takes two\nwireframes from different images and leverages the connectivity information\nbetween nodes to better glue them together. In addition to the increased\nefficiency brought by the joint matching, we also demonstrate a large boost of\nperformance when leveraging the complementary nature of these two features in a\nsingle architecture. We show that our matching strategy outperforms the\nstate-of-the-art approaches independently matching line segments and points for\na wide variety of datasets and tasks. The code is available at\nhttps://github.com/cvg/GlueStick.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-04T17:58:14Z",
    "updated": "2023-10-20T14:27:45Z",
    "doi": null
  },
  "2303.12326": {
    "id": "http://arxiv.org/abs/2303.12326v1",
    "title": "Make Encoder Great Again in 3D GAN Inversion through Geometry and\n  Occlusion-Aware Encoding",
    "authors": [
      "Ziyang Yuan",
      "Yiming Zhu",
      "Yu Li",
      "Hongyu Liu",
      "Chun Yuan"
    ],
    "abstract": "  3D GAN inversion aims to achieve high reconstruction fidelity and reasonable\n3D geometry simultaneously from a single image input. However, existing 3D GAN\ninversion methods rely on time-consuming optimization for each individual case.\nIn this work, we introduce a novel encoder-based inversion framework based on\nEG3D, one of the most widely-used 3D GAN models. We leverage the inherent\nproperties of EG3D's latent space to design a discriminator and a background\ndepth regularization. This enables us to train a geometry-aware encoder capable\nof converting the input image into corresponding latent code. Additionally, we\nexplore the feature space of EG3D and develop an adaptive refinement stage that\nimproves the representation ability of features in EG3D to enhance the recovery\nof fine-grained textural details. Finally, we propose an occlusion-aware fusion\noperation to prevent distortion in unobserved regions. Our method achieves\nimpressive results comparable to optimization-based methods while operating up\nto 500 times faster. Our framework is well-suited for applications such as\nsemantic editing.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "14J60 (Primary) 14F05, 14J26 (Secondary)",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-22T05:51:53Z",
    "updated": "2023-03-22T05:51:53Z",
    "doi": null
  },
  "2104.13562": {
    "id": "http://arxiv.org/abs/2104.13562v2",
    "title": "Neural Ray-Tracing: Learning Surfaces and Reflectance for Relighting and\n  View Synthesis",
    "authors": [
      "Julian Knodt",
      "Joe Bartusek",
      "Seung-Hwan Baek",
      "Felix Heide"
    ],
    "abstract": "  Recent neural rendering methods have demonstrated accurate view interpolation\nby predicting volumetric density and color with a neural network. Although such\nvolumetric representations can be supervised on static and dynamic scenes,\nexisting methods implicitly bake the complete scene light transport into a\nsingle neural network for a given scene, including surface modeling,\nbidirectional scattering distribution functions, and indirect lighting effects.\nIn contrast to traditional rendering pipelines, this prohibits changing surface\nreflectance, illumination, or composing other objects in the scene.\n  In this work, we explicitly model the light transport between scene surfaces\nand we rely on traditional integration schemes and the rendering equation to\nreconstruct a scene. The proposed method allows BSDF recovery with unknown\nlight conditions and classic light transports such as pathtracing. By learning\ndecomposed transport with surface representations established in conventional\nrendering methods, the method naturally facilitates editing shape, reflectance,\nlighting and scene composition. The method outperforms NeRV for relighting\nunder known lighting conditions, and produces realistic reconstructions for\nrelit and edited scenes. We validate the proposed approach for scene editing,\nrelighting and reflectance estimation learned from synthetic and captured views\non a subset of NeRV's datasets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-28T03:47:48Z",
    "updated": "2021-12-04T20:40:20Z",
    "doi": null
  },
  "2305.16223": {
    "id": "http://arxiv.org/abs/2305.16223v2",
    "title": "Prompt-Free Diffusion: Taking \"Text\" out of Text-to-Image Diffusion\n  Models",
    "authors": [
      "Xingqian Xu",
      "Jiayi Guo",
      "Zhangyang Wang",
      "Gao Huang",
      "Irfan Essa",
      "Humphrey Shi"
    ],
    "abstract": "  Text-to-image (T2I) research has grown explosively in the past year, owing to\nthe large-scale pre-trained diffusion models and many emerging personalization\nand editing approaches. Yet, one pain point persists: the text prompt\nengineering, and searching high-quality text prompts for customized results is\nmore art than science. Moreover, as commonly argued: \"an image is worth a\nthousand words\" - the attempt to describe a desired image with texts often ends\nup being ambiguous and cannot comprehensively cover delicate visual details,\nhence necessitating more additional controls from the visual domain. In this\npaper, we take a bold step forward: taking \"Text\" out of a pre-trained T2I\ndiffusion model, to reduce the burdensome prompt engineering efforts for users.\nOur proposed framework, Prompt-Free Diffusion, relies on only visual inputs to\ngenerate new images: it takes a reference image as \"context\", an optional image\nstructural conditioning, and an initial noise, with absolutely no text prompt.\nThe core architecture behind the scene is Semantic Context Encoder (SeeCoder),\nsubstituting the commonly used CLIP-based or LLM-based text encoder. The\nreusability of SeeCoder also makes it a convenient drop-in component: one can\nalso pre-train a SeeCoder in one T2I model and reuse it for another. Through\nextensive experiments, Prompt-Free Diffusion is experimentally found to (i)\noutperform prior exemplar-based image synthesis approaches; (ii) perform on par\nwith state-of-the-art T2I models using prompts following the best practice; and\n(iii) be naturally extensible to other downstream applications such as anime\nfigure generation and virtual try-on, with promising quality. Our code and\nmodels are open-sourced at https://github.com/SHI-Labs/Prompt-Free-Diffusion.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-25T16:30:07Z",
    "updated": "2023-06-01T02:27:42Z",
    "doi": null
  },
  "2305.13516": {
    "id": "http://arxiv.org/abs/2305.13516v1",
    "title": "Scaling Speech Technology to 1,000+ Languages",
    "authors": [
      "Vineel Pratap",
      "Andros Tjandra",
      "Bowen Shi",
      "Paden Tomasello",
      "Arun Babu",
      "Sayani Kundu",
      "Ali Elkahky",
      "Zhaoheng Ni",
      "Apoorv Vyas",
      "Maryam Fazel-Zarandi",
      "Alexei Baevski",
      "Yossi Adi",
      "Xiaohui Zhang",
      "Wei-Ning Hsu",
      "Alexis Conneau",
      "Michael Auli"
    ],
    "abstract": "  Expanding the language coverage of speech technology has the potential to\nimprove access to information for many more people. However, current speech\ntechnology is restricted to about one hundred languages which is a small\nfraction of the over 7,000 languages spoken around the world. The Massively\nMultilingual Speech (MMS) project increases the number of supported languages\nby 10-40x, depending on the task. The main ingredients are a new dataset based\non readings of publicly available religious texts and effectively leveraging\nself-supervised learning. We built pre-trained wav2vec 2.0 models covering\n1,406 languages, a single multilingual automatic speech recognition model for\n1,107 languages, speech synthesis models for the same number of languages, as\nwell as a language identification model for 4,017 languages. Experiments show\nthat our multilingual speech recognition model more than halves the word error\nrate of Whisper on 54 languages of the FLEURS benchmark while being trained on\na small fraction of the labeled data.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-22T22:09:41Z",
    "updated": "2023-05-22T22:09:41Z",
    "doi": null
  },
  "2308.04995": {
    "id": "http://arxiv.org/abs/2308.04995v2",
    "title": "IDiff-Face: Synthetic-based Face Recognition through Fizzy\n  Identity-Conditioned Diffusion Models",
    "authors": [
      "Fadi Boutros",
      "Jonas Henry Grebe",
      "Arjan Kuijper",
      "Naser Damer"
    ],
    "abstract": "  The availability of large-scale authentic face databases has been crucial to\nthe significant advances made in face recognition research over the past\ndecade. However, legal and ethical concerns led to the recent retraction of\nmany of these databases by their creators, raising questions about the\ncontinuity of future face recognition research without one of its key\nresources. Synthetic datasets have emerged as a promising alternative to\nprivacy-sensitive authentic data for face recognition development. However,\nrecent synthetic datasets that are used to train face recognition models suffer\neither from limitations in intra-class diversity or cross-class (identity)\ndiscrimination, leading to less optimal accuracies, far away from the\naccuracies achieved by models trained on authentic data. This paper targets\nthis issue by proposing IDiff-Face, a novel approach based on conditional\nlatent diffusion models for synthetic identity generation with realistic\nidentity variations for face recognition training. Through extensive\nevaluations, our proposed synthetic-based face recognition approach pushed the\nlimits of state-of-the-art performances, achieving, for example, 98.00%\naccuracy on the Labeled Faces in the Wild (LFW) benchmark, far ahead from the\nrecent synthetic-based face recognition solutions with 95.40% and bridging the\ngap to authentic-based face recognition with 99.82% accuracy.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-09T14:48:31Z",
    "updated": "2023-08-10T10:43:53Z",
    "doi": null
  },
  "2310.00106": {
    "id": "http://arxiv.org/abs/2310.00106v2",
    "title": "FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video\n  Synthesis from Static Imagery",
    "authors": [
      "Tasin Islam",
      "Alina Miron",
      "XiaoHui Liu",
      "Yongmin Li"
    ],
    "abstract": "  Our study introduces a new image-to-video generator called FashionFlow to\ngenerate fashion videos. By utilising a diffusion model, we are able to create\nshort videos from still fashion images. Our approach involves developing and\nconnecting relevant components with the diffusion model, which results in the\ncreation of high-fidelity videos that are aligned with the conditional image.\nThe components include the use of pseudo-3D convolutional layers to generate\nvideos efficiently. VAE and CLIP encoders capture vital characteristics from\nstill images to condition the diffusion model at a global level. Our research\ndemonstrates a successful synthesis of fashion videos featuring models posing\nfrom various angles, showcasing the fit and appearance of the garment. Our\nfindings hold great promise for improving and enhancing the shopping experience\nfor the online fashion industry.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-29T19:34:32Z",
    "updated": "2024-01-20T09:57:47Z",
    "doi": null
  },
  "2312.04687": {
    "id": "http://arxiv.org/abs/2312.04687v1",
    "title": "LLM4TDD: Best Practices for Test Driven Development Using Large Language\n  Models",
    "authors": [
      "Sanyogita Piya",
      "Allison Sullivan"
    ],
    "abstract": "  In today's society, we are becoming increasingly dependent on software\nsystems. However, we also constantly witness the negative impacts of buggy\nsoftware. Program synthesis aims to improve software correctness by\nautomatically generating the program given an outline of the expected behavior.\nFor decades, program synthesis has been an active research field, with recent\napproaches looking to incorporate Large Language Models to help generate code.\nThis paper explores the concept of LLM4TDD, where we guide Large Language\nModels to generate code iteratively using a test-driven development\nmethodology. We conduct an empirical evaluation using ChatGPT and coding\nproblems from LeetCode to investigate the impact of different test, prompt and\nproblem attributes on the efficacy of LLM4TDD.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-07T20:37:54Z",
    "updated": "2023-12-07T20:37:54Z",
    "doi": null
  },
  "2403.11415": {
    "id": "http://arxiv.org/abs/2403.11415v2",
    "title": "DreamSampler: Unifying Diffusion Sampling and Score Distillation for\n  Image Manipulation",
    "authors": [
      "Jeongsol Kim",
      "Geon Yeong Park",
      "Jong Chul Ye"
    ],
    "abstract": "  Reverse sampling and score-distillation have emerged as main workhorses in\nrecent years for image manipulation using latent diffusion models (LDMs). While\nreverse diffusion sampling often requires adjustments of LDM architecture or\nfeature engineering, score distillation offers a simple yet powerful\nmodel-agnostic approach, but it is often prone to mode-collapsing. To address\nthese limitations and leverage the strengths of both approaches, here we\nintroduce a novel framework called {\\em DreamSampler}, which seamlessly\nintegrates these two distinct approaches through the lens of regularized latent\noptimization. Similar to score-distillation, DreamSampler is a model-agnostic\napproach applicable to any LDM architecture, but it allows both distillation\nand reverse sampling with additional guidance for image editing and\nreconstruction. Through experiments involving image editing, SVG reconstruction\nand etc, we demonstrate the competitive performance of DreamSampler compared to\nexisting approaches, while providing new applications. Code:\nhttps://github.com/DreamSampler/dream-sampler\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-18T02:08:58Z",
    "updated": "2024-09-23T06:47:08Z",
    "doi": null
  },
  "2210.15947": {
    "id": "http://arxiv.org/abs/2210.15947v2",
    "title": "NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed\n  Neural Radiance Fields",
    "authors": [
      "Liangchen Song",
      "Anpei Chen",
      "Zhong Li",
      "Zhang Chen",
      "Lele Chen",
      "Junsong Yuan",
      "Yi Xu",
      "Andreas Geiger"
    ],
    "abstract": "  Visually exploring in a real-world 4D spatiotemporal space freely in VR has\nbeen a long-term quest. The task is especially appealing when only a few or\neven single RGB cameras are used for capturing the dynamic scene. To this end,\nwe present an efficient framework capable of fast reconstruction, compact\nmodeling, and streamable rendering. First, we propose to decompose the 4D\nspatiotemporal space according to temporal characteristics. Points in the 4D\nspace are associated with probabilities of belonging to three categories:\nstatic, deforming, and new areas. Each area is represented and regularized by a\nseparate neural field. Second, we propose a hybrid representations based\nfeature streaming scheme for efficiently modeling the neural fields. Our\napproach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single\nhand-held cameras and multi-camera arrays, achieving comparable or superior\nrendering performance in terms of quality and speed comparable to recent\nstate-of-the-art methods, achieving reconstruction in 10 seconds per frame and\ninteractive rendering.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-28T07:11:05Z",
    "updated": "2023-02-18T07:00:29Z",
    "doi": null
  },
  "2205.04421": {
    "id": "http://arxiv.org/abs/2205.04421v2",
    "title": "NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level\n  Quality",
    "authors": [
      "Xu Tan",
      "Jiawei Chen",
      "Haohe Liu",
      "Jian Cong",
      "Chen Zhang",
      "Yanqing Liu",
      "Xi Wang",
      "Yichong Leng",
      "Yuanhao Yi",
      "Lei He",
      "Frank Soong",
      "Tao Qin",
      "Sheng Zhao",
      "Tie-Yan Liu"
    ],
    "abstract": "  Text to speech (TTS) has made rapid progress in both academia and industry in\nrecent years. Some questions naturally arise that whether a TTS system can\nachieve human-level quality, how to define/judge that quality and how to\nachieve it. In this paper, we answer these questions by first defining the\nhuman-level quality based on the statistical significance of subjective measure\nand introducing appropriate guidelines to judge it, and then developing a TTS\nsystem called NaturalSpeech that achieves human-level quality on a benchmark\ndataset. Specifically, we leverage a variational autoencoder (VAE) for\nend-to-end text to waveform generation, with several key modules to enhance the\ncapacity of the prior from text and reduce the complexity of the posterior from\nspeech, including phoneme pre-training, differentiable duration modeling,\nbidirectional prior/posterior modeling, and a memory mechanism in VAE.\nExperiment evaluations on popular LJSpeech dataset show that our proposed\nNaturalSpeech achieves -0.01 CMOS (comparative mean opinion score) to human\nrecordings at the sentence level, with Wilcoxon signed rank test at p-level p\n>> 0.05, which demonstrates no statistically significant difference from human\nrecordings for the first time on this dataset.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-09T16:57:35Z",
    "updated": "2022-05-10T15:25:20Z",
    "doi": null
  },
  "2201.13195": {
    "id": "http://arxiv.org/abs/2201.13195v3",
    "title": "Memory-Efficient Backpropagation through Large Linear Layers",
    "authors": [
      "Daniel Bershatsky",
      "Aleksandr Mikhalev",
      "Alexandr Katrutsa",
      "Julia Gusak",
      "Daniil Merkulov",
      "Ivan Oseledets"
    ],
    "abstract": "  In modern neural networks like Transformers, linear layers require\nsignificant memory to store activations during backward pass. This study\nproposes a memory reduction approach to perform backpropagation through linear\nlayers. Since the gradients of linear layers are computed by matrix\nmultiplications, we consider methods for randomized matrix multiplications and\ndemonstrate that they require less memory with a moderate decrease of the test\naccuracy. Also, we investigate the variance of the gradient estimate induced by\nthe randomized matrix multiplication. We compare this variance with the\nvariance coming from gradient estimation based on the batch of samples. We\ndemonstrate the benefits of the proposed method on the fine-tuning of the\npre-trained RoBERTa model on GLUE tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-01-31T13:02:41Z",
    "updated": "2022-02-02T21:24:49Z",
    "doi": null
  },
  "2303.05125": {
    "id": "http://arxiv.org/abs/2303.05125v1",
    "title": "Cones: Concept Neurons in Diffusion Models for Customized Generation",
    "authors": [
      "Zhiheng Liu",
      "Ruili Feng",
      "Kai Zhu",
      "Yifei Zhang",
      "Kecheng Zheng",
      "Yu Liu",
      "Deli Zhao",
      "Jingren Zhou",
      "Yang Cao"
    ],
    "abstract": "  Human brains respond to semantic features of presented stimuli with different\nneurons. It is then curious whether modern deep neural networks admit a similar\nbehavior pattern. Specifically, this paper finds a small cluster of neurons in\na diffusion model corresponding to a particular subject. We call those neurons\nthe concept neurons. They can be identified by statistics of network gradients\nto a stimulation connected with the given subject. The concept neurons\ndemonstrate magnetic properties in interpreting and manipulating generation\nresults. Shutting them can directly yield the related subject contextualized in\ndifferent scenes. Concatenating multiple clusters of concept neurons can\nvividly generate all related concepts in a single image. A few steps of further\nfine-tuning can enhance the multi-concept capability, which may be the first to\nmanage to generate up to four different subjects in a single image. For\nlarge-scale applications, the concept neurons are environmentally friendly as\nwe only need to store a sparse cluster of int index instead of dense float32\nvalues of the parameters, which reduces storage consumption by 90\\% compared\nwith previous subject-driven generation methods. Extensive qualitative and\nquantitative studies on diverse scenarios show the superiority of our method in\ninterpreting and manipulating diffusion models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-09T09:16:04Z",
    "updated": "2023-03-09T09:16:04Z",
    "doi": null
  },
  "2104.07689": {
    "id": "http://arxiv.org/abs/2104.07689v1",
    "title": "Dual Contrastive Learning for Unsupervised Image-to-Image Translation",
    "authors": [
      "Junlin Han",
      "Mehrdad Shoeiby",
      "Lars Petersson",
      "Mohammad Ali Armin"
    ],
    "abstract": "  Unsupervised image-to-image translation tasks aim to find a mapping between a\nsource domain X and a target domain Y from unpaired training data. Contrastive\nlearning for Unpaired image-to-image Translation (CUT) yields state-of-the-art\nresults in modeling unsupervised image-to-image translation by maximizing\nmutual information between input and output patches using only one encoder for\nboth domains. In this paper, we propose a novel method based on contrastive\nlearning and a dual learning setting (exploiting two encoders) to infer an\nefficient mapping between unpaired data. Additionally, while CUT suffers from\nmode collapse, a variant of our method efficiently addresses this issue. We\nfurther demonstrate the advantage of our approach through extensive ablation\nstudies demonstrating superior performance comparing to recent approaches in\nmultiple challenging image translation tasks. Lastly, we demonstrate that the\ngap between unsupervised methods and supervised methods can be efficiently\nclosed.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-15T18:00:22Z",
    "updated": "2021-04-15T18:00:22Z",
    "doi": null
  },
  "1812.01187": {
    "id": "http://arxiv.org/abs/1812.01187v2",
    "title": "Bag of Tricks for Image Classification with Convolutional Neural\n  Networks",
    "authors": [
      "Tong He",
      "Zhi Zhang",
      "Hang Zhang",
      "Zhongyue Zhang",
      "Junyuan Xie",
      "Mu Li"
    ],
    "abstract": "  Much of the recent progress made in image classification research can be\ncredited to training procedure refinements, such as changes in data\naugmentations and optimization methods. In the literature, however, most\nrefinements are either briefly mentioned as implementation details or only\nvisible in source code. In this paper, we will examine a collection of such\nrefinements and empirically evaluate their impact on the final model accuracy\nthrough ablation study. We will show that, by combining these refinements\ntogether, we are able to improve various CNN models significantly. For example,\nwe raise ResNet-50's top-1 validation accuracy from 75.3% to 79.29% on\nImageNet. We will also demonstrate that improvement on image classification\naccuracy leads to better transfer learning performance in other application\ndomains such as object detection and semantic segmentation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-12-04T03:07:35Z",
    "updated": "2018-12-05T22:17:01Z",
    "doi": null
  },
  "2110.10596": {
    "id": "http://arxiv.org/abs/2110.10596v2",
    "title": "Look at What I'm Doing: Self-Supervised Spatial Grounding of Narrations\n  in Instructional Videos",
    "authors": [
      "Reuben Tan",
      "Bryan A. Plummer",
      "Kate Saenko",
      "Hailin Jin",
      "Bryan Russell"
    ],
    "abstract": "  We introduce the task of spatially localizing narrated interactions in\nvideos. Key to our approach is the ability to learn to spatially localize\ninteractions with self-supervision on a large corpus of videos with\naccompanying transcribed narrations. To achieve this goal, we propose a\nmultilayer cross-modal attention network that enables effective optimization of\na contrastive loss during training. We introduce a divided strategy that\nalternates between computing inter- and intra-modal attention across the visual\nand natural language modalities, which allows effective training via directly\ncontrasting the two modalities' representations. We demonstrate the\neffectiveness of our approach by self-training on the HowTo100M instructional\nvideo dataset and evaluating on a newly collected dataset of localized\ndescribed interactions in the YouCook2 dataset. We show that our approach\noutperforms alternative baselines, including shallow co-attention and full\ncross-modal attention. We also apply our approach to grounding phrases in\nimages with weak supervision on Flickr30K and show that stacking multiple\nattention layers is effective and, when combined with a word-to-region loss,\nachieves state of the art on recall-at-one and pointing hand accuracies.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-20T14:45:13Z",
    "updated": "2021-12-02T16:55:56Z",
    "doi": null
  },
  "2307.05950": {
    "id": "http://arxiv.org/abs/2307.05950v2",
    "title": "Exploring the Effectiveness of LLMs in Automated Logging Generation: An\n  Empirical Study",
    "authors": [
      "Yichen Li",
      "Yintong Huo",
      "Zhihan Jiang",
      "Renyi Zhong",
      "Pinjia He",
      "Yuxin Su",
      "Lionel Briand",
      "Michael R. Lyu"
    ],
    "abstract": "  Automated logging statement generation supports developers in documenting\ncritical software runtime behavior. Given the great success in natural language\ngeneration and programming language comprehension, large language models (LLMs)\nmight help developers generate logging statements, but this has not yet been\ninvestigated. To fill the gap, this paper performs the first study on exploring\nLLMs for logging statement generation.We first build a logging statement\ngeneration dataset, LogBench, with two parts: (1) LogBench-O: logging\nstatements collected from GitHub repositories, and (2) LogBench-T: the\ntransformed unseen code from LogBench-O. Then, we leverage LogBench to evaluate\nthe effectiveness and generalization capabilities (using LogBench-T) of eleven\ntop-performing LLMs. In addition, we examine the performance of these LLMs\nagainst classical retrieval-based and machine learning-based logging methods\nfrom the era preceding LLMs. We further evaluate LLM's logging generalization\ncapabilities using unseen data (LogBench-T) derived from code transformation\ntechniques. While existing LLMs deliver decent predictions on logging levels\nand logging variables, our study indicates that they only achieve a maximum\nBLEU score of 0.249, thus calling for improvements. The paper also highlights\nthe importance of prompt constructions and external factors (e.g., programming\ncontexts and code comments) for LLMs' logging performance. Based on these\nfindings, we identify five implications and provide practical advice for future\nlogging research. Our empirical analysis discloses the limitations of current\nlogging approaches while showcasing the potential of LLM-based logging tools,\nand provides actionable guidance for building more practical models.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-12T06:32:51Z",
    "updated": "2024-04-01T12:19:55Z",
    "doi": null
  },
  "2311.16918": {
    "id": "http://arxiv.org/abs/2311.16918v2",
    "title": "RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail\n  Richness in Text-to-3D",
    "authors": [
      "Lingteng Qiu",
      "Guanying Chen",
      "Xiaodong Gu",
      "Qi Zuo",
      "Mutian Xu",
      "Yushuang Wu",
      "Weihao Yuan",
      "Zilong Dong",
      "Liefeng Bo",
      "Xiaoguang Han"
    ],
    "abstract": "  Lifting 2D diffusion for 3D generation is a challenging problem due to the\nlack of geometric prior and the complex entanglement of materials and lighting\nin natural images. Existing methods have shown promise by first creating the\ngeometry through score-distillation sampling (SDS) applied to rendered surface\nnormals, followed by appearance modeling. However, relying on a 2D RGB\ndiffusion model to optimize surface normals is suboptimal due to the\ndistribution discrepancy between natural images and normals maps, leading to\ninstability in optimization. In this paper, recognizing that the normal and\ndepth information effectively describe scene geometry and be automatically\nestimated from images, we propose to learn a generalizable Normal-Depth\ndiffusion model for 3D generation. We achieve this by training on the\nlarge-scale LAION dataset together with the generalizable image-to-depth and\nnormal prior models. In an attempt to alleviate the mixed illumination effects\nin the generated materials, we introduce an albedo diffusion model to impose\ndata-driven constraints on the albedo component. Our experiments show that when\nintegrated into existing text-to-3D pipelines, our models significantly enhance\nthe detail richness, achieving state-of-the-art results. Our project page is\nhttps://aigc3d.github.io/richdreamer/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-28T16:22:33Z",
    "updated": "2023-12-24T16:36:09Z",
    "doi": null
  },
  "1912.00552": {
    "id": "http://arxiv.org/abs/1912.00552v2",
    "title": "Sparse Graph Attention Networks",
    "authors": [
      "Yang Ye",
      "Shihao Ji"
    ],
    "abstract": "  Graph Neural Networks (GNNs) have proved to be an effective representation\nlearning framework for graph-structured data, and have achieved\nstate-of-the-art performance on many practical predictive tasks, such as node\nclassification, link prediction and graph classification. Among the variants of\nGNNs, Graph Attention Networks (GATs) learn to assign dense attention\ncoefficients over all neighbors of a node for feature aggregation, and improve\nthe performance of many graph learning tasks. However, real-world graphs are\noften very large and noisy, and GATs are prone to overfitting if not\nregularized properly. Even worse, the local aggregation mechanism of GATs may\nfail on disassortative graphs, where nodes within local neighborhood provide\nmore noise than useful information for feature aggregation. In this paper, we\npropose Sparse Graph Attention Networks (SGATs) that learn sparse attention\ncoefficients under an $L_0$-norm regularization, and the learned sparse\nattentions are then used for all GNN layers, resulting in an edge-sparsified\ngraph. By doing so, we can identify noisy/task-irrelevant edges, and thus\nperform feature aggregation on most informative neighbors. Extensive\nexperiments on synthetic and real-world graph learning benchmarks demonstrate\nthe superior performance of SGATs. In particular, SGATs can remove about\n50\\%-80\\% edges from large assortative graphs, while retaining similar\nclassification accuracies. On disassortative graphs, SGATs prune majority of\nnoisy edges and outperform GATs in classification accuracies by significant\nmargins. Furthermore, the removed edges can be interpreted intuitively and\nquantitatively. To the best of our knowledge, this is the first graph learning\nalgorithm that shows significant redundancies in graphs and edge-sparsified\ngraphs can achieve similar or sometimes higher predictive performances than\noriginal graphs.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-12-02T02:25:01Z",
    "updated": "2021-04-11T02:54:26Z",
    "doi": null
  },
  "1704.03976": {
    "id": "http://arxiv.org/abs/1704.03976v2",
    "title": "Virtual Adversarial Training: A Regularization Method for Supervised and\n  Semi-Supervised Learning",
    "authors": [
      "Takeru Miyato",
      "Shin-ichi Maeda",
      "Masanori Koyama",
      "Shin Ishii"
    ],
    "abstract": "  We propose a new regularization method based on virtual adversarial loss: a\nnew measure of local smoothness of the conditional label distribution given\ninput. Virtual adversarial loss is defined as the robustness of the conditional\nlabel distribution around each input data point against local perturbation.\nUnlike adversarial training, our method defines the adversarial direction\nwithout label information and is hence applicable to semi-supervised learning.\nBecause the directions in which we smooth the model are only \"virtually\"\nadversarial, we call our method virtual adversarial training (VAT). The\ncomputational cost of VAT is relatively low. For neural networks, the\napproximated gradient of virtual adversarial loss can be computed with no more\nthan two pairs of forward- and back-propagations. In our experiments, we\napplied VAT to supervised and semi-supervised learning tasks on multiple\nbenchmark datasets. With a simple enhancement of the algorithm based on the\nentropy minimization principle, our VAT achieves state-of-the-art performance\nfor semi-supervised learning tasks on SVHN and CIFAR-10.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-04-13T02:45:27Z",
    "updated": "2018-06-27T04:52:47Z",
    "doi": null
  },
  "2012.15370": {
    "id": "http://arxiv.org/abs/2012.15370v1",
    "title": "OSTeC: One-Shot Texture Completion",
    "authors": [
      "Baris Gecer",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ],
    "abstract": "  The last few years have witnessed the great success of non-linear generative\nmodels in synthesizing high-quality photorealistic face images. Many recent 3D\nfacial texture reconstruction and pose manipulation from a single image\napproaches still rely on large and clean face datasets to train image-to-image\nGenerative Adversarial Networks (GANs). Yet the collection of such a large\nscale high-resolution 3D texture dataset is still very costly and difficult to\nmaintain age/ethnicity balance. Moreover, regression-based approaches suffer\nfrom generalization to the in-the-wild conditions and are unable to fine-tune\nto a target-image. In this work, we propose an unsupervised approach for\none-shot 3D facial texture completion that does not require large-scale texture\ndatasets, but rather harnesses the knowledge stored in 2D face generators. The\nproposed approach rotates an input image in 3D and fill-in the unseen regions\nby reconstructing the rotated image in a 2D face generator, based on the\nvisible parts. Finally, we stitch the most visible textures at different angles\nin the UV image-plane. Further, we frontalize the target image by projecting\nthe completed texture into the generator. The qualitative and quantitative\nexperiments demonstrate that the completed UV textures and frontalized images\nare of high quality, resembles the original identity, can be used to train a\ntexture GAN model for 3DMM fitting and improve pose-invariant face recognition.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-12-30T23:53:26Z",
    "updated": "2020-12-30T23:53:26Z",
    "doi": "10.1109/CVPR46437.2021.00754"
  },
  "2104.04670": {
    "id": "http://arxiv.org/abs/2104.04670v5",
    "title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on\n  Dataset and Prompt Collections",
    "authors": [
      "Ruiqi Zhong",
      "Kristy Lee",
      "Zheng Zhang",
      "Dan Klein"
    ],
    "abstract": "  Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-10T02:57:22Z",
    "updated": "2021-09-08T16:29:59Z",
    "doi": null
  },
  "2109.00783": {
    "id": "http://arxiv.org/abs/2109.00783v4",
    "title": "Computer Vision Self-supervised Learning Methods on Time Series",
    "authors": [
      "Daesoo Lee",
      "Erlend Aune"
    ],
    "abstract": "  Self-supervised learning (SSL) has had great success in both computer vision.\nMost of the current mainstream computer vision SSL frameworks are based on\nSiamese network architecture. These approaches often rely on cleverly crafted\nloss functions and training setups to avoid feature collapse. In this study, we\nevaluate if those computer-vision SSL frameworks are also effective on a\ndifferent modality (\\textit{i.e.,} time series). The effectiveness is\nexperimented and evaluated on the UCR and UEA archives, and we show that the\ncomputer vision SSL frameworks can be effective even for time series. In\naddition, we propose a new method that improves on the recently proposed VICReg\nmethod. Our method improves on a \\textit{covariance} term proposed in VICReg,\nand in addition we augment the head of the architecture by an iterative\nnormalization layer that accelerates the convergence of the model.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-02T08:45:53Z",
    "updated": "2024-01-26T22:16:07Z",
    "doi": null
  },
  "2303.06573": {
    "id": "http://arxiv.org/abs/2303.06573v2",
    "title": "Large Language Models Know Your Contextual Search Intent: A Prompting\n  Framework for Conversational Search",
    "authors": [
      "Kelong Mao",
      "Zhicheng Dou",
      "Fengran Mo",
      "Jiewen Hou",
      "Haonan Chen",
      "Hongjin Qian"
    ],
    "abstract": "  Precisely understanding users' contextual search intent has been an important\nchallenge for conversational search. As conversational search sessions are much\nmore diverse and long-tailed, existing methods trained on limited data still\nshow unsatisfactory effectiveness and robustness to handle real conversational\nsearch scenarios. Recently, large language models (LLMs) have demonstrated\namazing capabilities for text generation and conversation understanding. In\nthis work, we present a simple yet effective prompting framework, called\nLLM4CS, to leverage LLMs as a text-based search intent interpreter to help\nconversational search. Under this framework, we explore three prompting methods\nto generate multiple query rewrites and hypothetical responses, and propose to\naggregate them into an integrated representation that can robustly represent\nthe user's real contextual search intent. Extensive automatic evaluations and\nhuman evaluations on three widely used conversational search benchmarks,\nincluding CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance\nof our simple LLM4CS framework compared with existing methods and even using\nhuman rewrites. Our findings provide important evidence to better understand\nand leverage LLMs for conversational search.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-12T05:08:16Z",
    "updated": "2023-10-19T23:45:30Z",
    "doi": null
  },
  "2303.16765": {
    "id": "http://arxiv.org/abs/2303.16765v2",
    "title": "MDP: A Generalized Framework for Text-Guided Image Editing by\n  Manipulating the Diffusion Path",
    "authors": [
      "Qian Wang",
      "Biao Zhang",
      "Michael Birsak",
      "Peter Wonka"
    ],
    "abstract": "  Image generation using diffusion can be controlled in multiple ways. In this\npaper, we systematically analyze the equations of modern generative diffusion\nnetworks to propose a framework, called MDP, that explains the design space of\nsuitable manipulations. We identify 5 different manipulations, including\nintermediate latent, conditional embedding, cross attention maps, guidance, and\npredicted noise. We analyze the corresponding parameters of these manipulations\nand the manipulation schedule. We show that some previous editing methods fit\nnicely into our framework. Particularly, we identified one specific\nconfiguration as a new type of control by manipulating the predicted noise,\nwhich can perform higher-quality edits than previous work for a variety of\nlocal and global edits.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-29T14:57:54Z",
    "updated": "2023-03-30T11:42:41Z",
    "doi": null
  },
  "2005.01703": {
    "id": "http://arxiv.org/abs/2005.01703v2",
    "title": "Transforming and Projecting Images into Class-conditional Generative\n  Networks",
    "authors": [
      "Minyoung Huh",
      "Richard Zhang",
      "Jun-Yan Zhu",
      "Sylvain Paris",
      "Aaron Hertzmann"
    ],
    "abstract": "  We present a method for projecting an input image into the space of a\nclass-conditional generative neural network. We propose a method that optimizes\nfor transformation to counteract the model biases in generative neural\nnetworks. Specifically, we demonstrate that one can solve for image\ntranslation, scale, and global color transformation, during the projection\noptimization to address the object-center bias and color bias of a Generative\nAdversarial Network. This projection process poses a difficult optimization\nproblem, and purely gradient-based optimizations fail to find good solutions.\nWe describe a hybrid optimization strategy that finds good projections by\nestimating transformations and class parameters. We show the effectiveness of\nour method on real images and further demonstrate how the corresponding\nprojections lead to better editability of these images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-05-04T17:57:47Z",
    "updated": "2020-08-27T18:10:52Z",
    "doi": null
  },
  "1710.02971": {
    "id": "http://arxiv.org/abs/1710.02971v4",
    "title": "Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,\n  and node2vec",
    "authors": [
      "Jiezhong Qiu",
      "Yuxiao Dong",
      "Hao Ma",
      "Jian Li",
      "Kuansan Wang",
      "Jie Tang"
    ],
    "abstract": "  Since the invention of word2vec, the skip-gram model has significantly\nadvanced the research of network embedding, such as the recent emergence of the\nDeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of\nthe aforementioned models with negative sampling can be unified into the matrix\nfactorization framework with closed forms. Our analysis and proofs reveal that:\n(1) DeepWalk empirically produces a low-rank transformation of a network's\nnormalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk\nwhen the size of vertices' context is set to one; (3) As an extension of LINE,\nPTE can be viewed as the joint factorization of multiple networks' Laplacians;\n(4) node2vec is factorizing a matrix related to the stationary distribution and\ntransition probability tensor of a 2nd-order random walk. We further provide\nthe theoretical connections between skip-gram based network embedding\nalgorithms and the theory of graph Laplacian. Finally, we present the NetMF\nmethod as well as its approximation algorithm for computing network embedding.\nOur method offers significant improvements over DeepWalk and LINE for\nconventional network mining tasks. This work lays the theoretical foundation\nfor skip-gram based network embedding methods, leading to a better\nunderstanding of latent network representation learning.\n",
    "categories": [
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-10-09T07:28:46Z",
    "updated": "2018-02-08T09:51:03Z",
    "doi": "10.1145/3159652.3159706"
  },
  "2310.19248": {
    "id": "http://arxiv.org/abs/2310.19248v1",
    "title": "IMPRESS: Evaluating the Resilience of Imperceptible Perturbations\n  Against Unauthorized Data Usage in Diffusion-Based Generative AI",
    "authors": [
      "Bochuan Cao",
      "Changjiang Li",
      "Ting Wang",
      "Jinyuan Jia",
      "Bo Li",
      "Jinghui Chen"
    ],
    "abstract": "  Diffusion-based image generation models, such as Stable Diffusion or DALL-E\n2, are able to learn from given images and generate high-quality samples\nfollowing the guidance from prompts. For instance, they can be used to create\nartistic images that mimic the style of an artist based on his/her original\nartworks or to maliciously edit the original images for fake content. However,\nsuch ability also brings serious ethical issues without proper authorization\nfrom the owner of the original images. In response, several attempts have been\nmade to protect the original images from such unauthorized data usage by adding\nimperceptible perturbations, which are designed to mislead the diffusion model\nand make it unable to properly generate new samples. In this work, we introduce\na perturbation purification platform, named IMPRESS, to evaluate the\neffectiveness of imperceptible perturbations as a protective measure. IMPRESS\nis based on the key observation that imperceptible perturbations could lead to\na perceptible inconsistency between the original image and the\ndiffusion-reconstructed image, which can be used to devise a new optimization\nstrategy for purifying the image, which may weaken the protection of the\noriginal image from unauthorized data usage (e.g., style mimicking, malicious\nediting). The proposed IMPRESS platform offers a comprehensive evaluation of\nseveral contemporary protection methods, and can be used as an evaluation\nplatform for future protection methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-30T03:33:41Z",
    "updated": "2023-10-30T03:33:41Z",
    "doi": null
  },
  "2302.00438": {
    "id": "http://arxiv.org/abs/2302.00438v1",
    "title": "On the Robustness of Code Generation Techniques: An Empirical Study on\n  GitHub Copilot",
    "authors": [
      "Antonio Mastropaolo",
      "Luca Pascarella",
      "Emanuela Guglielmi",
      "Matteo Ciniselli",
      "Simone Scalabrino",
      "Rocco Oliveto",
      "Gabriele Bavota"
    ],
    "abstract": "  Software engineering research has always being concerned with the improvement\nof code completion approaches, which suggest the next tokens a developer will\nlikely type while coding. The release of GitHub Copilot constitutes a big step\nforward, also because of its unprecedented ability to automatically generate\neven entire functions from their natural language description. While the\nusefulness of Copilot is evident, it is still unclear to what extent it is\nrobust. Specifically, we do not know the extent to which semantic-preserving\nchanges in the natural language description provided to the model have an\neffect on the generated code function. In this paper we present an empirical\nstudy in which we aim at understanding whether different but semantically\nequivalent natural language descriptions result in the same recommended\nfunction. A negative answer would pose questions on the robustness of deep\nlearning (DL)-based code generators since it would imply that developers using\ndifferent wordings to describe the same code would obtain different\nrecommendations. We asked Copilot to automatically generate 892 Java methods\nstarting from their original Javadoc description. Then, we generated different\nsemantically equivalent descriptions for each method both manually and\nautomatically, and we analyzed the extent to which predictions generated by\nCopilot changed. Our results show that modifying the description results in\ndifferent code recommendations in ~46% of cases. Also, differences in the\nsemantically equivalent descriptions might impact the correctness of the\ngenerated code ~28%.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-01T13:36:53Z",
    "updated": "2023-02-01T13:36:53Z",
    "doi": null
  },
  "2206.05442": {
    "id": "http://arxiv.org/abs/2206.05442v7",
    "title": "From Human Days to Machine Seconds: Automatically Answering and\n  Generating Machine Learning Final Exams",
    "authors": [
      "Iddo Drori",
      "Sarah J. Zhang",
      "Reece Shuttleworth",
      "Sarah Zhang",
      "Keith Tyser",
      "Zad Chin",
      "Pedro Lantigua",
      "Saisamrit Surbehera",
      "Gregory Hunter",
      "Derek Austin",
      "Leonard Tang",
      "Yann Hicke",
      "Sage Simhon",
      "Sathwik Karnik",
      "Darnell Granberry",
      "Madeleine Udell"
    ],
    "abstract": "  A final exam in machine learning at a top institution such as MIT, Harvard,\nor Cornell typically takes faculty days to write, and students hours to solve.\nWe demonstrate that large language models pass machine learning finals at a\nhuman level, on finals available online after the models were trained, and\nautomatically generate new human-quality final exam questions in seconds.\nPrevious work has developed program synthesis and few-shot learning methods to\nsolve university-level problem set questions in mathematics and STEM courses.\nIn this work, we develop and compare methods that solve final exams, which\ndiffer from problem sets in several ways: the questions are longer, have\nmultiple parts, are more complicated, and span a broader set of topics. We\ncurate a dataset and benchmark of questions from machine learning final exams\navailable online and code for answering these questions and generating new\nquestions. We show how to generate new questions from other questions and\ncourse notes. For reproducibility and future research on this final exam\nbenchmark, we use automatic checkers for multiple-choice, numeric, and\nquestions with expression answers. We perform ablation studies comparing\nzero-shot learning with few-shot learning and chain-of-thought prompting using\nGPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that\nfew-shot learning methods perform best. We highlight the transformative\npotential of language models to streamline the writing and solution of\nlarge-scale assessments, significantly reducing the workload from human days to\nmere machine seconds. Our results suggest that rather than banning large\nlanguage models such as ChatGPT in class, instructors should teach students to\nharness them by asking students meta-questions about correctness, completeness,\nand originality of the responses generated, encouraging critical thinking in\nacademic studies.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-11T06:38:06Z",
    "updated": "2023-06-28T04:42:05Z",
    "doi": null
  },
  "2207.10642": {
    "id": "http://arxiv.org/abs/2207.10642v1",
    "title": "Generative Multiplane Images: Making a 2D GAN 3D-Aware",
    "authors": [
      "Xiaoming Zhao",
      "Fangchang Ma",
      "David G\u00fcera",
      "Zhile Ren",
      "Alexander G. Schwing",
      "Alex Colburn"
    ],
    "abstract": "  What is really needed to make an existing 2D GAN 3D-aware? To answer this\nquestion, we modify a classical GAN, i.e., StyleGANv2, as little as possible.\nWe find that only two modifications are absolutely necessary: 1) a multiplane\nimage style generator branch which produces a set of alpha maps conditioned on\ntheir depth; 2) a pose-conditioned discriminator. We refer to the generated\noutput as a 'generative multiplane image' (GMPI) and emphasize that its\nrenderings are not only high-quality but also guaranteed to be view-consistent,\nwhich makes GMPIs different from many prior works. Importantly, the number of\nalpha maps can be dynamically adjusted and can differ between training and\ninference, alleviating memory concerns and enabling fast training of GMPIs in\nless than half a day at a resolution of $1024^2$. Our findings are consistent\nacross three challenging and common high-resolution datasets, including FFHQ,\nAFHQv2, and MetFaces.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-21T17:50:16Z",
    "updated": "2022-07-21T17:50:16Z",
    "doi": null
  },
  "2401.08815": {
    "id": "http://arxiv.org/abs/2401.08815v1",
    "title": "Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive",
    "authors": [
      "Yumeng Li",
      "Margret Keuper",
      "Dan Zhang",
      "Anna Khoreva"
    ],
    "abstract": "  Despite the recent advances in large-scale diffusion models, little progress\nhas been made on the layout-to-image (L2I) synthesis task. Current L2I models\neither suffer from poor editability via text or weak alignment between the\ngenerated image and the input layout. This limits their usability in practice.\nTo mitigate this, we propose to integrate adversarial supervision into the\nconventional training pipeline of L2I diffusion models (ALDM). Specifically, we\nemploy a segmentation-based discriminator which provides explicit feedback to\nthe diffusion generator on the pixel-level alignment between the denoised image\nand the input layout. To encourage consistent adherence to the input layout\nover the sampling steps, we further introduce the multistep unrolling strategy.\nInstead of looking at a single timestep, we unroll a few steps recursively to\nimitate the inference process, and ask the discriminator to assess the\nalignment of denoised images with the layout over a certain time window. Our\nexperiments show that ALDM enables layout faithfulness of the generated images,\nwhile allowing broad editability via text prompts. Moreover, we showcase its\nusefulness for practical applications: by synthesizing target distribution\nsamples via text control, we improve domain generalization of semantic\nsegmentation models by a large margin (~12 mIoU points).\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-16T20:31:46Z",
    "updated": "2024-01-16T20:31:46Z",
    "doi": null
  },
  "2312.05664": {
    "id": "http://arxiv.org/abs/2312.05664v2",
    "title": "CoGS: Controllable Gaussian Splatting",
    "authors": [
      "Heng Yu",
      "Joel Julin",
      "Zolt\u00e1n \u00c1. Milacski",
      "Koichiro Niinuma",
      "L\u00e1szl\u00f3 A. Jeni"
    ],
    "abstract": "  Capturing and re-animating the 3D structure of articulated objects present\nsignificant barriers. On one hand, methods requiring extensively calibrated\nmulti-view setups are prohibitively complex and resource-intensive, limiting\ntheir practical applicability. On the other hand, while single-camera Neural\nRadiance Fields (NeRFs) offer a more streamlined approach, they have excessive\ntraining and rendering costs. 3D Gaussian Splatting would be a suitable\nalternative but for two reasons. Firstly, existing methods for 3D dynamic\nGaussians require synchronized multi-view cameras, and secondly, the lack of\ncontrollability in dynamic scenarios. We present CoGS, a method for\nControllable Gaussian Splatting, that enables the direct manipulation of scene\nelements, offering real-time control of dynamic scenes without the prerequisite\nof pre-computing control signals. We evaluated CoGS using both synthetic and\nreal-world datasets that include dynamic objects that differ in degree of\ndifficulty. In our evaluations, CoGS consistently outperformed existing dynamic\nand controllable neural representations in terms of visual fidelity.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-09T20:06:29Z",
    "updated": "2024-04-22T17:28:30Z",
    "doi": null
  },
  "2009.03300": {
    "id": "http://arxiv.org/abs/2009.03300v3",
    "title": "Measuring Massive Multitask Language Understanding",
    "authors": [
      "Dan Hendrycks",
      "Collin Burns",
      "Steven Basart",
      "Andy Zou",
      "Mantas Mazeika",
      "Dawn Song",
      "Jacob Steinhardt"
    ],
    "abstract": "  We propose a new test to measure a text model's multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess\nextensive world knowledge and problem solving ability. We find that while most\nrecent models have near random-chance accuracy, the very largest GPT-3 model\nimproves over random chance by almost 20 percentage points on average. However,\non every one of the 57 tasks, the best models still need substantial\nimprovements before they can reach expert-level accuracy. Models also have\nlopsided performance and frequently do not know when they are wrong. Worse,\nthey still have near-random accuracy on some socially important subjects such\nas morality and law. By comprehensively evaluating the breadth and depth of a\nmodel's academic and professional understanding, our test can be used to\nanalyze models across many tasks and to identify important shortcomings.\n",
    "categories": [
      {
        "@term": "cs.CY",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-09-07T17:59:25Z",
    "updated": "2021-01-12T18:57:11Z",
    "doi": null
  },
  "2307.15058": {
    "id": "http://arxiv.org/abs/2307.15058v1",
    "title": "MARS: An Instance-aware, Modular and Realistic Simulator for Autonomous\n  Driving",
    "authors": [
      "Zirui Wu",
      "Tianyu Liu",
      "Liyi Luo",
      "Zhide Zhong",
      "Jianteng Chen",
      "Hongmin Xiao",
      "Chao Hou",
      "Haozhe Lou",
      "Yuantao Chen",
      "Runyi Yang",
      "Yuxin Huang",
      "Xiaoyu Ye",
      "Zike Yan",
      "Yongliang Shi",
      "Yiyi Liao",
      "Hao Zhao"
    ],
    "abstract": "  Nowadays, autonomous cars can drive smoothly in ordinary cases, and it is\nwidely recognized that realistic sensor simulation will play a critical role in\nsolving remaining corner cases by simulating them. To this end, we propose an\nautonomous driving simulator based upon neural radiance fields (NeRFs).\nCompared with existing works, ours has three notable features: (1)\nInstance-aware. Our simulator models the foreground instances and background\nenvironments separately with independent networks so that the static (e.g.,\nsize and appearance) and dynamic (e.g., trajectory) properties of instances can\nbe controlled separately. (2) Modular. Our simulator allows flexible switching\nbetween different modern NeRF-related backbones, sampling strategies, input\nmodalities, etc. We expect this modular design to boost academic progress and\nindustrial deployment of NeRF-based autonomous driving simulation. (3)\nRealistic. Our simulator set new state-of-the-art photo-realism results given\nthe best module selection. Our simulator will be open-sourced while most of our\ncounterparts are not. Project page: https://open-air-sun.github.io/mars/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-27T17:59:52Z",
    "updated": "2023-07-27T17:59:52Z",
    "doi": "10.1007/978-981-99-8850-1_1"
  },
  "2409.16165": {
    "id": "http://arxiv.org/abs/2409.16165v1",
    "title": "EnIGMA: Enhanced Interactive Generative Model Agent for CTF Challenges",
    "authors": [
      "Talor Abramovich",
      "Meet Udeshi",
      "Minghao Shao",
      "Kilian Lieret",
      "Haoran Xi",
      "Kimberly Milner",
      "Sofija Jancheska",
      "John Yang",
      "Carlos E. Jimenez",
      "Farshad Khorrami",
      "Prashanth Krishnamurthy",
      "Brendan Dolan-Gavitt",
      "Muhammad Shafique",
      "Karthik Narasimhan",
      "Ramesh Karri",
      "Ofir Press"
    ],
    "abstract": "  Although language model (LM) agents are demonstrating growing potential in\nmany domains, their success in cybersecurity has been limited due to simplistic\ndesign and the lack of fundamental features for this domain. We present EnIGMA,\nan LM agent for autonomously solving Capture The Flag (CTF) challenges. EnIGMA\nintroduces new Agent-Computer Interfaces (ACIs) to improve the success rate on\nCTF challenges. We establish the novel Interactive Agent Tool concept, which\nenables LM agents to run interactive command-line utilities essential for these\nchallenges. Empirical analysis of EnIGMA on over 350 CTF challenges from three\ndifferent benchmarks indicates that providing a robust set of new tools with\ndemonstration of their usage helps the LM solve complex problems and achieves\nstate-of-the-art results on the NYU CTF and Intercode-CTF benchmarks. Finally,\nwe discuss insights on ACI design and agent behavior on cybersecurity tasks\nthat highlight the need to adapt real-world tools for LM agents.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-09-24T15:06:01Z",
    "updated": "2024-09-24T15:06:01Z",
    "doi": null
  },
  "2208.12415": {
    "id": "http://arxiv.org/abs/2208.12415v1",
    "title": "MuLan: A Joint Embedding of Music Audio and Natural Language",
    "authors": [
      "Qingqing Huang",
      "Aren Jansen",
      "Joonseok Lee",
      "Ravi Ganti",
      "Judith Yue Li",
      "Daniel P. W. Ellis"
    ],
    "abstract": "  Music tagging and content-based retrieval systems have traditionally been\nconstructed using pre-defined ontologies covering a rigid set of music\nattributes or text queries. This paper presents MuLan: a first attempt at a new\ngeneration of acoustic models that link music audio directly to unconstrained\nnatural language music descriptions. MuLan takes the form of a two-tower, joint\naudio-text embedding model trained using 44 million music recordings (370K\nhours) and weakly-associated, free-form text annotations. Through its\ncompatibility with a wide range of music genres and text styles (including\nconventional music tags), the resulting audio-text representation subsumes\nexisting ontologies while graduating to true zero-shot functionalities. We\ndemonstrate the versatility of the MuLan embeddings with a range of experiments\nincluding transfer learning, zero-shot music tagging, language understanding in\nthe music domain, and cross-modal retrieval applications.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-26T03:13:21Z",
    "updated": "2022-08-26T03:13:21Z",
    "doi": null
  },
  "2302.03453": {
    "id": "http://arxiv.org/abs/2302.03453v2",
    "title": "OSRT: Omnidirectional Image Super-Resolution with Distortion-aware\n  Transformer",
    "authors": [
      "Fanghua Yu",
      "Xintao Wang",
      "Mingdeng Cao",
      "Gen Li",
      "Ying Shan",
      "Chao Dong"
    ],
    "abstract": "  Omnidirectional images (ODIs) have obtained lots of research interest for\nimmersive experiences. Although ODIs require extremely high resolution to\ncapture details of the entire scene, the resolutions of most ODIs are\ninsufficient. Previous methods attempt to solve this issue by image\nsuper-resolution (SR) on equirectangular projection (ERP) images. However, they\nomit geometric properties of ERP in the degradation process, and their models\ncan hardly generalize to real ERP images. In this paper, we propose Fisheye\ndownsampling, which mimics the real-world imaging process and synthesizes more\nrealistic low-resolution samples. Then we design a distortion-aware Transformer\n(OSRT) to modulate ERP distortions continuously and self-adaptively. Without a\ncumbersome process, OSRT outperforms previous methods by about 0.2dB on PSNR.\nMoreover, we propose a convenient data augmentation strategy, which synthesizes\npseudo ERP images from plain images. This simple strategy can alleviate the\nover-fitting problem of large networks and significantly boost the performance\nof ODISR. Extensive experiments have demonstrated the state-of-the-art\nperformance of our OSRT. Codes and models will be available at\nhttps://github.com/Fanghua-Yu/OSRT.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-07T13:19:59Z",
    "updated": "2023-02-09T10:45:41Z",
    "doi": null
  },
  "2203.13301": {
    "id": "http://arxiv.org/abs/2203.13301v2",
    "title": "Multi-modal Multi-label Facial Action Unit Detection with Transformer",
    "authors": [
      "Lingfeng Wang",
      "Shisen Wang",
      "Jin Qi"
    ],
    "abstract": "  Facial Action Coding System is an important approach of facial expression\nanalysis.This paper describes our submission to the third Affective Behavior\nAnalysis (ABAW) 2022 competition. We proposed a transfomer based model to\ndetect facial action unit (FAU) in video. To be specific, we firstly trained a\nmulti-modal model to extract both audio and visual feature. After that, we\nproposed a action units correlation module to learn relationships between each\naction unit labels and refine action unit detection result. Experimental\nresults on validation dataset shows that our method achieves better performance\nthan baseline model, which verifies that the effectiveness of proposed network.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-24T18:59:31Z",
    "updated": "2022-03-28T05:17:31Z",
    "doi": null
  },
  "1810.00553": {
    "id": "http://arxiv.org/abs/1810.00553v1",
    "title": "Optimal Adaptive and Accelerated Stochastic Gradient Descent",
    "authors": [
      "Qi Deng",
      "Yi Cheng",
      "Guanghui Lan"
    ],
    "abstract": "  Stochastic gradient descent (\\textsc{Sgd}) methods are the most powerful\noptimization tools in training machine learning and deep learning models.\nMoreover, acceleration (a.k.a. momentum) methods and diagonal scaling (a.k.a.\nadaptive gradient) methods are the two main techniques to improve the slow\nconvergence of \\textsc{Sgd}. While empirical studies have demonstrated\npotential advantages of combining these two techniques, it remains unknown\nwhether these methods can achieve the optimal rate of convergence for\nstochastic optimization. In this paper, we present a new class of adaptive and\naccelerated stochastic gradient descent methods and show that they exhibit the\noptimal sampling and iteration complexity for stochastic optimization. More\nspecifically, we show that diagonal scaling, initially designed to improve\nvanilla stochastic gradient, can be incorporated into accelerated stochastic\ngradient descent to achieve the optimal rate of convergence for smooth\nstochastic optimization. We also show that momentum, apart from being known to\nspeed up the convergence rate of deterministic optimization, also provides us\nnew ways of designing non-uniform and aggressive moving average schemes in\nstochastic optimization. Finally, we present some heuristics that help to\nimplement adaptive accelerated stochastic gradient descent methods and to\nfurther improve their practical performance for machine learning and deep\nlearning.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-10-01T07:07:47Z",
    "updated": "2018-10-01T07:07:47Z",
    "doi": null
  },
  "2102.02808": {
    "id": "http://arxiv.org/abs/2102.02808v2",
    "title": "Multi-Stage Progressive Image Restoration",
    "authors": [
      "Syed Waqas Zamir",
      "Aditya Arora",
      "Salman Khan",
      "Munawar Hayat",
      "Fahad Shahbaz Khan",
      "Ming-Hsuan Yang",
      "Ling Shao"
    ],
    "abstract": "  Image restoration tasks demand a complex balance between spatial details and\nhigh-level contextualized information while recovering images. In this paper,\nwe propose a novel synergistic design that can optimally balance these\ncompeting goals. Our main proposal is a multi-stage architecture, that\nprogressively learns restoration functions for the degraded inputs, thereby\nbreaking down the overall recovery process into more manageable steps.\nSpecifically, our model first learns the contextualized features using\nencoder-decoder architectures and later combines them with a high-resolution\nbranch that retains local information. At each stage, we introduce a novel\nper-pixel adaptive design that leverages in-situ supervised attention to\nreweight the local features. A key ingredient in such a multi-stage\narchitecture is the information exchange between different stages. To this end,\nwe propose a two-faceted approach where the information is not only exchanged\nsequentially from early to late stages, but lateral connections between feature\nprocessing blocks also exist to avoid any loss of information. The resulting\ntightly interlinked multi-stage architecture, named as MPRNet, delivers strong\nperformance gains on ten datasets across a range of tasks including image\nderaining, deblurring, and denoising. The source code and pre-trained models\nare available at https://github.com/swz30/MPRNet.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-02-04T18:57:07Z",
    "updated": "2021-03-16T11:02:52Z",
    "doi": null
  },
  "2309.06180": {
    "id": "http://arxiv.org/abs/2309.06180v1",
    "title": "Efficient Memory Management for Large Language Model Serving with\n  PagedAttention",
    "authors": [
      "Woosuk Kwon",
      "Zhuohan Li",
      "Siyuan Zhuang",
      "Ying Sheng",
      "Lianmin Zheng",
      "Cody Hao Yu",
      "Joseph E. Gonzalez",
      "Hao Zhang",
      "Ion Stoica"
    ],
    "abstract": "  High throughput serving of large language models (LLMs) requires batching\nsufficiently many requests at a time. However, existing systems struggle\nbecause the key-value cache (KV cache) memory for each request is huge and\ngrows and shrinks dynamically. When managed inefficiently, this memory can be\nsignificantly wasted by fragmentation and redundant duplication, limiting the\nbatch size. To address this problem, we propose PagedAttention, an attention\nalgorithm inspired by the classical virtual memory and paging techniques in\noperating systems. On top of it, we build vLLM, an LLM serving system that\nachieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV\ncache within and across requests to further reduce memory usage. Our\nevaluations show that vLLM improves the throughput of popular LLMs by\n2-4$\\times$ with the same level of latency compared to the state-of-the-art\nsystems, such as FasterTransformer and Orca. The improvement is more pronounced\nwith longer sequences, larger models, and more complex decoding algorithms.\nvLLM's source code is publicly available at\nhttps://github.com/vllm-project/vllm\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DC",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-12T12:50:04Z",
    "updated": "2023-09-12T12:50:04Z",
    "doi": null
  },
  "2302.06235": {
    "id": "http://arxiv.org/abs/2302.06235v2",
    "title": "A Simple Zero-shot Prompt Weighting Technique to Improve Prompt\n  Ensembling in Text-Image Models",
    "authors": [
      "James Urquhart Allingham",
      "Jie Ren",
      "Michael W Dusenberry",
      "Xiuye Gu",
      "Yin Cui",
      "Dustin Tran",
      "Jeremiah Zhe Liu",
      "Balaji Lakshminarayanan"
    ],
    "abstract": "  Contrastively trained text-image models have the remarkable ability to\nperform zero-shot classification, that is, classifying previously unseen images\ninto categories that the model has never been explicitly trained to identify.\nHowever, these zero-shot classifiers need prompt engineering to achieve high\naccuracy. Prompt engineering typically requires hand-crafting a set of prompts\nfor individual downstream tasks. In this work, we aim to automate this prompt\nengineering and improve zero-shot accuracy through prompt ensembling. In\nparticular, we ask \"Given a large pool of prompts, can we automatically score\nthe prompts and ensemble those that are most suitable for a particular\ndownstream dataset, without needing access to labeled validation data?\". We\ndemonstrate that this is possible. In doing so, we identify several pathologies\nin a naive prompt scoring method where the score can be easily overconfident\ndue to biases in pre-training and test data, and we propose a novel prompt\nscoring method that corrects for the biases. Using our proposed scoring method\nto create a weighted average prompt ensemble, our method outperforms equal\naverage ensemble, as well as hand-crafted prompts, on ImageNet, 4 of its\nvariants, and 11 fine-grained classification benchmarks, all while being fully\nautomatic, optimization-free, and not requiring access to labeled validation\ndata.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-13T10:19:58Z",
    "updated": "2023-07-15T11:12:59Z",
    "doi": null
  },
  "2209.01578": {
    "id": "http://arxiv.org/abs/2209.01578v2",
    "title": "Spatial-Temporal Transformer for Video Snapshot Compressive Imaging",
    "authors": [
      "Lishun Wang",
      "Miao Cao",
      "Yong Zhong",
      "Xin Yuan"
    ],
    "abstract": "  Video snapshot compressive imaging (SCI) captures multiple sequential video\nframes by a single measurement using the idea of computational imaging. The\nunderlying principle is to modulate high-speed frames through different masks\nand these modulated frames are summed to a single measurement captured by a\nlow-speed 2D sensor (dubbed optical encoder); following this, algorithms are\nemployed to reconstruct the desired high-speed frames (dubbed software decoder)\nif needed. In this paper, we consider the reconstruction algorithm in video\nSCI, i.e., recovering a series of video frames from a compressed measurement.\nSpecifically, we propose a Spatial-Temporal transFormer (STFormer) to exploit\nthe correlation in both spatial and temporal domains. STFormer network is\ncomposed of a token generation block, a video reconstruction block, and these\ntwo blocks are connected by a series of STFormer blocks. Each STFormer block\nconsists of a spatial self-attention branch, a temporal self-attention branch\nand the outputs of these two branches are integrated by a fusion network.\nExtensive results on both simulated and real data demonstrate the\nstate-of-the-art performance of STFormer. The code and models are publicly\navailable at https://github.com/ucaswangls/STFormer.git\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-04T09:24:17Z",
    "updated": "2022-09-08T04:56:25Z",
    "doi": null
  },
  "2401.05998": {
    "id": "http://arxiv.org/abs/2401.05998v1",
    "title": "Combating Adversarial Attacks with Multi-Agent Debate",
    "authors": [
      "Steffi Chern",
      "Zhen Fan",
      "Andy Liu"
    ],
    "abstract": "  While state-of-the-art language models have achieved impressive results, they\nremain susceptible to inference-time adversarial attacks, such as adversarial\nprompts generated by red teams arXiv:2209.07858. One approach proposed to\nimprove the general quality of language model generations is multi-agent\ndebate, where language models self-evaluate through discussion and feedback\narXiv:2305.14325. We implement multi-agent debate between current\nstate-of-the-art language models and evaluate models' susceptibility to red\nteam attacks in both single- and multi-agent settings. We find that multi-agent\ndebate can reduce model toxicity when jailbroken or less capable models are\nforced to debate with non-jailbroken or more capable models. We also find\nmarginal improvements through the general usage of multi-agent interactions. We\nfurther perform adversarial prompt content classification via embedding\nclustering, and analyze the susceptibility of different models to different\ntypes of attack topics.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-11T15:57:38Z",
    "updated": "2024-01-11T15:57:38Z",
    "doi": null
  },
  "1907.01879": {
    "id": "http://arxiv.org/abs/1907.01879v1",
    "title": "Learning to Predict Robot Keypoints Using Artificially Generated Images",
    "authors": [
      "Christoph Heindl",
      "Sebastian Zambal",
      "Josef Scharinger"
    ],
    "abstract": "  This work considers robot keypoint estimation on color images as a supervised\nmachine learning task. We propose the use of probabilistically created\nrenderings to overcome the lack of labeled real images. Rather than sampling\nfrom stationary distributions, our approach introduces a feedback mechanism\nthat constantly adapts probability distributions according to current training\nprogress. Initial results show, our approach achieves near-human-level accuracy\non real images. Additionally, we demonstrate that feedback leads to fewer\nrequired training steps, while maintaining the same model quality on synthetic\ndata sets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-07-03T12:20:16Z",
    "updated": "2019-07-03T12:20:16Z",
    "doi": null
  },
  "2309.14751": {
    "id": "http://arxiv.org/abs/2309.14751v1",
    "title": "Text-image guided Diffusion Model for generating Deepfake celebrity\n  interactions",
    "authors": [
      "Yunzhuo Chen",
      "Nur Al Hasan Haldar",
      "Naveed Akhtar",
      "Ajmal Mian"
    ],
    "abstract": "  Deepfake images are fast becoming a serious concern due to their realism.\nDiffusion models have recently demonstrated highly realistic visual content\ngeneration, which makes them an excellent potential tool for Deepfake\ngeneration. To curb their exploitation for Deepfakes, it is imperative to first\nexplore the extent to which diffusion models can be used to generate realistic\ncontent that is controllable with convenient prompts. This paper devises and\nexplores a novel method in that regard. Our technique alters the popular stable\ndiffusion model to generate a controllable high-quality Deepfake image with\ntext and image prompts. In addition, the original stable model lacks severely\nin generating quality images that contain multiple persons. The modified\ndiffusion model is able to address this problem, it add input anchor image's\nlatent at the beginning of inferencing rather than Gaussian random latent as\ninput. Hence, we focus on generating forged content for celebrity interactions,\nwhich may be used to spread rumors. We also apply Dreambooth to enhance the\nrealism of our fake images. Dreambooth trains the pairing of center words and\nspecific features to produce more refined and personalized output images. Our\nresults show that with the devised scheme, it is possible to create fake visual\ncontent with alarming realism, such that the content can serve as believable\nevidence of meetings between powerful political figures.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-26T08:24:37Z",
    "updated": "2023-09-26T08:24:37Z",
    "doi": null
  },
  "2304.08477": {
    "id": "http://arxiv.org/abs/2304.08477v2",
    "title": "Latent-Shift: Latent Diffusion with Temporal Shift for Efficient\n  Text-to-Video Generation",
    "authors": [
      "Jie An",
      "Songyang Zhang",
      "Harry Yang",
      "Sonal Gupta",
      "Jia-Bin Huang",
      "Jiebo Luo",
      "Xi Yin"
    ],
    "abstract": "  We propose Latent-Shift -- an efficient text-to-video generation method based\non a pretrained text-to-image generation model that consists of an autoencoder\nand a U-Net diffusion model. Learning a video diffusion model in the latent\nspace is much more efficient than in the pixel space. The latter is often\nlimited to first generating a low-resolution video followed by a sequence of\nframe interpolation and super-resolution models, which makes the entire\npipeline very complex and computationally expensive. To extend a U-Net from\nimage generation to video generation, prior work proposes to add additional\nmodules like 1D temporal convolution and/or temporal attention layers. In\ncontrast, we propose a parameter-free temporal shift module that can leverage\nthe spatial U-Net as is for video generation. We achieve this by shifting two\nportions of the feature map channels forward and backward along the temporal\ndimension. The shifted features of the current frame thus receive the features\nfrom the previous and the subsequent frames, enabling motion learning without\nadditional parameters. We show that Latent-Shift achieves comparable or better\nresults while being significantly more efficient. Moreover, Latent-Shift can\ngenerate images despite being finetuned for T2V generation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-17T17:57:06Z",
    "updated": "2023-04-18T03:27:52Z",
    "doi": null
  },
  "1906.01277": {
    "id": "http://arxiv.org/abs/1906.01277v2",
    "title": "Wasserstein Weisfeiler-Lehman Graph Kernels",
    "authors": [
      "Matteo Togninalli",
      "Elisabetta Ghisu",
      "Felipe Llinares-L\u00f3pez",
      "Bastian Rieck",
      "Karsten Borgwardt"
    ],
    "abstract": "  Most graph kernels are an instance of the class of $\\mathcal{R}$-Convolution\nkernels, which measure the similarity of objects by comparing their\nsubstructures. Despite their empirical success, most graph kernels use a naive\naggregation of the final set of substructures, usually a sum or average,\nthereby potentially discarding valuable information about the distribution of\nindividual components. Furthermore, only a limited instance of these approaches\ncan be extended to continuously attributed graphs. We propose a novel method\nthat relies on the Wasserstein distance between the node feature vector\ndistributions of two graphs, which allows to find subtler differences in data\nsets by considering graphs as high-dimensional objects, rather than simple\nmeans. We further propose a Weisfeiler-Lehman inspired embedding scheme for\ngraphs with continuous node attributes and weighted edges, enhance it with the\ncomputed Wasserstein distance, and thus improve the state-of-the-art prediction\nperformance on several graph classification tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "q-bio.MN",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-06-04T08:52:47Z",
    "updated": "2019-10-30T14:25:05Z",
    "doi": null
  },
  "2203.08713": {
    "id": "http://arxiv.org/abs/2203.08713v2",
    "title": "DeciWatch: A Simple Baseline for 10x Efficient 2D and 3D Pose Estimation",
    "authors": [
      "Ailing Zeng",
      "Xuan Ju",
      "Lei Yang",
      "Ruiyuan Gao",
      "Xizhou Zhu",
      "Bo Dai",
      "Qiang Xu"
    ],
    "abstract": "  This paper proposes a simple baseline framework for video-based 2D/3D human\npose estimation that can achieve 10 times efficiency improvement over existing\nworks without any performance degradation, named DeciWatch. Unlike current\nsolutions that estimate each frame in a video, DeciWatch introduces a simple\nyet effective sample-denoise-recover framework that only watches sparsely\nsampled frames, taking advantage of the continuity of human motions and the\nlightweight pose representation. Specifically, DeciWatch uniformly samples less\nthan 10% video frames for detailed estimation, denoises the estimated 2D/3D\nposes with an efficient Transformer architecture, and then accurately recovers\nthe rest of the frames using another Transformer-based network. Comprehensive\nexperimental results on three video-based human pose estimation and body mesh\nrecovery tasks with four datasets validate the efficiency and effectiveness of\nDeciWatch. Code is available at https://github.com/cure-lab/DeciWatch.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-16T16:03:37Z",
    "updated": "2022-07-20T18:02:53Z",
    "doi": null
  },
  "2203.04067": {
    "id": "http://arxiv.org/abs/2203.04067v1",
    "title": "Lane Detection with Versatile AtrousFormer and Local Semantic Guidance",
    "authors": [
      "Jiaxing Yang",
      "Lihe Zhang",
      "Huchuan Lu"
    ],
    "abstract": "  Lane detection is one of the core functions in autonomous driving and has\naroused widespread attention recently. The networks to segment lane instances,\nespecially with bad appearance, must be able to explore lane distribution\nproperties. Most existing methods tend to resort to CNN-based techniques. A few\nhave a try on incorporating the recent adorable, the seq2seq Transformer\n\\cite{transformer}. However, their innate drawbacks of weak global information\ncollection ability and exorbitant computation overhead prohibit a wide range of\nthe further applications. In this work, we propose Atrous Transformer\n(AtrousFormer) to solve the problem. Its variant local AtrousFormer is\ninterleaved into feature extractor to enhance extraction. Their collecting\ninformation first by rows and then by columns in a dedicated manner finally\nequips our network with stronger information gleaning ability and better\ncomputation efficiency. To further improve the performance, we also propose a\nlocal semantic guided decoder to delineate the identities and shapes of lanes\nmore accurately, in which the predicted Gaussian map of the starting point of\neach lane serves to guide the process. Extensive results on three challenging\nbenchmarks (CULane, TuSimple, and BDD100K) show that our network performs\nfavorably against the state of the arts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-08T13:25:35Z",
    "updated": "2022-03-08T13:25:35Z",
    "doi": null
  },
  "2305.18453": {
    "id": "http://arxiv.org/abs/2305.18453v5",
    "title": "Conditional Diffusion Models for Semantic 3D Brain MRI Synthesis",
    "authors": [
      "Zolnamar Dorjsembe",
      "Hsing-Kuo Pao",
      "Sodtavilan Odonchimed",
      "Furen Xiao"
    ],
    "abstract": "  Artificial intelligence (AI) in healthcare, especially in medical imaging,\nfaces challenges due to data scarcity and privacy concerns. Addressing these,\nwe introduce Med-DDPM, a diffusion model designed for 3D semantic brain MRI\nsynthesis. This model effectively tackles data scarcity and privacy issues by\nintegrating semantic conditioning. This involves the channel-wise concatenation\nof a conditioning image to the model input, enabling control in image\ngeneration. Med-DDPM demonstrates superior stability and performance compared\nto existing 3D brain imaging synthesis methods. It generates diverse,\nanatomically coherent images with high visual fidelity. In terms of dice score\naccuracy in the tumor segmentation task, Med-DDPM achieves 0.6207, close to the\n0.6531 accuracy of real images, and outperforms baseline models. Combined with\nreal images, it further increases segmentation accuracy to 0.6675, showing the\npotential of our proposed method for data augmentation. This model represents\nthe first use of a diffusion model in 3D semantic brain MRI synthesis,\nproducing high-quality images. Its semantic conditioning feature also shows\npotential for image anonymization in biomedical imaging, addressing data and\nprivacy issues. We provide the code and model weights for Med-DDPM on our\nGitHub repository (https://github.com/mobaidoctor/med-ddpm/) to support\nreproducibility.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-29T04:14:38Z",
    "updated": "2024-04-19T13:37:18Z",
    "doi": "10.1109/JBHI.2024.3385504"
  },
  "2203.13253": {
    "id": "http://arxiv.org/abs/2203.13253v1",
    "title": "Video Instance Segmentation via Multi-scale Spatio-temporal Split\n  Attention Transformer",
    "authors": [
      "Omkar Thawakar",
      "Sanath Narayan",
      "Jiale Cao",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Muhammad Haris Khan",
      "Salman Khan",
      "Michael Felsberg",
      "Fahad Shahbaz Khan"
    ],
    "abstract": "  State-of-the-art transformer-based video instance segmentation (VIS)\napproaches typically utilize either single-scale spatio-temporal features or\nper-frame multi-scale features during the attention computations. We argue that\nsuch an attention computation ignores the multi-scale spatio-temporal feature\nrelationships that are crucial to tackle target appearance deformations in\nvideos. To address this issue, we propose a transformer-based VIS framework,\nnamed MS-STS VIS, that comprises a novel multi-scale spatio-temporal split\n(MS-STS) attention module in the encoder. The proposed MS-STS module\neffectively captures spatio-temporal feature relationships at multiple scales\nacross frames in a video. We further introduce an attention block in the\ndecoder to enhance the temporal consistency of the detected instances in\ndifferent frames of a video. Moreover, an auxiliary discriminator is introduced\nduring training to ensure better foreground-background separability within the\nmulti-scale spatio-temporal feature space. We conduct extensive experiments on\ntwo benchmarks: Youtube-VIS (2019 and 2021). Our MS-STS VIS achieves\nstate-of-the-art performance on both benchmarks. When using the ResNet50\nbackbone, our MS-STS achieves a mask AP of 50.1 %, outperforming the best\nreported results in literature by 2.7 % and by 4.8 % at higher overlap\nthreshold of AP_75, while being comparable in model size and speed on\nYoutube-VIS 2019 val. set. When using the Swin Transformer backbone, MS-STS VIS\nachieves mask AP of 61.0 % on Youtube-VIS 2019 val. set. Our code and models\nare available at https://github.com/OmkarThawakar/MSSTS-VIS.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-24T17:59:20Z",
    "updated": "2022-03-24T17:59:20Z",
    "doi": null
  },
  "2312.04410": {
    "id": "http://arxiv.org/abs/2312.04410v1",
    "title": "Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models",
    "authors": [
      "Jiayi Guo",
      "Xingqian Xu",
      "Yifan Pu",
      "Zanlin Ni",
      "Chaofei Wang",
      "Manushree Vasu",
      "Shiji Song",
      "Gao Huang",
      "Humphrey Shi"
    ],
    "abstract": "  Recently, diffusion models have made remarkable progress in text-to-image\n(T2I) generation, synthesizing images with high fidelity and diverse contents.\nDespite this advancement, latent space smoothness within diffusion models\nremains largely unexplored. Smooth latent spaces ensure that a perturbation on\nan input latent corresponds to a steady change in the output image. This\nproperty proves beneficial in downstream tasks, including image interpolation,\ninversion, and editing. In this work, we expose the non-smoothness of diffusion\nlatent spaces by observing noticeable visual fluctuations resulting from minor\nlatent variations. To tackle this issue, we propose Smooth Diffusion, a new\ncategory of diffusion models that can be simultaneously high-performing and\nsmooth. Specifically, we introduce Step-wise Variation Regularization to\nenforce the proportion between the variations of an arbitrary input latent and\nthat of the output image is a constant at any diffusion training step. In\naddition, we devise an interpolation standard deviation (ISTD) metric to\neffectively assess the latent space smoothness of a diffusion model. Extensive\nquantitative and qualitative experiments demonstrate that Smooth Diffusion\nstands out as a more desirable solution not only in T2I generation but also\nacross various downstream tasks. Smooth Diffusion is implemented as a\nplug-and-play Smooth-LoRA to work with various community models. Code is\navailable at https://github.com/SHI-Labs/Smooth-Diffusion.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-07T16:26:23Z",
    "updated": "2023-12-07T16:26:23Z",
    "doi": null
  },
  "2311.16465": {
    "id": "http://arxiv.org/abs/2311.16465v1",
    "title": "TextDiffuser-2: Unleashing the Power of Language Models for Text\n  Rendering",
    "authors": [
      "Jingye Chen",
      "Yupan Huang",
      "Tengchao Lv",
      "Lei Cui",
      "Qifeng Chen",
      "Furu Wei"
    ],
    "abstract": "  The diffusion model has been proven a powerful generative model in recent\nyears, yet remains a challenge in generating visual text. Several methods\nalleviated this issue by incorporating explicit text position and content as\nguidance on where and what text to render. However, these methods still suffer\nfrom several drawbacks, such as limited flexibility and automation, constrained\ncapability of layout prediction, and restricted style diversity. In this paper,\nwe present TextDiffuser-2, aiming to unleash the power of language models for\ntext rendering. Firstly, we fine-tune a large language model for layout\nplanning. The large language model is capable of automatically generating\nkeywords for text rendering and also supports layout modification through\nchatting. Secondly, we utilize the language model within the diffusion model to\nencode the position and texts at the line level. Unlike previous methods that\nemployed tight character-level guidance, this approach generates more diverse\ntext images. We conduct extensive experiments and incorporate user studies\ninvolving human participants as well as GPT-4V, validating TextDiffuser-2's\ncapacity to achieve a more rational text layout and generation with enhanced\ndiversity. The code and model will be available at\n\\url{https://aka.ms/textdiffuser-2}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-28T04:02:40Z",
    "updated": "2023-11-28T04:02:40Z",
    "doi": null
  },
  "2209.07521": {
    "id": "http://arxiv.org/abs/2209.07521v2",
    "title": "On-Device Domain Generalization",
    "authors": [
      "Kaiyang Zhou",
      "Yuanhan Zhang",
      "Yuhang Zang",
      "Jingkang Yang",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "abstract": "  We present a systematic study of domain generalization (DG) for tiny neural\nnetworks. This problem is critical to on-device machine learning applications\nbut has been overlooked in the literature where research has been merely\nfocused on large models. Tiny neural networks have much fewer parameters and\nlower complexity and therefore should not be trained the same way as their\nlarge counterparts for DG applications. By conducting extensive experiments, we\nfind that knowledge distillation (KD), a well-known technique for model\ncompression, is much better for tackling the on-device DG problem than\nconventional DG methods. Another interesting observation is that the\nteacher-student gap on out-of-distribution data is bigger than that on\nin-distribution data, which highlights the capacity mismatch issue as well as\nthe shortcoming of KD. We further propose a method called out-of-distribution\nknowledge distillation (OKD) where the idea is to teach the student how the\nteacher handles out-of-distribution data synthesized via disruptive data\naugmentation. Without adding any extra parameter to the model -- hence keeping\nthe deployment cost unchanged -- OKD significantly improves DG performance for\ntiny neural networks in a variety of on-device DG scenarios for image and\nspeech applications. We also contribute a scalable approach for synthesizing\nvisual domain shifts, along with a new suite of DG datasets to complement\nexisting testbeds.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-15T17:59:31Z",
    "updated": "2022-11-08T04:32:30Z",
    "doi": null
  },
  "2406.11617": {
    "id": "http://arxiv.org/abs/2406.11617v1",
    "title": "DELLA-Merging: Reducing Interference in Model Merging through\n  Magnitude-Based Sampling",
    "authors": [
      "Pala Tej Deep",
      "Rishabh Bhardwaj",
      "Soujanya Poria"
    ],
    "abstract": "  With the proliferation of domain-specific models, model merging has emerged\nas a set of techniques that combine the capabilities of multiple models into\none that can multitask without the cost of additional training. In this paper,\nwe propose a new model merging technique, Drop and rEscaLe via sampLing with\nmAgnitude (DELLA-Merging), that employs a novel pruning technique, MAGPRUNE,\nwhich shows significant advantages over DARE and TIES. MAGPRUNE first ranks the\nparameters in order of their magnitude and assigns higher dropout probabilities\n(p) to parameters with lower ranks corresponding to lower magnitudes. To\napproximate the original embeddings, MAGPRUNE employs a rescaling operation on\nthe parameters that survive the random dropping by 1/(1 - p). On three\ndifferent expert models considered for merging (LM, Math, Code) and\ncorresponding benchmark datasets (AlpacaEval, GSM8K, MBPP), DELLA shows an\naverage improvement of 2.4 points over baseline methods employing delta\nparameter pruning (an improvement of 3.6 points over TIES, 1.2 points over\nDARE), and 11.1 points over the no-pruning baseline (TA). We release the source\ncode at: https://github.com/declare-lab/della.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-17T15:02:45Z",
    "updated": "2024-06-17T15:02:45Z",
    "doi": null
  },
  "2304.08483": {
    "id": "http://arxiv.org/abs/2304.08483v1",
    "title": "Text2Performer: Text-Driven Human Video Generation",
    "authors": [
      "Yuming Jiang",
      "Shuai Yang",
      "Tong Liang Koh",
      "Wayne Wu",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "abstract": "  Text-driven content creation has evolved to be a transformative technique\nthat revolutionizes creativity. Here we study the task of text-driven human\nvideo generation, where a video sequence is synthesized from texts describing\nthe appearance and motions of a target performer. Compared to general\ntext-driven video generation, human-centric video generation requires\nmaintaining the appearance of synthesized human while performing complex\nmotions. In this work, we present Text2Performer to generate vivid human videos\nwith articulated motions from texts. Text2Performer has two novel designs: 1)\ndecomposed human representation and 2) diffusion-based motion sampler. First,\nwe decompose the VQVAE latent space into human appearance and pose\nrepresentation in an unsupervised manner by utilizing the nature of human\nvideos. In this way, the appearance is well maintained along the generated\nframes. Then, we propose continuous VQ-diffuser to sample a sequence of pose\nembeddings. Unlike existing VQ-based methods that operate in the discrete\nspace, continuous VQ-diffuser directly outputs the continuous pose embeddings\nfor better motion modeling. Finally, motion-aware masking strategy is designed\nto mask the pose embeddings spatial-temporally to enhance the temporal\ncoherence. Moreover, to facilitate the task of text-driven human video\ngeneration, we contribute a Fashion-Text2Video dataset with manually annotated\naction labels and text descriptions. Extensive experiments demonstrate that\nText2Performer generates high-quality human videos (up to 512x256 resolution)\nwith diverse appearances and flexible motions.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-17T17:59:02Z",
    "updated": "2023-04-17T17:59:02Z",
    "doi": null
  },
  "2204.05991": {
    "id": "http://arxiv.org/abs/2204.05991v2",
    "title": "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression\n  Comprehension",
    "authors": [
      "Sanjay Subramanian",
      "William Merrill",
      "Trevor Darrell",
      "Matt Gardner",
      "Sameer Singh",
      "Anna Rohrbach"
    ],
    "abstract": "  Training a referring expression comprehension (ReC) model for a new visual\ndomain requires collecting referring expressions, and potentially corresponding\nbounding boxes, for images in the domain. While large-scale pre-trained models\nare useful for image classification across domains, it remains unclear if they\ncan be applied in a zero-shot manner to more complex tasks like ReC. We present\nReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a\nstate-of-the-art large-scale model, for ReC. Motivated by the close connection\nbetween ReC and CLIP's contrastive pre-training objective, the first component\nof ReCLIP is a region-scoring method that isolates object proposals via\ncropping and blurring, and passes them to CLIP. However, through controlled\nexperiments on a synthetic dataset, we find that CLIP is largely incapable of\nperforming spatial reasoning off-the-shelf. Thus, the second component of\nReCLIP is a spatial relation resolver that handles several types of spatial\nrelations. We reduce the gap between zero-shot baselines from prior work and\nsupervised models by as much as 29% on RefCOCOg, and on RefGTA (video game\nimagery), ReCLIP's relative improvement over supervised ReC models trained on\nreal images is 8%.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-12T17:55:38Z",
    "updated": "2022-05-02T20:08:17Z",
    "doi": null
  },
  "1906.00377": {
    "id": "http://arxiv.org/abs/1906.00377v1",
    "title": "Hierarchical Video Frame Sequence Representation with Deep Convolutional\n  Graph Network",
    "authors": [
      "Feng Mao",
      "Xiang Wu",
      "Hui Xue",
      "Rong Zhang"
    ],
    "abstract": "  High accuracy video label prediction (classification) models are attributed\nto large scale data. These data could be frame feature sequences extracted by a\npre-trained convolutional-neural-network, which promote the efficiency for\ncreating models. Unsupervised solutions such as feature average pooling, as a\nsimple label-independent parameter-free based method, has limited ability to\nrepresent the video. While the supervised methods, like RNN, can greatly\nimprove the recognition accuracy. However, the video length is usually long,\nand there are hierarchical relationships between frames across events in the\nvideo, the performance of RNN based models are decreased. In this paper, we\nproposes a novel video classification method based on a deep convolutional\ngraph neural network(DCGN). The proposed method utilize the characteristics of\nthe hierarchical structure of the video, and performed multi-level feature\nextraction on the video frame sequence through the graph network, obtained a\nvideo representation re ecting the event semantics hierarchically. We test our\nmodel on YouTube-8M Large-Scale Video Understanding dataset, and the result\noutperforms RNN based benchmarks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-06-02T10:02:39Z",
    "updated": "2019-06-02T10:02:39Z",
    "doi": "10.1007/978-3-030-11018-5_24"
  },
  "2402.06088": {
    "id": "http://arxiv.org/abs/2402.06088v1",
    "title": "Animated Stickers: Bringing Stickers to Life with Video Diffusion",
    "authors": [
      "David Yan",
      "Winnie Zhang",
      "Luxin Zhang",
      "Anmol Kalia",
      "Dingkang Wang",
      "Ankit Ramchandani",
      "Miao Liu",
      "Albert Pumarola",
      "Edgar Schoenfeld",
      "Elliot Blanchard",
      "Krishna Narni",
      "Yaqiao Luo",
      "Lawrence Chen",
      "Guan Pang",
      "Ali Thabet",
      "Peter Vajda",
      "Amy Bearman",
      "Licheng Yu"
    ],
    "abstract": "  We introduce animated stickers, a video diffusion model which generates an\nanimation conditioned on a text prompt and static sticker image. Our model is\nbuilt on top of the state-of-the-art Emu text-to-image model, with the addition\nof temporal layers to model motion. Due to the domain gap, i.e. differences in\nvisual and motion style, a model which performed well on generating natural\nvideos can no longer generate vivid videos when applied to stickers. To bridge\nthis gap, we employ a two-stage finetuning pipeline: first with weakly\nin-domain data, followed by human-in-the-loop (HITL) strategy which we term\nensemble-of-teachers. It distills the best qualities of multiple teachers into\na smaller student model. We show that this strategy allows us to specifically\ntarget improvements to motion quality while maintaining the style from the\nstatic image. With inference optimizations, our model is able to generate an\neight-frame video with high-quality, interesting, and relevant motion in under\none second.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-08T22:49:32Z",
    "updated": "2024-02-08T22:49:32Z",
    "doi": null
  },
  "2107.01358": {
    "id": "http://arxiv.org/abs/2107.01358v1",
    "title": "CInC Flow: Characterizable Invertible 3x3 Convolution",
    "authors": [
      "Sandeep Nagar",
      "Marius Dufraisse",
      "Girish Varma"
    ],
    "abstract": "  Normalizing flows are an essential alternative to GANs for generative\nmodelling, which can be optimized directly on the maximum likelihood of the\ndataset. They also allow computation of the exact latent vector corresponding\nto an image since they are composed of invertible transformations. However, the\nrequirement of invertibility of the transformation prevents standard and\nexpressive neural network models such as CNNs from being directly used.\nEmergent convolutions were proposed to construct an invertible 3$\\times$3 CNN\nlayer using a pair of masked CNN layers, making them inefficient. We study\nconditions such that 3$\\times$3 CNNs are invertible, allowing them to construct\nexpressive normalizing flows. We derive necessary and sufficient conditions on\na padded CNN for it to be invertible. Our conditions for invertibility are\nsimple, can easily be maintained during the training process. Since we require\nonly a single CNN layer for every effective invertible CNN layer, our approach\nis more efficient than emerging convolutions. We also proposed a coupling\nmethod, Quad-coupling. We benchmark our approach and show similar performance\nresults to emergent convolutions while improving the model's efficiency.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-07-03T06:55:24Z",
    "updated": "2021-07-03T06:55:24Z",
    "doi": null
  },
  "2403.07379": {
    "id": "http://arxiv.org/abs/2403.07379v2",
    "title": "Hallmarks of Optimization Trajectories in Neural Networks: Directional\n  Exploration and Redundancy",
    "authors": [
      "Sidak Pal Singh",
      "Bobby He",
      "Thomas Hofmann",
      "Bernhard Sch\u00f6lkopf"
    ],
    "abstract": "  We propose a fresh take on understanding the mechanisms of neural networks by\nanalyzing the rich directional structure of optimization trajectories,\nrepresented by their pointwise parameters. Towards this end, we introduce some\nnatural notions of the complexity of optimization trajectories, both\nqualitative and quantitative, which hallmark the directional nature of\noptimization in neural networks: when is there redundancy, and when\nexploration. We use them to reveal the inherent nuance and interplay involved\nbetween various optimization choices, such as momentum and weight decay.\nFurther, the trajectory perspective helps us see the effect of scale on\nregularizing the directional nature of trajectories, and as a by-product, we\nalso observe an intriguing heterogeneity of Q,K,V dynamics in the middle\nattention layers in LLMs and which is homogenized by scale. Importantly, we put\nthe significant directional redundancy observed to the test by demonstrating\nthat training only scalar batchnorm parameters some while into training matches\nthe performance of training the entire network, which thus exhibits the\npotential of hybrid optimization schemes that are geared towards efficiency.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-12T07:32:47Z",
    "updated": "2024-06-24T04:53:34Z",
    "doi": null
  },
  "1803.05268": {
    "id": "http://arxiv.org/abs/1803.05268v2",
    "title": "Transparency by Design: Closing the Gap Between Performance and\n  Interpretability in Visual Reasoning",
    "authors": [
      "David Mascharka",
      "Philip Tran",
      "Ryan Soklaski",
      "Arjun Majumdar"
    ],
    "abstract": "  Visual question answering requires high-order reasoning about an image, which\nis a fundamental capability needed by machine systems to follow complex\ndirectives. Recently, modular networks have been shown to be an effective\nframework for performing visual reasoning tasks. While modular networks were\ninitially designed with a degree of model transparency, their performance on\ncomplex visual reasoning benchmarks was lacking. Current state-of-the-art\napproaches do not provide an effective mechanism for understanding the\nreasoning process. In this paper, we close the performance gap between\ninterpretable models and state-of-the-art visual reasoning methods. We propose\na set of visual-reasoning primitives which, when composed, manifest as a model\ncapable of performing complex reasoning tasks in an explicitly-interpretable\nmanner. The fidelity and interpretability of the primitives' outputs enable an\nunparalleled ability to diagnose the strengths and weaknesses of the resulting\nmodel. Critically, we show that these primitives are highly performant,\nachieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show\nthat our model is able to effectively learn generalized representations when\nprovided a small amount of data containing novel object attributes. Using the\nCoGenT generalization task, we show more than a 20 percentage point improvement\nover the current state of the art.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-03-14T13:33:06Z",
    "updated": "2018-07-02T18:48:31Z",
    "doi": "10.1109/CVPR.2018.00519"
  },
  "1708.07747": {
    "id": "http://arxiv.org/abs/1708.07747v2",
    "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n  Algorithms",
    "authors": [
      "Han Xiao",
      "Kashif Rasul",
      "Roland Vollgraf"
    ],
    "abstract": "  We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images\nof 70,000 fashion products from 10 categories, with 7,000 images per category.\nThe training set has 60,000 images and the test set has 10,000 images.\nFashion-MNIST is intended to serve as a direct drop-in replacement for the\noriginal MNIST dataset for benchmarking machine learning algorithms, as it\nshares the same image size, data format and the structure of training and\ntesting splits. The dataset is freely available at\nhttps://github.com/zalandoresearch/fashion-mnist\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-08-25T14:01:29Z",
    "updated": "2017-09-15T21:29:49Z",
    "doi": null
  },
  "1606.09549": {
    "id": "http://arxiv.org/abs/1606.09549v3",
    "title": "Fully-Convolutional Siamese Networks for Object Tracking",
    "authors": [
      "Luca Bertinetto",
      "Jack Valmadre",
      "Jo\u00e3o F. Henriques",
      "Andrea Vedaldi",
      "Philip H. S. Torr"
    ],
    "abstract": "  The problem of arbitrary object tracking has traditionally been tackled by\nlearning a model of the object's appearance exclusively online, using as sole\ntraining data the video itself. Despite the success of these methods, their\nonline-only approach inherently limits the richness of the model they can\nlearn. Recently, several attempts have been made to exploit the expressive\npower of deep convolutional networks. However, when the object to track is not\nknown beforehand, it is necessary to perform Stochastic Gradient Descent online\nto adapt the weights of the network, severely compromising the speed of the\nsystem. In this paper we equip a basic tracking algorithm with a novel\nfully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset\nfor object detection in video. Our tracker operates at frame-rates beyond\nreal-time and, despite its extreme simplicity, achieves state-of-the-art\nperformance in multiple benchmarks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-06-30T16:00:43Z",
    "updated": "2021-12-01T19:21:43Z",
    "doi": null
  },
  "2303.17076": {
    "id": "http://arxiv.org/abs/2303.17076v1",
    "title": "DiffCollage: Parallel Generation of Large Content with Diffusion Models",
    "authors": [
      "Qinsheng Zhang",
      "Jiaming Song",
      "Xun Huang",
      "Yongxin Chen",
      "Ming-Yu Liu"
    ],
    "abstract": "  We present DiffCollage, a compositional diffusion model that can generate\nlarge content by leveraging diffusion models trained on generating pieces of\nthe large content. Our approach is based on a factor graph representation where\neach factor node represents a portion of the content and a variable node\nrepresents their overlap. This representation allows us to aggregate\nintermediate outputs from diffusion models defined on individual nodes to\ngenerate content of arbitrary size and shape in parallel without resorting to\nan autoregressive generation procedure. We apply DiffCollage to various tasks,\nincluding infinite image generation, panorama image generation, and\nlong-duration text-guided motion generation. Extensive experimental results\nwith a comparison to strong autoregressive baselines verify the effectiveness\nof our approach.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-30T00:51:12Z",
    "updated": "2023-03-30T00:51:12Z",
    "doi": null
  },
  "2205.11423": {
    "id": "http://arxiv.org/abs/2205.11423v1",
    "title": "Decoder Denoising Pretraining for Semantic Segmentation",
    "authors": [
      "Emmanuel Brempong Asiedu",
      "Simon Kornblith",
      "Ting Chen",
      "Niki Parmar",
      "Matthias Minderer",
      "Mohammad Norouzi"
    ],
    "abstract": "  Semantic segmentation labels are expensive and time consuming to acquire.\nHence, pretraining is commonly used to improve the label-efficiency of\nsegmentation models. Typically, the encoder of a segmentation model is\npretrained as a classifier and the decoder is randomly initialized. Here, we\nargue that random initialization of the decoder can be suboptimal, especially\nwhen few labeled examples are available. We propose a decoder pretraining\napproach based on denoising, which can be combined with supervised pretraining\nof the encoder. We find that decoder denoising pretraining on the ImageNet\ndataset strongly outperforms encoder-only supervised pretraining. Despite its\nsimplicity, decoder denoising pretraining achieves state-of-the-art results on\nlabel-efficient semantic segmentation and offers considerable gains on the\nCityscapes, Pascal Context, and ADE20K datasets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.4.6; I.5.4; I.2.10",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-23T16:08:31Z",
    "updated": "2022-05-23T16:08:31Z",
    "doi": null
  },
  "1603.08575": {
    "id": "http://arxiv.org/abs/1603.08575v3",
    "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
    "authors": [
      "S. M. Ali Eslami",
      "Nicolas Heess",
      "Theophane Weber",
      "Yuval Tassa",
      "David Szepesvari",
      "Koray Kavukcuoglu",
      "Geoffrey E. Hinton"
    ],
    "abstract": "  We present a framework for efficient inference in structured image models\nthat explicitly reason about objects. We achieve this by performing\nprobabilistic inference using a recurrent neural network that attends to scene\nelements and processes them one at a time. Crucially, the model itself learns\nto choose the appropriate number of inference steps. We use this scheme to\nlearn to perform inference in partially specified 2D models (variable-sized\nvariational auto-encoders) and fully specified 3D models (probabilistic\nrenderers). We show that such models learn to identify multiple objects -\ncounting, locating and classifying the elements of a scene - without any\nsupervision, e.g., decomposing 3D images with various numbers of objects in a\nsingle forward pass of a neural network. We further show that the networks\nproduce accurate inferences when compared to supervised counterparts, and that\ntheir structure leads to improved generalization.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-03-28T21:59:08Z",
    "updated": "2016-08-12T16:05:08Z",
    "doi": null
  },
  "2111.10701": {
    "id": "http://arxiv.org/abs/2111.10701v1",
    "title": "Self-Supervised Point Cloud Completion via Inpainting",
    "authors": [
      "Himangi Mittal",
      "Brian Okorn",
      "Arpit Jangid",
      "David Held"
    ],
    "abstract": "  When navigating in urban environments, many of the objects that need to be\ntracked and avoided are heavily occluded. Planning and tracking using these\npartial scans can be challenging. The aim of this work is to learn to complete\nthese partial point clouds, giving us a full understanding of the object's\ngeometry using only partial observations. Previous methods achieve this with\nthe help of complete, ground-truth annotations of the target objects, which are\navailable only for simulated datasets. However, such ground truth is\nunavailable for real-world LiDAR data. In this work, we present a\nself-supervised point cloud completion algorithm, PointPnCNet, which is trained\nonly on partial scans without assuming access to complete, ground-truth\nannotations. Our method achieves this via inpainting. We remove a portion of\nthe input data and train the network to complete the missing region. As it is\ndifficult to determine which regions were occluded in the initial cloud and\nwhich were synthetically removed, our network learns to complete the full\ncloud, including the missing regions in the initial partial cloud. We show that\nour method outperforms previous unsupervised and weakly-supervised methods on\nboth the synthetic dataset, ShapeNet, and real-world LiDAR dataset, Semantic\nKITTI.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-21T00:03:11Z",
    "updated": "2021-11-21T00:03:11Z",
    "doi": null
  },
  "2309.03895": {
    "id": "http://arxiv.org/abs/2309.03895v1",
    "title": "InstructDiffusion: A Generalist Modeling Interface for Vision Tasks",
    "authors": [
      "Zigang Geng",
      "Binxin Yang",
      "Tiankai Hang",
      "Chen Li",
      "Shuyang Gu",
      "Ting Zhang",
      "Jianmin Bao",
      "Zheng Zhang",
      "Han Hu",
      "Dong Chen",
      "Baining Guo"
    ],
    "abstract": "  We present InstructDiffusion, a unifying and generic framework for aligning\ncomputer vision tasks with human instructions. Unlike existing approaches that\nintegrate prior knowledge and pre-define the output space (e.g., categories and\ncoordinates) for each vision task, we cast diverse vision tasks into a\nhuman-intuitive image-manipulating process whose output space is a flexible and\ninteractive pixel space. Concretely, the model is built upon the diffusion\nprocess and is trained to predict pixels according to user instructions, such\nas encircling the man's left shoulder in red or applying a blue mask to the\nleft car. InstructDiffusion could handle a variety of vision tasks, including\nunderstanding tasks (such as segmentation and keypoint detection) and\ngenerative tasks (such as editing and enhancement). It even exhibits the\nability to handle unseen tasks and outperforms prior methods on novel datasets.\nThis represents a significant step towards a generalist modeling interface for\nvision tasks, advancing artificial general intelligence in the field of\ncomputer vision.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-07T17:56:57Z",
    "updated": "2023-09-07T17:56:57Z",
    "doi": null
  },
  "2305.06500": {
    "id": "http://arxiv.org/abs/2305.06500v2",
    "title": "InstructBLIP: Towards General-purpose Vision-Language Models with\n  Instruction Tuning",
    "authors": [
      "Wenliang Dai",
      "Junnan Li",
      "Dongxu Li",
      "Anthony Meng Huat Tiong",
      "Junqi Zhao",
      "Weisheng Wang",
      "Boyang Li",
      "Pascale Fung",
      "Steven Hoi"
    ],
    "abstract": "  Large-scale pre-training and instruction tuning have been successful at\ncreating general-purpose language models with broad competence. However,\nbuilding general-purpose vision-language models is challenging due to the rich\ninput distributions and task diversity resulting from the additional visual\ninput. Although vision-language pretraining has been widely studied,\nvision-language instruction tuning remains under-explored. In this paper, we\nconduct a systematic and comprehensive study on vision-language instruction\ntuning based on the pretrained BLIP-2 models. We gather 26 publicly available\ndatasets, covering a wide variety of tasks and capabilities, and transform them\ninto instruction tuning format. Additionally, we introduce an instruction-aware\nQuery Transformer, which extracts informative features tailored to the given\ninstruction. Trained on 13 held-in datasets, InstructBLIP attains\nstate-of-the-art zero-shot performance across all 13 held-out datasets,\nsubstantially outperforming BLIP-2 and larger Flamingo models. Our models also\nlead to state-of-the-art performance when finetuned on individual downstream\ntasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts).\nFurthermore, we qualitatively demonstrate the advantages of InstructBLIP over\nconcurrent multimodal models. All InstructBLIP models are open-sourced at\nhttps://github.com/salesforce/LAVIS/tree/main/projects/instructblip.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-11T00:38:10Z",
    "updated": "2023-06-15T08:00:18Z",
    "doi": null
  },
  "2005.08575": {
    "id": "http://arxiv.org/abs/2005.08575v5",
    "title": "Audio ALBERT: A Lite BERT for Self-supervised Learning of Audio\n  Representation",
    "authors": [
      "Po-Han Chi",
      "Pei-Hung Chung",
      "Tsung-Han Wu",
      "Chun-Cheng Hsieh",
      "Yen-Hao Chen",
      "Shang-Wen Li",
      "Hung-yi Lee"
    ],
    "abstract": "  For self-supervised speech processing, it is crucial to use pretrained models\nas speech representation extractors. In recent works, increasing the size of\nthe model has been utilized in acoustic model training in order to achieve\nbetter performance. In this paper, we propose Audio ALBERT, a lite version of\nthe self-supervised speech representation model. We use the representations\nwith two downstream tasks, speaker identification, and phoneme classification.\nWe show that Audio ALBERT is capable of achieving competitive performance with\nthose huge models in the downstream tasks while utilizing 91\\% fewer\nparameters. Moreover, we use some simple probing models to measure how much the\ninformation of the speaker and phoneme is encoded in latent representations. In\nprobing experiments, we find that the latent representations encode richer\ninformation of both phoneme and speaker than that of the last layer.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-05-18T10:42:44Z",
    "updated": "2021-05-03T09:33:31Z",
    "doi": null
  },
  "1509.06825": {
    "id": "http://arxiv.org/abs/1509.06825v1",
    "title": "Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700\n  Robot Hours",
    "authors": [
      "Lerrel Pinto",
      "Abhinav Gupta"
    ],
    "abstract": "  Current learning-based robot grasping approaches exploit human-labeled\ndatasets for training the models. However, there are two problems with such a\nmethodology: (a) since each object can be grasped in multiple ways, manually\nlabeling grasp locations is not a trivial task; (b) human labeling is biased by\nsemantics. While there have been attempts to train robots using trial-and-error\nexperiments, the amount of data used in such experiments remains substantially\nlow and hence makes the learner prone to over-fitting. In this paper, we take\nthe leap of increasing the available training data to 40 times more than prior\nwork, leading to a dataset size of 50K data points collected over 700 hours of\nrobot grasping attempts. This allows us to train a Convolutional Neural Network\n(CNN) for the task of predicting grasp locations without severe overfitting. In\nour formulation, we recast the regression problem to an 18-way binary\nclassification over image patches. We also present a multi-stage learning\napproach where a CNN trained in one stage is used to collect hard negatives in\nsubsequent stages. Our experiments clearly show the benefit of using\nlarge-scale datasets (and multi-stage training) for the task of grasping. We\nalso compare to several baselines and show state-of-the-art performance on\ngeneralization to unseen objects for grasping.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2015-09-23T02:08:02Z",
    "updated": "2015-09-23T02:08:02Z",
    "doi": null
  },
  "2406.04324": {
    "id": "http://arxiv.org/abs/2406.04324v1",
    "title": "SF-V: Single Forward Video Generation Model",
    "authors": [
      "Zhixing Zhang",
      "Yanyu Li",
      "Yushu Wu",
      "Yanwu Xu",
      "Anil Kag",
      "Ivan Skorokhodov",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Junli Cao",
      "Dimitris Metaxas",
      "Sergey Tulyakov",
      "Jian Ren"
    ],
    "abstract": "  Diffusion-based video generation models have demonstrated remarkable success\nin obtaining high-fidelity videos through the iterative denoising process.\nHowever, these models require multiple denoising steps during sampling,\nresulting in high computational costs. In this work, we propose a novel\napproach to obtain single-step video generation models by leveraging\nadversarial training to fine-tune pre-trained video diffusion models. We show\nthat, through the adversarial training, the multi-steps video diffusion model,\ni.e., Stable Video Diffusion (SVD), can be trained to perform single forward\npass to synthesize high-quality videos, capturing both temporal and spatial\ndependencies in the video data. Extensive experiments demonstrate that our\nmethod achieves competitive generation quality of synthesized videos with\nsignificantly reduced computational overhead for the denoising process (i.e.,\naround $23\\times$ speedup compared with SVD and $6\\times$ speedup compared with\nexisting works, with even better generation quality), paving the way for\nreal-time video synthesis and editing. More visualization results are made\npublicly available at https://snap-research.github.io/SF-V.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-06T17:58:27Z",
    "updated": "2024-06-06T17:58:27Z",
    "doi": null
  },
  "2305.06161": {
    "id": "http://arxiv.org/abs/2305.06161v2",
    "title": "StarCoder: may the source be with you!",
    "authors": [
      "Raymond Li",
      "Loubna Ben Allal",
      "Yangtian Zi",
      "Niklas Muennighoff",
      "Denis Kocetkov",
      "Chenghao Mou",
      "Marc Marone",
      "Christopher Akiki",
      "Jia Li",
      "Jenny Chim",
      "Qian Liu",
      "Evgenii Zheltonozhskii",
      "Terry Yue Zhuo",
      "Thomas Wang",
      "Olivier Dehaene",
      "Mishig Davaadorj",
      "Joel Lamy-Poirier",
      "Jo\u00e3o Monteiro",
      "Oleh Shliazhko",
      "Nicolas Gontier",
      "Nicholas Meade",
      "Armel Zebaze",
      "Ming-Ho Yee",
      "Logesh Kumar Umapathi",
      "Jian Zhu",
      "Benjamin Lipkin",
      "Muhtasham Oblokulov",
      "Zhiruo Wang",
      "Rudra Murthy",
      "Jason Stillerman",
      "Siva Sankalp Patel",
      "Dmitry Abulkhanov",
      "Marco Zocca",
      "Manan Dey",
      "Zhihan Zhang",
      "Nour Fahmy",
      "Urvashi Bhattacharyya",
      "Wenhao Yu",
      "Swayam Singh",
      "Sasha Luccioni",
      "Paulo Villegas",
      "Maxim Kunakov",
      "Fedor Zhdanov",
      "Manuel Romero",
      "Tony Lee",
      "Nadav Timor",
      "Jennifer Ding",
      "Claire Schlesinger",
      "Hailey Schoelkopf",
      "Jan Ebert",
      "Tri Dao",
      "Mayank Mishra",
      "Alex Gu",
      "Jennifer Robinson",
      "Carolyn Jane Anderson",
      "Brendan Dolan-Gavitt",
      "Danish Contractor",
      "Siva Reddy",
      "Daniel Fried",
      "Dzmitry Bahdanau",
      "Yacine Jernite",
      "Carlos Mu\u00f1oz Ferrandis",
      "Sean Hughes",
      "Thomas Wolf",
      "Arjun Guha",
      "Leandro von Werra",
      "Harm de Vries"
    ],
    "abstract": "  The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-09T08:16:42Z",
    "updated": "2023-12-13T14:44:10Z",
    "doi": null
  },
  "2210.14868": {
    "id": "http://arxiv.org/abs/2210.14868v3",
    "title": "Multi-lingual Evaluation of Code Generation Models",
    "authors": [
      "Ben Athiwaratkun",
      "Sanjay Krishna Gouda",
      "Zijian Wang",
      "Xiaopeng Li",
      "Yuchen Tian",
      "Ming Tan",
      "Wasi Uddin Ahmad",
      "Shiqi Wang",
      "Qing Sun",
      "Mingyue Shang",
      "Sujan Kumar Gonugondla",
      "Hantian Ding",
      "Varun Kumar",
      "Nathan Fulton",
      "Arash Farahani",
      "Siddhartha Jain",
      "Robert Giaquinto",
      "Haifeng Qian",
      "Murali Krishna Ramanathan",
      "Ramesh Nallapati",
      "Baishakhi Ray",
      "Parminder Bhatia",
      "Sudipta Sengupta",
      "Dan Roth",
      "Bing Xiang"
    ],
    "abstract": "  We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-26T17:17:06Z",
    "updated": "2023-03-28T19:02:34Z",
    "doi": null
  },
  "2408.12340": {
    "id": "http://arxiv.org/abs/2408.12340v2",
    "title": "VTON-HandFit: Virtual Try-on for Arbitrary Hand Pose Guided by Hand\n  Priors Embedding",
    "authors": [
      "Yujie Liang",
      "Xiaobin Hu",
      "Boyuan Jiang",
      "Donghao Luo",
      "Kai WU",
      "Wenhui Han",
      "Taisong Jin",
      "Chengjie Wang"
    ],
    "abstract": "  Although diffusion-based image virtual try-on has made considerable progress,\nemerging approaches still struggle to effectively address the issue of hand\nocclusion (i.e., clothing regions occluded by the hand part), leading to a\nnotable degradation of the try-on performance. To tackle this issue widely\nexisting in real-world scenarios, we propose VTON-HandFit, leveraging the power\nof hand priors to reconstruct the appearance and structure for hand occlusion\ncases. Firstly, we tailor a Handpose Aggregation Net using the ControlNet-based\nstructure explicitly and adaptively encoding the global hand and pose priors.\nBesides, to fully exploit the hand-related structure and appearance\ninformation, we propose Hand-feature Disentanglement Embedding module to\ndisentangle the hand priors into the hand structure-parametric and\nvisual-appearance features, and customize a masked cross attention for further\ndecoupled feature embedding. Lastly, we customize a hand-canny constraint loss\nto better learn the structure edge knowledge from the hand template of model\nimage. VTON-HandFit outperforms the baselines in qualitative and quantitative\nevaluations on the public dataset and our self-collected hand-occlusion\nHandfit-3K dataset particularly for the arbitrary hand pose occlusion cases in\nreal-world scenarios. The Code and dataset will be available at\n\\url{https://github.com/VTON-HandFit/VTON-HandFit}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-22T12:36:10Z",
    "updated": "2024-08-27T02:53:37Z",
    "doi": null
  },
  "2406.04806": {
    "id": "http://arxiv.org/abs/2406.04806v4",
    "title": "Streaming Diffusion Policy: Fast Policy Synthesis with Variable Noise\n  Diffusion Models",
    "authors": [
      "Sigmund H. H\u00f8eg",
      "Yilun Du",
      "Olav Egeland"
    ],
    "abstract": "  Diffusion models have seen rapid adoption in robotic imitation learning,\nenabling autonomous execution of complex dexterous tasks. However, action\nsynthesis is often slow, requiring many steps of iterative denoising, limiting\nthe extent to which models can be used in tasks that require fast reactive\npolicies. To sidestep this, recent works have explored how the distillation of\nthe diffusion process can be used to accelerate policy synthesis. However,\ndistillation is computationally expensive and can hurt both the accuracy and\ndiversity of synthesized actions. We propose SDP (Streaming Diffusion Policy),\nan alternative method to accelerate policy synthesis, leveraging the insight\nthat generating a partially denoised action trajectory is substantially faster\nthan a full output action trajectory. At each observation, our approach outputs\na partially denoised action trajectory with variable levels of noise\ncorruption, where the immediate action to execute is noise-free, with\nsubsequent actions having increasing levels of noise and uncertainty. The\npartially denoised action trajectory for a new observation can then be quickly\ngenerated by applying a few steps of denoising to the previously predicted\nnoisy action trajectory (rolled over by one timestep). We illustrate the\nefficacy of this approach, dramatically speeding up policy synthesis while\npreserving performance across both simulated and real-world settings.\n",
    "categories": [
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-07T10:13:44Z",
    "updated": "2024-10-11T16:04:49Z",
    "doi": null
  },
  "2408.12483": {
    "id": "http://arxiv.org/abs/2408.12483v1",
    "title": "Not All Samples Should Be Utilized Equally: Towards Understanding and\n  Improving Dataset Distillation",
    "authors": [
      "Shaobo Wang",
      "Yantai Yang",
      "Qilong Wang",
      "Kaixin Li",
      "Linfeng Zhang",
      "Junchi Yan"
    ],
    "abstract": "  Dataset Distillation (DD) aims to synthesize a small dataset capable of\nperforming comparably to the original dataset. Despite the success of numerous\nDD methods, theoretical exploration of this area remains unaddressed. In this\npaper, we take an initial step towards understanding various matching-based DD\nmethods from the perspective of sample difficulty. We begin by empirically\nexamining sample difficulty, measured by gradient norm, and observe that\ndifferent matching-based methods roughly correspond to specific difficulty\ntendencies. We then extend the neural scaling laws of data pruning to DD to\ntheoretically explain these matching-based methods. Our findings suggest that\nprioritizing the synthesis of easier samples from the original dataset can\nenhance the quality of distilled datasets, especially in low IPC\n(image-per-class) settings. Based on our empirical observations and theoretical\nanalysis, we introduce the Sample Difficulty Correction (SDC) approach,\ndesigned to predominantly generate easier samples to achieve higher dataset\nquality. Our SDC can be seamlessly integrated into existing methods as a plugin\nwith minimal code adjustments. Experimental results demonstrate that adding SDC\ngenerates higher-quality distilled datasets across 7 distillation methods and 6\ndatasets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-22T15:20:32Z",
    "updated": "2024-08-22T15:20:32Z",
    "doi": null
  },
  "2306.02741": {
    "id": "http://arxiv.org/abs/2306.02741v1",
    "title": "ZIGNeRF: Zero-shot 3D Scene Representation with Invertible Generative\n  Neural Radiance Fields",
    "authors": [
      "Kanghyeok Ko",
      "Minhyeok Lee"
    ],
    "abstract": "  Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable\nproficiency in synthesizing multi-view images by learning the distribution of a\nset of unposed images. Despite the aptitude of existing generative NeRFs in\ngenerating 3D-consistent high-quality random samples within data distribution,\nthe creation of a 3D representation of a singular input image remains a\nformidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative\nmodel that executes zero-shot Generative Adversarial Network (GAN) inversion\nfor the generation of multi-view images from a single out-of-domain image. The\nmodel is underpinned by a novel inverter that maps out-of-domain images into\nthe latent code of the generator manifold. Notably, ZIGNeRF is capable of\ndisentangling the object from the background and executing 3D operations such\nas 360-degree rotation or depth and horizontal translation. The efficacy of our\nmodel is validated using multiple real-image datasets: Cats, AFHQ, CelebA,\nCelebA-HQ, and CompCars.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-05T09:41:51Z",
    "updated": "2023-06-05T09:41:51Z",
    "doi": null
  },
  "2307.00522": {
    "id": "http://arxiv.org/abs/2307.00522v1",
    "title": "LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance",
    "authors": [
      "Linoy Tsaban",
      "Apolin\u00e1rio Passos"
    ],
    "abstract": "  Recent large-scale text-guided diffusion models provide powerful\nimage-generation capabilities. Currently, a significant effort is given to\nenable the modification of these images using text only as means to offer\nintuitive and versatile editing. However, editing proves to be difficult for\nthese generative models due to the inherent nature of editing techniques, which\ninvolves preserving certain content from the original image. Conversely, in\ntext-based models, even minor modifications to the text prompt frequently\nresult in an entirely distinct result, making attaining one-shot generation\nthat accurately corresponds to the users intent exceedingly challenging. In\naddition, to edit a real image using these state-of-the-art tools, one must\nfirst invert the image into the pre-trained models domain - adding another\nfactor affecting the edit quality, as well as latency. In this exploratory\nreport, we propose LEDITS - a combined lightweight approach for real-image\nediting, incorporating the Edit Friendly DDPM inversion technique with Semantic\nGuidance, thus extending Semantic Guidance to real image editing, while\nharnessing the editing capabilities of DDPM inversion as well. This approach\nachieves versatile edits, both subtle and extensive as well as alterations in\ncomposition and style, while requiring no optimization nor extensions to the\narchitecture.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-02T09:11:09Z",
    "updated": "2023-07-02T09:11:09Z",
    "doi": null
  },
  "2404.17774": {
    "id": "http://arxiv.org/abs/2404.17774v2",
    "title": "High-quality Surface Reconstruction using Gaussian Surfels",
    "authors": [
      "Pinxuan Dai",
      "Jiamin Xu",
      "Wenxiang Xie",
      "Xinguo Liu",
      "Huamin Wang",
      "Weiwei Xu"
    ],
    "abstract": "  We propose a novel point-based representation, Gaussian surfels, to combine\nthe advantages of the flexible optimization procedure in 3D Gaussian points and\nthe surface alignment property of surfels. This is achieved by directly setting\nthe z-scale of 3D Gaussian points to 0, effectively flattening the original 3D\nellipsoid into a 2D ellipse. Such a design provides clear guidance to the\noptimizer. By treating the local z-axis as the normal direction, it greatly\nimproves optimization stability and surface alignment. While the derivatives to\nthe local z-axis computed from the covariance matrix are zero in this setting,\nwe design a self-supervised normal-depth consistency loss to remedy this issue.\nMonocular normal priors and foreground masks are incorporated to enhance the\nquality of the reconstruction, mitigating issues related to highlights and\nbackground. We propose a volumetric cutting method to aggregate the information\nof Gaussian surfels so as to remove erroneous points in depth maps generated by\nalpha blending. Finally, we apply screened Poisson reconstruction method to the\nfused depth maps to extract the surface mesh. Experimental results show that\nour method demonstrates superior performance in surface reconstruction compared\nto state-of-the-art neural volume rendering and point-based rendering methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-27T04:13:39Z",
    "updated": "2024-04-30T01:53:27Z",
    "doi": null
  },
  "2404.05674": {
    "id": "http://arxiv.org/abs/2404.05674v1",
    "title": "MoMA: Multimodal LLM Adapter for Fast Personalized Image Generation",
    "authors": [
      "Kunpeng Song",
      "Yizhe Zhu",
      "Bingchen Liu",
      "Qing Yan",
      "Ahmed Elgammal",
      "Xiao Yang"
    ],
    "abstract": "  In this paper, we present MoMA: an open-vocabulary, training-free\npersonalized image model that boasts flexible zero-shot capabilities. As\nfoundational text-to-image models rapidly evolve, the demand for robust\nimage-to-image translation grows. Addressing this need, MoMA specializes in\nsubject-driven personalized image generation. Utilizing an open-source,\nMultimodal Large Language Model (MLLM), we train MoMA to serve a dual role as\nboth a feature extractor and a generator. This approach effectively synergizes\nreference image and text prompt information to produce valuable image features,\nfacilitating an image diffusion model. To better leverage the generated\nfeatures, we further introduce a novel self-attention shortcut method that\nefficiently transfers image features to an image diffusion model, improving the\nresemblance of the target object in generated images. Remarkably, as a\ntuning-free plug-and-play module, our model requires only a single reference\nimage and outperforms existing methods in generating images with high detail\nfidelity, enhanced identity-preservation and prompt faithfulness. Our work is\nopen-source, thereby providing universal access to these advancements.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-08T16:55:49Z",
    "updated": "2024-04-08T16:55:49Z",
    "doi": null
  },
  "2108.06152": {
    "id": "http://arxiv.org/abs/2108.06152v3",
    "title": "Conditional DETR for Fast Training Convergence",
    "authors": [
      "Depu Meng",
      "Xiaokang Chen",
      "Zejia Fan",
      "Gang Zeng",
      "Houqiang Li",
      "Yuhui Yuan",
      "Lei Sun",
      "Jingdong Wang"
    ],
    "abstract": "  The recently-developed DETR approach applies the transformer encoder and\ndecoder architecture to object detection and achieves promising performance. In\nthis paper, we handle the critical issue, slow training convergence, and\npresent a conditional cross-attention mechanism for fast DETR training. Our\napproach is motivated by that the cross-attention in DETR relies highly on the\ncontent embeddings for localizing the four extremities and predicting the box,\nwhich increases the need for high-quality content embeddings and thus the\ntraining difficulty. Our approach, named conditional DETR, learns a conditional\nspatial query from the decoder embedding for decoder multi-head\ncross-attention. The benefit is that through the conditional spatial query,\neach cross-attention head is able to attend to a band containing a distinct\nregion, e.g., one object extremity or a region inside the object box. This\nnarrows down the spatial range for localizing the distinct regions for object\nclassification and box regression, thus relaxing the dependence on the content\nembeddings and easing the training. Empirical results show that conditional\nDETR converges 6.7x faster for the backbones R50 and R101 and 10x faster for\nstronger backbones DC5-R50 and DC5-R101. Code is available at\nhttps://github.com/Atten4Vis/ConditionalDETR.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-08-13T10:07:46Z",
    "updated": "2023-09-29T13:21:57Z",
    "doi": null
  },
  "2207.10075": {
    "id": "http://arxiv.org/abs/2207.10075v2",
    "title": "Is an Object-Centric Video Representation Beneficial for Transfer?",
    "authors": [
      "Chuhan Zhang",
      "Ankush Gupta",
      "Andrew Zisserman"
    ],
    "abstract": "  The objective of this work is to learn an object-centric video\nrepresentation, with the aim of improving transferability to novel tasks, i.e.,\ntasks different from the pre-training task of action classification. To this\nend, we introduce a new object-centric video recognition model based on a\ntransformer architecture. The model learns a set of object-centric summary\nvectors for the video, and uses these vectors to fuse the visual and\nspatio-temporal trajectory 'modalities' of the video clip. We also introduce a\nnovel trajectory contrast loss to further enhance objectness in these summary\nvectors. With experiments on four datasets -- SomethingSomething-V2,\nSomethingElse, Action Genome and EpicKitchens -- we show that the\nobject-centric model outperforms prior video representations (both\nobject-agnostic and object-aware), when: (1) classifying actions on unseen\nobjects and unseen environments; (2) low-shot learning of novel classes; (3)\nlinear probe to other downstream tasks; as well as (4) for standard action\nclassification.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-20T17:59:44Z",
    "updated": "2022-10-08T18:19:42Z",
    "doi": null
  },
  "2210.04802": {
    "id": "http://arxiv.org/abs/2210.04802v2",
    "title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in\n  Fine-tuned Source Code Models",
    "authors": [
      "Hossein Hajipour",
      "Ning Yu",
      "Cristian-Alexandru Staicu",
      "Mario Fritz"
    ],
    "abstract": "  Large code datasets have become increasingly accessible for pre-training\nsource code models. However, for the fine-tuning phase, obtaining\nrepresentative training data that fully covers the code distribution for\nspecific downstream tasks remains challenging due to the task-specific nature\nand limited labeling resources. Moreover, fine-tuning pretrained models can\nresult in forgetting previously acquired pre-training knowledge. These lead to\nout-of-distribution (OOD) generalization issues with unexpected model inference\nbehaviors that have not been systematically studied yet. In this paper, we\ncontribute the first systematic approach that simulates various OOD scenarios\nalong different dimensions of source code data properties and study the\nfine-tuned model behaviors in such scenarios. We investigate the behaviors of\nmodels under different fine-tuning methodologies, including full fine-tuning\nand Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis,\nconducted on four state-of-the-art pretrained models and applied to two code\ngeneration tasks, exposes multiple failure modes attributed to OOD\ngeneralization issues. Additionally, our analysis uncovers that LoRA\nfine-tuning consistently exhibits significantly better OOD generalization\nperformance than full fine-tuning across various scenarios.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-10T16:07:24Z",
    "updated": "2023-10-30T14:25:57Z",
    "doi": null
  },
  "2110.09408": {
    "id": "http://arxiv.org/abs/2110.09408v3",
    "title": "HRFormer: High-Resolution Transformer for Dense Prediction",
    "authors": [
      "Yuhui Yuan",
      "Rao Fu",
      "Lang Huang",
      "Weihong Lin",
      "Chao Zhang",
      "Xilin Chen",
      "Jingdong Wang"
    ],
    "abstract": "  We present a High-Resolution Transformer (HRFormer) that learns\nhigh-resolution representations for dense prediction tasks, in contrast to the\noriginal Vision Transformer that produces low-resolution representations and\nhas high memory and computational cost. We take advantage of the\nmulti-resolution parallel design introduced in high-resolution convolutional\nnetworks (HRNet), along with local-window self-attention that performs\nself-attention over small non-overlapping image windows, for improving the\nmemory and computation efficiency. In addition, we introduce a convolution into\nthe FFN to exchange information across the disconnected image windows. We\ndemonstrate the effectiveness of the High-Resolution Transformer on both human\npose estimation and semantic segmentation tasks, e.g., HRFormer outperforms\nSwin transformer by $1.3$ AP on COCO pose estimation with $50\\%$ fewer\nparameters and $30\\%$ fewer FLOPs. Code is available at:\nhttps://github.com/HRNet/HRFormer.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-18T15:37:58Z",
    "updated": "2021-11-07T14:39:41Z",
    "doi": null
  },
  "1907.03395": {
    "id": "http://arxiv.org/abs/1907.03395v2",
    "title": "Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and\n  Graph Attention Networks",
    "authors": [
      "Vineet Kosaraju",
      "Amir Sadeghian",
      "Roberto Mart\u00edn-Mart\u00edn",
      "Ian Reid",
      "S. Hamid Rezatofighi",
      "Silvio Savarese"
    ],
    "abstract": "  Predicting the future trajectories of multiple interacting agents in a scene\nhas become an increasingly important problem for many different applications\nranging from control of autonomous vehicles and social robots to security and\nsurveillance. This problem is compounded by the presence of social interactions\nbetween humans and their physical interactions with the scene. While the\nexisting literature has explored some of these cues, they mainly ignored the\nmultimodal nature of each human's future trajectory. In this paper, we present\nSocial-BiGAT, a graph-based generative adversarial network that generates\nrealistic, multimodal trajectory predictions by better modelling the social\ninteractions of pedestrians in a scene. Our method is based on a graph\nattention network (GAT) that learns reliable feature representations that\nencode the social interactions between humans in the scene, and a recurrent\nencoder-decoder architecture that is trained adversarially to predict, based on\nthe features, the humans' paths. We explicitly account for the multimodal\nnature of the prediction problem by forming a reversible transformation between\neach scene and its latent noise vector, as in Bicycle-GAN. We show that our\nframework achieves state-of-the-art performance comparing it to several\nbaselines on existing trajectory forecasting benchmarks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-07-04T23:48:07Z",
    "updated": "2019-07-17T01:05:26Z",
    "doi": null
  },
  "1809.03627": {
    "id": "http://arxiv.org/abs/1809.03627v2",
    "title": "ClusterGAN : Latent Space Clustering in Generative Adversarial Networks",
    "authors": [
      "Sudipto Mukherjee",
      "Himanshu Asnani",
      "Eugene Lin",
      "Sreeram Kannan"
    ],
    "abstract": "  Generative Adversarial networks (GANs) have obtained remarkable success in\nmany unsupervised learning tasks and unarguably, clustering is an important\nunsupervised learning problem. While one can potentially exploit the\nlatent-space back-projection in GANs to cluster, we demonstrate that the\ncluster structure is not retained in the GAN latent space.\n  In this paper, we propose ClusterGAN as a new mechanism for clustering using\nGANs. By sampling latent variables from a mixture of one-hot encoded variables\nand continuous latent variables, coupled with an inverse network (which\nprojects the data to the latent space) trained jointly with a clustering\nspecific loss, we are able to achieve clustering in the latent space. Our\nresults show a remarkable phenomenon that GANs can preserve latent space\ninterpolation across categories, even though the discriminator is never exposed\nto such vectors. We compare our results with various clustering baselines and\ndemonstrate superior performance on both synthetic and real datasets.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-09-10T23:00:37Z",
    "updated": "2019-01-26T23:28:35Z",
    "doi": null
  },
  "2101.00529": {
    "id": "http://arxiv.org/abs/2101.00529v2",
    "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
    "authors": [
      "Pengchuan Zhang",
      "Xiujun Li",
      "Xiaowei Hu",
      "Jianwei Yang",
      "Lei Zhang",
      "Lijuan Wang",
      "Yejin Choi",
      "Jianfeng Gao"
    ],
    "abstract": "  This paper presents a detailed study of improving visual representations for\nvision language (VL) tasks and develops an improved object detection model to\nprovide object-centric representations of images. Compared to the most widely\nused \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new\nmodel is bigger, better-designed for VL tasks, and pre-trained on much larger\ntraining corpora that combine multiple public annotated object detection\ndatasets. Therefore, it can generate representations of a richer collection of\nvisual objects and concepts. While previous VL research focuses mainly on\nimproving the vision-language fusion model and leaves the object detection\nmodel improvement untouched, we show that visual features matter significantly\nin VL models. In our experiments we feed the visual features generated by the\nnew object detection model into a Transformer-based VL fusion model \\oscar\n\\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the\nVL model and fine-tune it on a wide range of downstream VL tasks. Our results\nshow that the new visual features significantly improve the performance across\nall VL tasks, creating new state-of-the-art results on seven public benchmarks.\nWe will release the new object detection model to public.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-01-02T23:35:27Z",
    "updated": "2021-03-10T01:27:16Z",
    "doi": null
  },
  "2106.11251": {
    "id": "http://arxiv.org/abs/2106.11251v2",
    "title": "Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval",
    "authors": [
      "Xiao Wang",
      "Craig Macdonald",
      "Nicola Tonellotto",
      "Iadh Ounis"
    ],
    "abstract": "  Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,\nhave shown the usefulness of expanding and reweighting the users' initial\nqueries using information occurring in an initial set of retrieved documents,\nknown as the pseudo-relevant set. Recently, dense retrieval -- through the use\nof neural contextual language models such as BERT for analysing the documents'\nand queries' contents and computing their relevance scores -- has shown a\npromising performance on several information retrieval tasks still relying on\nthe traditional inverted index for identifying documents relevant to a query.\nTwo different dense retrieval families have emerged: the use of single embedded\nrepresentations for each passage and query (e.g. using BERT's [CLS] token), or\nvia multiple representations (e.g. using an embedding for each token of the\nquery and document). In this work, we conduct the first study into the\npotential for multiple representation dense retrieval to be enhanced using\npseudo-relevance feedback. In particular, based on the pseudo-relevant set of\ndocuments identified using a first-pass dense retrieval, we extract\nrepresentative feedback embeddings (using KMeans clustering) -- while ensuring\nthat these embeddings discriminate among passages (based on IDF) -- which are\nthen added to the query representation. These additional feedback embeddings\nare shown to both enhance the effectiveness of a reranking as well as an\nadditional dense retrieval operation. Indeed, experiments on the MSMARCO\npassage ranking dataset show that MAP can be improved by upto 26% on the TREC\n2019 query set and 10% on the TREC 2020 query set by the application of our\nproposed ColBERT-PRF method on a ColBERT dense retrieval approach.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-21T16:49:03Z",
    "updated": "2021-07-01T14:02:56Z",
    "doi": "10.1145/3471158.3472250"
  },
  "2105.09377": {
    "id": "http://arxiv.org/abs/2105.09377v1",
    "title": "Pure Tensor Program Rewriting via Access Patterns (Representation Pearl)",
    "authors": [
      "Gus Henry Smith",
      "Andrew Liu",
      "Steven Lyubomirsky",
      "Scott Davidson",
      "Joseph McMahan",
      "Michael Taylor",
      "Luis Ceze",
      "Zachary Tatlock"
    ],
    "abstract": "  Tensor kernels in machine learning (ML) often correspond to pure mathematical\nexpressions, making term rewriting an attractive strategy for optimization and\nmapping to specialized hardware accelerators. However, existing ML intermediate\nrepresentations (IRs) tend to either be \\textit{pure but high-level}, making\nlow-level rewrites to hardware targets inexpressible, or \\textit{low-level but\nimpure}, hampering the use of term rewriting altogether. This paper introduces\nGlenside, a pure IR whose core abstraction -- the \\textit{access pattern} --\nenables low-level, layout-aware, hardware-centric program rewrites.\n  We demonstrate how term rewriting in Glenside can be used to map program\nfragments to hardware accelerator invocations and automatically discover\nclassic data layout transformations like \\texttt{im2col}. Glenside establishes\na new foundation for exploring further term rewriting techniques in optimizing\nlow-level tensor programs.\n",
    "categories": [
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-05-19T19:56:44Z",
    "updated": "2021-05-19T19:56:44Z",
    "doi": "10.1145/3460945.3464953"
  },
  "2201.13433": {
    "id": "http://arxiv.org/abs/2201.13433v1",
    "title": "Third Time's the Charm? Image and Video Editing with StyleGAN3",
    "authors": [
      "Yuval Alaluf",
      "Or Patashnik",
      "Zongze Wu",
      "Asif Zamir",
      "Eli Shechtman",
      "Dani Lischinski",
      "Daniel Cohen-Or"
    ],
    "abstract": "  StyleGAN is arguably one of the most intriguing and well-studied generative\nmodels, demonstrating impressive performance in image generation, inversion,\nand manipulation. In this work, we explore the recent StyleGAN3 architecture,\ncompare it to its predecessor, and investigate its unique advantages, as well\nas drawbacks. In particular, we demonstrate that while StyleGAN3 can be trained\non unaligned data, one can still use aligned data for training, without\nhindering the ability to generate unaligned imagery. Next, our analysis of the\ndisentanglement of the different latent spaces of StyleGAN3 indicates that the\ncommonly used W/W+ spaces are more entangled than their StyleGAN2 counterparts,\nunderscoring the benefits of using the StyleSpace for fine-grained editing.\nConsidering image inversion, we observe that existing encoder-based techniques\nstruggle when trained on unaligned data. We therefore propose an encoding\nscheme trained solely on aligned data, yet can still invert unaligned images.\nFinally, we introduce a novel video inversion and editing workflow that\nleverages the capabilities of a fine-tuned StyleGAN3 generator to reduce\ntexture sticking and expand the field of view of the edited video.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-01-31T18:44:59Z",
    "updated": "2022-01-31T18:44:59Z",
    "doi": null
  },
  "2201.00112": {
    "id": "http://arxiv.org/abs/2201.00112v1",
    "title": "SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface\n  Discriminators",
    "authors": [
      "Andrew Luo",
      "Tianqin Li",
      "Wen-Hao Zhang",
      "Tai Sing Lee"
    ],
    "abstract": "  Recent advances in deep generative models have led to immense progress in 3D\nshape synthesis. While existing models are able to synthesize shapes\nrepresented as voxels, point-clouds, or implicit functions, these methods only\nindirectly enforce the plausibility of the final 3D shape surface. Here we\npresent a 3D shape synthesis framework (SurfGen) that directly applies\nadversarial training to the object surface. Our approach uses a differentiable\nspherical projection layer to capture and represent the explicit zero\nisosurface of an implicit 3D generator as functions defined on the unit sphere.\nBy processing the spherical representation of 3D object surfaces with a\nspherical CNN in an adversarial setting, our generator can better learn the\nstatistics of natural shape surfaces. We evaluate our model on large-scale\nshape datasets, and demonstrate that the end-to-end trained model is capable of\ngenerating high fidelity 3D shapes with diverse topology.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-01-01T04:44:42Z",
    "updated": "2022-01-01T04:44:42Z",
    "doi": null
  },
  "2305.13657": {
    "id": "http://arxiv.org/abs/2305.13657v1",
    "title": "ChatGPT as your Personal Data Scientist",
    "authors": [
      "Md Mahadi Hassan",
      "Alex Knipper",
      "Shubhra Kanti Karmaker Santu"
    ],
    "abstract": "  The rise of big data has amplified the need for efficient, user-friendly\nautomated machine learning (AutoML) tools. However, the intricacy of\nunderstanding domain-specific data and defining prediction tasks necessitates\nhuman intervention making the process time-consuming while preventing full\nautomation. Instead, envision an intelligent agent capable of assisting users\nin conducting AutoML tasks through intuitive, natural conversations without\nrequiring in-depth knowledge of the underlying machine learning (ML) processes.\nThis agent's key challenge is to accurately comprehend the user's prediction\ngoals and, consequently, formulate precise ML tasks, adjust data sets and model\nparameters accordingly, and articulate results effectively. In this paper, we\ntake a pioneering step towards this ambitious goal by introducing a\nChatGPT-based conversational data-science framework to act as a \"personal data\nscientist\". Precisely, we utilize Large Language Models (ChatGPT) to build a\nnatural interface between the users and the ML models (Scikit-Learn), which in\nturn, allows us to approach this ambitious problem with a realistic solution.\n  Our model pivots around four dialogue states: Data Visualization, Task\nFormulation, Prediction Engineering, and Result Summary and Recommendation.\nEach state marks a unique conversation phase, impacting the overall user-system\ninteraction. Multiple LLM instances, serving as \"micro-agents\", ensure a\ncohesive conversation flow, granting us granular control over the\nconversation's progression. In summary, we developed an end-to-end system that\nnot only proves the viability of the novel concept of conversational data\nscience but also underscores the potency of LLMs in solving complex tasks.\nInterestingly, its development spotlighted several critical weaknesses in the\ncurrent LLMs (ChatGPT) and highlighted substantial opportunities for\nimprovement.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-23T04:00:16Z",
    "updated": "2023-05-23T04:00:16Z",
    "doi": null
  },
  "1709.00513": {
    "id": "http://arxiv.org/abs/1709.00513v2",
    "title": "Training Shallow and Thin Networks for Acceleration via Knowledge\n  Distillation with Conditional Adversarial Networks",
    "authors": [
      "Zheng Xu",
      "Yen-Chang Hsu",
      "Jiawei Huang"
    ],
    "abstract": "  There is an increasing interest on accelerating neural networks for real-time\napplications. We study the student-teacher strategy, in which a small and fast\nstudent network is trained with the auxiliary information learned from a large\nand accurate teacher network. We propose to use conditional adversarial\nnetworks to learn the loss function to transfer knowledge from teacher to\nstudent. The proposed method is particularly effective for relatively small\nstudent networks. Moreover, experimental results show the effect of network\nsize when the modern networks are used as student. We empirically study the\ntrade-off between inference time and classification accuracy, and provide\nsuggestions on choosing a proper student network.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-09-02T01:03:08Z",
    "updated": "2018-04-16T18:42:13Z",
    "doi": null
  },
  "2209.06899": {
    "id": "http://arxiv.org/abs/2209.06899v1",
    "title": "Out of One, Many: Using Language Models to Simulate Human Samples",
    "authors": [
      "Lisa P. Argyle",
      "Ethan C. Busby",
      "Nancy Fulda",
      "Joshua Gubler",
      "Christopher Rytting",
      "David Wingate"
    ],
    "abstract": "  We propose and explore the possibility that language models can be studied as\neffective proxies for specific human sub-populations in social science\nresearch. Practical and research applications of artificial intelligence tools\nhave sometimes been limited by problematic biases (such as racism or sexism),\nwhich are often treated as uniform properties of the models. We show that the\n\"algorithmic bias\" within one such tool -- the GPT-3 language model -- is\ninstead both fine-grained and demographically correlated, meaning that proper\nconditioning will cause it to accurately emulate response distributions from a\nwide variety of human subgroups. We term this property \"algorithmic fidelity\"\nand explore its extent in GPT-3. We create \"silicon samples\" by conditioning\nthe model on thousands of socio-demographic backstories from real human\nparticipants in multiple large surveys conducted in the United States. We then\ncompare the silicon and human samples to demonstrate that the information\ncontained in GPT-3 goes far beyond surface similarity. It is nuanced,\nmultifaceted, and reflects the complex interplay between ideas, attitudes, and\nsocio-cultural context that characterize human attitudes. We suggest that\nlanguage models with sufficient algorithmic fidelity thus constitute a novel\nand powerful tool to advance understanding of humans and society across a\nvariety of disciplines.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-14T19:53:32Z",
    "updated": "2022-09-14T19:53:32Z",
    "doi": "10.1017/pan.2023.2"
  },
  "2309.14564": {
    "id": "http://arxiv.org/abs/2309.14564v4",
    "title": "Generative Escher Meshes",
    "authors": [
      "Noam Aigerman",
      "Thibault Groueix"
    ],
    "abstract": "  This paper proposes a fully-automatic, text-guided generative method for\nproducing perfectly-repeating, periodic, tile-able 2D imagery, such as the one\nseen on floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to\nsquare texture images that are seamless when tiled, our method generates\nnon-square tilings which comprise solely of repeating copies of the same\nobject. It achieves this by optimizing both geometry and texture of a 2D mesh,\nyielding a non-square tile in the shape and appearance of the desired object,\nwith close to no additional background details, that can tile the plane without\ngaps nor overlaps. We enable optimization of the tile's shape by an\nunconstrained, differentiable parameterization of the space of all valid\ntileable meshes for given boundary conditions stemming from a symmetry group.\nNamely, we construct a differentiable family of linear systems derived from a\n2D mesh-mapping technique - Orbifold Tutte Embedding - by considering the\nmesh's Laplacian matrix as differentiable parameters. We prove that the\nsolution space of these linear systems is exactly all possible valid tiling\nconfigurations, thereby providing an end-to-end differentiable representation\nfor the entire space of valid tiles. We render the textured mesh via a\ndifferentiable renderer, and leverage a pre-trained image diffusion model to\ninduce a loss on the resulting image, updating the mesh's parameters so as to\nmake its appearance match the text prompt. We show our method is able to\nproduce plausible, appealing results, with non-trivial tiles, for a variety of\ndifferent periodic tiling patterns.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-25T22:24:02Z",
    "updated": "2024-06-17T23:57:57Z",
    "doi": null
  },
  "1803.08475": {
    "id": "http://arxiv.org/abs/1803.08475v3",
    "title": "Attention, Learn to Solve Routing Problems!",
    "authors": [
      "Wouter Kool",
      "Herke van Hoof",
      "Max Welling"
    ],
    "abstract": "  The recently presented idea to learn heuristics for combinatorial\noptimization problems is promising as it can save costly development. However,\nto push this idea towards practical implementation, we need better models and\nbetter ways of training. We contribute in both directions: we propose a model\nbased on attention layers with benefits over the Pointer Network and we show\nhow to train this model using REINFORCE with a simple baseline based on a\ndeterministic greedy rollout, which we find is more efficient than using a\nvalue function. We significantly improve over recent learned heuristics for the\nTravelling Salesman Problem (TSP), getting close to optimal results for\nproblems up to 100 nodes. With the same hyperparameters, we learn strong\nheuristics for two variants of the Vehicle Routing Problem (VRP), the\nOrienteering Problem (OP) and (a stochastic variant of) the Prize Collecting\nTSP (PCTSP), outperforming a wide range of baselines and getting results close\nto highly optimized and specialized algorithms.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-03-22T17:22:24Z",
    "updated": "2019-02-07T09:10:08Z",
    "doi": null
  },
  "2007.08508": {
    "id": "http://arxiv.org/abs/2007.08508v1",
    "title": "RepPoints V2: Verification Meets Regression for Object Detection",
    "authors": [
      "Yihong Chen",
      "Zheng Zhang",
      "Yue Cao",
      "Liwei Wang",
      "Stephen Lin",
      "Han Hu"
    ],
    "abstract": "  Verification and regression are two general methodologies for prediction in\nneural networks. Each has its own strengths: verification can be easier to\ninfer accurately, and regression is more efficient and applicable to continuous\ntarget variables. Hence, it is often beneficial to carefully combine them to\ntake advantage of their benefits. In this paper, we take this philosophy to\nimprove state-of-the-art object detection, specifically by RepPoints. Though\nRepPoints provides high performance, we find that its heavy reliance on\nregression for object localization leaves room for improvement. We introduce\nverification tasks into the localization prediction of RepPoints, producing\nRepPoints v2, which provides consistent improvements of about 2.0 mAP over the\noriginal RepPoints on the COCO object detection benchmark using different\nbackbones and training methods. RepPoints v2 also achieves 52.1 mAP on COCO\n\\texttt{test-dev} by a single model. Moreover, we show that the proposed\napproach can more generally elevate other object detection frameworks as well\nas applications such as instance segmentation. The code is available at\nhttps://github.com/Scalsol/RepPointsV2.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-16T17:57:08Z",
    "updated": "2020-07-16T17:57:08Z",
    "doi": null
  },
  "2408.04631": {
    "id": "http://arxiv.org/abs/2408.04631v1",
    "title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior\n  for Part-Level Dynamics",
    "authors": [
      "Ruining Li",
      "Chuanxia Zheng",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "abstract": "  We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-08T17:59:38Z",
    "updated": "2024-08-08T17:59:38Z",
    "doi": null
  },
  "2212.10535": {
    "id": "http://arxiv.org/abs/2212.10535v2",
    "title": "A Survey of Deep Learning for Mathematical Reasoning",
    "authors": [
      "Pan Lu",
      "Liang Qiu",
      "Wenhao Yu",
      "Sean Welleck",
      "Kai-Wei Chang"
    ],
    "abstract": "  Mathematical reasoning is a fundamental aspect of human intelligence and is\napplicable in various fields, including science, engineering, finance, and\neveryday life. The development of artificial intelligence (AI) systems capable\nof solving math problems and proving theorems has garnered significant interest\nin the fields of machine learning and natural language processing. For example,\nmathematics serves as a testbed for aspects of reasoning that are challenging\nfor powerful deep learning models, driving new algorithmic and modeling\nadvances. On the other hand, recent advances in large-scale neural language\nmodels have opened up new benchmarks and opportunities to use deep learning for\nmathematical reasoning. In this survey paper, we review the key tasks,\ndatasets, and methods at the intersection of mathematical reasoning and deep\nlearning over the past decade. We also evaluate existing benchmarks and\nmethods, and discuss future research directions in this domain.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-20T18:46:16Z",
    "updated": "2023-06-22T01:37:02Z",
    "doi": null
  },
  "2311.18799": {
    "id": "http://arxiv.org/abs/2311.18799v2",
    "title": "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware\n  representations to LLMs and Emergent Cross-modal Reasoning",
    "authors": [
      "Artemis Panagopoulou",
      "Le Xue",
      "Ning Yu",
      "Junnan Li",
      "Dongxu Li",
      "Shafiq Joty",
      "Ran Xu",
      "Silvio Savarese",
      "Caiming Xiong",
      "Juan Carlos Niebles"
    ],
    "abstract": "  Recent research has achieved significant advancements in visual reasoning\ntasks through learning image-to-language projections and leveraging the\nimpressive reasoning abilities of Large Language Models (LLMs). This paper\nintroduces an efficient and effective framework that integrates multiple\nmodalities (images, 3D, audio and video) to a frozen LLM and demonstrates an\nemergent ability for cross-modal reasoning (2+ modality inputs). Our approach\nexplores two distinct projection mechanisms: Q-Formers and Linear Projections\n(LPs). Through extensive experimentation across all four modalities on 16\nbenchmarks, we explore both methods and assess their adaptability in integrated\nand separate cross-modal reasoning. The Q-Former projection demonstrates\nsuperior performance in single modality scenarios and adaptability in joint\nversus discriminative reasoning involving two or more modalities. However, it\nexhibits lower generalization capabilities than linear projection in contexts\nwhere task-modality data are limited. To enable this framework, we devise a\nscalable pipeline that automatically generates high-quality, instruction-tuning\ndatasets from readily available captioning data across different modalities,\nand contribute 24K QA data for audio and 250K QA data for 3D. To facilitate\nfurther research in cross-modal reasoning, we introduce the DisCRn\n(Discriminative Cross-modal Reasoning) benchmark comprising 9K audio-video QA\nsamples and 28K image-3D QA samples that require the model to reason\ndiscriminatively across disparate input modalities.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-30T18:43:51Z",
    "updated": "2024-09-09T16:00:04Z",
    "doi": null
  },
  "2207.13861": {
    "id": "http://arxiv.org/abs/2207.13861v2",
    "title": "DnSwin: Toward Real-World Denoising via Continuous Wavelet\n  Sliding-Transformer",
    "authors": [
      "Hao Li",
      "Zhijing Yang",
      "Xiaobin Hong",
      "Ziying Zhao",
      "Junyang Chen",
      "Yukai Shi",
      "Jinshan Pan"
    ],
    "abstract": "  Real-world image denoising is a practical image restoration problem that aims\nto obtain clean images from in-the-wild noisy inputs. Recently, the Vision\nTransformer (ViT) has exhibited a strong ability to capture long-range\ndependencies, and many researchers have attempted to apply the ViT to image\ndenoising tasks. However, a real-world image is an isolated frame that makes\nthe ViT build long-range dependencies based on the internal patches, which\ndivides images into patches, disarranges noise patterns and damages gradient\ncontinuity. In this article, we propose to resolve this issue by using a\ncontinuous Wavelet Sliding-Transformer that builds frequency correspondences\nunder real-world scenes, called DnSwin. Specifically, we first extract the\nbottom features from noisy input images by using a convolutional neural network\n(CNN) encoder. The key to DnSwin is to extract high-frequency and low-frequency\ninformation from the observed features and build frequency dependencies. To\nthis end, we propose a Wavelet Sliding-Window Transformer (WSWT) that utilizes\nthe discrete wavelet transform (DWT), self-attention and the inverse DWT (IDWT)\nto extract deep features. Finally, we reconstruct the deep features into\ndenoised images using a CNN decoder. Both quantitative and qualitative\nevaluations conducted on real-world denoising benchmarks demonstrate that the\nproposed DnSwin performs favorably against the state-of-the-art methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-28T02:33:57Z",
    "updated": "2022-09-13T05:14:07Z",
    "doi": "10.1016/j.knosys.2022.109815"
  },
  "1807.09251": {
    "id": "http://arxiv.org/abs/1807.09251v2",
    "title": "GANimation: Anatomically-aware Facial Animation from a Single Image",
    "authors": [
      "Albert Pumarola",
      "Antonio Agudo",
      "Aleix M. Martinez",
      "Alberto Sanfeliu",
      "Francesc Moreno-Noguer"
    ],
    "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have shown\nimpressive results for task of facial expression synthesis. The most successful\narchitecture is StarGAN, that conditions GANs generation process with images of\na specific domain, namely a set of images of persons sharing the same\nexpression. While effective, this approach can only generate a discrete number\nof expressions, determined by the content of the dataset. To address this\nlimitation, in this paper, we introduce a novel GAN conditioning scheme based\non Action Units (AU) annotations, which describes in a continuous manifold the\nanatomical facial movements defining a human expression. Our approach allows\ncontrolling the magnitude of activation of each AU and combine several of them.\nAdditionally, we propose a fully unsupervised strategy to train the model, that\nonly requires images annotated with their activated AUs, and exploit attention\nmechanisms that make our network robust to changing backgrounds and lighting\nconditions. Extensive evaluation show that our approach goes beyond competing\nconditional generators both in the capability to synthesize a much wider range\nof expressions ruled by anatomically feasible muscle movements, as in the\ncapacity of dealing with images in the wild.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-07-24T17:47:09Z",
    "updated": "2018-08-28T23:46:23Z",
    "doi": null
  },
  "2402.12204": {
    "id": "http://arxiv.org/abs/2402.12204v1",
    "title": "Enhancing Multilingual Capabilities of Large Language Models through\n  Self-Distillation from Resource-Rich Languages",
    "authors": [
      "Yuanchi Zhang",
      "Yile Wang",
      "Zijun Liu",
      "Shuo Wang",
      "Xiaolong Wang",
      "Peng Li",
      "Maosong Sun",
      "Yang Liu"
    ],
    "abstract": "  While large language models (LLMs) have been pre-trained on multilingual\ncorpora, their performance still lags behind in most languages compared to a\nfew resource-rich languages. One common approach to mitigate this issue is to\ntranslate training data from resource-rich languages into other languages and\nthen continue training. However, using the data obtained solely relying on\ntranslation while ignoring the original capabilities of LLMs across languages\nis not always effective, which we show will limit the performance of\ncross-lingual knowledge transfer. In this work, we propose SDRRL, a method\nbased on Self-Distillation from Resource-Rich Languages that effectively\nimprove multilingual performance by leveraging the internal capabilities of\nLLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and\nSeaLLM) and source languages across various comprehension and generation tasks,\nexperimental results demonstrate that SDRRL can significantly enhance\nmultilingual capabilities while minimizing the impact on original performance\nin resource-rich languages.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-19T15:07:32Z",
    "updated": "2024-02-19T15:07:32Z",
    "doi": null
  },
  "2010.10952": {
    "id": "http://arxiv.org/abs/2010.10952v4",
    "title": "A Wigner-Eckart Theorem for Group Equivariant Convolution Kernels",
    "authors": [
      "Leon Lang",
      "Maurice Weiler"
    ],
    "abstract": "  Group equivariant convolutional networks (GCNNs) endow classical\nconvolutional networks with additional symmetry priors, which can lead to a\nconsiderably improved performance. Recent advances in the theoretical\ndescription of GCNNs revealed that such models can generally be understood as\nperforming convolutions with G-steerable kernels, that is, kernels that satisfy\nan equivariance constraint themselves. While the G-steerability constraint has\nbeen derived, it has to date only been solved for specific use cases - a\ngeneral characterization of G-steerable kernel spaces is still missing. This\nwork provides such a characterization for the practically relevant case of G\nbeing any compact group. Our investigation is motivated by a striking analogy\nbetween the constraints underlying steerable kernels on the one hand and\nspherical tensor operators from quantum mechanics on the other hand. By\ngeneralizing the famous Wigner-Eckart theorem for spherical tensor operators,\nwe prove that steerable kernel spaces are fully understood and parameterized in\nterms of 1) generalized reduced matrix elements, 2) Clebsch-Gordan\ncoefficients, and 3) harmonic basis functions on homogeneous spaces.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-10-21T12:42:23Z",
    "updated": "2021-01-21T10:00:28Z",
    "doi": null
  },
  "2111.10023": {
    "id": "http://arxiv.org/abs/2111.10023v1",
    "title": "UFO: A UniFied TransfOrmer for Vision-Language Representation Learning",
    "authors": [
      "Jianfeng Wang",
      "Xiaowei Hu",
      "Zhe Gan",
      "Zhengyuan Yang",
      "Xiyang Dai",
      "Zicheng Liu",
      "Yumao Lu",
      "Lijuan Wang"
    ],
    "abstract": "  In this paper, we propose a single UniFied transfOrmer (UFO), which is\ncapable of processing either unimodal inputs (e.g., image or language) or\nmultimodal inputs (e.g., the concatenation of the image and the question), for\nvision-language (VL) representation learning. Existing approaches typically\ndesign an individual network for each modality and/or a specific fusion network\nfor multimodal tasks. To simplify the network architecture, we use a single\ntransformer network and enforce multi-task learning during VL pre-training,\nwhich includes the image-text contrastive loss, image-text matching loss, and\nmasked language modeling loss based on the bidirectional and the seq2seq\nattention mask. The same transformer network is used as the image encoder, the\ntext encoder, or the fusion network in different pre-training tasks.\nEmpirically, we observe less conflict among different tasks and achieve new\nstate of the arts on visual question answering, COCO image captioning\n(cross-entropy optimization) and nocaps (in SPICE). On other downstream tasks,\ne.g., image-text retrieval, we also achieve competitive performance.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-19T03:23:10Z",
    "updated": "2021-11-19T03:23:10Z",
    "doi": null
  },
  "2312.05133": {
    "id": "http://arxiv.org/abs/2312.05133v2",
    "title": "GIR: 3D Gaussian Inverse Rendering for Relightable Scene Factorization",
    "authors": [
      "Yahao Shi",
      "Yanmin Wu",
      "Chenming Wu",
      "Xing Liu",
      "Chen Zhao",
      "Haocheng Feng",
      "Jian Zhang",
      "Bin Zhou",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "abstract": "  This paper presents a 3D Gaussian Inverse Rendering (GIR) method, employing\n3D Gaussian representations to effectively factorize the scene into material\nproperties, light, and geometry. The key contributions lie in three-fold. We\ncompute the normal of each 3D Gaussian using the shortest eigenvector, with a\ndirectional masking scheme forcing accurate normal estimation without external\nsupervision. We adopt an efficient voxel-based indirect illumination tracing\nscheme that stores direction-aware outgoing radiance in each 3D Gaussian to\ndisentangle secondary illumination for approximating multi-bounce light\ntransport. To further enhance the illumination disentanglement, we represent a\nhigh-resolution environmental map with a learnable low-resolution map and a\nlightweight, fully convolutional network. Our method achieves state-of-the-art\nperformance in both relighting and novel view synthesis tasks among the\nrecently proposed inverse rendering methods while achieving real-time\nrendering. This substantiates our proposed method's efficacy and broad\napplicability, highlighting its potential as an influential tool in various\nreal-time interactive graphics applications such as material editing and\nrelighting. The code will be released at https://github.com/guduxiaolang/GIR.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-08T16:05:15Z",
    "updated": "2024-08-15T15:40:48Z",
    "doi": null
  },
  "1701.02547": {
    "id": "http://arxiv.org/abs/1701.02547v4",
    "title": "A Convenient Category for Higher-Order Probability Theory",
    "authors": [
      "Chris Heunen",
      "Ohad Kammar",
      "Sam Staton",
      "Hongseok Yang"
    ],
    "abstract": "  Higher-order probabilistic programming languages allow programmers to write\nsophisticated models in machine learning and statistics in a succinct and\nstructured way, but step outside the standard measure-theoretic formalization\nof probability theory. Programs may use both higher-order functions and\ncontinuous distributions, or even define a probability distribution on\nfunctions. But standard probability theory does not handle higher-order\nfunctions well: the category of measurable spaces is not cartesian closed.\n  Here we introduce quasi-Borel spaces. We show that these spaces: form a new\nformalization of probability theory replacing measurable spaces; form a\ncartesian closed category and so support higher-order functions; form a\nwell-pointed category and so support good proof principles for equational\nreasoning; and support continuous probability distributions. We demonstrate the\nuse of quasi-Borel spaces for higher-order functions and probability by:\nshowing that a well-known construction of probability theory involving random\nfunctions gains a cleaner expression; and generalizing de Finetti's theorem,\nthat is a crucial theorem in probability theory, to quasi-Borel spaces.\n",
    "categories": [
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LO",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.CT",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.PR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-01-10T12:19:05Z",
    "updated": "2020-11-20T08:56:11Z",
    "doi": "10.1109/LICS.2017.8005137"
  },
  "2211.05781": {
    "id": "http://arxiv.org/abs/2211.05781v2",
    "title": "Demystify Transformers & Convolutions in Modern Image Deep Networks",
    "authors": [
      "Xiaowei Hu",
      "Min Shi",
      "Weiyun Wang",
      "Sitong Wu",
      "Linjie Xing",
      "Wenhai Wang",
      "Xizhou Zhu",
      "Lewei Lu",
      "Jie Zhou",
      "Xiaogang Wang",
      "Yu Qiao",
      "Jifeng Dai"
    ],
    "abstract": "  Vision transformers have gained popularity recently, leading to the\ndevelopment of new vision backbones with improved features and consistent\nperformance gains. However, these advancements are not solely attributable to\nnovel feature transformation designs; certain benefits also arise from advanced\nnetwork-level and block-level architectures. This paper aims to identify the\nreal gains of popular convolution and attention operators through a detailed\nstudy. We find that the key difference among these feature transformation\nmodules, such as attention or convolution, lies in their spatial feature\naggregation approach, known as the \"spatial token mixer\" (STM). To facilitate\nan impartial comparison, we introduce a unified architecture to neutralize the\nimpact of divergent network-level and block-level designs. Subsequently,\nvarious STMs are integrated into this unified framework for comprehensive\ncomparative analysis. Our experiments on various tasks and an analysis of\ninductive bias show a significant performance boost due to advanced\nnetwork-level and block-level designs, but performance differences persist\namong different STMs. Our detailed analysis also reveals various findings about\ndifferent STMs, such as effective receptive fields and invariance tests. All\nmodels and codes used in this study are publicly available at\n\\url{https://github.com/OpenGVLab/STM-Evaluation}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-10T18:59:43Z",
    "updated": "2023-12-01T08:00:51Z",
    "doi": null
  },
  "2404.00604": {
    "id": "http://arxiv.org/abs/2404.00604v1",
    "title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment",
    "authors": [
      "Xiao Liu",
      "Xixuan Song",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "  Reinforcement learning from human feedback (RLHF) has been a central\ntechnique for recent large language model (LLM) alignment. However, its heavy\ndependence on costly human or LLM-as-Judge preference feedback could stymie its\nwider applications. In this work, we introduce Self-Contrast, a feedback-free\nlarge language model alignment method via exploiting extensive self-generated\nnegatives. With only supervised fine-tuning (SFT) targets, Self-Contrast\nleverages the LLM itself to generate massive diverse candidates, and harnesses\na pre-trained embedding model to filter multiple negatives according to text\nsimilarity. Theoretically, we illustrate that in this setting, merely scaling\nnegative responses can still effectively approximate situations with more\nbalanced positive and negative preference annotations. Our experiments with\ndirect preference optimization (DPO) on three datasets show that, Self-Contrast\ncould consistently outperform SFT and standard DPO training by large margins.\nAnd as the number of self-generated negatives increases, the performance of\nSelf-Contrast continues to grow. Code and data are available at\nhttps://github.com/THUDM/Self-Contrast.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-31T08:30:15Z",
    "updated": "2024-03-31T08:30:15Z",
    "doi": null
  },
  "2305.08296": {
    "id": "http://arxiv.org/abs/2305.08296v1",
    "title": "Neural Face Rigging for Animating and Retargeting Facial Meshes in the\n  Wild",
    "authors": [
      "Dafei Qin",
      "Jun Saito",
      "Noam Aigerman",
      "Thibault Groueix",
      "Taku Komura"
    ],
    "abstract": "  We propose an end-to-end deep-learning approach for automatic rigging and\nretargeting of 3D models of human faces in the wild. Our approach, called\nNeural Face Rigging (NFR), holds three key properties:\n  (i) NFR's expression space maintains human-interpretable editing parameters\nfor artistic controls;\n  (ii) NFR is readily applicable to arbitrary facial meshes with different\nconnectivity and expressions;\n  (iii) NFR can encode and produce fine-grained details of complex expressions\nperformed by arbitrary subjects.\n  To the best of our knowledge, NFR is the first approach to provide realistic\nand controllable deformations of in-the-wild facial meshes, without the manual\ncreation of blendshapes or correspondence. We design a deformation autoencoder\nand train it through a multi-dataset training scheme, which benefits from the\nunique advantages of two data sources: a linear 3DMM with interpretable control\nparameters as in FACS, and 4D captures of real faces with fine-grained details.\nThrough various experiments, we show NFR's ability to automatically produce\nrealistic and accurate facial deformations across a wide range of existing\ndatasets as well as noisy facial scans in-the-wild, while providing\nartist-controlled, editable parameters.\n",
    "categories": [
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-15T01:51:08Z",
    "updated": "2023-05-15T01:51:08Z",
    "doi": null
  },
  "2202.13562": {
    "id": "http://arxiv.org/abs/2202.13562v2",
    "title": "Name Your Style: An Arbitrary Artist-aware Image Style Transfer",
    "authors": [
      "Zhi-Song Liu",
      "Li-Wen Wang",
      "Wan-Chi Siu",
      "Vicky Kalogeiton"
    ],
    "abstract": "  Image style transfer has attracted widespread attention in the past few\nyears. Despite its remarkable results, it requires additional style images\navailable as references, making it less flexible and inconvenient. Using text\nis the most natural way to describe the style. More importantly, text can\ndescribe implicit abstract styles, like styles of specific artists or art\nmovements. In this paper, we propose a text-driven image style transfer (TxST)\nthat leverages advanced image-text encoders to control arbitrary style\ntransfer. We introduce a contrastive training strategy to effectively extract\nstyle descriptions from the image-text model (i.e., CLIP), which aligns\nstylization with the text description. To this end, we also propose a novel and\nefficient attention module that explores cross-attentions to fuse style and\ncontent features. Finally, we achieve an arbitrary artist-aware image style\ntransfer to learn and transfer specific artistic characters such as Picasso,\noil painting, or a rough sketch. Extensive experiments demonstrate that our\napproach outperforms the state-of-the-art methods on both image and textual\nstyles. Moreover, it can mimic the styles of one or many artists to achieve\nattractive results, thus highlighting a promising direction in image style\ntransfer.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-02-28T06:21:38Z",
    "updated": "2022-03-05T00:47:30Z",
    "doi": null
  },
  "2204.04788": {
    "id": "http://arxiv.org/abs/2204.04788v2",
    "title": "Representation Learning by Detecting Incorrect Location Embeddings",
    "authors": [
      "Sepehr Sameni",
      "Simon Jenni",
      "Paolo Favaro"
    ],
    "abstract": "  In this paper, we introduce a novel self-supervised learning (SSL) loss for\nimage representation learning. There is a growing belief that generalization in\ndeep neural networks is linked to their ability to discriminate object shapes.\nSince object shape is related to the location of its parts, we propose to\ndetect those that have been artificially misplaced. We represent object parts\nwith image tokens and train a ViT to detect which token has been combined with\nan incorrect positional embedding. We then introduce sparsity in the inputs to\nmake the model more robust to occlusions and to speed up the training. We call\nour method DILEMMA, which stands for Detection of Incorrect Location EMbeddings\nwith MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an\nimprovement in their performance of respectively 4.41%, 3.97%, and 0.5% under\nthe same training time and with a linear probing transfer on ImageNet-1K. We\nalso show full fine-tuning improvements of MAE combined with our method on\nImageNet-100. We evaluate our method via fine-tuning on common SSL benchmarks.\nMoreover, we show that when downstream tasks are strongly reliant on shape\n(such as in the YOGA-82 pose dataset), our pre-trained features yield a\nsignificant gain over prior work.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-10T22:58:02Z",
    "updated": "2023-03-13T10:13:00Z",
    "doi": null
  },
  "2305.15399": {
    "id": "http://arxiv.org/abs/2305.15399v2",
    "title": "Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape",
    "authors": [
      "Rundi Wu",
      "Ruoshi Liu",
      "Carl Vondrick",
      "Changxi Zheng"
    ],
    "abstract": "  Synthesizing novel 3D models that resemble the input example has long been\npursued by graphics artists and machine learning researchers. In this paper, we\npresent Sin3DM, a diffusion model that learns the internal patch distribution\nfrom a single 3D textured shape and generates high-quality variations with fine\ngeometry and texture details. Training a diffusion model directly in 3D would\ninduce large memory and computational cost. Therefore, we first compress the\ninput into a lower-dimensional latent space and then train a diffusion model on\nit. Specifically, we encode the input 3D textured shape into triplane feature\nmaps that represent the signed distance and texture fields of the input. The\ndenoising network of our diffusion model has a limited receptive field to avoid\noverfitting, and uses triplane-aware 2D convolution blocks to improve the\nresult quality. Aside from randomly generating new samples, our model also\nfacilitates applications such as retargeting, outpainting and local editing.\nThrough extensive qualitative and quantitative evaluation, we show that our\nmethod outperforms prior methods in generation quality of 3D shapes.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-24T17:57:15Z",
    "updated": "2024-02-21T01:25:36Z",
    "doi": null
  },
  "2311.01410": {
    "id": "http://arxiv.org/abs/2311.01410v2",
    "title": "The Blessing of Randomness: SDE Beats ODE in General Diffusion-based\n  Image Editing",
    "authors": [
      "Shen Nie",
      "Hanzhong Allan Guo",
      "Cheng Lu",
      "Yuhao Zhou",
      "Chenyu Zheng",
      "Chongxuan Li"
    ],
    "abstract": "  We present a unified probabilistic formulation for diffusion-based image\nediting, where a latent variable is edited in a task-specific manner and\ngenerally deviates from the corresponding marginal distribution induced by the\noriginal stochastic or ordinary differential equation (SDE or ODE). Instead, it\ndefines a corresponding SDE or ODE for editing. In the formulation, we prove\nthat the Kullback-Leibler divergence between the marginal distributions of the\ntwo SDEs gradually decreases while that for the ODEs remains as the time\napproaches zero, which shows the promise of SDE in image editing. Inspired by\nit, we provide the SDE counterparts for widely used ODE baselines in various\ntasks including inpainting and image-to-image translation, where SDE shows a\nconsistent and substantial improvement. Moreover, we propose SDE-Drag -- a\nsimple yet effective method built upon the SDE formulation for point-based\ncontent dragging. We build a challenging benchmark (termed DragBench) with\nopen-set natural, art, and AI-generated images for evaluation. A user study on\nDragBench indicates that SDE-Drag significantly outperforms our ODE baseline,\nexisting diffusion-based methods, and the renowned DragGAN. Our results\ndemonstrate the superiority and versatility of SDE in image editing and push\nthe boundary of diffusion-based editing methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-02T17:23:14Z",
    "updated": "2024-02-29T14:31:13Z",
    "doi": null
  },
  "2407.06642": {
    "id": "http://arxiv.org/abs/2407.06642v2",
    "title": "Powerful and Flexible: Personalized Text-to-Image Generation via\n  Reinforcement Learning",
    "authors": [
      "Fanyue Wei",
      "Wei Zeng",
      "Zhenyang Li",
      "Dawei Yin",
      "Lixin Duan",
      "Wen Li"
    ],
    "abstract": "  Personalized text-to-image models allow users to generate varied styles of\nimages (specified with a sentence) for an object (specified with a set of\nreference images). While remarkable results have been achieved using\ndiffusion-based generation models, the visual structure and details of the\nobject are often unexpectedly changed during the diffusion process. One major\nreason is that these diffusion-based approaches typically adopt a simple\nreconstruction objective during training, which can hardly enforce appropriate\nstructural consistency between the generated and the reference images. To this\nend, in this paper, we design a novel reinforcement learning framework by\nutilizing the deterministic policy gradient method for personalized\ntext-to-image generation, with which various objectives, differential or even\nnon-differential, can be easily incorporated to supervise the diffusion models\nto improve the quality of the generated images. Experimental results on\npersonalized text-to-image generation benchmark datasets demonstrate that our\nproposed approach outperforms existing state-of-the-art methods by a large\nmargin on visual fidelity while maintaining text-alignment. Our code is\navailable at: \\url{https://github.com/wfanyue/DPG-T2I-Personalization}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-09T08:11:53Z",
    "updated": "2024-07-18T15:34:04Z",
    "doi": null
  },
  "2205.05131": {
    "id": "http://arxiv.org/abs/2205.05131v3",
    "title": "UL2: Unifying Language Learning Paradigms",
    "authors": [
      "Yi Tay",
      "Mostafa Dehghani",
      "Vinh Q. Tran",
      "Xavier Garcia",
      "Jason Wei",
      "Xuezhi Wang",
      "Hyung Won Chung",
      "Siamak Shakeri",
      "Dara Bahri",
      "Tal Schuster",
      "Huaixiu Steven Zheng",
      "Denny Zhou",
      "Neil Houlsby",
      "Donald Metzler"
    ],
    "abstract": "  Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized & unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 & GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-10T19:32:20Z",
    "updated": "2023-02-28T17:20:36Z",
    "doi": null
  },
  "1909.05372": {
    "id": "http://arxiv.org/abs/1909.05372v1",
    "title": "Overton: A Data System for Monitoring and Improving Machine-Learned\n  Products",
    "authors": [
      "Christopher R\u00e9",
      "Feng Niu",
      "Pallavi Gudipati",
      "Charles Srisuwananukorn"
    ],
    "abstract": "  We describe a system called Overton, whose main design goal is to support\nengineers in building, monitoring, and improving production machine learning\nsystems. Key challenges engineers face are monitoring fine-grained quality,\ndiagnosing errors in sophisticated applications, and handling contradictory or\nincomplete supervision data. Overton automates the life cycle of model\nconstruction, deployment, and monitoring by providing a set of novel\nhigh-level, declarative abstractions. Overton's vision is to shift developers\nto these higher-level tasks instead of lower-level machine learning tasks. In\nfact, using Overton, engineers can build deep-learning-based applications\nwithout writing any code in frameworks like TensorFlow. For over a year,\nOverton has been used in production to support multiple applications in both\nnear-real-time applications and back-of-house processing. In that time,\nOverton-based applications have answered billions of queries in multiple\nlanguages and processed trillions of records reducing errors 1.7-2.9 times\nversus production systems.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DB",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-09-07T03:51:13Z",
    "updated": "2019-09-07T03:51:13Z",
    "doi": null
  },
  "2211.11679": {
    "id": "http://arxiv.org/abs/2211.11679v3",
    "title": "Mean Shift Mask Transformer for Unseen Object Instance Segmentation",
    "authors": [
      "Yangxiao Lu",
      "Yuqiao Chen",
      "Nicholas Ruozzi",
      "Yu Xiang"
    ],
    "abstract": "  Segmenting unseen objects from images is a critical perception skill that a\nrobot needs to acquire. In robot manipulation, it can facilitate a robot to\ngrasp and manipulate unseen objects. Mean shift clustering is a widely used\nmethod for image segmentation tasks. However, the traditional mean shift\nclustering algorithm is not differentiable, making it difficult to integrate it\ninto an end-to-end neural network training framework. In this work, we propose\nthe Mean Shift Mask Transformer (MSMFormer), a new transformer architecture\nthat simulates the von Mises-Fisher (vMF) mean shift clustering algorithm,\nallowing for the joint training and inference of both the feature extractor and\nthe clustering. Its central component is a hypersphere attention mechanism,\nwhich updates object queries on a hypersphere. To illustrate the effectiveness\nof our method, we apply MSMFormer to unseen object instance segmentation. Our\nexperiments show that MSMFormer achieves competitive performance compared to\nstate-of-the-art methods for unseen object instance segmentation. The project\npage, appendix, video, and code are available at\nhttps://irvlutd.github.io/MSMFormer\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-21T17:47:48Z",
    "updated": "2023-09-21T23:04:42Z",
    "doi": null
  },
  "2311.18834": {
    "id": "http://arxiv.org/abs/2311.18834v1",
    "title": "ART$\\boldsymbol{\\cdot}$V: Auto-Regressive Text-to-Video Generation with\n  Diffusion Models",
    "authors": [
      "Wenming Weng",
      "Ruoyu Feng",
      "Yanhui Wang",
      "Qi Dai",
      "Chunyu Wang",
      "Dacheng Yin",
      "Zhiyuan Zhao",
      "Kai Qiu",
      "Jianmin Bao",
      "Yuhui Yuan",
      "Chong Luo",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ],
    "abstract": "  We present ART$\\boldsymbol{\\cdot}$V, an efficient framework for\nauto-regressive video generation with diffusion models. Unlike existing methods\nthat generate entire videos in one-shot, ART$\\boldsymbol{\\cdot}$V generates a\nsingle frame at a time, conditioned on the previous ones. The framework offers\nthree distinct advantages. First, it only learns simple continual motions\nbetween adjacent frames, therefore avoiding modeling complex long-range motions\nthat require huge training data. Second, it preserves the high-fidelity\ngeneration ability of the pre-trained image diffusion models by making only\nminimal network modifications. Third, it can generate arbitrarily long videos\nconditioned on a variety of prompts such as text, image or their combinations,\nmaking it highly versatile and flexible. To combat the common drifting issue in\nAR models, we propose masked diffusion model which implicitly learns which\ninformation can be drawn from reference images rather than network predictions,\nin order to reduce the risk of generating inconsistent appearances that cause\ndrifting. Moreover, we further enhance generation coherence by conditioning it\non the initial frame, which typically contains minimal noise. This is\nparticularly useful for long video generation. When trained for only two weeks\non four GPUs, ART$\\boldsymbol{\\cdot}$V already can generate videos with natural\nmotions, rich details and a high level of aesthetic quality. Besides, it\nenables various appealing applications, e.g., composing a long video from\nmultiple text prompts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-30T18:59:47Z",
    "updated": "2023-11-30T18:59:47Z",
    "doi": null
  },
  "2007.08509": {
    "id": "http://arxiv.org/abs/2007.08509v1",
    "title": "World-Consistent Video-to-Video Synthesis",
    "authors": [
      "Arun Mallya",
      "Ting-Chun Wang",
      "Karan Sapra",
      "Ming-Yu Liu"
    ],
    "abstract": "  Video-to-video synthesis (vid2vid) aims for converting high-level semantic\ninputs to photorealistic videos. While existing vid2vid methods can achieve\nshort-term temporal consistency, they fail to ensure the long-term one. This is\nbecause they lack knowledge of the 3D world being rendered and generate each\nframe only based on the past few frames. To address the limitation, we\nintroduce a novel vid2vid framework that efficiently and effectively utilizes\nall past generated frames during rendering. This is achieved by condensing the\n3D world rendered so far into a physically-grounded estimate of the current\nframe, which we call the guidance image. We further propose a novel neural\nnetwork architecture to take advantage of the information stored in the\nguidance images. Extensive experimental results on several challenging datasets\nverify the effectiveness of our approach in achieving world consistency - the\noutput video is consistent within the entire rendered 3D world.\n  https://nvlabs.github.io/wc-vid2vid/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-16T17:58:13Z",
    "updated": "2020-07-16T17:58:13Z",
    "doi": null
  },
  "2206.01720": {
    "id": "http://arxiv.org/abs/2206.01720v1",
    "title": "Revisiting the \"Video\" in Video-Language Understanding",
    "authors": [
      "Shyamal Buch",
      "Crist\u00f3bal Eyzaguirre",
      "Adrien Gaidon",
      "Jiajun Wu",
      "Li Fei-Fei",
      "Juan Carlos Niebles"
    ],
    "abstract": "  What makes a video task uniquely suited for videos, beyond what can be\nunderstood from a single image? Building on recent progress in self-supervised\nimage-language models, we revisit this question in the context of video and\nlanguage tasks. We propose the atemporal probe (ATP), a new model for\nvideo-language analysis which provides a stronger bound on the baseline\naccuracy of multimodal models constrained by image-level understanding. By\napplying this model to standard discriminative video and language tasks, such\nas video question answering and text-to-video retrieval, we characterize the\nlimitations and potential of current video-language benchmarks. We find that\nunderstanding of event temporality is often not necessary to achieve strong or\nstate-of-the-art performance, even compared with recent large-scale\nvideo-language models and in contexts intended to benchmark deeper video-level\nunderstanding. We also demonstrate how ATP can improve both video-language\ndataset and model design. We describe a technique for leveraging ATP to better\ndisentangle dataset subsets with a higher concentration of temporally\nchallenging data, improving benchmarking efficacy for causal and temporal\nunderstanding. Further, we show that effectively integrating ATP into full\nvideo-level temporal models can improve efficiency and state-of-the-art\naccuracy.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-03T17:57:33Z",
    "updated": "2022-06-03T17:57:33Z",
    "doi": null
  },
  "1811.07605": {
    "id": "http://arxiv.org/abs/1811.07605v3",
    "title": "Adversarial Autoencoders for Compact Representations of 3D Point Clouds",
    "authors": [
      "Maciej Zamorski",
      "Maciej Zi\u0119ba",
      "Piotr Klukowski",
      "Rafa\u0142 Nowak",
      "Karol Kurach",
      "Wojciech Stokowiec",
      "Tomasz Trzci\u0144ski"
    ],
    "abstract": "  Deep generative architectures provide a way to model not only images but also\ncomplex, 3-dimensional objects, such as point clouds. In this work, we present\na novel method to obtain meaningful representations of 3D shapes that can be\nused for challenging tasks including 3D points generation, reconstruction,\ncompression, and clustering. Contrary to existing methods for 3D point cloud\ngeneration that train separate decoupled models for representation learning and\ngeneration, our approach is the first end-to-end solution that allows to\nsimultaneously learn a latent space of representation and generate 3D shape out\nof it. Moreover, our model is capable of learning meaningful compact binary\ndescriptors with adversarial training conducted on a latent space. To achieve\nthis goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D\ninput and create 3D output. Thanks to our end-to-end training regime, the\nresulting method called 3D Adversarial Autoencoder (3dAAE) obtains either\nbinary or continuous latent space that covers a much wider portion of training\ndata distribution. Finally, our quantitative evaluation shows that 3dAAE\nprovides state-of-the-art results for 3D points clustering and 3D object\nretrieval.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-11-19T10:51:09Z",
    "updated": "2019-05-01T19:22:36Z",
    "doi": null
  },
  "2203.03014": {
    "id": "http://arxiv.org/abs/2203.03014v2",
    "title": "Learnable Irrelevant Modality Dropout for Multimodal Action Recognition\n  on Modality-Specific Annotated Videos",
    "authors": [
      "Saghir Alfasly",
      "Jian Lu",
      "Chen Xu",
      "Yuru Zou"
    ],
    "abstract": "  With the assumption that a video dataset is multimodality annotated in which\nauditory and visual modalities both are labeled or class-relevant, current\nmultimodal methods apply modality fusion or cross-modality attention. However,\neffectively leveraging the audio modality in vision-specific annotated videos\nfor action recognition is of particular challenge. To tackle this challenge, we\npropose a novel audio-visual framework that effectively leverages the audio\nmodality in any solely vision-specific annotated dataset. We adopt the language\nmodels (e.g., BERT) to build a semantic audio-video label dictionary (SAVLD)\nthat maps each video label to its most K-relevant audio labels in which SAVLD\nserves as a bridge between audio and video datasets. Then, SAVLD along with a\npretrained audio multi-label model are used to estimate the audio-visual\nmodality relevance during the training phase. Accordingly, a novel learnable\nirrelevant modality dropout (IMD) is proposed to completely drop out the\nirrelevant audio modality and fuse only the relevant modalities. Moreover, we\npresent a new two-stream video Transformer for efficiently modeling the visual\nmodalities. Results on several vision-specific annotated datasets including\nKinetics400 and UCF-101 validated our framework as it outperforms most relevant\naction recognition methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-06T17:31:06Z",
    "updated": "2022-03-27T03:26:40Z",
    "doi": null
  },
  "2308.15930": {
    "id": "http://arxiv.org/abs/2308.15930v3",
    "title": "LLaSM: Large Language and Speech Model",
    "authors": [
      "Yu Shu",
      "Siwei Dong",
      "Guangyao Chen",
      "Wenhao Huang",
      "Ruihua Zhang",
      "Daochen Shi",
      "Qiqi Xiang",
      "Yemin Shi"
    ],
    "abstract": "  Multi-modal large language models have garnered significant interest\nrecently. Though, most of the works focus on vision-language multi-modal models\nproviding strong capabilities in following vision-and-language instructions.\nHowever, we claim that speech is also an important modality through which\nhumans interact with the world. Hence, it is crucial for a general-purpose\nassistant to be able to follow multi-modal speech-and-language instructions. In\nthis work, we propose Large Language and Speech Model (LLaSM). LLaSM is an\nend-to-end trained large multi-modal speech-language model with cross-modal\nconversational abilities, capable of following speech-and-language\ninstructions. Our early experiments show that LLaSM demonstrates a more\nconvenient and natural way for humans to interact with artificial intelligence.\nSpecifically, we also release a large Speech Instruction Following dataset\nLLaSM-Audio-Instructions. Code and demo are available at\nhttps://github.com/LinkSoul-AI/LLaSM and\nhttps://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions\ndataset is available at\nhttps://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-30T10:12:39Z",
    "updated": "2023-09-16T06:14:54Z",
    "doi": null
  },
  "2409.00786": {
    "id": "http://arxiv.org/abs/2409.00786v1",
    "title": "Zero-Shot Paragraph-level Handwriting Imitation with Latent Diffusion\n  Models",
    "authors": [
      "Martin Mayr",
      "Marcel Dreier",
      "Florian Kordon",
      "Mathias Seuret",
      "Jochen Z\u00f6llner",
      "Fei Wu",
      "Andreas Maier",
      "Vincent Christlein"
    ],
    "abstract": "  The imitation of cursive handwriting is mainly limited to generating\nhandwritten words or lines. Multiple synthetic outputs must be stitched\ntogether to create paragraphs or whole pages, whereby consistency and layout\ninformation are lost. To close this gap, we propose a method for imitating\nhandwriting at the paragraph level that also works for unseen writing styles.\nTherefore, we introduce a modified latent diffusion model that enriches the\nencoder-decoder mechanism with specialized loss functions that explicitly\npreserve the style and content. We enhance the attention mechanism of the\ndiffusion model with adaptive 2D positional encoding and the conditioning\nmechanism to work with two modalities simultaneously: a style image and the\ntarget text. This significantly improves the realism of the generated\nhandwriting. Our approach sets a new benchmark in our comprehensive evaluation.\nIt outperforms all existing imitation methods at both line and paragraph\nlevels, considering combined style and content preservation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-09-01T17:33:31Z",
    "updated": "2024-09-01T17:33:31Z",
    "doi": null
  },
  "2407.14245": {
    "id": "http://arxiv.org/abs/2407.14245v1",
    "title": "Dataset Distillation by Automatic Training Trajectories",
    "authors": [
      "Dai Liu",
      "Jindong Gu",
      "Hu Cao",
      "Carsten Trinitis",
      "Martin Schulz"
    ],
    "abstract": "  Dataset Distillation is used to create a concise, yet informative, synthetic\ndataset that can replace the original dataset for training purposes. Some\nleading methods in this domain prioritize long-range matching, involving the\nunrolling of training trajectories with a fixed number of steps (NS) on the\nsynthetic dataset to align with various expert training trajectories. However,\ntraditional long-range matching methods possess an overfitting-like problem,\nthe fixed step size NS forces synthetic dataset to distortedly conform seen\nexpert training trajectories, resulting in a loss of generality-especially to\nthose from unencountered architecture. We refer to this as the Accumulated\nMismatching Problem (AMP), and propose a new approach, Automatic Training\nTrajectories (ATT), which dynamically and adaptively adjusts trajectory length\nNS to address the AMP. Our method outperforms existing methods particularly in\ntests involving cross-architectures. Moreover, owing to its adaptive nature, it\nexhibits enhanced stability in the face of parameter variations.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-19T12:27:11Z",
    "updated": "2024-07-19T12:27:11Z",
    "doi": null
  },
  "2302.04973": {
    "id": "http://arxiv.org/abs/2302.04973v2",
    "title": "Invariant Slot Attention: Object Discovery with Slot-Centric Reference\n  Frames",
    "authors": [
      "Ondrej Biza",
      "Sjoerd van Steenkiste",
      "Mehdi S. M. Sajjadi",
      "Gamaleldin F. Elsayed",
      "Aravindh Mahendran",
      "Thomas Kipf"
    ],
    "abstract": "  Automatically discovering composable abstractions from raw perceptual data is\na long-standing challenge in machine learning. Recent slot-based neural\nnetworks that learn about objects in a self-supervised manner have made\nexciting progress in this direction. However, they typically fall short at\nadequately capturing spatial symmetries present in the visual world, which\nleads to sample inefficiency, such as when entangling object appearance and\npose. In this paper, we present a simple yet highly effective method for\nincorporating spatial symmetries via slot-centric reference frames. We\nincorporate equivariance to per-object pose transformations into the attention\nand generation mechanism of Slot Attention by translating, scaling, and\nrotating position encodings. These changes result in little computational\noverhead, are easy to implement, and can result in large gains in terms of data\nefficiency and overall improvements to object discovery. We evaluate our method\non a wide range of synthetic object discovery benchmarks namely CLEVR,\nTetrominoes, CLEVRTex, Objects Room and MultiShapeNet, and show promising\nimprovements on the challenging real-world Waymo Open dataset.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-09T23:25:28Z",
    "updated": "2023-07-21T01:40:31Z",
    "doi": null
  },
  "2204.14095": {
    "id": "http://arxiv.org/abs/2204.14095v2",
    "title": "PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model\n  Pretraining",
    "authors": [
      "Yuting Gao",
      "Jinfeng Liu",
      "Zihan Xu",
      "Jun Zhang",
      "Ke Li",
      "Rongrong Ji",
      "Chunhua Shen"
    ],
    "abstract": "  Large-scale vision-language pre-training has achieved promising results on\ndownstream tasks. Existing methods highly rely on the assumption that the\nimage-text pairs crawled from the Internet are in perfect one-to-one\ncorrespondence. However, in real scenarios, this assumption can be difficult to\nhold: the text description, obtained by crawling the affiliated metadata of the\nimage, often suffers from the semantic mismatch and the mutual compatibility.\nTo address these issues, we introduce PyramidCLIP, which constructs an input\npyramid with different semantic levels for each modality, and aligns visual\nelements and linguistic elements in the form of hierarchy via peer-level\nsemantics alignment and cross-level relation alignment. Furthermore, we soften\nthe loss of negative samples (unpaired samples) so as to weaken the strict\nconstraint during the pre-training stage, thus mitigating the risk of forcing\nthe model to distinguish compatible negative pairs. Experiments on five\ndownstream tasks demonstrate the effectiveness of the proposed PyramidCLIP. In\nparticular, with the same amount of 15 million pre-training image-text pairs,\nPyramidCLIP exceeds CLIP on ImageNet zero-shot classification top-1 accuracy by\n10.6%/13.2%/10.0% with ResNet50/ViT-B32/ViT-B16 based image encoder\nrespectively. When scaling to larger datasets, PyramidCLIP achieves the\nstate-of-the-art results on several downstream tasks. In particular, the\nresults of PyramidCLIP-ResNet50 trained on 143M image-text pairs surpass that\nof CLIP using 400M data on ImageNet zero-shot classification task,\nsignificantly improving the data efficiency of CLIP.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-29T13:38:42Z",
    "updated": "2022-05-28T08:52:58Z",
    "doi": null
  },
  "2009.12559": {
    "id": "http://arxiv.org/abs/2009.12559v1",
    "title": "Affinity Space Adaptation for Semantic Segmentation Across Domains",
    "authors": [
      "Wei Zhou",
      "Yukang Wang",
      "Jiajia Chu",
      "Jiehua Yang",
      "Xiang Bai",
      "Yongchao Xu"
    ],
    "abstract": "  Semantic segmentation with dense pixel-wise annotation has achieved excellent\nperformance thanks to deep learning. However, the generalization of semantic\nsegmentation in the wild remains challenging. In this paper, we address the\nproblem of unsupervised domain adaptation (UDA) in semantic segmentation.\nMotivated by the fact that source and target domain have invariant semantic\nstructures, we propose to exploit such invariance across domains by leveraging\nco-occurring patterns between pairwise pixels in the output of structured\nsemantic segmentation. This is different from most existing approaches that\nattempt to adapt domains based on individual pixel-wise information in image,\nfeature, or output level. Specifically, we perform domain adaptation on the\naffinity relationship between adjacent pixels termed affinity space of source\nand target domain. To this end, we develop two affinity space adaptation\nstrategies: affinity space cleaning and adversarial affinity space alignment.\nExtensive experiments demonstrate that the proposed method achieves superior\nperformance against some state-of-the-art methods on several challenging\nbenchmarks for semantic segmentation across domains. The code is available at\nhttps://github.com/idealwei/ASANet.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-09-26T10:28:11Z",
    "updated": "2020-09-26T10:28:11Z",
    "doi": "10.1109/TIP.2020.3018221"
  },
  "2207.01887": {
    "id": "http://arxiv.org/abs/2207.01887v2",
    "title": "Open-Vocabulary Multi-Label Classification via Multi-Modal Knowledge\n  Transfer",
    "authors": [
      "Sunan He",
      "Taian Guo",
      "Tao Dai",
      "Ruizhi Qiao",
      "Bo Ren",
      "Shu-Tao Xia"
    ],
    "abstract": "  Real-world recognition system often encounters the challenge of unseen\nlabels. To identify such unseen labels, multi-label zero-shot learning (ML-ZSL)\nfocuses on transferring knowledge by a pre-trained textual label embedding\n(e.g., GloVe). However, such methods only exploit single-modal knowledge from a\nlanguage model, while ignoring the rich semantic information inherent in\nimage-text pairs. Instead, recently developed open-vocabulary (OV) based\nmethods succeed in exploiting such information of image-text pairs in object\ndetection, and achieve impressive performance. Inspired by the success of\nOV-based methods, we propose a novel open-vocabulary framework, named\nmulti-modal knowledge transfer (MKT), for multi-label classification.\nSpecifically, our method exploits multi-modal knowledge of image-text pairs\nbased on a vision and language pre-training (VLP) model. To facilitate\ntransferring the image-text matching ability of VLP model, knowledge\ndistillation is employed to guarantee the consistency of image and label\nembeddings, along with prompt tuning to further update the label embeddings. To\nfurther enable the recognition of multiple objects, a simple but effective\ntwo-stream module is developed to capture both local and global features.\nExtensive experimental results show that our method significantly outperforms\nstate-of-the-art methods on public benchmark datasets. The source code is\navailable at https://github.com/sunanhe/MKT.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-05T08:32:18Z",
    "updated": "2023-02-01T10:59:03Z",
    "doi": null
  },
  "2406.19680": {
    "id": "http://arxiv.org/abs/2406.19680v1",
    "title": "MimicMotion: High-Quality Human Motion Video Generation with\n  Confidence-aware Pose Guidance",
    "authors": [
      "Yuang Zhang",
      "Jiaxi Gu",
      "Li-Wen Wang",
      "Han Wang",
      "Junqi Cheng",
      "Yuefeng Zhu",
      "Fangyuan Zou"
    ],
    "abstract": "  In recent years, generative artificial intelligence has achieved significant\nadvancements in the field of image generation, spawning a variety of\napplications. However, video generation still faces considerable challenges in\nvarious aspects, such as controllability, video length, and richness of\ndetails, which hinder the application and popularization of this technology. In\nthis work, we propose a controllable video generation framework, dubbed\nMimicMotion, which can generate high-quality videos of arbitrary length\nmimicking specific motion guidance. Compared with previous methods, our\napproach has several highlights. Firstly, we introduce confidence-aware pose\nguidance that ensures high frame quality and temporal smoothness. Secondly, we\nintroduce regional loss amplification based on pose confidence, which\nsignificantly reduces image distortion. Lastly, for generating long and smooth\nvideos, we propose a progressive latent fusion strategy. By this means, we can\nproduce videos of arbitrary length with acceptable resource consumption. With\nextensive experiments and user studies, MimicMotion demonstrates significant\nimprovements over previous approaches in various aspects. Detailed results and\ncomparisons are available on our project page:\nhttps://tencent.github.io/MimicMotion .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-28T06:40:53Z",
    "updated": "2024-06-28T06:40:53Z",
    "doi": null
  },
  "2209.12325": {
    "id": "http://arxiv.org/abs/2209.12325v1",
    "title": "An Empirical Study on Cross-X Transfer for Legal Judgment Prediction",
    "authors": [
      "Joel Niklaus",
      "Matthias St\u00fcrmer",
      "Ilias Chalkidis"
    ],
    "abstract": "  Cross-lingual transfer learning has proven useful in a variety of Natural\nLanguage Processing (NLP) tasks, but it is understudied in the context of legal\nNLP, and not at all in Legal Judgment Prediction (LJP). We explore transfer\nlearning techniques on LJP using the trilingual Swiss-Judgment-Prediction\ndataset, including cases written in three languages. We find that cross-lingual\ntransfer improves the overall results across languages, especially when we use\nadapter-based fine-tuning. Finally, we further improve the model's performance\nby augmenting the training dataset with machine-translated versions of the\noriginal documents, using a 3x larger training corpus. Further on, we perform\nan analysis exploring the effect of cross-domain and cross-regional transfer,\ni.e., train a model across domains (legal areas), or regions. We find that in\nboth settings (legal areas, origin regions), models trained across all groups\nperform overall better, while they also have improved results in the worst-case\nscenarios. Finally, we report improved results when we ambitiously apply\ncross-jurisdiction transfer, where we further augment our dataset with Indian\nlegal cases.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68T50",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.2",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-25T21:41:56Z",
    "updated": "2022-09-25T21:41:56Z",
    "doi": null
  },
  "1806.02920": {
    "id": "http://arxiv.org/abs/1806.02920v1",
    "title": "GAIN: Missing Data Imputation using Generative Adversarial Nets",
    "authors": [
      "Jinsung Yoon",
      "James Jordon",
      "Mihaela van der Schaar"
    ],
    "abstract": "  We propose a novel method for imputing missing data by adapting the\nwell-known Generative Adversarial Nets (GAN) framework. Accordingly, we call\nour method Generative Adversarial Imputation Nets (GAIN). The generator (G)\nobserves some components of a real data vector, imputes the missing components\nconditioned on what is actually observed, and outputs a completed vector. The\ndiscriminator (D) then takes a completed vector and attempts to determine which\ncomponents were actually observed and which were imputed. To ensure that D\nforces G to learn the desired distribution, we provide D with some additional\ninformation in the form of a hint vector. The hint reveals to D partial\ninformation about the missingness of the original sample, which is used by D to\nfocus its attention on the imputation quality of particular components. This\nhint ensures that G does in fact learn to generate according to the true data\ndistribution. We tested our method on various datasets and found that GAIN\nsignificantly outperforms state-of-the-art imputation methods.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-06-07T22:57:16Z",
    "updated": "2018-06-07T22:57:16Z",
    "doi": null
  },
  "2402.11487": {
    "id": "http://arxiv.org/abs/2402.11487v2",
    "title": "Visual Concept-driven Image Generation with Text-to-Image Diffusion\n  Model",
    "authors": [
      "Tanzila Rahman",
      "Shweta Mahajan",
      "Hsin-Ying Lee",
      "Jian Ren",
      "Sergey Tulyakov",
      "Leonid Sigal"
    ],
    "abstract": "  Text-to-image (TTI) diffusion models have demonstrated impressive results in\ngenerating high-resolution images of complex and imaginative scenes. Recent\napproaches have further extended these methods with personalization techniques\nthat allow them to integrate user-illustrated concepts (e.g., the user\nhim/herself) using a few sample image illustrations. However, the ability to\ngenerate images with multiple interacting concepts, such as human subjects, as\nwell as concepts that may be entangled in one, or across multiple, image\nillustrations remains illusive. In this work, we propose a concept-driven TTI\npersonalization framework that addresses these core challenges. We build on\nexisting works that learn custom tokens for user-illustrated concepts, allowing\nthose to interact with existing text tokens in the TTI model. However,\nimportantly, to disentangle and better learn the concepts in question, we\njointly learn (latent) segmentation masks that disentangle these concepts in\nuser-provided image illustrations. We do so by introducing an Expectation\nMaximization (EM)-like optimization procedure where we alternate between\nlearning the custom tokens and estimating (latent) masks encompassing\ncorresponding concepts in user-supplied images. We obtain these masks based on\ncross-attention, from within the U-Net parameterized latent diffusion model and\nsubsequent DenseCRF optimization. We illustrate that such joint alternating\nrefinement leads to the learning of better tokens for concepts and, as a\nby-product, latent masks. We illustrate the benefits of the proposed approach\nqualitatively and quantitatively with several examples and use cases that can\ncombine three or more entangled concepts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-18T07:28:37Z",
    "updated": "2024-07-17T01:47:16Z",
    "doi": null
  },
  "2406.11132": {
    "id": "http://arxiv.org/abs/2406.11132v1",
    "title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language\n  Models Agents",
    "authors": [
      "Weizhe Chen",
      "Sven Koenig",
      "Bistra Dilkina"
    ],
    "abstract": "  In this past year, large language models (LLMs) have had remarkable success\nin domains outside the traditional natural language processing, and people are\nstarting to explore the usage of LLMs in more general and close to application\ndomains like code generation, travel planning, and robot controls. Connecting\nthese LLMs with great capacity and external tools, people are building the\nso-called LLM agents, which are supposed to help people do all kinds of work in\neveryday life. In all these domains, the prompt to the LLMs has been shown to\nmake a big difference in what the LLM would generate and thus affect the\nperformance of the LLM agents. Therefore, automatic prompt engineering has\nbecome an important question for many researchers and users of LLMs. In this\npaper, we propose a novel method, \\textsc{RePrompt}, which does \"gradient\ndescent\" to optimize the step-by-step instructions in the prompt of the LLM\nagents based on the chat history obtained from interactions with LLM agents. By\noptimizing the prompt, the LLM will learn how to plan in specific domains. We\nhave used experiments in PDDL generation and travel planning to show that our\nmethod could generally improve the performance for different reasoning tasks\nwhen using the updated prompt as the initial prompt.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-17T01:23:11Z",
    "updated": "2024-06-17T01:23:11Z",
    "doi": null
  },
  "2312.06742": {
    "id": "http://arxiv.org/abs/2312.06742v2",
    "title": "Honeybee: Locality-enhanced Projector for Multimodal LLM",
    "authors": [
      "Junbum Cha",
      "Wooyoung Kang",
      "Jonghwan Mun",
      "Byungseok Roh"
    ],
    "abstract": "  In Multimodal Large Language Models (MLLMs), a visual projector plays a\ncrucial role in bridging pre-trained vision encoders with LLMs, enabling\nprofound visual understanding while harnessing the LLMs' robust capabilities.\nDespite the importance of the visual projector, it has been relatively less\nexplored. In this study, we first identify two essential projector properties:\n(i) flexibility in managing the number of visual tokens, crucial for MLLMs'\noverall efficiency, and (ii) preservation of local context from visual\nfeatures, vital for spatial understanding. Based on these findings, we propose\na novel projector design that is both flexible and locality-enhanced,\neffectively satisfying the two desirable properties. Additionally, we present\ncomprehensive strategies to effectively utilize multiple and multifaceted\ninstruction datasets. Through extensive experiments, we examine the impact of\nindividual design choices. Finally, our proposed MLLM, Honeybee, remarkably\noutperforms previous state-of-the-art methods across various benchmarks,\nincluding MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly\nhigher efficiency. Code and models are available at\nhttps://github.com/kakaobrain/honeybee.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-11T18:59:06Z",
    "updated": "2024-04-01T03:00:06Z",
    "doi": null
  },
  "2402.09966": {
    "id": "http://arxiv.org/abs/2402.09966v1",
    "title": "Textual Localization: Decomposing Multi-concept Images for\n  Subject-Driven Text-to-Image Generation",
    "authors": [
      "Junjie Shentu",
      "Matthew Watson",
      "Noura Al Moubayed"
    ],
    "abstract": "  Subject-driven text-to-image diffusion models empower users to tailor the\nmodel to new concepts absent in the pre-training dataset using a few sample\nimages. However, prevalent subject-driven models primarily rely on\nsingle-concept input images, facing challenges in specifying the target concept\nwhen dealing with multi-concept input images. To this end, we introduce a\ntextual localized text-to-image model (Texual Localization) to handle\nmulti-concept input images. During fine-tuning, our method incorporates a novel\ncross-attention guidance to decompose multiple concepts, establishing distinct\nconnections between the visual representation of the target concept and the\nidentifier token in the text prompt. Experimental results reveal that our\nmethod outperforms or performs comparably to the baseline models in terms of\nimage fidelity and image-text alignment on multi-concept input images. In\ncomparison to Custom Diffusion, our method with hard guidance achieves CLIP-I\nscores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85%\nhigher in single-concept and multi-concept generation, respectively. Notably,\nour method generates cross-attention maps consistent with the target concept in\nthe generated images, a capability absent in existing models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-15T14:19:42Z",
    "updated": "2024-02-15T14:19:42Z",
    "doi": null
  },
  "2104.08718": {
    "id": "http://arxiv.org/abs/2104.08718v3",
    "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
    "authors": [
      "Jack Hessel",
      "Ari Holtzman",
      "Maxwell Forbes",
      "Ronan Le Bras",
      "Yejin Choi"
    ],
    "abstract": "  Image captioning has conventionally relied on reference-based automatic\nevaluations, where machine captions are compared against captions written by\nhumans. This is in contrast to the reference-free manner in which humans assess\ncaption quality.\n  In this paper, we report the surprising empirical finding that CLIP (Radford\net al., 2021), a cross-modal model pretrained on 400M image+caption pairs from\nthe web, can be used for robust automatic evaluation of image captioning\nwithout the need for references. Experiments spanning several corpora\ndemonstrate that our new reference-free metric, CLIPScore, achieves the highest\ncorrelation with human judgements, outperforming existing reference-based\nmetrics like CIDEr and SPICE. Information gain experiments demonstrate that\nCLIPScore, with its tight focus on image-text compatibility, is complementary\nto existing reference-based metrics that emphasize text-text similarities.\nThus, we also present a reference-augmented version, RefCLIPScore, which\nachieves even higher correlation. Beyond literal description tasks, several\ncase studies reveal domains where CLIPScore performs well (clip-art images,\nalt-text rating), but also where it is relatively weaker in comparison to\nreference-based metrics, e.g., news captions that require richer contextual\nknowledge.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-18T05:00:29Z",
    "updated": "2022-03-23T19:47:21Z",
    "doi": null
  },
  "2312.06635": {
    "id": "http://arxiv.org/abs/2312.06635v6",
    "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
    "authors": [
      "Songlin Yang",
      "Bailin Wang",
      "Yikang Shen",
      "Rameswar Panda",
      "Yoon Kim"
    ],
    "abstract": "  Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-11T18:51:59Z",
    "updated": "2024-08-27T01:27:29Z",
    "doi": null
  },
  "2008.05221": {
    "id": "http://arxiv.org/abs/2008.05221v4",
    "title": "Compression of Deep Learning Models for Text: A Survey",
    "authors": [
      "Manish Gupta",
      "Puneet Agrawal"
    ],
    "abstract": "  In recent years, the fields of natural language processing (NLP) and\ninformation retrieval (IR) have made tremendous progress thanksto deep learning\nmodels like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and\nLong Short-Term Memory (LSTMs)networks, and Transformer [120] based models like\nBidirectional Encoder Representations from Transformers (BERT) [24],\nGenerativePre-training Transformer (GPT-2) [94], Multi-task Deep Neural Network\n(MT-DNN) [73], Extra-Long Network (XLNet) [134], Text-to-text transfer\ntransformer (T5) [95], T-NLG [98] and GShard [63]. But these models are\nhumongous in size. On the other hand,real world applications demand small model\nsize, low response times and low computational power wattage. In this survey,\nwediscuss six different types of methods (Pruning, Quantization, Knowledge\nDistillation, Parameter Sharing, Tensor Decomposition, andSub-quadratic\nTransformer based methods) for compression of such models to enable their\ndeployment in real industry NLP projects.Given the critical need of building\napplications with efficient and small models, and the large amount of recently\npublished work inthis area, we believe that this survey organizes the plethora\nof work done by the 'deep learning for NLP' community in the past fewyears and\npresents it as a coherent story.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-08-12T10:42:14Z",
    "updated": "2021-06-13T17:47:28Z",
    "doi": null
  },
  "2309.17179": {
    "id": "http://arxiv.org/abs/2309.17179v2",
    "title": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and\n  Training",
    "authors": [
      "Xidong Feng",
      "Ziyu Wan",
      "Muning Wen",
      "Stephen Marcus McAleer",
      "Ying Wen",
      "Weinan Zhang",
      "Jun Wang"
    ],
    "abstract": "  Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim\nto augment the reasoning capabilities of LLMs by using tree-search algorithms\nto guide multi-step reasoning. These methods rely on prompting a pre-trained\nmodel to serve as a value function and focus on problems with low search depth.\nAs a result, these methods will not work in domains where the pre-trained LLM\ndoes not have enough knowledge to serve as an effective value function or in\ndomains that require long-horizon planning. To address these limitations, we\npresent an AlphaZero-like tree-search learning framework for LLMs (termed\nTS-LLM), systematically illustrating how tree-search with a learned value\nfunction can guide LLM decoding. TS-LLM distinguishes itself in two key ways.\n(1) Leveraging a learned value function and AlphaZero-like algorithms, our\napproach can be generally adaptable to a wide range of tasks, language models\nof any size, and tasks of varying search depths. (2) Our approach can guide\nLLMs during both inference and training, iteratively improving the LLM.\nEmpirical results across reasoning, planning, alignment, and decision-making\ntasks show that TS-LLM outperforms existing approaches and can handle trees\nwith a depth of 64.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-29T12:20:19Z",
    "updated": "2024-02-09T00:13:46Z",
    "doi": null
  },
  "1909.01066": {
    "id": "http://arxiv.org/abs/1909.01066v2",
    "title": "Language Models as Knowledge Bases?",
    "authors": [
      "Fabio Petroni",
      "Tim Rockt\u00e4schel",
      "Patrick Lewis",
      "Anton Bakhtin",
      "Yuxiang Wu",
      "Alexander H. Miller",
      "Sebastian Riedel"
    ],
    "abstract": "  Recent progress in pretraining language models on large textual corpora led\nto a surge of improvements for downstream NLP tasks. Whilst learning linguistic\nknowledge, these models may also be storing relational knowledge present in the\ntraining data, and may be able to answer queries structured as\n\"fill-in-the-blank\" cloze statements. Language models have many advantages over\nstructured knowledge bases: they require no schema engineering, allow\npractitioners to query about an open class of relations, are easy to extend to\nmore data, and require no human supervision to train. We present an in-depth\nanalysis of the relational knowledge already present (without fine-tuning) in a\nwide range of state-of-the-art pretrained language models. We find that (i)\nwithout fine-tuning, BERT contains relational knowledge competitive with\ntraditional NLP methods that have some access to oracle knowledge, (ii) BERT\nalso does remarkably well on open-domain question answering against a\nsupervised baseline, and (iii) certain types of factual knowledge are learned\nmuch more readily than others by standard language model pretraining\napproaches. The surprisingly strong ability of these models to recall factual\nknowledge without any fine-tuning demonstrates their potential as unsupervised\nopen-domain QA systems. The code to reproduce our analysis is available at\nhttps://github.com/facebookresearch/LAMA.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-09-03T11:11:08Z",
    "updated": "2019-09-04T09:33:20Z",
    "doi": null
  },
  "2111.07991": {
    "id": "http://arxiv.org/abs/2111.07991v3",
    "title": "LiT: Zero-Shot Transfer with Locked-image text Tuning",
    "authors": [
      "Xiaohua Zhai",
      "Xiao Wang",
      "Basil Mustafa",
      "Andreas Steiner",
      "Daniel Keysers",
      "Alexander Kolesnikov",
      "Lucas Beyer"
    ],
    "abstract": "  This paper presents contrastive-tuning, a simple method employing contrastive\ntraining to align image and text models while still taking advantage of their\npre-training. In our empirical study we find that locked pre-trained image\nmodels with unlocked text models work best. We call this instance of\ncontrastive-tuning \"Locked-image Tuning\" (LiT), which just teaches a text model\nto read out good representations from a pre-trained image model for new tasks.\nA LiT model gains the capability of zero-shot transfer to new vision tasks,\nsuch as image classification or retrieval. The proposed LiT is widely\napplicable; it works reliably with multiple pre-training methods (supervised\nand unsupervised) and across diverse architectures (ResNet, Vision Transformers\nand MLP-Mixer) using three different image-text datasets. With the\ntransformer-based pre-trained ViT-g/14 model, the LiT model achieves 85.2%\nzero-shot transfer accuracy on the ImageNet test set, and 82.5% on the\nchallenging out-of-distribution ObjectNet test set.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-15T18:53:48Z",
    "updated": "2022-06-22T14:43:02Z",
    "doi": null
  },
  "2207.04324": {
    "id": "http://arxiv.org/abs/2207.04324v2",
    "title": "Video Coding Using Learned Latent GAN Compression",
    "authors": [
      "Mustafa Shukor",
      "Bharath Bhushan Damodaran",
      "Xu Yao",
      "Pierre Hellier"
    ],
    "abstract": "  We propose in this paper a new paradigm for facial video compression. We\nleverage the generative capacity of GANs such as StyleGAN to represent and\ncompress a video, including intra and inter compression. Each frame is inverted\nin the latent space of StyleGAN, from which the optimal compression is learned.\nTo do so, a diffeomorphic latent representation is learned using a normalizing\nflows model, where an entropy model can be optimized for image coding. In\naddition, we propose a new perceptual loss that is more efficient than other\ncounterparts. Finally, an entropy model for video inter coding with residual is\nalso learned in the previously constructed latent representation. Our method\n(SGANC) is simple, faster to train, and achieves better results for image and\nvideo coding compared to state-of-the-art codecs such as VTM, AV1, and recent\ndeep learning techniques. In particular, it drastically minimizes perceptual\ndistortion at low bit rates.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-09T19:07:43Z",
    "updated": "2022-07-12T21:17:39Z",
    "doi": null
  },
  "1709.01509": {
    "id": "http://arxiv.org/abs/1709.01509v1",
    "title": "Linking Generative Adversarial Learning and Binary Classification",
    "authors": [
      "Akshay Balsubramani"
    ],
    "abstract": "  In this note, we point out a basic link between generative adversarial (GA)\ntraining and binary classification -- any powerful discriminator essentially\ncomputes an (f-)divergence between real and generated samples. The result,\nrepeatedly re-derived in decision theory, has implications for GA Networks\n(GANs), providing an alternative perspective on training f-GANs by designing\nthe discriminator loss function.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-09-05T17:55:59Z",
    "updated": "2017-09-05T17:55:59Z",
    "doi": null
  },
  "2306.00914": {
    "id": "http://arxiv.org/abs/2306.00914v3",
    "title": "Conditioning Diffusion Models via Attributes and Semantic Masks for Face\n  Generation",
    "authors": [
      "Nico Giambi",
      "Giuseppe Lisanti"
    ],
    "abstract": "  Deep generative models have shown impressive results in generating realistic\nimages of faces. GANs managed to generate high-quality, high-fidelity images\nwhen conditioned on semantic masks, but they still lack the ability to\ndiversify their output. Diffusion models partially solve this problem and are\nable to generate diverse samples given the same condition. In this paper, we\npropose a multi-conditioning approach for diffusion models via cross-attention\nexploiting both attributes and semantic masks to generate high-quality and\ncontrollable face images. We also studied the impact of applying\nperceptual-focused loss weighting into the latent space instead of the pixel\nspace. Our method extends the previous approaches by introducing conditioning\non more than one set of features, guaranteeing a more fine-grained control over\nthe generated face images. We evaluate our approach on the CelebA-HQ dataset,\nand we show that it can generate realistic and diverse samples while allowing\nfor fine-grained control over multiple attributes and semantic regions.\nAdditionally, we perform an ablation study to evaluate the impact of different\nconditioning strategies on the quality and diversity of the generated images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-01T17:16:37Z",
    "updated": "2023-09-27T18:13:12Z",
    "doi": null
  },
  "2401.14361": {
    "id": "http://arxiv.org/abs/2401.14361v2",
    "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
    "authors": [
      "Leyang Xue",
      "Yao Fu",
      "Zhan Lu",
      "Luo Mai",
      "Mahesh Marina"
    ],
    "abstract": "  This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PF",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-25T18:07:50Z",
    "updated": "2024-08-01T13:21:24Z",
    "doi": null
  },
  "2301.13622": {
    "id": "http://arxiv.org/abs/2301.13622v2",
    "title": "Learning Data Representations with Joint Diffusion Models",
    "authors": [
      "Kamil Deja",
      "Tomasz Trzcinski",
      "Jakub M. Tomczak"
    ],
    "abstract": "  Joint machine learning models that allow synthesizing and classifying data\noften offer uneven performance between those tasks or are unstable to train. In\nthis work, we depart from a set of empirical observations that indicate the\nusefulness of internal representations built by contemporary deep\ndiffusion-based generative models not only for generating but also predicting.\nWe then propose to extend the vanilla diffusion model with a classifier that\nallows for stable joint end-to-end training with shared parameterization\nbetween those objectives. The resulting joint diffusion model outperforms\nrecent state-of-the-art hybrid methods in terms of both classification and\ngeneration quality on all evaluated benchmarks. On top of our joint training\napproach, we present how we can directly benefit from shared generative and\ndiscriminative representations by introducing a method for visual\ncounterfactual explanations.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-31T13:29:19Z",
    "updated": "2023-04-05T13:09:54Z",
    "doi": null
  },
  "2302.14859": {
    "id": "http://arxiv.org/abs/2302.14859v2",
    "title": "BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis",
    "authors": [
      "Lior Yariv",
      "Peter Hedman",
      "Christian Reiser",
      "Dor Verbin",
      "Pratul P. Srinivasan",
      "Richard Szeliski",
      "Jonathan T. Barron",
      "Ben Mildenhall"
    ],
    "abstract": "  We present a method for reconstructing high-quality meshes of large unbounded\nreal-world scenes suitable for photorealistic novel view synthesis. We first\noptimize a hybrid neural volume-surface scene representation designed to have\nwell-behaved level sets that correspond to surfaces in the scene. We then bake\nthis representation into a high-quality triangle mesh, which we equip with a\nsimple and fast view-dependent appearance model based on spherical Gaussians.\nFinally, we optimize this baked representation to best reproduce the captured\nviewpoints, resulting in a model that can leverage accelerated polygon\nrasterization pipelines for real-time view synthesis on commodity hardware. Our\napproach outperforms previous scene representations for real-time rendering in\nterms of accuracy, speed, and power consumption, and produces high quality\nmeshes that enable applications such as appearance editing and physical\nsimulation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-28T18:58:03Z",
    "updated": "2023-05-16T15:01:42Z",
    "doi": null
  },
  "2211.00593": {
    "id": "http://arxiv.org/abs/2211.00593v1",
    "title": "Interpretability in the Wild: a Circuit for Indirect Object\n  Identification in GPT-2 small",
    "authors": [
      "Kevin Wang",
      "Alexandre Variengien",
      "Arthur Conmy",
      "Buck Shlegeris",
      "Jacob Steinhardt"
    ],
    "abstract": "  Research in mechanistic interpretability seeks to explain behaviors of\nmachine learning models in terms of their internal components. However, most\nprevious work either focuses on simple behaviors in small models, or describes\ncomplicated behaviors in larger models with broad strokes. In this work, we\nbridge this gap by presenting an explanation for how GPT-2 small performs a\nnatural language task called indirect object identification (IOI). Our\nexplanation encompasses 26 attention heads grouped into 7 main classes, which\nwe discovered using a combination of interpretability approaches relying on\ncausal interventions. To our knowledge, this investigation is the largest\nend-to-end attempt at reverse-engineering a natural behavior \"in the wild\" in a\nlanguage model. We evaluate the reliability of our explanation using three\nquantitative criteria--faithfulness, completeness and minimality. Though these\ncriteria support our explanation, they also point to remaining gaps in our\nunderstanding. Our work provides evidence that a mechanistic understanding of\nlarge ML models is feasible, opening opportunities to scale our understanding\nto both larger models and more complex tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-01T17:08:44Z",
    "updated": "2022-11-01T17:08:44Z",
    "doi": null
  },
  "2106.00291": {
    "id": "http://arxiv.org/abs/2106.00291v1",
    "title": "Preview, Attend and Review: Schema-Aware Curriculum Learning for\n  Multi-Domain Dialog State Tracking",
    "authors": [
      "Yinpei Dai",
      "Hangyu Li",
      "Yongbin Li",
      "Jian Sun",
      "Fei Huang",
      "Luo Si",
      "Xiaodan Zhu"
    ],
    "abstract": "  Existing dialog state tracking (DST) models are trained with dialog data in a\nrandom order, neglecting rich structural information in a dataset. In this\npaper, we propose to use curriculum learning (CL) to better leverage both the\ncurriculum structure and schema structure for task-oriented dialogs.\nSpecifically, we propose a model-agnostic framework called Schema-aware\nCurriculum Learning for Dialog State Tracking (SaCLog), which consists of a\npreview module that pre-trains a DST model with schema information, a\ncurriculum module that optimizes the model with CL, and a review module that\naugments mispredicted data to reinforce the CL training. We show that our\nproposed approach improves DST performance over both a transformer-based and\nRNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art\nresults on WOZ2.0 and MultiWOZ2.1.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-01T07:52:35Z",
    "updated": "2021-06-01T07:52:35Z",
    "doi": null
  },
  "2210.12254": {
    "id": "http://arxiv.org/abs/2210.12254v2",
    "title": "Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models",
    "authors": [
      "Vikram Voleti",
      "Christopher Pal",
      "Adam Oberman"
    ],
    "abstract": "  Generative models based on denoising diffusion techniques have led to an\nunprecedented increase in the quality and diversity of imagery that is now\npossible to create with neural generative models. However, most contemporary\nstate-of-the-art methods are derived from a standard isotropic Gaussian\nformulation. In this work we examine the situation where non-isotropic Gaussian\ndistributions are used. We present the key mathematical derivations for\ncreating denoising diffusion models using an underlying non-isotropic Gaussian\nnoise model. We also provide initial experiments with the CIFAR-10 dataset to\nhelp verify empirically that this more general modeling approach can also yield\nhigh-quality samples.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-21T21:16:46Z",
    "updated": "2022-11-23T00:40:58Z",
    "doi": null
  },
  "2310.08785": {
    "id": "http://arxiv.org/abs/2310.08785v1",
    "title": "DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided\n  Image Editing",
    "authors": [
      "Yueming Lyu",
      "Kang Zhao",
      "Bo Peng",
      "Yue Jiang",
      "Yingya Zhang",
      "Jing Dong"
    ],
    "abstract": "  Text-guided image editing faces significant challenges to training and\ninference flexibility. Much literature collects large amounts of annotated\nimage-text pairs to train text-conditioned generative models from scratch,\nwhich is expensive and not efficient. After that, some approaches that leverage\npre-trained vision-language models are put forward to avoid data collection,\nbut they are also limited by either per text-prompt optimization or\ninference-time hyper-parameters tuning. To address these issues, we investigate\nand identify a specific space, referred to as CLIP DeltaSpace, where the CLIP\nvisual feature difference of two images is semantically aligned with the CLIP\ntextual feature difference of their corresponding text descriptions. Based on\nDeltaSpace, we propose a novel framework called DeltaEdit, which maps the CLIP\nvisual feature differences to the latent space directions of a generative model\nduring the training phase, and predicts the latent space directions from the\nCLIP textual feature differences during the inference phase. And this design\nendows DeltaEdit with two advantages: (1) text-free training; (2)\ngeneralization to various text prompts for zero-shot inference. Extensive\nexperiments validate the effectiveness and versatility of DeltaEdit with\ndifferent generative models, including both the GAN model and the diffusion\nmodel, in achieving flexible text-guided image editing. Code is available at\nhttps://github.com/Yueming6568/DeltaEdit.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-12T15:43:12Z",
    "updated": "2023-10-12T15:43:12Z",
    "doi": null
  },
  "2106.15326": {
    "id": "http://arxiv.org/abs/2106.15326v1",
    "title": "Source-free Domain Adaptation via Avatar Prototype Generation and\n  Adaptation",
    "authors": [
      "Zhen Qiu",
      "Yifan Zhang",
      "Hongbin Lin",
      "Shuaicheng Niu",
      "Yanxia Liu",
      "Qing Du",
      "Mingkui Tan"
    ],
    "abstract": "  We study a practical domain adaptation task, called source-free unsupervised\ndomain adaptation (UDA) problem, in which we cannot access source domain data\ndue to data privacy issues but only a pre-trained source model and unlabeled\ntarget data are available. This task, however, is very difficult due to one key\nchallenge: the lack of source data and target domain labels makes model\nadaptation very challenging. To address this, we propose to mine the hidden\nknowledge in the source model and exploit it to generate source avatar\nprototypes (i.e., representative features for each source class) as well as\ntarget pseudo labels for domain alignment. To this end, we propose a\nContrastive Prototype Generation and Adaptation (CPGA) method. Specifically,\nCPGA consists of two stages: (1) prototype generation: by exploring the\nclassification boundary information of the source model, we train a prototype\ngenerator to generate avatar prototypes via contrastive learning. (2) prototype\nadaptation: based on the generated source prototypes and target pseudo labels,\nwe develop a new robust contrastive prototype adaptation strategy to align each\npseudo-labeled target data to the corresponding source prototypes. Extensive\nexperiments on three UDA benchmark datasets demonstrate the effectiveness and\nsuperiority of the proposed method.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-18T08:30:54Z",
    "updated": "2021-06-18T08:30:54Z",
    "doi": null
  },
  "1712.02514": {
    "id": "http://arxiv.org/abs/1712.02514v1",
    "title": "TV-GAN: Generative Adversarial Network Based Thermal to Visible Face\n  Recognition",
    "authors": [
      "Teng Zhang",
      "Arnold Wiliem",
      "Siqi Yang",
      "Brian C. Lovell"
    ],
    "abstract": "  This work tackles the face recognition task on images captured using thermal\ncamera sensors which can operate in the non-light environment. While it can\ngreatly increase the scope and benefits of the current security surveillance\nsystems, performing such a task using thermal images is a challenging problem\ncompared to face recognition task in the Visible Light Domain (VLD). This is\npartly due to the much smaller amount number of thermal imagery data collected\ncompared to the VLD data. Unfortunately, direct application of the existing\nvery strong face recognition models trained using VLD data into the thermal\nimagery data will not produce a satisfactory performance. This is due to the\nexistence of the domain gap between the thermal and VLD images. To this end, we\npropose a Thermal-to-Visible Generative Adversarial Network (TV-GAN) that is\nable to transform thermal face images into their corresponding VLD images\nwhilst maintaining identity information which is sufficient enough for the\nexisting VLD face recognition models to perform recognition. Some examples are\npresented in Figure 1. Unlike the previous methods, our proposed TV-GAN uses an\nexplicit closed-set face recognition loss to regularize the discriminator\nnetwork training. This information will then be conveyed into the generator\nnetwork in the forms of gradient loss. In the experiment, we show that by using\nthis additional explicit regularization for the discriminator network, the\nTV-GAN is able to preserve more identity information when translating a thermal\nimage of a person which is not seen before by the TV-GAN.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-12-07T07:06:39Z",
    "updated": "2017-12-07T07:06:39Z",
    "doi": null
  },
  "2103.02885": {
    "id": "http://arxiv.org/abs/2103.02885v1",
    "title": "Extract the Knowledge of Graph Neural Networks and Go Beyond it: An\n  Effective Knowledge Distillation Framework",
    "authors": [
      "Cheng Yang",
      "Jiawei Liu",
      "Chuan Shi"
    ],
    "abstract": "  Semi-supervised learning on graphs is an important problem in the machine\nlearning area. In recent years, state-of-the-art classification methods based\non graph neural networks (GNNs) have shown their superiority over traditional\nones such as label propagation. However, the sophisticated architectures of\nthese neural models will lead to a complex prediction mechanism, which could\nnot make full use of valuable prior knowledge lying in the data, e.g.,\nstructurally correlated nodes tend to have the same class. In this paper, we\npropose a framework based on knowledge distillation to address the above\nissues. Our framework extracts the knowledge of an arbitrary learned GNN model\n(teacher model), and injects it into a well-designed student model. The student\nmodel is built with two simple prediction mechanisms, i.e., label propagation\nand feature transformation, which naturally preserves structure-based and\nfeature-based prior knowledge, respectively. In specific, we design the student\nmodel as a trainable combination of parameterized label propagation and feature\ntransformation modules. As a result, the learned student can benefit from both\nprior knowledge and the knowledge in GNN teachers for more effective\npredictions. Moreover, the learned student model has a more interpretable\nprediction process than GNNs. We conduct experiments on five public benchmark\ndatasets and employ seven GNN models including GCN, GAT, APPNP, SAGE, SGC,\nGCNII and GLP as the teacher models. Experimental results show that the learned\nstudent model can consistently outperform its corresponding teacher model by\n1.4% - 4.7% on average. Code and data are available at\nhttps://github.com/BUPT-GAMMA/CPF\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-03-04T08:13:55Z",
    "updated": "2021-03-04T08:13:55Z",
    "doi": null
  },
  "2308.00356": {
    "id": "http://arxiv.org/abs/2308.00356v1",
    "title": "Deep Image Harmonization with Globally Guided Feature Transformation and\n  Relation Distillation",
    "authors": [
      "Li Niu",
      "Linfeng Tan",
      "Xinhao Tao",
      "Junyan Cao",
      "Fengjun Guo",
      "Teng Long",
      "Liqing Zhang"
    ],
    "abstract": "  Given a composite image, image harmonization aims to adjust the foreground\nillumination to be consistent with background. Previous methods have explored\ntransforming foreground features to achieve competitive performance. In this\nwork, we show that using global information to guide foreground feature\ntransformation could achieve significant improvement. Besides, we propose to\ntransfer the foreground-background relation from real images to composite\nimages, which can provide intermediate supervision for the transformed encoder\nfeatures. Additionally, considering the drawbacks of existing harmonization\ndatasets, we also contribute a ccHarmony dataset which simulates the natural\nillumination variation. Extensive experiments on iHarmony4 and our contributed\ndataset demonstrate the superiority of our method. Our ccHarmony dataset is\nreleased at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-01T07:53:25Z",
    "updated": "2023-08-01T07:53:25Z",
    "doi": null
  },
  "2210.10828": {
    "id": "http://arxiv.org/abs/2210.10828v1",
    "title": "Grounded Video Situation Recognition",
    "authors": [
      "Zeeshan Khan",
      "C. V. Jawahar",
      "Makarand Tapaswi"
    ],
    "abstract": "  Dense video understanding requires answering several questions such as who is\ndoing what to whom, with what, how, why, and where. Recently, Video Situation\nRecognition (VidSitu) is framed as a task for structured prediction of multiple\nevents, their relationships, and actions and various verb-role pairs attached\nto descriptive entities. This task poses several challenges in identifying,\ndisambiguating, and co-referencing entities across multiple verb-role pairs,\nbut also faces some challenges of evaluation. In this work, we propose the\naddition of spatio-temporal grounding as an essential component of the\nstructured prediction task in a weakly supervised setting, and present a novel\nthree stage Transformer model, VideoWhisperer, that is empowered to make joint\npredictions. In stage one, we learn contextualised embeddings for video\nfeatures in parallel with key objects that appear in the video clips to enable\nfine-grained spatio-temporal reasoning. The second stage sees verb-role queries\nattend and pool information from object embeddings, localising answers to\nquestions posed about the action. The final stage generates these answers as\ncaptions to describe each verb-role pair present in the video. Our model\noperates on a group of events (clips) simultaneously and predicts verbs,\nverb-role pairs, their nouns, and their grounding on-the-fly. When evaluated on\na grounding-augmented version of the VidSitu dataset, we observe a large\nimprovement in entity captioning accuracy, as well as the ability to localize\nverb-roles without grounding annotations at training time.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-19T18:38:10Z",
    "updated": "2022-10-19T18:38:10Z",
    "doi": null
  },
  "2304.01373": {
    "id": "http://arxiv.org/abs/2304.01373v2",
    "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and\n  Scaling",
    "authors": [
      "Stella Biderman",
      "Hailey Schoelkopf",
      "Quentin Anthony",
      "Herbie Bradley",
      "Kyle O'Brien",
      "Eric Hallahan",
      "Mohammad Aflah Khan",
      "Shivanshu Purohit",
      "USVSN Sai Prashanth",
      "Edward Raff",
      "Aviya Skowron",
      "Lintang Sutawika",
      "Oskar van der Wal"
    ],
    "abstract": "  How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\n\\url{https://github.com/EleutherAI/pythia}.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-03T20:58:15Z",
    "updated": "2023-05-31T17:54:07Z",
    "doi": null
  },
  "2211.02077": {
    "id": "http://arxiv.org/abs/2211.02077v1",
    "title": "Scaling Multimodal Pre-Training via Cross-Modality Gradient\n  Harmonization",
    "authors": [
      "Junru Wu",
      "Yi Liang",
      "Feng Han",
      "Hassan Akbari",
      "Zhangyang Wang",
      "Cong Yu"
    ],
    "abstract": "  Self-supervised pre-training recently demonstrates success on large-scale\nmultimodal data, and state-of-the-art contrastive learning methods often\nenforce the feature consistency from cross-modality inputs, such as video/audio\nor video/text pairs. Despite its convenience to formulate and leverage in\npractice, such cross-modality alignment (CMA) is only a weak and noisy\nsupervision, since two modalities can be semantically misaligned even they are\ntemporally aligned. For example, even in the commonly adopted instructional\nvideos, a speaker can sometimes refer to something that is not visually present\nin the current frame; and the semantic misalignment would only be more\nunpredictable for the raw videos from the internet. We conjecture that might\ncause conflicts and biases among modalities, and may hence prohibit CMA from\nscaling up to training with larger and more heterogeneous data. This paper\nfirst verifies our conjecture by observing that, even in the latest VATT\npre-training using only instructional videos, there exist strong gradient\nconflicts between different CMA losses within the same video, audio, text\ntriplet, indicating them as the noisy source of supervision. We then propose to\nharmonize such gradients, via two techniques: (i) cross-modality gradient\nrealignment: modifying different CMA loss gradients for each sample triplet, so\nthat their gradient directions are more aligned; and (ii) gradient-based\ncurriculum learning: leveraging the gradient conflict information on an\nindicator of sample noisiness, to develop a curriculum learning strategy to\nprioritize training on less noisy sample triplets. Applying those techniques to\npre-training VATT on the HowTo100M dataset, we consistently improve its\nperformance on different downstream tasks. Moreover, we are able to scale VATT\npre-training to more complicated non-narrative Youtube8M dataset to further\nimprove the state-of-the-arts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-03T18:12:32Z",
    "updated": "2022-11-03T18:12:32Z",
    "doi": null
  },
  "2406.02106": {
    "id": "http://arxiv.org/abs/2406.02106v1",
    "title": "MARS: Benchmarking the Metaphysical Reasoning Abilities of Language\n  Models with a Multi-task Evaluation Dataset",
    "authors": [
      "Weiqi Wang",
      "Yangqiu Song"
    ],
    "abstract": "  To enable Large Language Models (LLMs) to function as conscious agents with\ngeneralizable reasoning capabilities, it is crucial that they possess the\nreasoning ability to comprehend situational changes (transitions) in\ndistribution triggered by environmental factors or actions from other agents.\nDespite its fundamental significance, this ability remains underexplored due to\nthe complexity of modeling infinite possible changes in an event and their\nassociated distributions, coupled with the lack of benchmark data with\nsituational transitions. Addressing these gaps, we propose a novel formulation\nof reasoning with distributional changes as a three-step discriminative\nprocess, termed as MetAphysical ReaSoning. We then introduce the first-ever\nbenchmark, MARS, comprising three tasks corresponding to each step. These tasks\nsystematically assess LLMs' capabilities in reasoning the plausibility of (i)\nchanges in actions, (ii) states caused by changed actions, and (iii)\nsituational transitions driven by changes in action. Extensive evaluations with\n20 (L)LMs of varying sizes and methods indicate that all three tasks in this\nprocess pose significant challenges, even for state-of-the-art LLMs and LMs\nafter fine-tuning. Further analyses reveal potential causes for the\nunderperformance of LLMs and demonstrate that pre-training them on large-scale\nconceptualization taxonomies can potentially enhance their metaphysical\nreasoning capabilities. Our data and models are publicly accessible at\nhttps://github.com/HKUST-KnowComp/MARS.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-04T08:35:04Z",
    "updated": "2024-06-04T08:35:04Z",
    "doi": null
  },
  "1807.01622": {
    "id": "http://arxiv.org/abs/1807.01622v1",
    "title": "Neural Processes",
    "authors": [
      "Marta Garnelo",
      "Jonathan Schwarz",
      "Dan Rosenbaum",
      "Fabio Viola",
      "Danilo J. Rezende",
      "S. M. Ali Eslami",
      "Yee Whye Teh"
    ],
    "abstract": "  A neural network (NN) is a parameterised function that can be tuned via\ngradient descent to approximate a labelled collection of data with high\nprecision. A Gaussian process (GP), on the other hand, is a probabilistic model\nthat defines a distribution over possible functions, and is updated in light of\ndata via the rules of probabilistic inference. GPs are probabilistic,\ndata-efficient and flexible, however they are also computationally intensive\nand thus limited in their applicability. We introduce a class of neural latent\nvariable models which we call Neural Processes (NPs), combining the best of\nboth worlds. Like GPs, NPs define distributions over functions, are capable of\nrapid adaptation to new observations, and can estimate the uncertainty in their\npredictions. Like NNs, NPs are computationally efficient during training and\nevaluation but also learn to adapt their priors to data. We demonstrate the\nperformance of NPs on a range of learning tasks, including regression and\noptimisation, and compare and contrast with related models in the literature.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-07-04T14:49:46Z",
    "updated": "2018-07-04T14:49:46Z",
    "doi": null
  },
  "2112.11542": {
    "id": "http://arxiv.org/abs/2112.11542v1",
    "title": "MIA-Former: Efficient and Robust Vision Transformers via Multi-grained\n  Input-Adaptation",
    "authors": [
      "Zhongzhi Yu",
      "Yonggan Fu",
      "Sicheng Li",
      "Chaojian Li",
      "Yingyan Lin"
    ],
    "abstract": "  ViTs are often too computationally expensive to be fitted onto real-world\nresource-constrained devices, due to (1) their quadratically increased\ncomplexity with the number of input tokens and (2) their overparameterized\nself-attention heads and model depth. In parallel, different images are of\nvaried complexity and their different regions can contain various levels of\nvisual information, indicating that treating all regions/tokens equally in\nterms of model complexity is unnecessary while such opportunities for trimming\ndown ViTs' complexity have not been fully explored. To this end, we propose a\nMulti-grained Input-adaptive Vision Transformer framework dubbed MIA-Former\nthat can input-adaptively adjust the structure of ViTs at three\ncoarse-to-fine-grained granularities (i.e., model depth and the number of model\nheads/tokens). In particular, our MIA-Former adopts a low-cost network trained\nwith a hybrid supervised and reinforcement training method to skip unnecessary\nlayers, heads, and tokens in an input adaptive manner, reducing the overall\ncomputational cost. Furthermore, an interesting side effect of our MIA-Former\nis that its resulting ViTs are naturally equipped with improved robustness\nagainst adversarial attacks over their static counterparts, because\nMIA-Former's multi-grained dynamic control improves the model diversity similar\nto the effect of ensemble and thus increases the difficulty of adversarial\nattacks against all its sub-models. Extensive experiments and ablation studies\nvalidate that the proposed MIA-Former framework can effectively allocate\ncomputation budgets adaptive to the difficulty of input images meanwhile\nincrease robustness, achieving state-of-the-art (SOTA) accuracy-efficiency\ntrade-offs, e.g., 20% computation savings with the same or even a higher\naccuracy compared with SOTA dynamic transformer models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-12-21T22:06:24Z",
    "updated": "2021-12-21T22:06:24Z",
    "doi": null
  },
  "2204.13509": {
    "id": "http://arxiv.org/abs/2204.13509v2",
    "title": "On the Effect of Pretraining Corpora on In-context Learning by a\n  Large-scale Language Model",
    "authors": [
      "Seongjin Shin",
      "Sang-Woo Lee",
      "Hwijeen Ahn",
      "Sungdong Kim",
      "HyoungSeok Kim",
      "Boseop Kim",
      "Kyunghyun Cho",
      "Gichang Lee",
      "Woomyoung Park",
      "Jung-Woo Ha",
      "Nako Sung"
    ],
    "abstract": "  Many recent studies on large-scale language models have reported successful\nin-context zero- and few-shot learning ability. However, the in-depth analysis\nof when in-context learning occurs is still lacking. For example, it is unknown\nhow in-context learning performance changes as the training corpus varies.\nHere, we investigate the effects of the source and size of the pretraining\ncorpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From\nour in-depth investigation, we introduce the following observations: (1)\nin-context learning performance heavily depends on the corpus domain source,\nand the size of the pretraining corpus does not necessarily determine the\nemergence of in-context learning, (2) in-context learning ability can emerge\nwhen a language model is trained on a combination of multiple corpora, even\nwhen each corpus does not result in in-context learning on its own, (3)\npretraining with a corpus related to a downstream task does not always\nguarantee the competitive in-context learning performance of the downstream\ntask, especially in the few-shot setting, and (4) the relationship between\nlanguage modeling (measured in perplexity) and in-context learning does not\nalways correlate: e.g., low perplexity does not always imply high in-context\nfew-shot learning performance.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-28T13:59:54Z",
    "updated": "2022-05-08T06:36:19Z",
    "doi": null
  },
  "2206.00048": {
    "id": "http://arxiv.org/abs/2206.00048v2",
    "title": "PandA: Unsupervised Learning of Parts and Appearances in the Feature\n  Maps of GANs",
    "authors": [
      "James Oldfield",
      "Christos Tzelepis",
      "Yannis Panagakis",
      "Mihalis A. Nicolaou",
      "Ioannis Patras"
    ],
    "abstract": "  Recent advances in the understanding of Generative Adversarial Networks\n(GANs) have led to remarkable progress in visual editing and synthesis tasks,\ncapitalizing on the rich semantics that are embedded in the latent spaces of\npre-trained GANs. However, existing methods are often tailored to specific GAN\narchitectures and are limited to either discovering global semantic directions\nthat do not facilitate localized control, or require some form of supervision\nthrough manually provided regions or segmentation masks. In this light, we\npresent an architecture-agnostic approach that jointly discovers factors\nrepresenting spatial parts and their appearances in an entirely unsupervised\nfashion. These factors are obtained by applying a semi-nonnegative tensor\nfactorization on the feature maps, which in turn enables context-aware local\nimage editing with pixel-level control. In addition, we show that the\ndiscovered appearance factors correspond to saliency maps that localize\nconcepts of interest, without using any labels. Experiments on a wide range of\nGAN architectures and datasets show that, in comparison to the state of the\nart, our method is far more efficient in terms of training time and, most\nimportantly, provides much more accurate localized control. Our code is\navailable at: https://github.com/james-oldfield/PandA.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-31T18:28:39Z",
    "updated": "2023-02-06T15:33:18Z",
    "doi": null
  },
  "2310.09213": {
    "id": "http://arxiv.org/abs/2310.09213v2",
    "title": "Discovery and Expansion of New Domains within Diffusion Models",
    "authors": [
      "Ye Zhu",
      "Yu Wu",
      "Duo Xu",
      "Zhiwei Deng",
      "Yan Yan",
      "Olga Russakovsky"
    ],
    "abstract": "  In this work, we study the generalization properties of diffusion models in a\nfew-shot setup, introduce a novel tuning-free paradigm to synthesize the target\nout-of-domain (OOD) data, and demonstrate its advantages compared to existing\nmethods in data-sparse scenarios with large domain gaps. Specifically, given a\npre-trained model and a small set of images that are OOD relative to the\nmodel's training distribution, we explore whether the frozen model is able to\ngeneralize to this new domain. We begin by revealing that Denoising Diffusion\nProbabilistic Models (DDPMs) trained on single-domain images are already\nequipped with sufficient representation abilities to reconstruct arbitrary\nimages from the inverted latent encoding following bi-directional deterministic\ndiffusion and denoising trajectories. We then demonstrate through both\ntheoretical and empirical perspectives that the OOD images establish Gaussian\npriors in latent spaces of the given model, and the inverted latent modes are\nseparable from their initial training domain. We then introduce our novel\ntuning-free paradigm to synthesize new images of the target unseen domain by\ndiscovering qualified OOD latent encodings in the inverted noisy spaces. This\nis fundamentally different from the current paradigm that seeks to modify the\ndenoising trajectory to achieve the same goal by tuning the model parameters.\nExtensive cross-model and domain experiments show that our proposed method can\nexpand the latent space and generate unseen images via frozen DDPMs without\nimpairing the quality of generation of their original domain. We also showcase\na practical application of our proposed heuristic approach in dramatically\ndifferent domains using astrophysical data, revealing the great potential of\nsuch a generalization paradigm in data spare fields such as scientific\nexplorations.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-13T16:07:31Z",
    "updated": "2024-05-26T20:17:35Z",
    "doi": null
  },
  "2305.00633": {
    "id": "http://arxiv.org/abs/2305.00633v3",
    "title": "Self-Evaluation Guided Beam Search for Reasoning",
    "authors": [
      "Yuxi Xie",
      "Kenji Kawaguchi",
      "Yiran Zhao",
      "Xu Zhao",
      "Min-Yen Kan",
      "Junxian He",
      "Qizhe Xie"
    ],
    "abstract": "  Breaking down a problem into intermediate steps has demonstrated impressive\nperformance in Large Language Model (LLM) reasoning. However, the growth of the\nreasoning chain introduces uncertainty and error accumulation, making it\nchallenging to elicit accurate final results. To tackle this challenge of\nuncertainty in multi-step reasoning, we introduce a stepwise self-evaluation\nmechanism to guide and calibrate the reasoning process of LLMs. We propose a\ndecoding algorithm integrating the self-evaluation guidance via stochastic beam\nsearch. The self-evaluation guidance serves as a better-calibrated automatic\ncriterion, facilitating an efficient search in the reasoning space and\nresulting in superior prediction quality. Stochastic beam search balances\nexploitation and exploration of the search space with temperature-controlled\nrandomness. Our approach surpasses the corresponding Codex-backboned baselines\nin few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA,\nand StrategyQA benchmarks, respectively. Experiment results with Llama-2 on\narithmetic reasoning demonstrate the efficiency of our method in outperforming\nthe baseline methods with comparable computational budgets. Further analysis in\nmulti-step reasoning finds our self-evaluation guidance pinpoints logic\nfailures and leads to higher consistency and robustness. Our code is publicly\navailable at https://guideddecoding.github.io/.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-01T02:37:59Z",
    "updated": "2023-10-26T01:43:17Z",
    "doi": null
  },
  "2405.05749": {
    "id": "http://arxiv.org/abs/2405.05749v2",
    "title": "NeRFFaceSpeech: One-shot Audio-driven 3D Talking Head Synthesis via\n  Generative Prior",
    "authors": [
      "Gihoon Kim",
      "Kwanggyoon Seo",
      "Sihun Cha",
      "Junyong Noh"
    ],
    "abstract": "  Audio-driven talking head generation is advancing from 2D to 3D content.\nNotably, Neural Radiance Field (NeRF) is in the spotlight as a means to\nsynthesize high-quality 3D talking head outputs. Unfortunately, this NeRF-based\napproach typically requires a large number of paired audio-visual data for each\nidentity, thereby limiting the scalability of the method. Although there have\nbeen attempts to generate audio-driven 3D talking head animations with a single\nimage, the results are often unsatisfactory due to insufficient information on\nobscured regions in the image. In this paper, we mainly focus on addressing the\noverlooked aspect of 3D consistency in the one-shot, audio-driven domain, where\nfacial animations are synthesized primarily in front-facing perspectives. We\npropose a novel method, NeRFFaceSpeech, which enables to produce high-quality\n3D-aware talking head. Using prior knowledge of generative models combined with\nNeRF, our method can craft a 3D-consistent facial feature space corresponding\nto a single image. Our spatial synchronization method employs audio-correlated\nvertex dynamics of a parametric face model to transform static image features\ninto dynamic visuals through ray deformation, ensuring realistic 3D facial\nmotion. Moreover, we introduce LipaintNet that can replenish the lacking\ninformation in the inner-mouth area, which can not be obtained from a given\nsingle image. The network is trained in a self-supervised manner by utilizing\nthe generative capabilities without additional data. The comprehensive\nexperiments demonstrate the superiority of our method in generating\naudio-driven talking heads from a single image with enhanced 3D consistency\ncompared to previous approaches. In addition, we introduce a quantitative way\nof measuring the robustness of a model against pose changes for the first time,\nwhich has been possible only qualitatively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-05-09T13:14:06Z",
    "updated": "2024-05-10T14:13:10Z",
    "doi": null
  },
  "2006.06466": {
    "id": "http://arxiv.org/abs/2006.06466v2",
    "title": "How Interpretable and Trustworthy are GAMs?",
    "authors": [
      "Chun-Hao Chang",
      "Sarah Tan",
      "Ben Lengerich",
      "Anna Goldenberg",
      "Rich Caruana"
    ],
    "abstract": "  Generalized additive models (GAMs) have become a leading modelclass for\ninterpretable machine learning. However, there are many algorithms for training\nGAMs, and these can learn different or even contradictory models, while being\nequally accurate. Which GAM should we trust? In this paper, we quantitatively\nand qualitatively investigate a variety of GAM algorithms on real and simulated\ndatasets. We find that GAMs with high feature sparsity (only using afew\nvariables to make predictions) can miss patterns in the data and be unfair to\nrare subpopulations. Our results suggest that inductive bias plays a crucial\nrole in what interpretable models learn and that tree-based GAMs represent the\nbest balance of sparsity, fidelity and accuracy and thus appear to be the most\ntrustworthy GAM.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-06-11T14:21:59Z",
    "updated": "2021-06-07T02:53:40Z",
    "doi": null
  },
  "2312.08885": {
    "id": "http://arxiv.org/abs/2312.08885v1",
    "title": "SceneWiz3D: Towards Text-guided 3D Scene Composition",
    "authors": [
      "Qihang Zhang",
      "Chaoyang Wang",
      "Aliaksandr Siarohin",
      "Peiye Zhuang",
      "Yinghao Xu",
      "Ceyuan Yang",
      "Dahua Lin",
      "Bolei Zhou",
      "Sergey Tulyakov",
      "Hsin-Ying Lee"
    ],
    "abstract": "  We are witnessing significant breakthroughs in the technology for generating\n3D objects from text. Existing approaches either leverage large text-to-image\nmodels to optimize a 3D representation or train 3D generators on object-centric\ndatasets. Generating entire scenes, however, remains very challenging as a\nscene contains multiple 3D objects, diverse and scattered. In this work, we\nintroduce SceneWiz3D, a novel approach to synthesize high-fidelity 3D scenes\nfrom text. We marry the locality of objects with globality of scenes by\nintroducing a hybrid 3D representation: explicit for objects and implicit for\nscenes. Remarkably, an object, being represented explicitly, can be either\ngenerated from text using conventional text-to-3D approaches, or provided by\nusers. To configure the layout of the scene and automatically place objects, we\napply the Particle Swarm Optimization technique during the optimization\nprocess. Furthermore, it is difficult for certain parts of the scene (e.g.,\ncorners, occlusion) to receive multi-view supervision, leading to inferior\ngeometry. We incorporate an RGBD panorama diffusion model to mitigate it,\nresulting in high-quality geometry. Extensive evaluation supports that our\napproach achieves superior quality over previous approaches, enabling the\ngeneration of detailed and view-consistent 3D scenes.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-13T18:59:30Z",
    "updated": "2023-12-13T18:59:30Z",
    "doi": null
  },
  "2311.10982": {
    "id": "http://arxiv.org/abs/2311.10982v1",
    "title": "Make Pixels Dance: High-Dynamic Video Generation",
    "authors": [
      "Yan Zeng",
      "Guoqiang Wei",
      "Jiani Zheng",
      "Jiaxin Zou",
      "Yang Wei",
      "Yuchen Zhang",
      "Hang Li"
    ],
    "abstract": "  Creating high-dynamic videos such as motion-rich actions and sophisticated\nvisual effects poses a significant challenge in the field of artificial\nintelligence. Unfortunately, current state-of-the-art video generation methods,\nprimarily focusing on text-to-video generation, tend to produce video clips\nwith minimal motions despite maintaining high fidelity. We argue that relying\nsolely on text instructions is insufficient and suboptimal for video\ngeneration. In this paper, we introduce PixelDance, a novel approach based on\ndiffusion models that incorporates image instructions for both the first and\nlast frames in conjunction with text instructions for video generation.\nComprehensive experimental results demonstrate that PixelDance trained with\npublic data exhibits significantly better proficiency in synthesizing videos\nwith complex scenes and intricate motions, setting a new standard for video\ngeneration.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-18T06:25:58Z",
    "updated": "2023-11-18T06:25:58Z",
    "doi": null
  },
  "2207.01696": {
    "id": "http://arxiv.org/abs/2207.01696v2",
    "title": "TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of\n  3D Human Motions and Texts",
    "authors": [
      "Chuan Guo",
      "Xinxin Zuo",
      "Sen Wang",
      "Li Cheng"
    ],
    "abstract": "  Inspired by the strong ties between vision and language, the two intimate\nhuman sensing and communication modalities, our paper aims to explore the\ngeneration of 3D human full-body motions from texts, as well as its reciprocal\ntask, shorthanded for text2motion and motion2text, respectively. To tackle the\nexisting challenges, especially to enable the generation of multiple distinct\nmotions from the same text, and to avoid the undesirable production of trivial\nmotionless pose sequences, we propose the use of motion token, a discrete and\ncompact motion representation. This provides one level playing ground when\nconsidering both motions and text signals, as the motion and text tokens,\nrespectively. Moreover, our motion2text module is integrated into the inverse\nalignment process of our text2motion training pipeline, where a significant\ndeviation of synthesized text from the input text would be penalized by a large\ntraining loss; empirically this is shown to effectively improve performance.\nFinally, the mappings in-between the two modalities of motions and texts are\nfacilitated by adapting the neural model for machine translation (NMT) to our\ncontext. This autoregressive modeling of the distribution over discrete motion\ntokens further enables non-deterministic production of pose sequences, of\nvariable lengths, from an input text. Our approach is flexible, could be used\nfor both text2motion and motion2text tasks. Empirical evaluations on two\nbenchmark datasets demonstrate the superior performance of our approach on both\ntasks over a variety of state-of-the-art methods. Project page:\nhttps://ericguo5513.github.io/TM2T/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-04T19:52:18Z",
    "updated": "2022-08-04T18:31:20Z",
    "doi": null
  },
  "1609.05158": {
    "id": "http://arxiv.org/abs/1609.05158v2",
    "title": "Real-Time Single Image and Video Super-Resolution Using an Efficient\n  Sub-Pixel Convolutional Neural Network",
    "authors": [
      "Wenzhe Shi",
      "Jose Caballero",
      "Ferenc Husz\u00e1r",
      "Johannes Totz",
      "Andrew P. Aitken",
      "Rob Bishop",
      "Daniel Rueckert",
      "Zehan Wang"
    ],
    "abstract": "  Recently, several models based on deep neural networks have achieved great\nsuccess in terms of both reconstruction accuracy and computational performance\nfor single image super-resolution. In these methods, the low resolution (LR)\ninput image is upscaled to the high resolution (HR) space using a single\nfilter, commonly bicubic interpolation, before reconstruction. This means that\nthe super-resolution (SR) operation is performed in HR space. We demonstrate\nthat this is sub-optimal and adds computational complexity. In this paper, we\npresent the first convolutional neural network (CNN) capable of real-time SR of\n1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN\narchitecture where the feature maps are extracted in the LR space. In addition,\nwe introduce an efficient sub-pixel convolution layer which learns an array of\nupscaling filters to upscale the final LR feature maps into the HR output. By\ndoing so, we effectively replace the handcrafted bicubic filter in the SR\npipeline with more complex upscaling filters specifically trained for each\nfeature map, whilst also reducing the computational complexity of the overall\nSR operation. We evaluate the proposed approach using images and videos from\npublicly available datasets and show that it performs significantly better\n(+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster\nthan previous CNN-based methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-09-16T17:58:14Z",
    "updated": "2016-09-23T17:16:37Z",
    "doi": null
  },
  "2302.05573": {
    "id": "http://arxiv.org/abs/2302.05573v1",
    "title": "3D Colored Shape Reconstruction from a Single RGB Image through\n  Diffusion",
    "authors": [
      "Bo Li",
      "Xiaolin Wei",
      "Fengwei Chen",
      "Bin Liu"
    ],
    "abstract": "  We propose a novel 3d colored shape reconstruction method from a single RGB\nimage through diffusion model. Diffusion models have shown great development\npotentials for high-quality 3D shape generation. However, most existing work\nbased on diffusion models only focus on geometric shape generation, they cannot\neither accomplish 3D reconstruction from a single image, or produce 3D\ngeometric shape with color information. In this work, we propose to reconstruct\na 3D colored shape from a single RGB image through a novel conditional\ndiffusion model. The reverse process of the proposed diffusion model is\nconsisted of three modules, shape prediction module, color prediction module\nand NeRF-like rendering module. In shape prediction module, the reference RGB\nimage is first encoded into a high-level shape feature and then the shape\nfeature is utilized as a condition to predict the reverse geometric noise in\ndiffusion model. Then the color of each 3D point updated in shape prediction\nmodule is predicted by color prediction module. Finally, a NeRF-like rendering\nmodule is designed to render the colored point cloud predicted by the former\ntwo modules to 2D image space to guide the training conditioned only on a\nreference image. As far as the authors know, the proposed method is the first\ndiffusion model for 3D colored shape reconstruction from a single RGB image.\nExperimental results demonstrate that the proposed method achieves competitive\nperformance on colored 3D shape reconstruction, and the ablation study\nvalidates the positive role of the color prediction module in improving the\nreconstruction quality of 3D geometric point cloud.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-11T02:15:00Z",
    "updated": "2023-02-11T02:15:00Z",
    "doi": null
  },
  "2208.08643": {
    "id": "http://arxiv.org/abs/2208.08643v2",
    "title": "Learning Program Representations with a Tree-Structured Transformer",
    "authors": [
      "Wenhan Wang",
      "Kechi Zhang",
      "Ge Li",
      "Shangqing Liu",
      "Anran Li",
      "Zhi Jin",
      "Yang Liu"
    ],
    "abstract": "  Learning vector representations for programs is a critical step in applying\ndeep learning techniques for program understanding tasks. Various neural\nnetwork models are proposed to learn from tree-structured program\nrepresentations, e.g., abstract syntax tree (AST) and concrete syntax tree\n(CST). However, most neural architectures either fail to capture long-range\ndependencies which are ubiquitous in programs, or cannot learn effective\nrepresentations for syntax tree nodes, making them incapable of performing the\nnode-level prediction tasks, e.g., bug localization. In this paper, we propose\nTree-Transformer, a novel recursive tree-structured neural network to learn the\nvector representations for source codes. We propose a multi-head attention\nmechanism to model the dependency between siblings and parent-children node\npairs. Moreover, we propose a bi-directional propagation strategy to allow node\ninformation passing in two directions, bottom-up and top-down along trees. In\nthis way, Tree-Transformer can learn the information of the node features as\nwell as the global contextual information. The extensive experimental results\nshow that our Tree-Transformer significantly outperforms the existing\ntree-based and graph-based program representation learning approaches in both\nthe tree-level and node-level prediction tasks.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-18T05:42:01Z",
    "updated": "2023-01-07T12:31:50Z",
    "doi": null
  },
  "2307.06507": {
    "id": "http://arxiv.org/abs/2307.06507v2",
    "title": "Improving Nonalcoholic Fatty Liver Disease Classification Performance\n  With Latent Diffusion Models",
    "authors": [
      "Romain Hardy",
      "Joe Klepich",
      "Ryan Mitchell",
      "Steve Hall",
      "Jericho Villareal",
      "Cornelia Ilin"
    ],
    "abstract": "  Integrating deep learning with clinical expertise holds great potential for\naddressing healthcare challenges and empowering medical professionals with\nimproved diagnostic tools. However, the need for annotated medical images is\noften an obstacle to leveraging the full power of machine learning models. Our\nresearch demonstrates that by combining synthetic images, generated using\ndiffusion models, with real images, we can enhance nonalcoholic fatty liver\ndisease (NAFLD) classification performance even in low-data regime settings. We\nevaluate the quality of the synthetic images by comparing two metrics:\nInception Score (IS) and Fr\\'{e}chet Inception Distance (FID), computed on\ndiffusion- and generative adversarial network (GAN)-generated images. Our\nresults show superior performance for the diffusion-generated images, with a\nmaximum IS score of $1.90$ compared to $1.67$ for GANs, and a minimum FID score\nof $69.45$ compared to $100.05$ for GANs. Utilizing a partially frozen CNN\nbackbone (EfficientNet v1), our synthetic augmentation method achieves a\nmaximum image-level ROC AUC of $0.904$ on a NAFLD prediction task.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-13T01:14:08Z",
    "updated": "2023-11-15T02:24:21Z",
    "doi": null
  },
  "2403.14572": {
    "id": "http://arxiv.org/abs/2403.14572v2",
    "title": "Implicit Style-Content Separation using B-LoRA",
    "authors": [
      "Yarden Frenkel",
      "Yael Vinker",
      "Ariel Shamir",
      "Daniel Cohen-Or"
    ],
    "abstract": "  Image stylization involves manipulating the visual appearance and texture\n(style) of an image while preserving its underlying objects, structures, and\nconcepts (content). The separation of style and content is essential for\nmanipulating the image's style independently from its content, ensuring a\nharmonious and visually pleasing result. Achieving this separation requires a\ndeep understanding of both the visual and semantic characteristics of images,\noften necessitating the training of specialized models or employing heavy\noptimization. In this paper, we introduce B-LoRA, a method that leverages LoRA\n(Low-Rank Adaptation) to implicitly separate the style and content components\nof a single image, facilitating various image stylization tasks. By analyzing\nthe architecture of SDXL combined with LoRA, we find that jointly learning the\nLoRA weights of two specific blocks (referred to as B-LoRAs) achieves\nstyle-content separation that cannot be achieved by training each B-LoRA\nindependently. Consolidating the training into only two blocks and separating\nstyle and content allows for significantly improving style manipulation and\novercoming overfitting issues often associated with model fine-tuning. Once\ntrained, the two B-LoRAs can be used as independent components to allow various\nimage stylization tasks, including image style transfer, text-based image\nstylization, consistent style generation, and style-content mixing.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-21T17:20:21Z",
    "updated": "2024-09-22T12:42:39Z",
    "doi": null
  },
  "2305.14808": {
    "id": "http://arxiv.org/abs/2305.14808v1",
    "title": "SAGA: Summarization-Guided Assert Statement Generation",
    "authors": [
      "Yuwei Zhang",
      "Zhi Jin",
      "Zejun Wang",
      "Ying Xing",
      "Ge Li"
    ],
    "abstract": "  Generating meaningful assert statements is one of the key challenges in\nautomated test case generation, which requires understanding the intended\nfunctionality of the tested code. Recently, deep learning-based models have\nshown promise in improving the performance of assert statement generation.\nHowever, existing models only rely on the test prefixes along with their\ncorresponding focal methods, yet ignore the developer-written summarization.\nBased on our observations, the summarization contents usually express the\nintended program behavior or contain parameters that will appear directly in\nthe assert statement. Such information will help existing models address their\ncurrent inability to accurately predict assert statements. This paper presents\na novel summarization-guided approach for automatically generating assert\nstatements. To derive generic representations for natural language (i.e.,\nsummarization) and programming language (i.e., test prefixes and focal\nmethods), we leverage a pre-trained language model as the reference\narchitecture and fine-tune it on the task of assert statement generation. To\nthe best of our knowledge, the proposed approach makes the first attempt to\nleverage the summarization of focal methods as the guidance for making the\ngenerated assert statements more accurate. We demonstrate the effectiveness of\nour approach on two real-world datasets when compared with state-of-the-art\nmodels.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-24T07:03:21Z",
    "updated": "2023-05-24T07:03:21Z",
    "doi": "10.1007/s11390-023-2878-6"
  },
  "2301.12662": {
    "id": "http://arxiv.org/abs/2301.12662v1",
    "title": "SingSong: Generating musical accompaniments from singing",
    "authors": [
      "Chris Donahue",
      "Antoine Caillon",
      "Adam Roberts",
      "Ethan Manilow",
      "Philippe Esling",
      "Andrea Agostinelli",
      "Mauro Verzetti",
      "Ian Simon",
      "Olivier Pietquin",
      "Neil Zeghidour",
      "Jesse Engel"
    ],
    "abstract": "  We present SingSong, a system that generates instrumental music to accompany\ninput vocals, potentially offering musicians and non-musicians alike an\nintuitive new way to create music featuring their own voice. To accomplish\nthis, we build on recent developments in musical source separation and audio\ngeneration. Specifically, we apply a state-of-the-art source separation\nalgorithm to a large corpus of music audio to produce aligned pairs of vocals\nand instrumental sources. Then, we adapt AudioLM (Borsos et al., 2022) -- a\nstate-of-the-art approach for unconditional audio generation -- to be suitable\nfor conditional \"audio-to-audio\" generation tasks, and train it on the\nsource-separated (vocal, instrumental) pairs. In a pairwise comparison with the\nsame vocal inputs, listeners expressed a significant preference for\ninstrumentals generated by SingSong compared to those from a strong retrieval\nbaseline.\n  Sound examples at https://g.co/magenta/singsong\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-30T04:53:23Z",
    "updated": "2023-01-30T04:53:23Z",
    "doi": null
  },
  "2211.09590": {
    "id": "http://arxiv.org/abs/2211.09590v5",
    "title": "Hypergraph Transformer for Skeleton-based Action Recognition",
    "authors": [
      "Yuxuan Zhou",
      "Zhi-Qi Cheng",
      "Chao Li",
      "Yanwen Fang",
      "Yifeng Geng",
      "Xuansong Xie",
      "Margret Keuper"
    ],
    "abstract": "  Skeleton-based action recognition aims to recognize human actions given human\njoint coordinates with skeletal interconnections. By defining a graph with\njoints as vertices and their natural connections as edges, previous works\nsuccessfully adopted Graph Convolutional networks (GCNs) to model joint\nco-occurrences and achieved superior performance. More recently, a limitation\nof GCNs is identified, i.e., the topology is fixed after training. To relax\nsuch a restriction, Self-Attention (SA) mechanism has been adopted to make the\ntopology of GCNs adaptive to the input, resulting in the state-of-the-art\nhybrid models. Concurrently, attempts with plain Transformers have also been\nmade, but they still lag behind state-of-the-art GCN-based methods due to the\nlack of structural prior. Unlike hybrid models, we propose a more elegant\nsolution to incorporate the bone connectivity into Transformer via a graph\ndistance embedding. Our embedding retains the information of skeletal structure\nduring training, whereas GCNs merely use it for initialization. More\nimportantly, we reveal an underlying issue of graph models in general, i.e.,\npairwise aggregation essentially ignores the high-order kinematic dependencies\nbetween body joints. To fill this gap, we propose a new self-attention (SA)\nmechanism on hypergraph, termed Hypergraph Self-Attention (HyperSA), to\nincorporate intrinsic higher-order relations into the model. We name the\nresulting model Hyperformer, and it beats state-of-the-art graph models w.r.t.\naccuracy and efficiency on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA\ndatasets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.2.10",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-17T15:36:48Z",
    "updated": "2023-03-21T17:34:34Z",
    "doi": null
  },
  "1608.05343": {
    "id": "http://arxiv.org/abs/1608.05343v2",
    "title": "Decoupled Neural Interfaces using Synthetic Gradients",
    "authors": [
      "Max Jaderberg",
      "Wojciech Marian Czarnecki",
      "Simon Osindero",
      "Oriol Vinyals",
      "Alex Graves",
      "David Silver",
      "Koray Kavukcuoglu"
    ],
    "abstract": "  Training directed neural networks typically requires forward-propagating data\nthrough a computation graph, followed by backpropagating error signal, to\nproduce weight updates. All layers, or more generally, modules, of the network\nare therefore locked, in the sense that they must wait for the remainder of the\nnetwork to execute forwards and propagate error backwards before they can be\nupdated. In this work we break this constraint by decoupling modules by\nintroducing a model of the future computation of the network graph. These\nmodels predict what the result of the modelled subgraph will produce using only\nlocal information. In particular we focus on modelling error gradients: by\nusing the modelled synthetic gradient in place of true backpropagated error\ngradients we decouple subgraphs, and can update them independently and\nasynchronously i.e. we realise decoupled neural interfaces. We show results for\nfeed-forward models, where every layer is trained asynchronously, recurrent\nneural networks (RNNs) where predicting one's future gradient extends the time\nover which the RNN can effectively model, and also a hierarchical RNN system\nwith ticking at different timescales. Finally, we demonstrate that in addition\nto predicting gradients, the same framework can be used to predict inputs,\nresulting in models which are decoupled in both the forward and backwards pass\n-- amounting to independent networks which co-learn such that they can be\ncomposed into a single functioning corporation.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-08-18T17:29:09Z",
    "updated": "2017-07-03T10:52:04Z",
    "doi": null
  },
  "1608.04207": {
    "id": "http://arxiv.org/abs/1608.04207v3",
    "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction\n  Tasks",
    "authors": [
      "Yossi Adi",
      "Einat Kermany",
      "Yonatan Belinkov",
      "Ofer Lavi",
      "Yoav Goldberg"
    ],
    "abstract": "  There is a lot of research interest in encoding variable length sentences\ninto fixed length vectors, in a way that preserves the sentence meanings. Two\ncommon methods include representations based on averaging word vectors, and\nrepresentations based on the hidden states of recurrent neural networks such as\nLSTMs. The sentence vectors are used as features for subsequent machine\nlearning tasks or for pre-training in the context of deep learning. However,\nnot much is known about the properties that are encoded in these sentence\nrepresentations and about the language information they capture. We propose a\nframework that facilitates better understanding of the encoded representations.\nWe define prediction tasks around isolated aspects of sentence structure\n(namely sentence length, word content, and word order), and score\nrepresentations by the ability to train a classifier to solve each prediction\ntask when using the representation as input. We demonstrate the potential\ncontribution of the approach by analyzing different sentence representation\nmechanisms. The analysis sheds light on the relative strengths of different\nsentence embedding methods with respect to these low level prediction tasks,\nand on the effect of the encoded vector's dimensionality on the resulting\nrepresentations.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-08-15T08:51:38Z",
    "updated": "2017-02-09T06:58:50Z",
    "doi": null
  },
  "2310.09484": {
    "id": "http://arxiv.org/abs/2310.09484v3",
    "title": "Fast-DiM: Towards Fast Diffusion Morphs",
    "authors": [
      "Zander W. Blasingame",
      "Chen Liu"
    ],
    "abstract": "  Diffusion Morphs (DiM) are a recent state-of-the-art method for creating high\nquality face morphs; however, they require a high number of network function\nevaluations (NFE) to create the morphs. We propose a new DiM pipeline,\nFast-DiM, which can create morphs of a similar quality but with fewer NFE. We\ninvestigate the ODE solvers used to solve the Probability Flow ODE and the\nimpact they have on the the creation of face morphs. Additionally, we employ an\nalternative method for encoding images into the latent space of the Diffusion\nmodel by solving the Probability Flow ODE as time runs forwards. Our\nexperiments show that we can reduce the NFE by upwards of 85% in the encoding\nprocess while experiencing only 1.6\\% reduction in Mated Morph Presentation\nMatch Rate (MMPMR). Likewise, we showed we could cut NFE, in the sampling\nprocess, in half with only a maximal reduction of 0.23% in MMPMR.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-14T04:11:01Z",
    "updated": "2024-06-29T17:23:42Z",
    "doi": "10.1109/MSEC.2024.3410112"
  },
  "2301.13126": {
    "id": "http://arxiv.org/abs/2301.13126v3",
    "title": "LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain",
    "authors": [
      "Joel Niklaus",
      "Veton Matoshi",
      "Pooja Rani",
      "Andrea Galassi",
      "Matthias St\u00fcrmer",
      "Ilias Chalkidis"
    ],
    "abstract": "  Lately, propelled by the phenomenal advances around the transformer\narchitecture, the legal NLP field has enjoyed spectacular growth. To measure\nprogress, well curated and challenging benchmarks are crucial. However, most\nbenchmarks are English only and in legal NLP specifically there is no\nmultilingual benchmark available yet. Additionally, many benchmarks are\nsaturated, with the best models clearly outperforming the best humans and\nachieving near perfect scores. We survey the legal NLP literature and select 11\ndatasets covering 24 languages, creating LEXTREME. To provide a fair\ncomparison, we propose two aggregate scores, one based on the datasets and one\non the languages. The best baseline (XLM-R large) achieves both a dataset\naggregate score a language aggregate score of 61.3. This indicates that\nLEXTREME is still very challenging and leaves ample room for improvement. To\nmake it easy for researchers and practitioners to use, we release LEXTREME on\nhuggingface together with all the code required to evaluate models and a public\nWeights and Biases project with all the runs.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68T50",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.2",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-30T18:05:08Z",
    "updated": "2024-01-08T10:08:40Z",
    "doi": "10.18653/v1/2023.findings-emnlp.200"
  },
  "1907.08292": {
    "id": "http://arxiv.org/abs/1907.08292v1",
    "title": "Compositional Deep Learning",
    "authors": [
      "Bruno Gavranovi\u0107"
    ],
    "abstract": "  Neural networks have become an increasingly popular tool for solving many\nreal-world problems. They are a general framework for differentiable\noptimization which includes many other machine learning approaches as special\ncases. In this thesis we build a category-theoretic formalism around a class of\nneural networks exemplified by CycleGAN. CycleGAN is a collection of neural\nnetworks, closed under composition, whose inductive bias is increased by\nenforcing composition invariants, i.e. cycle-consistencies. Inspired by\nFunctorial Data Migration, we specify the interconnection of these networks\nusing a categorical schema, and network instances as set-valued functors on\nthis schema. We also frame neural network architectures, datasets, models, and\na number of other concepts in a categorical setting and thus show a special\nclass of functors, rather than functions, can be learned using gradient\ndescent. We use the category-theoretic framework to conceive a novel neural\nnetwork architecture whose goal is to learn the task of object insertion and\nobject deletion in images with unpaired data. We test the architecture on three\ndifferent datasets and obtain promising results.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.CT",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-07-16T10:21:15Z",
    "updated": "2019-07-16T10:21:15Z",
    "doi": null
  },
  "2208.01748": {
    "id": "http://arxiv.org/abs/2208.01748v1",
    "title": "A Fast Text-Driven Approach for Generating Artistic Content",
    "authors": [
      "Marian Lupascu",
      "Ryan Murdock",
      "Ionut Mironic\u0103",
      "Yijun Li"
    ],
    "abstract": "  In this work, we propose a complete framework that generates visual art.\nUnlike previous stylization methods that are not flexible with style parameters\n(i.e., they allow stylization with only one style image, a single stylization\ntext or stylization of a content image from a certain domain), our method has\nno such restriction. In addition, we implement an improved version that can\ngenerate a wide range of results with varying degrees of detail, style and\nstructure, with a boost in generation speed. To further enhance the results, we\ninsert an artistic super-resolution module in the generative pipeline. This\nmodule will bring additional details such as patterns specific to painters,\nslight brush marks, and so on.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-22T14:34:59Z",
    "updated": "2022-06-22T14:34:59Z",
    "doi": null
  },
  "1807.06358": {
    "id": "http://arxiv.org/abs/1807.06358v2",
    "title": "IntroVAE: Introspective Variational Autoencoders for Photographic Image\n  Synthesis",
    "authors": [
      "Huaibo Huang",
      "Zhihang Li",
      "Ran He",
      "Zhenan Sun",
      "Tieniu Tan"
    ],
    "abstract": "  We present a novel introspective variational autoencoder (IntroVAE) model for\nsynthesizing high-resolution photographic images. IntroVAE is capable of\nself-evaluating the quality of its generated samples and improving itself\naccordingly. Its inference and generator models are jointly trained in an\nintrospective way. On one hand, the generator is required to reconstruct the\ninput images from the noisy outputs of the inference model as normal VAEs. On\nthe other hand, the inference model is encouraged to classify between the\ngenerated and real samples while the generator tries to fool it as GANs. These\ntwo famous generative frameworks are integrated in a simple yet efficient\nsingle-stream architecture that can be trained in a single stage. IntroVAE\npreserves the advantages of VAEs, such as stable training and nice latent\nmanifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires\nno extra discriminators, because the inference model itself serves as a\ndiscriminator to distinguish between the generated and real samples.\nExperiments demonstrate that our method produces high-resolution\nphoto-realistic images (e.g., CELEBA images at \\(1024^{2}\\)), which are\ncomparable to or better than the state-of-the-art GANs.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-07-17T11:37:31Z",
    "updated": "2018-10-27T13:46:18Z",
    "doi": null
  },
  "2405.07178": {
    "id": "http://arxiv.org/abs/2405.07178v1",
    "title": "Hologram: Realtime Holographic Overlays via LiDAR Augmented\n  Reconstruction",
    "authors": [
      "Ekansh Agrawal"
    ],
    "abstract": "  Guided by the hologram technology of the infamous Star Wars franchise, I\npresent an application that creates real-time holographic overlays using LiDAR\naugmented 3D reconstruction. Prior attempts involve SLAM or NeRFs which either\nrequire highly calibrated scenes, incur steep computation costs, or fail to\nrender dynamic scenes. I propose 3 high-fidelity reconstruction tools that can\nrun on a portable device, such as a iPhone 14 Pro, which can allow for metric\naccurate facial reconstructions. My systems enable interactive and immersive\nholographic experiences that can be used for a wide range of applications,\nincluding augmented reality, telepresence, and entertainment.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-05-12T06:35:10Z",
    "updated": "2024-05-12T06:35:10Z",
    "doi": null
  },
  "2311.09265": {
    "id": "http://arxiv.org/abs/2311.09265v1",
    "title": "FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier",
    "authors": [
      "Zhongjie Duan",
      "Chengyu Wang",
      "Cen Chen",
      "Weining Qian",
      "Jun Huang",
      "Mingyi Jin"
    ],
    "abstract": "  With the emergence of diffusion models and rapid development in image\nprocessing, it has become effortless to generate fancy images in tasks such as\nstyle transfer and image editing. However, these impressive image processing\napproaches face consistency issues in video processing. In this paper, we\npropose a powerful model-free toolkit called FastBlend to address the\nconsistency problem for video processing. Based on a patch matching algorithm,\nwe design two inference modes, including blending and interpolation. In the\nblending mode, FastBlend eliminates video flicker by blending the frames within\na sliding window. Moreover, we optimize both computational efficiency and video\nquality according to different application scenarios. In the interpolation\nmode, given one or more keyframes rendered by diffusion models, FastBlend can\nrender the whole video. Since FastBlend does not modify the generation process\nof diffusion models, it exhibits excellent compatibility. Extensive experiments\nhave demonstrated the effectiveness of FastBlend. In the blending mode,\nFastBlend outperforms existing methods for video deflickering and video\nsynthesis. In the interpolation mode, FastBlend surpasses video interpolation\nand model-based video processing approaches. The source codes have been\nreleased on GitHub.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-15T08:28:28Z",
    "updated": "2023-11-15T08:28:28Z",
    "doi": null
  },
  "2308.11200": {
    "id": "http://arxiv.org/abs/2308.11200v1",
    "title": "SegRNN: Segment Recurrent Neural Network for Long-Term Time Series\n  Forecasting",
    "authors": [
      "Shengsheng Lin",
      "Weiwei Lin",
      "Wentai Wu",
      "Feiyu Zhao",
      "Ruichao Mo",
      "Haotong Zhang"
    ],
    "abstract": "  RNN-based methods have faced challenges in the Long-term Time Series\nForecasting (LTSF) domain when dealing with excessively long look-back windows\nand forecast horizons. Consequently, the dominance in this domain has shifted\ntowards Transformer, MLP, and CNN approaches. The substantial number of\nrecurrent iterations are the fundamental reasons behind the limitations of RNNs\nin LTSF. To address these issues, we propose two novel strategies to reduce the\nnumber of iterations in RNNs for LTSF tasks: Segment-wise Iterations and\nParallel Multi-step Forecasting (PMF). RNNs that combine these strategies,\nnamely SegRNN, significantly reduce the required recurrent iterations for LTSF,\nresulting in notable improvements in forecast accuracy and inference speed.\nExtensive experiments demonstrate that SegRNN not only outperforms SOTA\nTransformer-based models but also reduces runtime and memory usage by more than\n78%. These achievements provide strong evidence that RNNs continue to excel in\nLTSF tasks and encourage further exploration of this domain with more RNN-based\napproaches. The source code is coming soon.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-22T05:23:04Z",
    "updated": "2023-08-22T05:23:04Z",
    "doi": null
  },
  "2106.07447": {
    "id": "http://arxiv.org/abs/2106.07447v1",
    "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked\n  Prediction of Hidden Units",
    "authors": [
      "Wei-Ning Hsu",
      "Benjamin Bolte",
      "Yao-Hung Hubert Tsai",
      "Kushal Lakhotia",
      "Ruslan Salakhutdinov",
      "Abdelrahman Mohamed"
    ],
    "abstract": "  Self-supervised approaches for speech representation learning are challenged\nby three unique problems: (1) there are multiple sound units in each input\nutterance, (2) there is no lexicon of input sound units during the pre-training\nphase, and (3) sound units have variable lengths with no explicit segmentation.\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\napproach for self-supervised speech representation learning, which utilizes an\noffline clustering step to provide aligned target labels for a BERT-like\nprediction loss. A key ingredient of our approach is applying the prediction\nloss over the masked regions only, which forces the model to learn a combined\nacoustic and language model over the continuous inputs. HuBERT relies primarily\non the consistency of the unsupervised clustering step rather than the\nintrinsic quality of the assigned cluster labels. Starting with a simple\nk-means teacher of 100 clusters, and using two iterations of clustering, the\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\ndev-other and test-other evaluation subsets.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-14T14:14:28Z",
    "updated": "2021-06-14T14:14:28Z",
    "doi": null
  },
  "2407.16125": {
    "id": "http://arxiv.org/abs/2407.16125v1",
    "title": "Diffusion Prior-Based Amortized Variational Inference for Noisy Inverse\n  Problems",
    "authors": [
      "Sojin Lee",
      "Dogyun Park",
      "Inho Kong",
      "Hyunwoo J. Kim"
    ],
    "abstract": "  Recent studies on inverse problems have proposed posterior samplers that\nleverage the pre-trained diffusion models as powerful priors. These attempts\nhave paved the way for using diffusion models in a wide range of inverse\nproblems. However, the existing methods entail computationally demanding\niterative sampling procedures and optimize a separate solution for each\nmeasurement, which leads to limited scalability and lack of generalization\ncapability across unseen samples. To address these limitations, we propose a\nnovel approach, Diffusion prior-based Amortized Variational Inference (DAVI)\nthat solves inverse problems with a diffusion prior from an amortized\nvariational inference perspective. Specifically, instead of separate\nmeasurement-wise optimization, our amortized inference learns a function that\ndirectly maps measurements to the implicit posterior distributions of\ncorresponding clean data, enabling a single-step posterior sampling even for\nunseen measurements. Extensive experiments on image restoration tasks, e.g.,\nGaussian deblur, 4$\\times$ super-resolution, and box inpainting with two\nbenchmark datasets, demonstrate our approach's superior performance over strong\nbaselines. Code is available at https://github.com/mlvlab/DAVI.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-23T02:14:18Z",
    "updated": "2024-07-23T02:14:18Z",
    "doi": null
  },
  "1611.05009": {
    "id": "http://arxiv.org/abs/1611.05009v4",
    "title": "OctNet: Learning Deep 3D Representations at High Resolutions",
    "authors": [
      "Gernot Riegler",
      "Ali Osman Ulusoy",
      "Andreas Geiger"
    ],
    "abstract": "  We present OctNet, a representation for deep learning with sparse 3D data. In\ncontrast to existing models, our representation enables 3D convolutional\nnetworks which are both deep and high resolution. Towards this goal, we exploit\nthe sparsity in the input data to hierarchically partition the space using a\nset of unbalanced octrees where each leaf node stores a pooled feature\nrepresentation. This allows to focus memory allocation and computation to the\nrelevant dense regions and enables deeper networks without compromising\nresolution. We demonstrate the utility of our OctNet representation by\nanalyzing the impact of resolution on several 3D tasks including 3D object\nclassification, orientation estimation and point cloud labeling.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-11-15T20:05:45Z",
    "updated": "2017-04-10T08:46:56Z",
    "doi": null
  },
  "1712.00559": {
    "id": "http://arxiv.org/abs/1712.00559v3",
    "title": "Progressive Neural Architecture Search",
    "authors": [
      "Chenxi Liu",
      "Barret Zoph",
      "Maxim Neumann",
      "Jonathon Shlens",
      "Wei Hua",
      "Li-Jia Li",
      "Li Fei-Fei",
      "Alan Yuille",
      "Jonathan Huang",
      "Kevin Murphy"
    ],
    "abstract": "  We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-12-02T06:23:16Z",
    "updated": "2018-07-26T19:51:26Z",
    "doi": null
  },
  "2404.07762": {
    "id": "http://arxiv.org/abs/2404.07762v4",
    "title": "NeuroNCAP: Photorealistic Closed-loop Safety Testing for Autonomous\n  Driving",
    "authors": [
      "William Ljungbergh",
      "Adam Tonderski",
      "Joakim Johnander",
      "Holger Caesar",
      "Kalle \u00c5str\u00f6m",
      "Michael Felsberg",
      "Christoffer Petersson"
    ],
    "abstract": "  We present a versatile NeRF-based simulator for testing autonomous driving\n(AD) software systems, designed with a focus on sensor-realistic closed-loop\nevaluation and the creation of safety-critical scenarios. The simulator learns\nfrom sequences of real-world driving sensor data and enables reconfigurations\nand renderings of new, unseen scenarios. In this work, we use our simulator to\ntest the responses of AD models to safety-critical scenarios inspired by the\nEuropean New Car Assessment Programme (Euro NCAP). Our evaluation reveals that,\nwhile state-of-the-art end-to-end planners excel in nominal driving scenarios\nin an open-loop setting, they exhibit critical flaws when navigating our\nsafety-critical scenarios in a closed-loop setting. This highlights the need\nfor advancements in the safety and real-world usability of end-to-end planners.\nBy publicly releasing our simulator and scenarios as an easy-to-run evaluation\nsuite, we invite the research community to explore, refine, and validate their\nAD models in controlled, yet highly configurable and challenging\nsensor-realistic environments. Code and instructions can be found at\nhttps://github.com/atonderski/neuro-ncap\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-11T14:03:16Z",
    "updated": "2024-04-23T07:29:18Z",
    "doi": null
  },
  "2004.05675": {
    "id": "http://arxiv.org/abs/2004.05675v1",
    "title": "A Non-Parametric Test to Detect Data-Copying in Generative Models",
    "authors": [
      "Casey Meehan",
      "Kamalika Chaudhuri",
      "Sanjoy Dasgupta"
    ],
    "abstract": "  Detecting overfitting in generative models is an important challenge in\nmachine learning. In this work, we formalize a form of overfitting that we call\n{\\em{data-copying}} -- where the generative model memorizes and outputs\ntraining samples or small variations thereof. We provide a three sample\nnon-parametric test for detecting data-copying that uses the training set, a\nseparate sample from the target distribution, and a generated sample from the\nmodel, and study the performance of our test on several canonical models and\ndatasets.\n  For code \\& examples, visit https://github.com/casey-meehan/data-copying\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-12T18:59:29Z",
    "updated": "2020-04-12T18:59:29Z",
    "doi": null
  },
  "2206.02967": {
    "id": "http://arxiv.org/abs/2206.02967v2",
    "title": "Masked Unsupervised Self-training for Label-free Image Classification",
    "authors": [
      "Junnan Li",
      "Silvio Savarese",
      "Steven C. H. Hoi"
    ],
    "abstract": "  State-of-the-art computer vision models are mostly trained with supervised\nlearning using human-labeled images, which limits their scalability due to the\nexpensive annotation cost. While self-supervised representation learning has\nachieved impressive progress, it still requires a second stage of finetuning on\nlabeled data. On the other hand, models pre-trained with large-scale text-image\nsupervision (e.g., CLIP) have enabled zero-shot transfer to downstream image\nclassification tasks. However, the zero-shot performance of CLIP-like models\nare often insufficient for real-world adoption. In this paper, we aim to\nleverage the abundant unlabeled data from a target domain to improve the\nperformance of a pre-trained zero-shot classifier, by unsupervised finetuning\nof the pre-trained model. We propose Masked Unsupervised Self-Training (MUST),\na new unsupervised adaptation method which leverages two different and\ncomplementary sources of training signals: pseudo-labels and raw images. MUST\njointly optimizes three objectives to learn both class-level global feature and\npixel-level local feature and enforces a regularization between the two. We\ndemonstrate the efficacy of MUST on a variety of downstream tasks, where it\nimproves upon CLIP by a large margin. MUST also outperforms supervised few-shot\nadaptation methods. It achieves a top-1 accuracy of 77.7% on ImageNet using\nViT-B, +9.4% higher than CLIP, and +6.2% higher than 16-shot CLIP adaptation.\nOur code is available at https://github.com/salesforce/MUST.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-07T02:03:06Z",
    "updated": "2023-03-10T01:15:56Z",
    "doi": null
  },
  "2205.12654": {
    "id": "http://arxiv.org/abs/2205.12654v1",
    "title": "Bitext Mining Using Distilled Sentence Representations for Low-Resource\n  Languages",
    "authors": [
      "Kevin Heffernan",
      "Onur \u00c7elebi",
      "Holger Schwenk"
    ],
    "abstract": "  Scaling multilingual representation learning beyond the hundred most frequent\nlanguages is challenging, in particular to cover the long tail of low-resource\nlanguages. A promising approach has been to train one-for-all multilingual\nmodels capable of cross-lingual transfer, but these models often suffer from\ninsufficient capacity and interference between unrelated languages. Instead, we\nmove away from this approach and focus on training multiple language (family)\nspecific representations, but most prominently enable all languages to still be\nencoded in the same representational space. To achieve this, we focus on\nteacher-student training, allowing all encoders to be mutually compatible for\nbitext mining, and enabling fast learning of new languages. We introduce a new\nteacher-student training scheme which combines supervised and self-supervised\ntraining, allowing encoders to take advantage of monolingual training data,\nwhich is valuable in the low-resource setting.\n  Our approach significantly outperforms the original LASER encoder. We study\nvery low-resource languages and handle 50 African languages, many of which are\nnot covered by any other model. For these languages, we train sentence\nencoders, mine bitexts, and validate the bitexts by training NMT systems.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-25T10:53:24Z",
    "updated": "2022-05-25T10:53:24Z",
    "doi": null
  },
  "2312.14125": {
    "id": "http://arxiv.org/abs/2312.14125v4",
    "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
    "authors": [
      "Dan Kondratyuk",
      "Lijun Yu",
      "Xiuye Gu",
      "Jos\u00e9 Lezama",
      "Jonathan Huang",
      "Grant Schindler",
      "Rachel Hornung",
      "Vighnesh Birodkar",
      "Jimmy Yan",
      "Ming-Chang Chiu",
      "Krishna Somandepalli",
      "Hassan Akbari",
      "Yair Alon",
      "Yong Cheng",
      "Josh Dillon",
      "Agrim Gupta",
      "Meera Hahn",
      "Anja Hauth",
      "David Hendon",
      "Alonso Martinez",
      "David Minnen",
      "Mikhail Sirotenko",
      "Kihyuk Sohn",
      "Xuan Yang",
      "Hartwig Adam",
      "Ming-Hsuan Yang",
      "Irfan Essa",
      "Huisheng Wang",
      "David A. Ross",
      "Bryan Seybold",
      "Lu Jiang"
    ],
    "abstract": "  We present VideoPoet, a language model capable of synthesizing high-quality\nvideo, with matching audio, from a large variety of conditioning signals.\nVideoPoet employs a decoder-only transformer architecture that processes\nmultimodal inputs -- including images, videos, text, and audio. The training\nprotocol follows that of Large Language Models (LLMs), consisting of two\nstages: pretraining and task-specific adaptation. During pretraining, VideoPoet\nincorporates a mixture of multimodal generative objectives within an\nautoregressive Transformer framework. The pretrained LLM serves as a foundation\nthat can be adapted for a range of video generation tasks. We present empirical\nresults demonstrating the model's state-of-the-art capabilities in zero-shot\nvideo generation, specifically highlighting VideoPoet's ability to generate\nhigh-fidelity motions. Project page: http://sites.research.google/videopoet/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-21T18:46:41Z",
    "updated": "2024-06-04T17:25:20Z",
    "doi": null
  },
  "2310.14804": {
    "id": "http://arxiv.org/abs/2310.14804v2",
    "title": "Large Language Models can Share Images, Too!",
    "authors": [
      "Young-Jun Lee",
      "Dokyong Lee",
      "Joo Won Sung",
      "Jonghwan Hyeon",
      "Ho-Jin Choi"
    ],
    "abstract": "  This paper explores the image-sharing capability of Large Language Models\n(LLMs), such as GPT-4 and LLaMA 2, in a zero-shot setting. To facilitate a\ncomprehensive evaluation of LLMs, we introduce the PhotoChat++ dataset, which\nincludes enriched annotations (i.e., intent, triggering sentence, image\ndescription, and salient information). Furthermore, we present the\ngradient-free and extensible Decide, Describe, and Retrieve (DribeR) framework.\nWith extensive experiments, we unlock the image-sharing capability of DribeR\nequipped with LLMs in zero-shot prompting, with ChatGPT achieving the best\nperformance. Our findings also reveal the emergent image-sharing ability in\nLLMs under zero-shot conditions, validating the effectiveness of DribeR. We use\nthis framework to demonstrate its practicality and effectiveness in two\nreal-world scenarios: (1) human-bot interaction and (2) dataset augmentation.\nTo the best of our knowledge, this is the first study to assess the\nimage-sharing ability of various LLMs in a zero-shot setting. We make our\nsource code and dataset publicly available at\nhttps://github.com/passing2961/DribeR.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-23T10:59:21Z",
    "updated": "2024-07-04T13:55:33Z",
    "doi": null
  },
  "2307.04349": {
    "id": "http://arxiv.org/abs/2307.04349v2",
    "title": "RLTF: Reinforcement Learning from Unit Test Feedback",
    "authors": [
      "Jiate Liu",
      "Yiqin Zhu",
      "Kaiwen Xiao",
      "Qiang Fu",
      "Xiao Han",
      "Wei Yang",
      "Deheng Ye"
    ],
    "abstract": "  The goal of program synthesis, or code generation, is to generate executable\ncode based on given descriptions. Recently, there has been an increasing number\nof studies employing reinforcement learning (RL) to improve the performance of\nlarge language models (LLMs) for code. However, current representative works\neither rely solely on offline frameworks, limiting the exploration of new\nsample spaces, or fall short in the utilization of unit test signals, not\naccounting for specific error locations within the code. To address these\nissues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback,\na novel online RL framework with unit test feedback of multi-granularity for\nrefining code LLMs. Our approach generates data in real-time during training\nand simultaneously utilizes fine-grained feedback signals to guide the model\ntowards producing higher-quality code. Extensive experiments show that RLTF\nachieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our\ncode is available at: https://github.com/Zyq-scut/RLTF.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-10T05:18:18Z",
    "updated": "2023-11-13T03:49:27Z",
    "doi": null
  },
  "2406.02820": {
    "id": "http://arxiv.org/abs/2406.02820v1",
    "title": "ORACLE: Leveraging Mutual Information for Consistent Character\n  Generation with LoRAs in Diffusion Models",
    "authors": [
      "Kiymet Akdemir",
      "Pinar Yanardag"
    ],
    "abstract": "  Text-to-image diffusion models have recently taken center stage as pivotal\ntools in promoting visual creativity across an array of domains such as comic\nbook artistry, children's literature, game development, and web design. These\nmodels harness the power of artificial intelligence to convert textual\ndescriptions into vivid images, thereby enabling artists and creators to bring\ntheir imaginative concepts to life with unprecedented ease. However, one of the\nsignificant hurdles that persist is the challenge of maintaining consistency in\ncharacter generation across diverse contexts. Variations in textual prompts,\neven if minor, can yield vastly different visual outputs, posing a considerable\nproblem in projects that require a uniform representation of characters\nthroughout. In this paper, we introduce a novel framework designed to produce\nconsistent character representations from a single text prompt across diverse\nsettings. Through both quantitative and qualitative analyses, we demonstrate\nthat our framework outperforms existing methods in generating characters with\nconsistent visual identities, underscoring its potential to transform creative\nindustries. By addressing the critical challenge of character consistency, we\nnot only enhance the practical utility of these models but also broaden the\nhorizons for artistic and creative expression.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-04T23:39:08Z",
    "updated": "2024-06-04T23:39:08Z",
    "doi": null
  },
  "2309.00952": {
    "id": "http://arxiv.org/abs/2309.00952v1",
    "title": "Bridge Diffusion Model: bridge non-English language-native text-to-image\n  diffusion model with English communities",
    "authors": [
      "Shanyuan Liu",
      "Dawei Leng",
      "Yuhui Yin"
    ],
    "abstract": "  Text-to-Image generation (TTI) technologies are advancing rapidly, especially\nin the English language communities. However, English-native TTI models\ninherently carry biases from English world centric training data, which creates\na dilemma for development of other language-native TTI models. One common\nchoice is fine-tuning the English-native TTI model with translated samples from\nnon-English communities. It falls short of fully addressing the model bias\nproblem. Alternatively, training non-English language native models from\nscratch can effectively resolve the English world bias, but diverges from the\nEnglish TTI communities, thus not able to utilize the strides continuously\ngaining in the English TTI communities any more. To build non-English language\nnative TTI model meanwhile keep compatability with the English TTI communities,\nwe propose a novel model structure referred as \"Bridge Diffusion Model\" (BDM).\nThe proposed BDM employs a backbone-branch network structure to learn the\nnon-English language semantics while keep the latent space compatible with the\nEnglish-native TTI backbone, in an end-to-end manner. The unique advantages of\nthe proposed BDM are that it's not only adept at generating images that\nprecisely depict non-English language semantics, but also compatible with\nvarious English-native TTI plugins, such as different checkpoints, LoRA,\nControlNet, Dreambooth, and Textual Inversion, etc. Moreover, BDM can\nconcurrently generate content seamlessly combining both non-English native and\nEnglish-native semantics within a single image, fostering cultural interaction.\nWe verify our method by applying BDM to build a Chinese-native TTI model,\nwhereas the method is generic and applicable to any other language.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-02T14:30:56Z",
    "updated": "2023-09-02T14:30:56Z",
    "doi": null
  },
  "2402.07995": {
    "id": "http://arxiv.org/abs/2402.07995v2",
    "title": "How the Galaxy-Halo Connection Depends on Large-Scale Environment",
    "authors": [
      "John F. Wu",
      "Christian Kragh Jespersen",
      "Risa H. Wechsler"
    ],
    "abstract": "  We investigate the connection between galaxies, dark matter halos, and their\nlarge-scale environments at $z=0$ with Illustris TNG300 hydrodynamic simulation\ndata. We predict stellar masses from subhalo properties to test two types of\nmachine learning (ML) models: Explainable Boosting Machines (EBMs) with simple\ngalaxy environment features and $\\mathbb{E}(3)$-invariant graph neural networks\n(GNNs). The best-performing EBM models leverage spherically averaged\noverdensity features on $3$ Mpc scales. Interpretations via SHapley Additive\nexPlanations (SHAP) also suggest that, in the context of the TNG300 galaxy-halo\nconnection, simple spherical overdensity on $\\sim 3$ Mpc scales is more\nimportant than cosmic web distance features measured using the DisPerSE\nalgorithm. Meanwhile, a GNN with connectivity defined by a fixed linking\nlength, $L$, outperforms the EBM models by a significant margin. As we increase\nthe linking length scale, GNNs learn important environmental contributions up\nto the largest scales we probe ($L=10$ Mpc). We conclude that $3$ Mpc distance\nscales are most critical for describing the TNG galaxy-halo connection using\nthe spherical overdensity parameterization but that information on larger\nscales, which is not captured by simple environmental parameters or cosmic web\nfeatures, can further augment these models. Our study highlights the benefits\nof using interpretable ML algorithms to explain models of astrophysical\nphenomena, and the power of using GNNs to flexibly learn complex relationships\ndirectly from data while imposing constraints from physical symmetries.\n",
    "categories": [
      {
        "@term": "astro-ph.GA",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "astro-ph.CO",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "astro-ph.IM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-12T19:01:57Z",
    "updated": "2024-10-03T20:56:38Z",
    "doi": "10.3847/1538-4357/ad7bb3"
  },
  "2403.10701": {
    "id": "http://arxiv.org/abs/2403.10701v1",
    "title": "IMPRINT: Generative Object Compositing by Learning Identity-Preserving\n  Representation",
    "authors": [
      "Yizhi Song",
      "Zhifei Zhang",
      "Zhe Lin",
      "Scott Cohen",
      "Brian Price",
      "Jianming Zhang",
      "Soo Ye Kim",
      "He Zhang",
      "Wei Xiong",
      "Daniel Aliaga"
    ],
    "abstract": "  Generative object compositing emerges as a promising new avenue for\ncompositional image editing. However, the requirement of object identity\npreservation poses a significant challenge, limiting practical usage of most\nexisting methods. In response, this paper introduces IMPRINT, a novel\ndiffusion-based generative model trained with a two-stage learning framework\nthat decouples learning of identity preservation from that of compositing. The\nfirst stage is targeted for context-agnostic, identity-preserving pretraining\nof the object encoder, enabling the encoder to learn an embedding that is both\nview-invariant and conducive to enhanced detail preservation. The subsequent\nstage leverages this representation to learn seamless harmonization of the\nobject composited to the background. In addition, IMPRINT incorporates a\nshape-guidance mechanism offering user-directed control over the compositing\nprocess. Extensive experiments demonstrate that IMPRINT significantly\noutperforms existing methods and various baselines on identity preservation and\ncomposition quality.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-15T21:37:04Z",
    "updated": "2024-03-15T21:37:04Z",
    "doi": null
  },
  "2403.05518": {
    "id": "http://arxiv.org/abs/2403.05518v1",
    "title": "Bias-Augmented Consistency Training Reduces Biased Reasoning in\n  Chain-of-Thought",
    "authors": [
      "James Chua",
      "Edward Rees",
      "Hunar Batra",
      "Samuel R. Bowman",
      "Julian Michael",
      "Ethan Perez",
      "Miles Turpin"
    ],
    "abstract": "  While chain-of-thought prompting (CoT) has the potential to improve the\nexplainability of language model reasoning, it can systematically misrepresent\nthe factors influencing models' behavior--for example, rationalizing answers in\nline with a user's opinion without mentioning this bias. To mitigate this\nbiased reasoning problem, we introduce bias-augmented consistency training\n(BCT), an unsupervised fine-tuning scheme that trains models to give consistent\nreasoning across prompts with and without biasing features. We construct a\nsuite testing nine forms of biased reasoning on seven question-answering tasks,\nand find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of\nbiased reasoning by 86% on held-out tasks. Moreover, this model generalizes to\nother forms of bias, reducing biased reasoning on held-out biases by an average\nof 37%. As BCT generalizes to held-out biases and does not require gold labels,\nthis method may hold promise for reducing biased reasoning from as-of-yet\nunknown biases and on tasks where supervision for ground truth reasoning is\nunavailable.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-08T18:41:42Z",
    "updated": "2024-03-08T18:41:42Z",
    "doi": null
  },
  "2207.01821": {
    "id": "http://arxiv.org/abs/2207.01821v2",
    "title": "Toward Explainable and Fine-Grained 3D Grounding through Referring\n  Textual Phrases",
    "authors": [
      "Zhihao Yuan",
      "Xu Yan",
      "Zhuo Li",
      "Xuhao Li",
      "Yao Guo",
      "Shuguang Cui",
      "Zhen Li"
    ],
    "abstract": "  Recent progress in 3D scene understanding has explored visual grounding\n(3DVG) to localize a target object through a language description. However,\nexisting methods only consider the dependency between the entire sentence and\nthe target object, ignoring fine-grained relationships between contexts and\nnon-target ones. In this paper, we extend 3DVG to a more fine-grained and\ninterpretable task, called 3D Phrase Aware Grounding (3DPAG). The 3DPAG task\naims to localize the target objects in a 3D scene by explicitly identifying all\nphrase-related objects and then conducting the reasoning according to\ncontextual phrases. To tackle this problem, we manually labeled about 227K\nphrase-level annotations using a self-developed platform, from 88K sentences of\nwidely used 3DVG datasets, i.e., Nr3D, Sr3D and ScanRefer. By tapping on our\ndatasets, we can extend previous 3DVG methods to the fine-grained phrase-aware\nscenario. It is achieved through the proposed novel phrase-object alignment\noptimization and phrase-specific pre-training, boosting conventional 3DVG\nperformance as well. Extensive results confirm significant improvements, i.e.,\nprevious state-of-the-art method achieves 3.9%, 3.5% and 4.6% overall accuracy\ngains on Nr3D, Sr3D and ScanRefer respectively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-05T05:50:12Z",
    "updated": "2023-05-27T10:03:34Z",
    "doi": null
  },
  "2402.16021": {
    "id": "http://arxiv.org/abs/2402.16021v1",
    "title": "TMT: Tri-Modal Translation between Speech, Image, and Text by Processing\n  Different Modalities as Different Languages",
    "authors": [
      "Minsu Kim",
      "Jee-weon Jung",
      "Hyeongseop Rha",
      "Soumi Maiti",
      "Siddhant Arora",
      "Xuankai Chang",
      "Shinji Watanabe",
      "Yong Man Ro"
    ],
    "abstract": "  The capability to jointly process multi-modal information is becoming an\nessential task. However, the limited number of paired multi-modal data and the\nlarge computational requirements in multi-modal learning hinder the\ndevelopment. We propose a novel Tri-Modal Translation (TMT) model that\ntranslates between arbitrary modalities spanning speech, image, and text. We\nintroduce a novel viewpoint, where we interpret different modalities as\ndifferent languages, and treat multi-modal translation as a well-established\nmachine translation problem. To this end, we tokenize speech and image data\ninto discrete tokens, which provide a unified interface across modalities and\nsignificantly decrease the computational cost. In the proposed TMT, a\nmulti-modal encoder-decoder conducts the core translation, whereas\nmodality-specific processing is conducted only within the tokenization and\ndetokenization stages. We evaluate the proposed TMT on all six modality\ntranslation tasks. TMT outperforms single model counterparts consistently,\ndemonstrating that unifying tasks is beneficial not only for practicality but\nalso for performance.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-25T07:46:57Z",
    "updated": "2024-02-25T07:46:57Z",
    "doi": null
  },
  "2206.00800": {
    "id": "http://arxiv.org/abs/2206.00800v1",
    "title": "CcHarmony: Color-checker based Image Harmonization Dataset",
    "authors": [
      "Haoxu Huang",
      "Li Niu"
    ],
    "abstract": "  Image harmonization targets at adjusting the foreground in a composite image\nto make it compatible with the background, producing a more realistic and\nharmonious image. Training deep image harmonization network requires abundant\ntraining data, but it is extremely difficult to acquire training pairs of\ncomposite images and ground-truth harmonious images. Therefore, existing works\nturn to adjust the foreground appearance in a real image to create a synthetic\ncomposite image. However, such adjustment may not faithfully reflect the\nnatural illumination change of foreground. In this work, we explore a novel\ntransitive way to construct image harmonization dataset. Specifically, based on\nthe existing datasets with recorded illumination information, we first convert\nthe foreground in a real image to the standard illumination condition, and then\nconvert it to another illumination condition, which is combined with the\noriginal background to form a synthetic composite image. In this manner, we\nconstruct an image harmonization dataset called ccHarmony, which is named after\ncolor checker (cc). The dataset is available at\nhttps://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-01T23:57:16Z",
    "updated": "2022-06-01T23:57:16Z",
    "doi": null
  },
  "2301.13856": {
    "id": "http://arxiv.org/abs/2301.13856v2",
    "title": "Simplex Random Features",
    "authors": [
      "Isaac Reid",
      "Krzysztof Choromanski",
      "Valerii Likhosherstov",
      "Adrian Weller"
    ],
    "abstract": "  We present Simplex Random Features (SimRFs), a new random feature (RF)\nmechanism for unbiased approximation of the softmax and Gaussian kernels by\ngeometrical correlation of random projection vectors. We prove that SimRFs\nprovide the smallest possible mean square error (MSE) on unbiased estimates of\nthese kernels among the class of weight-independent geometrically-coupled\npositive random feature (PRF) mechanisms, substantially outperforming the\npreviously most accurate Orthogonal Random Features at no observable extra\ncost. We present a more computationally expensive SimRFs+ variant, which we\nprove is asymptotically optimal in the broader family of weight-dependent\ngeometrical coupling schemes (which permit correlations between random vector\ndirections and norms). In extensive empirical studies, we show consistent gains\nprovided by SimRFs in settings including pointwise kernel estimation,\nnonparametric classification and scalable Transformers.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-31T18:53:39Z",
    "updated": "2023-10-07T15:55:57Z",
    "doi": null
  },
  "2303.15060": {
    "id": "http://arxiv.org/abs/2303.15060v1",
    "title": "TMO: Textured Mesh Acquisition of Objects with a Mobile Device by using\n  Differentiable Rendering",
    "authors": [
      "Jaehoon Choi",
      "Dongki Jung",
      "Taejae Lee",
      "Sangwook Kim",
      "Youngdong Jung",
      "Dinesh Manocha",
      "Donghwan Lee"
    ],
    "abstract": "  We present a new pipeline for acquiring a textured mesh in the wild with a\nsingle smartphone which offers access to images, depth maps, and valid poses.\nOur method first introduces an RGBD-aided structure from motion, which can\nyield filtered depth maps and refines camera poses guided by corresponding\ndepth. Then, we adopt the neural implicit surface reconstruction method, which\nallows for high-quality mesh and develops a new training process for applying a\nregularization provided by classical multi-view stereo methods. Moreover, we\napply a differentiable rendering to fine-tune incomplete texture maps and\ngenerate textures which are perceptually closer to the original scene. Our\npipeline can be applied to any common objects in the real world without the\nneed for either in-the-lab environments or accurate mask images. We demonstrate\nresults of captured objects with complex shapes and validate our method\nnumerically against existing 3D reconstruction and texture mapping methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-27T10:07:52Z",
    "updated": "2023-03-27T10:07:52Z",
    "doi": null
  },
  "2211.10658": {
    "id": "http://arxiv.org/abs/2211.10658v2",
    "title": "EDGE: Editable Dance Generation From Music",
    "authors": [
      "Jonathan Tseng",
      "Rodrigo Castellon",
      "C. Karen Liu"
    ],
    "abstract": "  Dance is an important human art form, but creating new dances can be\ndifficult and time-consuming. In this work, we introduce Editable Dance\nGEneration (EDGE), a state-of-the-art method for editable dance generation that\nis capable of creating realistic, physically-plausible dances while remaining\nfaithful to the input music. EDGE uses a transformer-based diffusion model\npaired with Jukebox, a strong music feature extractor, and confers powerful\nediting capabilities well-suited to dance, including joint-wise conditioning,\nand in-betweening. We introduce a new metric for physical plausibility, and\nevaluate dance quality generated by our method extensively through (1) multiple\nquantitative metrics on physical plausibility, beat alignment, and diversity\nbenchmarks, and more importantly, (2) a large-scale user study, demonstrating a\nsignificant improvement over previous state-of-the-art methods. Qualitative\nsamples from our model can be found at our website.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-19T10:41:38Z",
    "updated": "2022-11-27T06:27:17Z",
    "doi": null
  },
  "2406.07522": {
    "id": "http://arxiv.org/abs/2406.07522v1",
    "title": "Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling",
    "authors": [
      "Liliang Ren",
      "Yang Liu",
      "Yadong Lu",
      "Yelong Shen",
      "Chen Liang",
      "Weizhu Chen"
    ],
    "abstract": "  Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-11T17:50:51Z",
    "updated": "2024-06-11T17:50:51Z",
    "doi": null
  },
  "1503.03585": {
    "id": "http://arxiv.org/abs/1503.03585v8",
    "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
    "authors": [
      "Jascha Sohl-Dickstein",
      "Eric A. Weiss",
      "Niru Maheswaranathan",
      "Surya Ganguli"
    ],
    "abstract": "  A central problem in machine learning involves modeling complex data-sets\nusing highly flexible families of probability distributions in which learning,\nsampling, inference, and evaluation are still analytically or computationally\ntractable. Here, we develop an approach that simultaneously achieves both\nflexibility and tractability. The essential idea, inspired by non-equilibrium\nstatistical physics, is to systematically and slowly destroy structure in a\ndata distribution through an iterative forward diffusion process. We then learn\na reverse diffusion process that restores structure in data, yielding a highly\nflexible and tractable generative model of the data. This approach allows us to\nrapidly learn, sample from, and evaluate probabilities in deep generative\nmodels with thousands of layers or time steps, as well as to compute\nconditional and posterior probabilities under the learned model. We\nadditionally release an open source reference implementation of the algorithm.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cond-mat.dis-nn",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "q-bio.NC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2015-03-12T04:51:37Z",
    "updated": "2015-11-18T21:50:51Z",
    "doi": null
  },
  "2210.16788": {
    "id": "http://arxiv.org/abs/2210.16788v1",
    "title": "Image-free Domain Generalization via CLIP for 3D Hand Pose Estimation",
    "authors": [
      "Seongyeong Lee",
      "Hansoo Park",
      "Dong Uk Kim",
      "Jihyeon Kim",
      "Muhammadjon Boboev",
      "Seungryul Baek"
    ],
    "abstract": "  RGB-based 3D hand pose estimation has been successful for decades thanks to\nlarge-scale databases and deep learning. However, the hand pose estimation\nnetwork does not operate well for hand pose images whose characteristics are\nfar different from the training data. This is caused by various factors such as\nilluminations, camera angles, diverse backgrounds in the input images, etc.\nMany existing methods tried to solve it by supplying additional large-scale\nunconstrained/target domain images to augment data space; however collecting\nsuch large-scale images takes a lot of labors. In this paper, we present a\nsimple image-free domain generalization approach for the hand pose estimation\nframework that uses only source domain data. We try to manipulate the image\nfeatures of the hand pose estimation network by adding the features from text\ndescriptions using the CLIP (Contrastive Language-Image Pre-training) model.\nThe manipulated image features are then exploited to train the hand pose\nestimation network via the contrastive learning framework. In experiments with\nSTB and RHD datasets, our algorithm shows improved performance over the\nstate-of-the-art domain generalization approaches.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-30T09:32:37Z",
    "updated": "2022-10-30T09:32:37Z",
    "doi": null
  },
  "2210.01033": {
    "id": "http://arxiv.org/abs/2210.01033v2",
    "title": "LPT: Long-tailed Prompt Tuning for Image Classification",
    "authors": [
      "Bowen Dong",
      "Pan Zhou",
      "Shuicheng Yan",
      "Wangmeng Zuo"
    ],
    "abstract": "  For long-tailed classification, most works often pretrain a big model on a\nlarge-scale dataset, and then fine-tune the whole model for adapting to\nlong-tailed data. Though promising, fine-tuning the whole pretrained model\ntends to suffer from high cost in computation and deployment of different\nmodels for different tasks, as well as weakened generalization ability for\noverfitting to certain features of long-tailed data. To alleviate these issues,\nwe propose an effective Long-tailed Prompt Tuning method for long-tailed\nclassification. LPT introduces several trainable prompts into a frozen\npretrained model to adapt it to long-tailed data. For better effectiveness, we\ndivide prompts into two groups: 1) a shared prompt for the whole long-tailed\ndataset to learn general features and to adapt a pretrained model into target\ndomain; and 2) group-specific prompts to gather group-specific features for the\nsamples which have similar features and also to empower the pretrained model\nwith discrimination ability. Then we design a two-phase training paradigm to\nlearn these prompts. In phase 1, we train the shared prompt via supervised\nprompt tuning to adapt a pretrained model to the desired long-tailed domain. In\nphase 2, we use the learnt shared prompt as query to select a small best\nmatched set for a group of similar samples from the group-specific prompt set\nto dig the common features of these similar samples, then optimize these\nprompts with dual sampling strategy and asymmetric GCL loss. By only\nfine-tuning a few prompts while fixing the pretrained model, LPT can reduce\ntraining and deployment cost by storing a few prompts, and enjoys a strong\ngeneralization ability of the pretrained model. Experiments show that on\nvarious long-tailed benchmarks, with only ~1.1% extra parameters, LPT achieves\ncomparable performance than previous whole model fine-tuning methods, and is\nmore robust to domain-shift.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-03T15:47:02Z",
    "updated": "2023-03-28T10:16:03Z",
    "doi": null
  },
  "2203.04904": {
    "id": "http://arxiv.org/abs/2203.04904v3",
    "title": "Rethinking Task Sampling for Few-shot Vision-Language Transfer Learning",
    "authors": [
      "Zhenhailong Wang",
      "Hang Yu",
      "Manling Li",
      "Han Zhao",
      "Heng Ji"
    ],
    "abstract": "  Despite achieving state-of-the-art zero-shot performance, existing\nvision-language models still fall short of few-shot transfer ability on\ndomain-specific problems. Classical fine-tuning often fails to prevent highly\nexpressive models from exploiting spurious correlations. Although\nmodel-agnostic meta-learning (MAML) presents as a natural alternative for\nfew-shot transfer learning, the expensive computation due to implicit\nsecond-order optimization limits its use on large-scale vision-language models\nsuch as CLIP. While much literature has been devoted to exploring alternative\noptimization strategies, we identify another essential aspect towards effective\nfew-shot transfer learning, task sampling, which is previously only be viewed\nas part of data pre-processing in MAML. To show the impact of task sampling, we\npropose a simple algorithm, Model-Agnostic Multitask Fine-tuning (MAMF), which\ndifferentiates classical fine-tuning only on uniformly sampling multiple tasks.\nDespite its simplicity, we show that MAMF consistently outperforms classical\nfine-tuning on five few-shot vision-language classification tasks. We further\nshow that the effectiveness of the bi-level optimization in MAML is highly\nsensitive to the zero-shot performance of a task in the context of few-shot\nvision-language classification. The goal of this paper is to provide new\ninsights on what makes few-shot learning work, and encourage more research into\ninvestigating better task sampling strategies.\n",
    "categories": [
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-09T17:26:53Z",
    "updated": "2022-07-15T04:16:21Z",
    "doi": null
  },
  "2205.01859": {
    "id": "http://arxiv.org/abs/2205.01859v1",
    "title": "DEAR: A Novel Deep Learning-based Approach for Automated Program Repair",
    "authors": [
      "Yi Li",
      "Shaohua Wang",
      "Tien N. Nguyen"
    ],
    "abstract": "  The existing deep learning (DL)-based automated program repair (APR) models\nare limited in fixing general software defects. % We present {\\tool}, a\nDL-based approach that supports fixing for the general bugs that require\ndependent changes at once to one or multiple consecutive statements in one or\nmultiple hunks of code. % We first design a novel fault localization (FL)\ntechnique for multi-hunk, multi-statement fixes that combines traditional\nspectrum-based (SB) FL with deep learning and data-flow analysis. It takes the\nbuggy statements returned by the SBFL model, detects the buggy hunks to be\nfixed at once, and expands a buggy statement $s$ in a hunk to include other\nsuspicious statements around $s$. We design a two-tier, tree-based LSTM model\nthat incorporates cycle training and uses a divide-and-conquer strategy to\nlearn proper code transformations for fixing multiple statements in the\nsuitable fixing context consisting of surrounding subtrees. We conducted\nseveral experiments to evaluate {\\tool} on three datasets: Defects4J (395\nbugs), BigFix (+26k bugs), and CPatMiner (+44k bugs). On Defects4J dataset,\n{\\tool} outperforms the baselines from 42\\%--683\\% in terms of the number of\nauto-fixed bugs with only the top-1 patches. On BigFix dataset, it fixes\n31--145 more bugs than existing DL-based APR models with the top-1 patches. On\nCPatMiner dataset, among 667 fixed bugs, there are 169 (25.3\\%)\nmulti-hunk/multi-statement bugs. {\\tool} fixes 71 and 164 more bugs, including\n52 and 61 more multi-hunk/multi-statement bugs, than the state-of-the-art,\nDL-based APR models.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-04T02:29:40Z",
    "updated": "2022-05-04T02:29:40Z",
    "doi": null
  },
  "2401.00909": {
    "id": "http://arxiv.org/abs/2401.00909v2",
    "title": "Taming Mode Collapse in Score Distillation for Text-to-3D Generation",
    "authors": [
      "Peihao Wang",
      "Dejia Xu",
      "Zhiwen Fan",
      "Dilin Wang",
      "Sreyas Mohan",
      "Forrest Iandola",
      "Rakesh Ranjan",
      "Yilei Li",
      "Qiang Liu",
      "Zhangyang Wang",
      "Vikas Chandra"
    ],
    "abstract": "  Despite the remarkable performance of score distillation in text-to-3D\ngeneration, such techniques notoriously suffer from view inconsistency issues,\nalso known as \"Janus\" artifact, where the generated objects fake each view with\nmultiple front faces. Although empirically effective methods have approached\nthis problem via score debiasing or prompt engineering, a more rigorous\nperspective to explain and tackle this problem remains elusive. In this paper,\nwe reveal that the existing score distillation-based text-to-3D generation\nframeworks degenerate to maximal likelihood seeking on each view independently\nand thus suffer from the mode collapse problem, manifesting as the Janus\nartifact in practice. To tame mode collapse, we improve score distillation by\nre-establishing the entropy term in the corresponding variational objective,\nwhich is applied to the distribution of rendered images. Maximizing the entropy\nencourages diversity among different views in generated 3D assets, thereby\nmitigating the Janus problem. Based on this new objective, we derive a new\nupdate rule for 3D score distillation, dubbed Entropic Score Distillation\n(ESD). We theoretically reveal that ESD can be simplified and implemented by\njust adopting the classifier-free guidance trick upon variational score\ndistillation. Although embarrassingly straightforward, our extensive\nexperiments successfully demonstrate that ESD can be an effective treatment for\nJanus artifacts in score distillation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-31T22:47:06Z",
    "updated": "2024-03-29T18:04:37Z",
    "doi": null
  },
  "2403.14472": {
    "id": "http://arxiv.org/abs/2403.14472v5",
    "title": "Detoxifying Large Language Models via Knowledge Editing",
    "authors": [
      "Mengru Wang",
      "Ningyu Zhang",
      "Ziwen Xu",
      "Zekun Xi",
      "Shumin Deng",
      "Yunzhi Yao",
      "Qishen Zhang",
      "Linyi Yang",
      "Jindong Wang",
      "Huajun Chen"
    ],
    "abstract": "  This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments with several\nknowledge editing approaches, indicating that knowledge editing has the\npotential to detoxify LLMs with a limited impact on general performance\nefficiently. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxifying\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.HC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-21T15:18:30Z",
    "updated": "2024-05-28T09:11:25Z",
    "doi": null
  },
  "2408.13413": {
    "id": "http://arxiv.org/abs/2408.13413v1",
    "title": "TVG: A Training-free Transition Video Generation Method with Diffusion\n  Models",
    "authors": [
      "Rui Zhang",
      "Yaosen Chen",
      "Yuegen Liu",
      "Wei Wang",
      "Xuming Wen",
      "Hongxia Wang"
    ],
    "abstract": "  Transition videos play a crucial role in media production, enhancing the flow\nand coherence of visual narratives. Traditional methods like morphing often\nlack artistic appeal and require specialized skills, limiting their\neffectiveness. Recent advances in diffusion model-based video generation offer\nnew possibilities for creating transitions but face challenges such as poor\ninter-frame relationship modeling and abrupt content changes. We propose a\nnovel training-free Transition Video Generation (TVG) approach using\nvideo-level diffusion models that addresses these limitations without\nadditional training. Our method leverages Gaussian Process Regression\n($\\mathcal{GPR}$) to model latent representations, ensuring smooth and dynamic\ntransitions between frames. Additionally, we introduce interpolation-based\nconditional controls and a Frequency-aware Bidirectional Fusion (FBiF)\narchitecture to enhance temporal control and transition reliability.\nEvaluations of benchmark datasets and custom image pairs demonstrate the\neffectiveness of our approach in generating high-quality smooth transition\nvideos. The code are provided in https://sobeymil.github.io/tvg.com.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-24T00:33:14Z",
    "updated": "2024-08-24T00:33:14Z",
    "doi": null
  },
  "1709.04326": {
    "id": "http://arxiv.org/abs/1709.04326v4",
    "title": "Learning with Opponent-Learning Awareness",
    "authors": [
      "Jakob N. Foerster",
      "Richard Y. Chen",
      "Maruan Al-Shedivat",
      "Shimon Whiteson",
      "Pieter Abbeel",
      "Igor Mordatch"
    ],
    "abstract": "  Multi-agent settings are quickly gathering importance in machine learning.\nThis includes a plethora of recent work on deep multi-agent reinforcement\nlearning, but also can be extended to hierarchical RL, generative adversarial\nnetworks and decentralised optimisation. In all these settings the presence of\nmultiple learning agents renders the training problem non-stationary and often\nleads to unstable training or undesired final results. We present Learning with\nOpponent-Learning Awareness (LOLA), a method in which each agent shapes the\nanticipated learning of the other agents in the environment. The LOLA learning\nrule includes a term that accounts for the impact of one agent's policy on the\nanticipated parameter update of the other agents. Results show that the\nencounter of two LOLA agents leads to the emergence of tit-for-tat and\ntherefore cooperation in the iterated prisoners' dilemma, while independent\nlearning does not. In this domain, LOLA also receives higher payouts compared\nto a naive learner, and is robust against exploitation by higher order\ngradient-based methods. Applied to repeated matching pennies, LOLA agents\nconverge to the Nash equilibrium. In a round robin tournament we show that LOLA\nagents successfully shape the learning of a range of multi-agent learning\nalgorithms from literature, resulting in the highest average returns on the\nIPD. We also show that the LOLA update rule can be efficiently calculated using\nan extension of the policy gradient estimator, making the method suitable for\nmodel-free RL. The method thus scales to large parameter and input spaces and\nnonlinear function approximators. We apply LOLA to a grid world task with an\nembedded social dilemma using recurrent policies and opponent modelling. By\nexplicitly considering the learning of the other agent, LOLA agents learn to\ncooperate out of self-interest. The code is at github.com/alshedivat/lola.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GT",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-09-13T13:42:15Z",
    "updated": "2018-09-19T19:22:48Z",
    "doi": null
  },
  "2210.10253": {
    "id": "http://arxiv.org/abs/2210.10253v1",
    "title": "On the Adversarial Robustness of Mixture of Experts",
    "authors": [
      "Joan Puigcerver",
      "Rodolphe Jenatton",
      "Carlos Riquelme",
      "Pranjal Awasthi",
      "Srinadh Bhojanapalli"
    ],
    "abstract": "  Adversarial robustness is a key desirable property of neural networks. It has\nbeen empirically shown to be affected by their sizes, with larger networks\nbeing typically more robust. Recently, Bubeck and Sellke proved a lower bound\non the Lipschitz constant of functions that fit the training data in terms of\ntheir number of parameters. This raises an interesting open question, do -- and\ncan -- functions with more parameters, but not necessarily more computational\ncost, have better robustness? We study this question for sparse Mixture of\nExpert models (MoEs), that make it possible to scale up the model size for a\nroughly constant computational cost. We theoretically show that under certain\nconditions on the routing and the structure of the data, MoEs can have\nsignificantly smaller Lipschitz constants than their dense counterparts. The\nrobustness of MoEs can suffer when the highest weighted experts for an input\nimplement sufficiently different functions. We next empirically evaluate the\nrobustness of MoEs on ImageNet using adversarial attacks and show they are\nindeed more robust than dense models with the same computational cost. We make\nkey observations showing the robustness of MoEs to the choice of experts,\nhighlighting the redundancy of experts in models trained in practice.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-19T02:24:57Z",
    "updated": "2022-10-19T02:24:57Z",
    "doi": null
  },
  "2205.11083": {
    "id": "http://arxiv.org/abs/2205.11083v3",
    "title": "Deep Digging into the Generalization of Self-Supervised Monocular Depth\n  Estimation",
    "authors": [
      "Jinwoo Bae",
      "Sungho Moon",
      "Sunghoon Im"
    ],
    "abstract": "  Self-supervised monocular depth estimation has been widely studied recently.\nMost of the work has focused on improving performance on benchmark datasets,\nsuch as KITTI, but has offered a few experiments on generalization performance.\nIn this paper, we investigate the backbone networks (e.g. CNNs, Transformers,\nand CNN-Transformer hybrid models) toward the generalization of monocular depth\nestimation. We first evaluate state-of-the-art models on diverse public\ndatasets, which have never been seen during the network training. Next, we\ninvestigate the effects of texture-biased and shape-biased representations\nusing the various texture-shifted datasets that we generated. We observe that\nTransformers exhibit a strong shape bias and CNNs do a strong texture-bias. We\nalso find that shape-biased models show better generalization performance for\nmonocular depth estimation compared to texture-biased models. Based on these\nobservations, we newly design a CNN-Transformer hybrid network with a\nmulti-level adaptive feature fusion module, called MonoFormer. The design\nintuition behind MonoFormer is to increase shape bias by employing Transformers\nwhile compensating for the weak locality bias of Transformers by adaptively\nfusing multi-level representations. Extensive experiments show that the\nproposed method achieves state-of-the-art performance with various public\ndatasets. Our method also shows the best generalization ability among the\ncompetitive methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-23T06:56:25Z",
    "updated": "2023-03-20T03:52:42Z",
    "doi": null
  },
  "2105.09755": {
    "id": "http://arxiv.org/abs/2105.09755v1",
    "title": "Generalized Wasserstein barycenters between probability measures living\n  on different subspaces",
    "authors": [
      "Julie Delon",
      "Natha\u00ebl Gozlan",
      "Alexandre Saint-Dizier"
    ],
    "abstract": "  In this paper, we introduce a generalization of the Wasserstein barycenter,\nto a case where the initial probability measures live on different subspaces of\nR^d. We study the existence and uniqueness of this barycenter, we show how it\nis related to a larger multi-marginal optimal transport problem, and we propose\na dual formulation. Finally, we explain how to compute numerically this\ngeneralized barycenter on discrete distributions, and we propose an explicit\nsolution for Gaussian distributions.\n",
    "categories": [
      {
        "@term": "math.PR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.FA",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-05-20T14:01:51Z",
    "updated": "2021-05-20T14:01:51Z",
    "doi": null
  },
  "2310.03295": {
    "id": "http://arxiv.org/abs/2310.03295v1",
    "title": "Can pre-trained models assist in dataset distillation?",
    "authors": [
      "Yao Lu",
      "Xuguang Chen",
      "Yuchen Zhang",
      "Jianyang Gu",
      "Tianle Zhang",
      "Yifan Zhang",
      "Xiaoniu Yang",
      "Qi Xuan",
      "Kai Wang",
      "Yang You"
    ],
    "abstract": "  Dataset Distillation (DD) is a prominent technique that encapsulates\nknowledge from a large-scale original dataset into a small synthetic dataset\nfor efficient training. Meanwhile, Pre-trained Models (PTMs) function as\nknowledge repositories, containing extensive information from the original\ndataset. This naturally raises a question: Can PTMs effectively transfer\nknowledge to synthetic datasets, guiding DD accurately? To this end, we conduct\npreliminary experiments, confirming the contribution of PTMs to DD. Afterwards,\nwe systematically study different options in PTMs, including initialization\nparameters, model architecture, training epoch and domain knowledge, revealing\nthat: 1) Increasing model diversity enhances the performance of synthetic\ndatasets; 2) Sub-optimal models can also assist in DD and outperform\nwell-trained ones in certain cases; 3) Domain-specific PTMs are not mandatory\nfor DD, but a reasonable domain match is crucial. Finally, by selecting optimal\noptions, we significantly improve the cross-architecture generalization over\nbaseline DD methods. We hope our work will facilitate researchers to develop\nbetter DD techniques. Our code is available at\nhttps://github.com/yaolu-zjut/DDInterpreter.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-05T03:51:21Z",
    "updated": "2023-10-05T03:51:21Z",
    "doi": null
  },
  "2302.14503": {
    "id": "http://arxiv.org/abs/2302.14503v1",
    "title": "Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?",
    "authors": [
      "Hyemin Ahn",
      "Esteve Valls Mascaro",
      "Dongheui Lee"
    ],
    "abstract": "  After many researchers observed fruitfulness from the recent diffusion\nprobabilistic model, its effectiveness in image generation is actively studied\nthese days. In this paper, our objective is to evaluate the potential of\ndiffusion probabilistic models for 3D human motion-related tasks. To this end,\nthis paper presents a study of employing diffusion probabilistic models to\npredict future 3D human motion(s) from the previously observed motion. Based on\nthe Human 3.6M and HumanEva-I datasets, our results show that diffusion\nprobabilistic models are competitive for both single (deterministic) and\nmultiple (stochastic) 3D motion prediction tasks, after finishing a single\ntraining process. In addition, we find out that diffusion probabilistic models\ncan offer an attractive compromise, since they can strike the right balance\nbetween the likelihood and diversity of the predicted future motions. Our code\nis publicly available on the project website:\nhttps://sites.google.com/view/diffusion-motion-prediction.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-28T11:34:55Z",
    "updated": "2023-02-28T11:34:55Z",
    "doi": null
  },
  "2302.03548": {
    "id": "http://arxiv.org/abs/2302.03548v1",
    "title": "PhysFormer++: Facial Video-based Physiological Measurement with SlowFast\n  Temporal Difference Transformer",
    "authors": [
      "Zitong Yu",
      "Yuming Shen",
      "Jingang Shi",
      "Hengshuang Zhao",
      "Yawen Cui",
      "Jiehua Zhang",
      "Philip Torr",
      "Guoying Zhao"
    ],
    "abstract": "  Remote photoplethysmography (rPPG), which aims at measuring heart activities\nand physiological signals from facial video without any contact, has great\npotential in many applications (e.g., remote healthcare and affective\ncomputing). Recent deep learning approaches focus on mining subtle rPPG clues\nusing convolutional neural networks with limited spatio-temporal receptive\nfields, which neglect the long-range spatio-temporal perception and interaction\nfor rPPG modeling. In this paper, we propose two end-to-end video transformer\nbased architectures, namely PhysFormer and PhysFormer++, to adaptively\naggregate both local and global spatio-temporal features for rPPG\nrepresentation enhancement. As key modules in PhysFormer, the temporal\ndifference transformers first enhance the quasi-periodic rPPG features with\ntemporal difference guided global attention, and then refine the local\nspatio-temporal representation against interference. To better exploit the\ntemporal contextual and periodic rPPG clues, we also extend the PhysFormer to\nthe two-pathway SlowFast based PhysFormer++ with temporal difference periodic\nand cross-attention transformers. Furthermore, we propose the label\ndistribution learning and a curriculum learning inspired dynamic constraint in\nfrequency domain, which provide elaborate supervisions for PhysFormer and\nPhysFormer++ and alleviate overfitting. Comprehensive experiments are performed\non four benchmark datasets to show our superior performance on both intra- and\ncross-dataset testings. Unlike most transformer networks needed pretraining\nfrom large-scale datasets, the proposed PhysFormer family can be easily trained\nfrom scratch on rPPG datasets, which makes it promising as a novel transformer\nbaseline for the rPPG community.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-07T15:56:03Z",
    "updated": "2023-02-07T15:56:03Z",
    "doi": null
  },
  "2203.00242": {
    "id": "http://arxiv.org/abs/2203.00242v1",
    "title": "Unsupervised Vision-and-Language Pre-training via Retrieval-based\n  Multi-Granular Alignment",
    "authors": [
      "Mingyang Zhou",
      "Licheng Yu",
      "Amanpreet Singh",
      "Mengjiao Wang",
      "Zhou Yu",
      "Ning Zhang"
    ],
    "abstract": "  Vision-and-Language (V+L) pre-training models have achieved tremendous\nsuccess in recent years on various multi-modal benchmarks. However, the\nmajority of existing models require pre-training on a large set of parallel\nimage-text data, which is costly to collect, compared to image-only or\ntext-only data. In this paper, we explore unsupervised Vision-and-Language\npre-training (UVLP) to learn the cross-modal representation from non-parallel\nimage and text datasets. We found two key factors that lead to good\nunsupervised V+L pre-training without parallel data: (i) joint image-and-text\ninput (ii) overall image-text alignment (even for non-parallel data).\nAccordingly, we propose a novel unsupervised V+L pre-training curriculum for\nnon-parallel texts and images. We first construct a weakly aligned image-text\ncorpus via a retrieval-based approach, then apply a set of multi-granular\nalignment pre-training tasks, including region-to-tag, region-to-phrase, and\nimage-to-sentence alignment, to bridge the gap between the two modalities. A\ncomprehensive ablation study shows each granularity is helpful to learn a\nstronger pre-trained model. We adapt our pre-trained model to a set of V+L\ndownstream tasks, including VQA, NLVR2, Visual Entailment, and RefCOCO+. Our\nmodel achieves the state-of-art performance in all these tasks under the\nunsupervised setting.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-01T05:34:01Z",
    "updated": "2022-03-01T05:34:01Z",
    "doi": null
  },
  "1703.01488": {
    "id": "http://arxiv.org/abs/1703.01488v1",
    "title": "Autoencoding Variational Inference For Topic Models",
    "authors": [
      "Akash Srivastava",
      "Charles Sutton"
    ],
    "abstract": "  Topic models are one of the most popular methods for learning representations\nof text, but a major challenge is that any change to the topic model requires\nmathematically deriving a new inference algorithm. A promising approach to\naddress this problem is autoencoding variational Bayes (AEVB), but it has\nproven diffi- cult to apply to topic models in practice. We present what is to\nour knowledge the first effective AEVB based inference method for latent\nDirichlet allocation (LDA), which we call Autoencoded Variational Inference For\nTopic Model (AVITM). This model tackles the problems caused for AEVB by the\nDirichlet prior and by component collapsing. We find that AVITM matches\ntraditional methods in accuracy with much better inference time. Indeed,\nbecause of the inference network, we find that it is unnecessary to pay the\ncomputational cost of running variational optimization on test data. Because\nAVITM is black box, it is readily applied to new topic models. As a dramatic\nillustration of this, we present a new topic model called ProdLDA, that\nreplaces the mixture model in LDA with a product of experts. By changing only\none line of code from LDA, we find that ProdLDA yields much more interpretable\ntopics, even if LDA is trained via collapsed Gibbs sampling.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-03-04T16:28:15Z",
    "updated": "2017-03-04T16:28:15Z",
    "doi": null
  },
  "1909.13387": {
    "id": "http://arxiv.org/abs/1909.13387v2",
    "title": "FaSNet: Low-latency Adaptive Beamforming for Multi-microphone Audio\n  Processing",
    "authors": [
      "Yi Luo",
      "Enea Ceolini",
      "Cong Han",
      "Shih-Chii Liu",
      "Nima Mesgarani"
    ],
    "abstract": "  Beamforming has been extensively investigated for multi-channel audio\nprocessing tasks. Recently, learning-based beamforming methods, sometimes\ncalled \\textit{neural beamformers}, have achieved significant improvements in\nboth signal quality (e.g. signal-to-noise ratio (SNR)) and speech recognition\n(e.g. word error rate (WER)). Such systems are generally non-causal and require\na large context for robust estimation of inter-channel features, which is\nimpractical in applications requiring low-latency responses. In this paper, we\npropose filter-and-sum network (FaSNet), a time-domain, filter-based\nbeamforming approach suitable for low-latency scenarios. FaSNet has a two-stage\nsystem design that first learns frame-level time-domain adaptive beamforming\nfilters for a selected reference channel, and then calculate the filters for\nall remaining channels. The filtered outputs at all channels are summed to\ngenerate the final output. Experiments show that despite its small model size,\nFaSNet is able to outperform several traditional oracle beamformers with\nrespect to scale-invariant signal-to-noise ratio (SI-SNR) in reverberant speech\nenhancement and separation tasks. Moreover, when trained with a\nfrequency-domain objective function on the CHiME-3 dataset, FaSNet achieves\n14.3\\% relative word error rate reduction (RWERR) compared with the baseline\nmodel. These results show the efficacy of FaSNet particularly in reverberant\nand noisy signal conditions.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.SP",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-09-29T22:26:06Z",
    "updated": "2019-10-01T02:07:14Z",
    "doi": null
  },
  "2301.11798": {
    "id": "http://arxiv.org/abs/2301.11798v2",
    "title": "MedSegDiff-V2: Diffusion based Medical Image Segmentation with\n  Transformer",
    "authors": [
      "Junde Wu",
      "Wei Ji",
      "Huazhu Fu",
      "Min Xu",
      "Yueming Jin",
      "Yanwu Xu"
    ],
    "abstract": "  The Diffusion Probabilistic Model (DPM) has recently gained popularity in the\nfield of computer vision, thanks to its image generation applications, such as\nImagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated\nimpressive capabilities and sparked much discussion within the community.\nRecent investigations have further unveiled the utility of DPM in the domain of\nmedical image analysis, as underscored by the commendable performance exhibited\nby the medical image segmentation model across various tasks. Although these\nmodels were originally underpinned by a UNet architecture, there exists a\npotential avenue for enhancing their performance through the integration of\nvision transformer mechanisms. However, we discovered that simply combining\nthese two models resulted in subpar performance. To effectively integrate these\ntwo cutting-edge techniques for the Medical image segmentation, we propose a\nnovel Transformer-based Diffusion framework, called MedSegDiff-V2. We verify\nits effectiveness on 20 medical image segmentation tasks with different image\nmodalities. Through comprehensive evaluation, our approach demonstrates\nsuperiority over prior state-of-the-art (SOTA) methodologies. Code is released\nat https://github.com/KidsWithTokens/MedSegDiff\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-19T03:42:36Z",
    "updated": "2023-12-24T03:02:12Z",
    "doi": null
  },
  "2111.15666": {
    "id": "http://arxiv.org/abs/2111.15666v2",
    "title": "HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing",
    "authors": [
      "Yuval Alaluf",
      "Omer Tov",
      "Ron Mokady",
      "Rinon Gal",
      "Amit H. Bermano"
    ],
    "abstract": "  The inversion of real images into StyleGAN's latent space is a well-studied\nproblem. Nevertheless, applying existing approaches to real-world scenarios\nremains an open challenge, due to an inherent trade-off between reconstruction\nand editability: latent space regions which can accurately represent real\nimages typically suffer from degraded semantic control. Recent work proposes to\nmitigate this trade-off by fine-tuning the generator to add the target image to\nwell-behaved, editable regions of the latent space. While promising, this\nfine-tuning scheme is impractical for prevalent use as it requires a lengthy\ntraining phase for each new image. In this work, we introduce this approach\ninto the realm of encoder-based inversion. We propose HyperStyle, a\nhypernetwork that learns to modulate StyleGAN's weights to faithfully express a\ngiven image in editable regions of the latent space. A naive modulation\napproach would require training a hypernetwork with over three billion\nparameters. Through careful network design, we reduce this to be in line with\nexisting encoders. HyperStyle yields reconstructions comparable to those of\noptimization techniques with the near real-time inference capabilities of\nencoders. Lastly, we demonstrate HyperStyle's effectiveness on several\napplications beyond the inversion task, including the editing of out-of-domain\nimages which were never seen during training.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-30T18:56:30Z",
    "updated": "2022-03-29T16:11:11Z",
    "doi": null
  },
  "2408.08495": {
    "id": "http://arxiv.org/abs/2408.08495v1",
    "title": "Achieving Complex Image Edits via Function Aggregation with Diffusion\n  Models",
    "authors": [
      "Mohammadreza Samadi",
      "Fred X. Han",
      "Mohammad Salameh",
      "Hao Wu",
      "Fengyu Sun",
      "Chunhua Zhou",
      "Di Niu"
    ],
    "abstract": "  Diffusion models have demonstrated strong performance in generative tasks,\nmaking them ideal candidates for image editing. Recent studies highlight their\nability to apply desired edits effectively by following textual instructions,\nyet two key challenges persist. First, these models struggle to apply multiple\nedits simultaneously, resulting in computational inefficiencies due to their\nreliance on sequential processing. Second, relying on textual prompts to\ndetermine the editing region can lead to unintended alterations in other parts\nof the image. In this work, we introduce FunEditor, an efficient diffusion\nmodel designed to learn atomic editing functions and perform complex edits by\naggregating simpler functions. This approach enables complex editing tasks,\nsuch as object movement, by aggregating multiple functions and applying them\nsimultaneously to specific areas. FunEditor is 5 to 24 times faster inference\nthan existing methods on complex tasks like object movement. Our experiments\ndemonstrate that FunEditor significantly outperforms recent baselines,\nincluding both inference-time optimization methods and fine-tuned models,\nacross various metrics, such as image quality assessment (IQA) and\nobject-background consistency.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-16T02:33:55Z",
    "updated": "2024-08-16T02:33:55Z",
    "doi": null
  },
  "2212.11263": {
    "id": "http://arxiv.org/abs/2212.11263v1",
    "title": "3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions",
    "authors": [
      "Dale Decatur",
      "Itai Lang",
      "Rana Hanocka"
    ],
    "abstract": "  We present 3D Highlighter, a technique for localizing semantic regions on a\nmesh using text as input. A key feature of our system is the ability to\ninterpret \"out-of-domain\" localizations. Our system demonstrates the ability to\nreason about where to place non-obviously related concepts on an input 3D\nshape, such as adding clothing to a bare 3D animal model. Our method\ncontextualizes the text description using a neural field and colors the\ncorresponding region of the shape using a probability-weighted blend. Our\nneural optimization is guided by a pre-trained CLIP encoder, which bypasses the\nneed for any 3D datasets or 3D annotations. Thus, 3D Highlighter is highly\nflexible, general, and capable of producing localizations on a myriad of input\nshapes. Our code is publicly available at\nhttps://github.com/threedle/3DHighlighter.\n",
    "categories": [
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-21T18:54:47Z",
    "updated": "2022-12-21T18:54:47Z",
    "doi": null
  },
  "2112.02466": {
    "id": "http://arxiv.org/abs/2112.02466v2",
    "title": "Pose-guided Feature Disentangling for Occluded Person Re-identification\n  Based on Transformer",
    "authors": [
      "Tao Wang",
      "Hong Liu",
      "Pinhao Song",
      "Tianyu Guo",
      "Wei Shi"
    ],
    "abstract": "  Occluded person re-identification is a challenging task as human body parts\ncould be occluded by some obstacles (e.g. trees, cars, and pedestrians) in\ncertain scenes. Some existing pose-guided methods solve this problem by\naligning body parts according to graph matching, but these graph-based methods\nare not intuitive and complicated. Therefore, we propose a transformer-based\nPose-guided Feature Disentangling (PFD) method by utilizing pose information to\nclearly disentangle semantic components (e.g. human body or joint parts) and\nselectively match non-occluded parts correspondingly. First, Vision Transformer\n(ViT) is used to extract the patch features with its strong capability. Second,\nto preliminarily disentangle the pose information from patch information, the\nmatching and distributing mechanism is leveraged in Pose-guided Feature\nAggregation (PFA) module. Third, a set of learnable semantic views are\nintroduced in transformer decoder to implicitly enhance the disentangled body\npart features. However, those semantic views are not guaranteed to be related\nto the body without additional supervision. Therefore, Pose-View Matching (PVM)\nmodule is proposed to explicitly match visible body parts and automatically\nseparate occlusion features. Fourth, to better prevent the interference of\nocclusions, we design a Pose-guided Push Loss to emphasize the features of\nvisible body parts. Extensive experiments over five challenging datasets for\ntwo tasks (occluded and holistic Re-ID) demonstrate that our proposed PFD is\nsuperior promising, which performs favorably against state-of-the-art methods.\nCode is available at https://github.com/WangTaoAs/PFD_Net\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-12-05T03:23:31Z",
    "updated": "2021-12-11T08:04:37Z",
    "doi": null
  },
  "2403.05808": {
    "id": "http://arxiv.org/abs/2403.05808v2",
    "title": "Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with\n  Diffusion Model for Blind Image Super-Resolution",
    "authors": [
      "Junxiong Lin",
      "Yan Wang",
      "Zeng Tao",
      "Boyang Wang",
      "Qing Zhao",
      "Haorang Wang",
      "Xuan Tong",
      "Xinji Mai",
      "Yuxuan Lin",
      "Wei Song",
      "Jiawen Yu",
      "Shaoqi Yan",
      "Wenqiang Zhang"
    ],
    "abstract": "  Pre-trained diffusion models utilized for image generation encapsulate a\nsubstantial reservoir of a priori knowledge pertaining to intricate textures.\nHarnessing the potential of leveraging this a priori knowledge in the context\nof image super-resolution presents a compelling avenue. Nonetheless, prevailing\ndiffusion-based methodologies presently overlook the constraints imposed by\ndegradation information on the diffusion process. Furthermore, these methods\nfail to consider the spatial variability inherent in the estimated blur kernel,\nstemming from factors such as motion jitter and out-of-focus elements in\nopen-environment scenarios. This oversight results in a notable deviation of\nthe image super-resolution effect from fundamental realities. To address these\nconcerns, we introduce a framework known as Adaptive Multi-modal Fusion of\n\\textbf{S}patially Variant Kernel Refinement with Diffusion Model for Blind\nImage \\textbf{S}uper-\\textbf{R}esolution (SSR). Within the SSR framework, we\npropose a Spatially Variant Kernel Refinement (SVKR) module. SVKR estimates a\nDepth-Informed Kernel, which takes the depth information into account and is\nspatially variant. Additionally, SVKR enhance the accuracy of depth information\nacquired from LR images, allowing for mutual enhancement between the depth map\nand blur kernel estimates. Finally, we introduce the Adaptive Multi-Modal\nFusion (AMF) module to align the information from three modalities:\nlow-resolution images, depth maps, and blur kernels. This alignment can\nconstrain the diffusion model to generate more authentic SR results.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-09T06:01:25Z",
    "updated": "2024-07-09T15:42:48Z",
    "doi": null
  },
  "1804.08275": {
    "id": "http://arxiv.org/abs/1804.08275v1",
    "title": "Deep Semantic Hashing with Generative Adversarial Networks",
    "authors": [
      "Zhaofan Qiu",
      "Yingwei Pan",
      "Ting Yao",
      "Tao Mei"
    ],
    "abstract": "  Hashing has been a widely-adopted technique for nearest neighbor search in\nlarge-scale image retrieval tasks. Recent research has shown that leveraging\nsupervised information can lead to high quality hashing. However, the cost of\nannotating data is often an obstacle when applying supervised hashing to a new\ndomain. Moreover, the results can suffer from the robustness problem as the\ndata at training and test stage could come from similar but different\ndistributions. This paper studies the exploration of generating synthetic data\nthrough semi-supervised generative adversarial networks (GANs), which leverages\nlargely unlabeled and limited labeled training data to produce highly\ncompelling data with intrinsic invariance and global coherence, for better\nunderstanding statistical structures of natural data. We demonstrate that the\nabove two limitations can be well mitigated by applying the synthetic data for\nhashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\npresented, which mainly consists of four components: a deep convolution neural\nnetworks (CNN) for learning image representations, an adversary stream to\ndistinguish synthetic images from real ones, a hash stream for encoding image\nrepresentations to hash codes and a classification stream. The whole\narchitecture is trained end-to-end by jointly optimizing three losses, i.e.,\nadversarial loss to correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the input\nreal-synthetic triplets and classification loss to classify each sample\naccurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\nbenchmarks validate the capability of exploiting synthetic images for hashing.\nOur framework also achieves superior results when compared to state-of-the-art\ndeep hash models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-23T08:19:55Z",
    "updated": "2018-04-23T08:19:55Z",
    "doi": null
  },
  "1910.12638": {
    "id": "http://arxiv.org/abs/1910.12638v2",
    "title": "Mockingjay: Unsupervised Speech Representation Learning with Deep\n  Bidirectional Transformer Encoders",
    "authors": [
      "Andy T. Liu",
      "Shu-wen Yang",
      "Po-Han Chi",
      "Po-chun Hsu",
      "Hung-yi Lee"
    ],
    "abstract": "  We present Mockingjay as a new speech representation learning approach, where\nbidirectional Transformer encoders are pre-trained on a large amount of\nunlabeled speech. Previous speech representation methods learn through\nconditioning on past frames and predicting information about future frames.\nWhereas Mockingjay is designed to predict the current frame through jointly\nconditioning on both past and future contexts. The Mockingjay representation\nimproves performance for a wide range of downstream tasks, including phoneme\nclassification, speaker recognition, and sentiment classification on spoken\ncontent, while outperforming other approaches. Mockingjay is empirically\npowerful and can be fine-tuned with downstream models, with only 2 epochs we\nfurther improve performance dramatically. In a low resource setting with only\n0.1% of labeled data, we outperform the result of Mel-features that uses all\n100% labeled data.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-10-25T01:55:12Z",
    "updated": "2020-02-02T15:08:39Z",
    "doi": "10.1109/ICASSP40776.2020.9054458"
  },
  "2402.12099": {
    "id": "http://arxiv.org/abs/2402.12099v2",
    "title": "Human Video Translation via Query Warping",
    "authors": [
      "Haiming Zhu",
      "Yangyang Xu",
      "Shengfeng He"
    ],
    "abstract": "  In this paper, we present QueryWarp, a novel framework for temporally\ncoherent human motion video translation. Existing diffusion-based video editing\napproaches that rely solely on key and value tokens to ensure temporal\nconsistency, which scarifies the preservation of local and structural regions.\nIn contrast, we aim to consider complementary query priors by constructing the\ntemporal correlations among query tokens from different frames. Initially, we\nextract appearance flows from source poses to capture continuous human\nforeground motion. Subsequently, during the denoising process of the diffusion\nmodel, we employ appearance flows to warp the previous frame's query token,\naligning it with the current frame's query. This query warping imposes explicit\nconstraints on the outputs of self-attention layers, effectively guaranteeing\ntemporally coherent translation. We perform experiments on various human motion\nvideo translation tasks, and the results demonstrate that our QueryWarp\nframework surpasses state-of-the-art methods both qualitatively and\nquantitatively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-19T12:28:45Z",
    "updated": "2024-05-21T04:04:01Z",
    "doi": null
  },
  "2208.04307": {
    "id": "http://arxiv.org/abs/2208.04307v1",
    "title": "PlaneFormers: From Sparse View Planes to 3D Reconstruction",
    "authors": [
      "Samir Agarwala",
      "Linyi Jin",
      "Chris Rockwell",
      "David F. Fouhey"
    ],
    "abstract": "  We present an approach for the planar surface reconstruction of a scene from\nimages with limited overlap. This reconstruction task is challenging since it\nrequires jointly reasoning about single image 3D reconstruction, correspondence\nbetween images, and the relative camera pose between images. Past work has\nproposed optimization-based approaches. We introduce a simpler approach, the\nPlaneFormer, that uses a transformer applied to 3D-aware plane tokens to\nperform 3D reasoning. Our experiments show that our approach is substantially\nmore effective than prior work, and that several 3D-specific design decisions\nare crucial for its success.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-08T17:58:13Z",
    "updated": "2022-08-08T17:58:13Z",
    "doi": null
  },
  "2208.11328": {
    "id": "http://arxiv.org/abs/2208.11328v2",
    "title": "K-Order Graph-oriented Transformer with GraAttention for 3D Pose and\n  Shape Estimation",
    "authors": [
      "Weixi Zhao",
      "Weiqiang Wang"
    ],
    "abstract": "  We propose a novel attention-based 2D-to-3D pose estimation network for\ngraph-structured data, named KOG-Transformer, and a 3D pose-to-shape estimation\nnetwork for hand data, named GASE-Net. Previous 3D pose estimation methods have\nfocused on various modifications to the graph convolution kernel, such as\nabandoning weight sharing or increasing the receptive field. Some of these\nmethods employ attention-based non-local modules as auxiliary modules. In order\nto better model the relationship between nodes in graph-structured data and\nfuse the information of different neighbor nodes in a differentiated way, we\nmake targeted modifications to the attention module and propose two modules\ndesigned for graph-structured data, graph relative positional encoding\nmulti-head self-attention (GR-MSA) and K-order graph-oriented multi-head\nself-attention (KOG-MSA). By stacking GR-MSA and KOG-MSA, we propose a novel\nnetwork KOG-Transformer for 2D-to-3D pose estimation. Furthermore, we propose a\nnetwork for shape estimation on hand data, called GraAttention shape estimation\nnetwork (GASE-Net), which takes a 3D pose as input and gradually models the\nshape of the hand from sparse to dense. We have empirically shown the\nsuperiority of KOG-Transformer through extensive experiments. Experimental\nresults show that KOG-Transformer significantly outperforms the previous\nstate-of-the-art methods on the benchmark dataset Human3.6M. We evaluate the\neffect of GASE-Net on two public available hand datasets, ObMan and\nInterHand2.6M. GASE-Net can predict the corresponding shape for input pose with\nstrong generalization ability.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-24T06:54:03Z",
    "updated": "2022-09-24T02:20:27Z",
    "doi": null
  },
  "1904.03468": {
    "id": "http://arxiv.org/abs/1904.03468v1",
    "title": "Deep Stacked Hierarchical Multi-patch Network for Image Deblurring",
    "authors": [
      "Hongguang Zhang",
      "Yuchao Dai",
      "Hongdong Li",
      "Piotr Koniusz"
    ],
    "abstract": "  Despite deep end-to-end learning methods have shown their superiority in\nremoving non-uniform motion blur, there still exist major challenges with the\ncurrent multi-scale and scale-recurrent models: 1) Deconvolution/upsampling\noperations in the coarse-to-fine scheme result in expensive runtime; 2) Simply\nincreasing the model depth with finer-scale levels cannot improve the quality\nof deblurring. To tackle the above problems, we present a deep hierarchical\nmulti-patch network inspired by Spatial Pyramid Matching to deal with blurry\nimages via a fine-to-coarse hierarchical representation. To deal with the\nperformance saturation w.r.t. depth, we propose a stacked version of our\nmulti-patch model. Our proposed basic multi-patch model achieves the\nstate-of-the-art performance on the GoPro dataset while enjoying a 40x faster\nruntime compared to current multi-scale methods. With 30ms to process an image\nat 1280x720 resolution, it is the first real-time deep motion deblurring model\nfor 720p images at 30fps. For stacked networks, significant improvements (over\n1.2dB) are achieved on the GoPro dataset by increasing the network depth.\nMoreover, by varying the depth of the stacked model, one can adapt the\nperformance and runtime of the same network for different application\nscenarios.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-04-06T15:15:36Z",
    "updated": "2019-04-06T15:15:36Z",
    "doi": null
  },
  "2002.02848": {
    "id": "http://arxiv.org/abs/2002.02848v1",
    "title": "Unsupervised pretraining transfers well across languages",
    "authors": [
      "Morgane Rivi\u00e8re",
      "Armand Joulin",
      "Pierre-Emmanuel Mazar\u00e9",
      "Emmanuel Dupoux"
    ],
    "abstract": "  Cross-lingual and multi-lingual training of Automatic Speech Recognition\n(ASR) has been extensively investigated in the supervised setting. This assumes\nthe existence of a parallel corpus of speech and orthographic transcriptions.\nRecently, contrastive predictive coding (CPC) algorithms have been proposed to\npretrain ASR systems with unlabelled data. In this work, we investigate whether\nunsupervised pretraining transfers well across languages. We show that a slight\nmodification of the CPC pretraining extracts features that transfer well to\nother languages, being on par or even outperforming supervised pretraining.\nThis shows the potential of unsupervised methods for languages with few\nlinguistic resources.\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-02-07T15:34:53Z",
    "updated": "2020-02-07T15:34:53Z",
    "doi": null
  },
  "2301.03846": {
    "id": "http://arxiv.org/abs/2301.03846v1",
    "title": "Practitioners' Expectations on Code Completion",
    "authors": [
      "Chaozheng Wang",
      "Junhao Hu",
      "Cuiyun Gao",
      "Yu Jin",
      "Tao Xie",
      "Hailiang Huang",
      "Zhenyu Lei",
      "Yuetang Deng"
    ],
    "abstract": "  Code completion has become a common practice for programmers during their\ndaily programming activities. It aims at automatically predicting the next\ntokens or lines that the programmers tend to use. A good code completion tool\ncan substantially save keystrokes and improve the programming efficiency for\nprogrammers. Recently, various techniques for code completion have been\nproposed for usage in practice. However, it is still unclear what are\npractitioners' expectations on code completion and whether existing research\nhas met their demands. To fill the gap, we perform an empirical study by first\ninterviewing 15 practitioners and then surveying 599 practitioners from 18 IT\ncompanies about their expectations on code completion. We then compare the\npractitioners' demands with current research via conducting a literature review\nof papers on code completion published in premier publication venues from 2012\nto 2022. Based on the comparison, we highlight the directions desirable for\nresearchers to invest efforts towards developing code completion techniques for\nmeeting practitioners' expectations.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-10T08:30:37Z",
    "updated": "2023-01-10T08:30:37Z",
    "doi": null
  },
  "2406.04961": {
    "id": "http://arxiv.org/abs/2406.04961v1",
    "title": "Multiplane Prior Guided Few-Shot Aerial Scene Rendering",
    "authors": [
      "Zihan Gao",
      "Licheng Jiao",
      "Lingling Li",
      "Xu Liu",
      "Fang Liu",
      "Puhua Chen",
      "Yuwei Guo"
    ],
    "abstract": "  Neural Radiance Fields (NeRF) have been successfully applied in various\naerial scenes, yet they face challenges with sparse views due to limited\nsupervision. The acquisition of dense aerial views is often prohibitive, as\nunmanned aerial vehicles (UAVs) may encounter constraints in perspective range\nand energy constraints. In this work, we introduce Multiplane Prior guided NeRF\n(MPNeRF), a novel approach tailored for few-shot aerial scene rendering-marking\na pioneering effort in this domain. Our key insight is that the intrinsic\ngeometric regularities specific to aerial imagery could be leveraged to enhance\nNeRF in sparse aerial scenes. By investigating NeRF's and Multiplane Image\n(MPI)'s behavior, we propose to guide the training process of NeRF with a\nMultiplane Prior. The proposed Multiplane Prior draws upon MPI's benefits and\nincorporates advanced image comprehension through a SwinV2 Transformer,\npre-trained via SimMIM. Our extensive experiments demonstrate that MPNeRF\noutperforms existing state-of-the-art methods applied in non-aerial contexts,\nby tripling the performance in SSIM and LPIPS even with three views available.\nWe hope our work offers insights into the development of NeRF-based\napplications in aerial scenes with limited data.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-07T14:25:53Z",
    "updated": "2024-06-07T14:25:53Z",
    "doi": null
  },
  "1806.06923": {
    "id": "http://arxiv.org/abs/1806.06923v1",
    "title": "Implicit Quantile Networks for Distributional Reinforcement Learning",
    "authors": [
      "Will Dabney",
      "Georg Ostrovski",
      "David Silver",
      "R\u00e9mi Munos"
    ],
    "abstract": "  In this work, we build on recent advances in distributional reinforcement\nlearning to give a generally applicable, flexible, and state-of-the-art\ndistributional variant of DQN. We achieve this by using quantile regression to\napproximate the full quantile function for the state-action return\ndistribution. By reparameterizing a distribution over the sample space, this\nyields an implicitly defined return distribution and gives rise to a large\nclass of risk-sensitive policies. We demonstrate improved performance on the 57\nAtari 2600 games in the ALE, and use our algorithm's implicitly defined\ndistributions to study the effects of risk-sensitive policies in Atari games.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-06-14T14:28:37Z",
    "updated": "2018-06-14T14:28:37Z",
    "doi": null
  },
  "2203.01726": {
    "id": "http://arxiv.org/abs/2203.01726v3",
    "title": "Ensembles of Vision Transformers as a New Paradigm for Automated\n  Classification in Ecology",
    "authors": [
      "S. Kyathanahally",
      "T. Hardeman",
      "M. Reyes",
      "E. Merz",
      "T. Bulas",
      "P. Brun",
      "F. Pomati",
      "M. Baity-Jesi"
    ],
    "abstract": "  Monitoring biodiversity is paramount to manage and protect natural resources.\nCollecting images of organisms over large temporal or spatial scales is a\npromising practice to monitor the biodiversity of natural ecosystems, providing\nlarge amounts of data with minimal interference with the environment. Deep\nlearning models are currently used to automate classification of organisms into\ntaxonomic units. However, imprecision in these classifiers introduces a\nmeasurement noise that is difficult to control and can significantly hinder the\nanalysis and interpretation of data. {We overcome this limitation through\nensembles of Data-efficient image Transformers (DeiTs), which not only are easy\nto train and implement, but also significantly outperform} the previous state\nof the art (SOTA). We validate our results on ten ecological imaging datasets\nof diverse origin, ranging from plankton to birds. On all the datasets, we\nachieve a new SOTA, with a reduction of the error with respect to the previous\nSOTA ranging from 29.35% to 100.00%, and often achieving performances very\nclose to perfect classification. Ensembles of DeiTs perform better not because\nof superior single-model performances but rather due to smaller overlaps in the\npredictions by independent models and lower top-1 probabilities. This increases\nthe benefit of ensembling, especially when using geometric averages to combine\nindividual learners. While we only test our approach on biodiversity image\ndatasets, our approach is generic and can be applied to any kind of images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-03T14:16:22Z",
    "updated": "2022-09-29T12:15:31Z",
    "doi": "10.1038/s41598-022-21910-0"
  },
  "2111.10659": {
    "id": "http://arxiv.org/abs/2111.10659v2",
    "title": "Are Vision Transformers Robust to Patch Perturbations?",
    "authors": [
      "Jindong Gu",
      "Volker Tresp",
      "Yao Qin"
    ],
    "abstract": "  Recent advances in Vision Transformer (ViT) have demonstrated its impressive\nperformance in image classification, which makes it a promising alternative to\nConvolutional Neural Network (CNN). Unlike CNNs, ViT represents an input image\nas a sequence of image patches. The patch-based input image representation\nmakes the following question interesting: How does ViT perform when individual\ninput image patches are perturbed with natural corruptions or adversarial\nperturbations, compared to CNNs? In this work, we study the robustness of ViT\nto patch-wise perturbations. Surprisingly, we find that ViTs are more robust to\nnaturally corrupted patches than CNNs, whereas they are more vulnerable to\nadversarial patches. Furthermore, we discover that the attention mechanism\ngreatly affects the robustness of vision transformers. Specifically, the\nattention module can help improve the robustness of ViT by effectively ignoring\nnatural corrupted patches. However, when ViTs are attacked by an adversary, the\nattention mechanism can be easily fooled to focus more on the adversarially\nperturbed patches and cause a mistake. Based on our analysis, we propose a\nsimple temperature-scaling based method to improve the robustness of ViT\nagainst adversarial patches. Extensive qualitative and quantitative experiments\nare performed to support our findings, understanding, and improvement of ViT\nrobustness to patch-wise perturbations across a set of transformer-based\narchitectures.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-20T19:00:51Z",
    "updated": "2022-07-18T17:24:18Z",
    "doi": null
  },
  "2207.10022": {
    "id": "http://arxiv.org/abs/2207.10022v2",
    "title": "Secrets of Event-Based Optical Flow",
    "authors": [
      "Shintaro Shiba",
      "Yoshimitsu Aoki",
      "Guillermo Gallego"
    ],
    "abstract": "  Event cameras respond to scene dynamics and offer advantages to estimate\nmotion. Following recent image-based deep-learning achievements, optical flow\nestimation methods for event cameras have rushed to combine those image-based\nmethods with event data. However, it requires several adaptations (data\nconversion, loss function, etc.) as they have very different properties. We\ndevelop a principled method to extend the Contrast Maximization framework to\nestimate optical flow from events alone. We investigate key elements: how to\ndesign the objective function to prevent overfitting, how to warp events to\ndeal better with occlusions, and how to improve convergence with multi-scale\nraw events. With these key elements, our method ranks first among unsupervised\nmethods on the MVSEC benchmark, and is competitive on the DSEC benchmark.\nMoreover, our method allows us to expose the issues of the ground truth flow in\nthose benchmarks, and produces remarkable results when it is transferred to\nunsupervised learning settings. Our code is available at\nhttps://github.com/tub-rip/event_based_optical_flow\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-20T16:40:38Z",
    "updated": "2022-07-21T17:26:51Z",
    "doi": "10.1007/978-3-031-19797-0_36"
  },
  "2208.11257": {
    "id": "http://arxiv.org/abs/2208.11257v1",
    "title": "3D-FM GAN: Towards 3D-Controllable Face Manipulation",
    "authors": [
      "Yuchen Liu",
      "Zhixin Shu",
      "Yijun Li",
      "Zhe Lin",
      "Richard Zhang",
      "S. Y. Kung"
    ],
    "abstract": "  3D-controllable portrait synthesis has significantly advanced, thanks to\nbreakthroughs in generative adversarial networks (GANs). However, it is still\nchallenging to manipulate existing face images with precise 3D control. While\nconcatenating GAN inversion and a 3D-aware, noise-to-image GAN is a\nstraight-forward solution, it is inefficient and may lead to noticeable drop in\nediting quality. To fill this gap, we propose 3D-FM GAN, a novel conditional\nGAN framework designed specifically for 3D-controllable face manipulation, and\ndoes not require any tuning after the end-to-end learning phase. By carefully\nencoding both the input face image and a physically-based rendering of 3D edits\ninto a StyleGAN's latent spaces, our image generator provides high-quality,\nidentity-preserved, 3D-controllable face manipulation. To effectively learn\nsuch novel framework, we develop two essential training strategies and a novel\nmultiplicative co-modulation architecture that improves significantly upon\nnaive schemes. With extensive evaluations, we show that our method outperforms\nthe prior arts on various tasks, with better editability, stronger identity\npreservation, and higher photo-realism. In addition, we demonstrate a better\ngeneralizability of our design on large pose editing and out-of-domain images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-24T01:33:13Z",
    "updated": "2022-08-24T01:33:13Z",
    "doi": null
  },
  "1811.05868": {
    "id": "http://arxiv.org/abs/1811.05868v2",
    "title": "Pitfalls of Graph Neural Network Evaluation",
    "authors": [
      "Oleksandr Shchur",
      "Maximilian Mumme",
      "Aleksandar Bojchevski",
      "Stephan G\u00fcnnemann"
    ],
    "abstract": "  Semi-supervised node classification in graphs is a fundamental problem in\ngraph mining, and the recently proposed graph neural networks (GNNs) have\nachieved unparalleled results on this task. Due to their massive success, GNNs\nhave attracted a lot of attention, and many novel architectures have been put\nforward. In this paper we show that existing evaluation strategies for GNN\nmodels have serious shortcomings. We show that using the same\ntrain/validation/test splits of the same datasets, as well as making\nsignificant changes to the training procedure (e.g. early stopping criteria)\nprecludes a fair comparison of different architectures. We perform a thorough\nempirical evaluation of four prominent GNN models and show that considering\ndifferent splits of the data leads to dramatically different rankings of\nmodels. Even more importantly, our findings suggest that simpler GNN\narchitectures are able to outperform the more sophisticated ones if the\nhyperparameters and the training procedure are tuned fairly for all models.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-11-14T15:53:19Z",
    "updated": "2019-06-18T13:15:39Z",
    "doi": null
  },
  "2301.04272": {
    "id": "http://arxiv.org/abs/2301.04272v2",
    "title": "Data Distillation: A Survey",
    "authors": [
      "Noveen Sachdeva",
      "Julian McAuley"
    ],
    "abstract": "  The popularity of deep learning has led to the curation of a vast number of\nmassive and multifarious datasets. Despite having close-to-human performance on\nindividual tasks, training parameter-hungry models on large datasets poses\nmulti-faceted problems such as (a) high model-training time; (b) slow research\niteration; and (c) poor eco-sustainability. As an alternative, data\ndistillation approaches aim to synthesize terse data summaries, which can serve\nas effective drop-in replacements of the original dataset for scenarios like\nmodel training, inference, architecture search, etc. In this survey, we present\na formal framework for data distillation, along with providing a detailed\ntaxonomy of existing approaches. Additionally, we cover data distillation\napproaches for different data modalities, namely images, graphs, and user-item\ninteractions (recommender systems), while also identifying current challenges\nand future research directions.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-11T02:25:10Z",
    "updated": "2023-09-26T04:43:31Z",
    "doi": null
  },
  "2005.08144": {
    "id": "http://arxiv.org/abs/2005.08144v1",
    "title": "High-dimensional Convolutional Networks for Geometric Pattern\n  Recognition",
    "authors": [
      "Christopher Choy",
      "Junha Lee",
      "Rene Ranftl",
      "Jaesik Park",
      "Vladlen Koltun"
    ],
    "abstract": "  Many problems in science and engineering can be formulated in terms of\ngeometric patterns in high-dimensional spaces. We present high-dimensional\nconvolutional networks (ConvNets) for pattern recognition problems that arise\nin the context of geometric registration. We first study the effectiveness of\nconvolutional networks in detecting linear subspaces in high-dimensional spaces\nwith up to 32 dimensions: much higher dimensionality than prior applications of\nConvNets. We then apply high-dimensional ConvNets to 3D registration under\nrigid motions and image correspondence estimation. Experiments indicate that\nour high-dimensional ConvNets outperform prior approaches that relied on deep\nnetworks based on global pooling operators.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-05-17T01:46:12Z",
    "updated": "2020-05-17T01:46:12Z",
    "doi": null
  },
  "1602.02410": {
    "id": "http://arxiv.org/abs/1602.02410v2",
    "title": "Exploring the Limits of Language Modeling",
    "authors": [
      "Rafal Jozefowicz",
      "Oriol Vinyals",
      "Mike Schuster",
      "Noam Shazeer",
      "Yonghui Wu"
    ],
    "abstract": "  In this work we explore recent advances in Recurrent Neural Networks for\nlarge scale Language Modeling, a task central to language understanding. We\nextend current models to deal with two key challenges present in this task:\ncorpora and vocabulary sizes, and complex, long term structure of language. We\nperform an exhaustive study on techniques such as character Convolutional\nNeural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.\nOur best single model significantly improves state-of-the-art perplexity from\n51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),\nwhile an ensemble of models sets a new record by improving perplexity from 41.0\ndown to 23.7. We also release these models for the NLP and ML community to\nstudy and improve upon.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-02-07T19:11:17Z",
    "updated": "2016-02-11T23:01:48Z",
    "doi": null
  },
  "2204.07955": {
    "id": "http://arxiv.org/abs/2204.07955v2",
    "title": "Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment\n  Analysis",
    "authors": [
      "Yan Ling",
      "Jianfei Yu",
      "Rui Xia"
    ],
    "abstract": "  As an important task in sentiment analysis, Multimodal Aspect-Based Sentiment\nAnalysis (MABSA) has attracted increasing attention in recent years. However,\nprevious approaches either (i) use separately pre-trained visual and textual\nmodels, which ignore the crossmodal alignment or (ii) use vision-language\nmodels pre-trained with general pre-training tasks, which are inadequate to\nidentify finegrained aspects, opinions, and their alignments across modalities.\nTo tackle these limitations, we propose a task-specific Vision-Language\nPre-training framework for MABSA (VLPMABSA), which is a unified multimodal\nencoder-decoder architecture for all the pretraining and downstream tasks. We\nfurther design three types of task-specific pre-training tasks from the\nlanguage, vision, and multimodal modalities, respectively. Experimental results\nshow that our approach generally outperforms the state-of-the-art approaches on\nthree MABSA subtasks. Further analysis demonstrates the effectiveness of each\npretraining task. The source code is publicly released at\nhttps://github.com/NUSTM/VLP-MABSA.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-17T08:44:00Z",
    "updated": "2022-04-21T12:46:38Z",
    "doi": null
  },
  "2401.02142": {
    "id": "http://arxiv.org/abs/2401.02142v2",
    "title": "GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion\n  Generation",
    "authors": [
      "Xuehao Gao",
      "Yang Yang",
      "Zhenyu Xie",
      "Shaoyi Du",
      "Zhongqian Sun",
      "Yang Wu"
    ],
    "abstract": "  In this paper, we propose a novel cascaded diffusion-based generative\nframework for text-driven human motion synthesis, which exploits a strategy\nnamed GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy\nsets up generation objectives by grouping body joints of detailed skeletons in\nclose semantic proximity together and then replacing each of such joint group\nwith a single body-part node. Such an operation recursively abstracts a human\npose to coarser and coarser skeletons at multiple granularity levels. With\ngradually increasing the abstraction level, human motion becomes more and more\nconcise and stable, significantly benefiting the cross-modal motion synthesis\ntask. The whole text-driven human motion synthesis problem is then divided into\nmultiple abstraction levels and solved with a multi-stage generation framework\nwith a cascaded latent diffusion model: an initial generator first generates\nthe coarsest human motion guess from a given text description; then, a series\nof successive generators gradually enrich the motion details based on the\ntextual description and the previous synthesized results. Notably, we further\nintegrate GUESS with the proposed dynamic multi-condition fusion mechanism to\ndynamically balance the cooperative effects of the given textual condition and\nsynthesized coarse motion prompt in different generation stages. Extensive\nexperiments on large-scale datasets verify that GUESS outperforms existing\nstate-of-the-art methods by large margins in terms of accuracy, realisticness,\nand diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-04T08:48:21Z",
    "updated": "2024-01-06T03:17:55Z",
    "doi": null
  },
  "2311.16945": {
    "id": "http://arxiv.org/abs/2311.16945v2",
    "title": "UC-NeRF: Neural Radiance Field for Under-Calibrated Multi-view Cameras\n  in Autonomous Driving",
    "authors": [
      "Kai Cheng",
      "Xiaoxiao Long",
      "Wei Yin",
      "Jin Wang",
      "Zhiqiang Wu",
      "Yuexin Ma",
      "Kaixuan Wang",
      "Xiaozhi Chen",
      "Xuejin Chen"
    ],
    "abstract": "  Multi-camera setups find widespread use across various applications, such as\nautonomous driving, as they greatly expand sensing capabilities. Despite the\nfast development of Neural radiance field (NeRF) techniques and their wide\napplications in both indoor and outdoor scenes, applying NeRF to multi-camera\nsystems remains very challenging. This is primarily due to the inherent\nunder-calibration issues in multi-camera setup, including inconsistent imaging\neffects stemming from separately calibrated image signal processing units in\ndiverse cameras, and system errors arising from mechanical vibrations during\ndriving that affect relative camera poses. In this paper, we present UC-NeRF, a\nnovel method tailored for novel view synthesis in under-calibrated multi-view\ncamera systems. Firstly, we propose a layer-based color correction to rectify\nthe color inconsistency in different image regions. Second, we propose virtual\nwarping to generate more viewpoint-diverse but color-consistent virtual views\nfor color correction and 3D recovery. Finally, a spatiotemporally constrained\npose refinement is designed for more robust and accurate pose calibration in\nmulti-camera systems. Our method not only achieves state-of-the-art performance\nof novel view synthesis in multi-camera setups, but also effectively\nfacilitates depth estimation in large-scale outdoor scenes with the synthesized\nnovel views.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-28T16:47:59Z",
    "updated": "2023-12-11T03:17:13Z",
    "doi": null
  },
  "2205.09753": {
    "id": "http://arxiv.org/abs/2205.09753v2",
    "title": "HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory\n  Prediction via Scene Encoding",
    "authors": [
      "Xiaosong Jia",
      "Penghao Wu",
      "Li Chen",
      "Yu Liu",
      "Hongyang Li",
      "Junchi Yan"
    ],
    "abstract": "  Encoding a driving scene into vector representations has been an essential\ntask for autonomous driving that can benefit downstream tasks e.g. trajectory\nprediction. The driving scene often involves heterogeneous elements such as the\ndifferent types of objects (agents, lanes, traffic signs) and the semantic\nrelations between objects are rich and diverse. Meanwhile, there also exist\nrelativity across elements, which means that the spatial relation is a relative\nconcept and need be encoded in a ego-centric manner instead of in a global\ncoordinate system. Based on these observations, we propose Heterogeneous\nDriving Graph Transformer (HDGT), a backbone modelling the driving scene as a\nheterogeneous graph with different types of nodes and edges. For heterogeneous\ngraph construction, we connect different types of nodes according to diverse\nsemantic relations. For spatial relation encoding, the coordinates of the node\nas well as its in-edges are in the local node-centric coordinate system. For\nthe aggregation module in the graph neural network (GNN), we adopt the\ntransformer structure in a hierarchical way to fit the heterogeneous nature of\ninputs. Experimental results show that HDGT achieves state-of-the-art\nperformance for the task of trajectory prediction, on INTERACTION Prediction\nChallenge and Waymo Open Motion Challenge.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-30T07:08:30Z",
    "updated": "2023-07-20T08:41:46Z",
    "doi": null
  },
  "2307.00781": {
    "id": "http://arxiv.org/abs/2307.00781v1",
    "title": "ACDMSR: Accelerated Conditional Diffusion Models for Single Image\n  Super-Resolution",
    "authors": [
      "Axi Niu",
      "Pham Xuan Trung",
      "Kang Zhang",
      "Jinqiu Sun",
      "Yu Zhu",
      "In So Kweon",
      "Yanning Zhang"
    ],
    "abstract": "  Diffusion models have gained significant popularity in the field of\nimage-to-image translation. Previous efforts applying diffusion models to image\nsuper-resolution (SR) have demonstrated that iteratively refining pure Gaussian\nnoise using a U-Net architecture trained on denoising at various noise levels\ncan yield satisfactory high-resolution images from low-resolution inputs.\nHowever, this iterative refinement process comes with the drawback of low\ninference speed, which strongly limits its applications. To speed up inference\nand further enhance the performance, our research revisits diffusion models in\nimage super-resolution and proposes a straightforward yet significant diffusion\nmodel-based super-resolution method called ACDMSR (accelerated conditional\ndiffusion model for image super-resolution). Specifically, our method adapts\nthe standard diffusion model to perform super-resolution through a\ndeterministic iterative denoising process. Our study also highlights the\neffectiveness of using a pre-trained SR model to provide the conditional image\nof the given low-resolution (LR) image to achieve superior high-resolution\nresults. We demonstrate that our method surpasses previous attempts in\nqualitative and quantitative results through extensive experiments conducted on\nbenchmark datasets such as Set5, Set14, Urban100, BSD100, and Manga109.\nMoreover, our approach generates more visually realistic counterparts for\nlow-resolution images, emphasizing its effectiveness in practical scenarios.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-03T06:49:04Z",
    "updated": "2023-07-03T06:49:04Z",
    "doi": null
  },
  "2302.02257": {
    "id": "http://arxiv.org/abs/2302.02257v4",
    "title": "Multi-Source Diffusion Models for Simultaneous Music Generation and\n  Separation",
    "authors": [
      "Giorgio Mariani",
      "Irene Tallini",
      "Emilian Postolache",
      "Michele Mancusi",
      "Luca Cosmo",
      "Emanuele Rodol\u00e0"
    ],
    "abstract": "  In this work, we define a diffusion-based generative model capable of both\nmusic synthesis and source separation by learning the score of the joint\nprobability density of sources sharing a context. Alongside the classic total\ninference tasks (i.e., generating a mixture, separating the sources), we also\nintroduce and experiment on the partial generation task of source imputation,\nwhere we generate a subset of the sources given the others (e.g., play a piano\ntrack that goes well with the drums). Additionally, we introduce a novel\ninference method for the separation task based on Dirac likelihood functions.\nWe train our model on Slakh2100, a standard dataset for musical source\nseparation, provide qualitative results in the generation settings, and\nshowcase competitive quantitative results in the source separation setting. Our\nmethod is the first example of a single model that can handle both generation\nand separation tasks, thus representing a step toward general audio models.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-04T23:18:36Z",
    "updated": "2024-03-18T11:39:29Z",
    "doi": null
  },
  "2212.07075": {
    "id": "http://arxiv.org/abs/2212.07075v1",
    "title": "Cross-Modal Similarity-Based Curriculum Learning for Image Captioning",
    "authors": [
      "Hongkuan Zhang",
      "Saku Sugawara",
      "Akiko Aizawa",
      "Lei Zhou",
      "Ryohei Sasano",
      "Koichi Takeda"
    ],
    "abstract": "  Image captioning models require the high-level generalization ability to\ndescribe the contents of various images in words. Most existing approaches\ntreat the image-caption pairs equally in their training without considering the\ndifferences in their learning difficulties. Several image captioning approaches\nintroduce curriculum learning methods that present training data with\nincreasing levels of difficulty. However, their difficulty measurements are\neither based on domain-specific features or prior model training. In this\npaper, we propose a simple yet efficient difficulty measurement for image\ncaptioning using cross-modal similarity calculated by a pretrained\nvision-language model. Experiments on the COCO and Flickr30k datasets show that\nour proposed approach achieves superior performance and competitive convergence\nspeed to baselines without requiring heuristics or incurring additional\ntraining costs. Moreover, the higher model performance on difficult examples\nand unseen data also demonstrates the generalization ability.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-14T07:52:36Z",
    "updated": "2022-12-14T07:52:36Z",
    "doi": null
  },
  "2410.02681": {
    "id": "http://arxiv.org/abs/2410.02681v1",
    "title": "Understanding and Mitigating Miscalibration in Prompt Tuning for\n  Vision-Language Models",
    "authors": [
      "Shuoyuan Wang",
      "Yixuan Li",
      "Hongxin Wei"
    ],
    "abstract": "  Confidence calibration is critical for the safe deployment of machine\nlearning models in the real world. However, such issue in vision-language\nmodels like CLIP, particularly after fine-tuning, has not been fully addressed.\nIn this work, we demonstrate that existing prompt tuning methods usually lead\nto a trade-off of calibration between base and new classes: the cross-entropy\nloss in CoOp causes overconfidence in new classes by increasing textual label\ndivergence, whereas the regularization of KgCoOp maintains the confidence level\nbut results in underconfidence in base classes due to the improved accuracy.\nInspired by the observations, we introduce Dynamic Outlier Regularization (DOR)\nto ensure the confidence calibration on both base and new classes after\nfine-tuning. In particular, we propose to minimize the feature deviation of\nnovel textual labels (instead of base classes) sampled from a large vocabulary.\nIn effect, DOR prevents the increase in textual divergence for new labels while\neasing restrictions on base classes. Extensive experiments demonstrate that DOR\ncan enhance the calibration performance of current fine-tuning methods on base\nand new classes.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-10-03T17:06:21Z",
    "updated": "2024-10-03T17:06:21Z",
    "doi": null
  },
  "2109.04144": {
    "id": "http://arxiv.org/abs/2109.04144v1",
    "title": "Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning",
    "authors": [
      "Prasetya Ajie Utama",
      "Nafise Sadat Moosavi",
      "Victor Sanh",
      "Iryna Gurevych"
    ],
    "abstract": "  Recent prompt-based approaches allow pretrained language models to achieve\nstrong performances on few-shot finetuning by reformulating downstream tasks as\na language modeling problem. In this work, we demonstrate that, despite its\nadvantages on low data regimes, finetuned prompt-based models for sentence pair\nclassification tasks still suffer from a common pitfall of adopting inference\nheuristics based on lexical overlap, e.g., models incorrectly assuming a\nsentence pair is of the same meaning because they consist of the same set of\nwords. Interestingly, we find that this particular inference heuristic is\nsignificantly less present in the zero-shot evaluation of the prompt-based\nmodel, indicating how finetuning can be destructive to useful knowledge learned\nduring the pretraining. We then show that adding a regularization that\npreserves pretraining weights is effective in mitigating this destructive\ntendency of few-shot finetuning. Our evaluation on three datasets demonstrates\npromising improvements on the three corresponding challenge datasets used to\ndiagnose the inference heuristics.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-09T10:10:29Z",
    "updated": "2021-09-09T10:10:29Z",
    "doi": null
  },
  "1609.09475": {
    "id": "http://arxiv.org/abs/1609.09475v3",
    "title": "Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the\n  Amazon Picking Challenge",
    "authors": [
      "Andy Zeng",
      "Kuan-Ting Yu",
      "Shuran Song",
      "Daniel Suo",
      "Ed Walker Jr.",
      "Alberto Rodriguez",
      "Jianxiong Xiao"
    ],
    "abstract": "  Robot warehouse automation has attracted significant interest in recent\nyears, perhaps most visibly in the Amazon Picking Challenge (APC). A fully\nautonomous warehouse pick-and-place system requires robust vision that reliably\nrecognizes and locates objects amid cluttered environments, self-occlusions,\nsensor noise, and a large variety of objects. In this paper we present an\napproach that leverages multi-view RGB-D data and self-supervised, data-driven\nlearning to overcome those difficulties. The approach was part of the\nMIT-Princeton Team system that took 3rd- and 4th- place in the stowing and\npicking tasks, respectively at APC 2016. In the proposed approach, we segment\nand label multiple views of a scene with a fully convolutional neural network,\nand then fit pre-scanned 3D object models to the resulting segmentation to get\nthe 6D object pose. Training a deep neural network for segmentation typically\nrequires a large amount of training data. We propose a self-supervised method\nto generate a large labeled dataset without tedious manual segmentation. We\ndemonstrate that our system can reliably estimate the 6D pose of objects under\na variety of scenarios. All code, data, and benchmarks are available at\nhttp://apc.cs.princeton.edu/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-09-29T19:39:13Z",
    "updated": "2017-05-07T20:12:55Z",
    "doi": null
  },
  "2006.04768": {
    "id": "http://arxiv.org/abs/2006.04768v3",
    "title": "Linformer: Self-Attention with Linear Complexity",
    "authors": [
      "Sinong Wang",
      "Belinda Z. Li",
      "Madian Khabsa",
      "Han Fang",
      "Hao Ma"
    ],
    "abstract": "  Large transformer models have shown extraordinary success in achieving\nstate-of-the-art results in many natural language processing applications.\nHowever, training and deploying these models can be prohibitively costly for\nlong sequences, as the standard self-attention mechanism of the Transformer\nuses $O(n^2)$ time and space with respect to sequence length. In this paper, we\ndemonstrate that the self-attention mechanism can be approximated by a low-rank\nmatrix. We further exploit this finding to propose a new self-attention\nmechanism, which reduces the overall self-attention complexity from $O(n^2)$ to\n$O(n)$ in both time and space. The resulting linear transformer, the\n\\textit{Linformer}, performs on par with standard Transformer models, while\nbeing much more memory- and time-efficient.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-06-08T17:37:52Z",
    "updated": "2020-06-14T08:15:54Z",
    "doi": null
  },
  "2305.05189": {
    "id": "http://arxiv.org/abs/2305.05189v4",
    "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with\n  Large Language Models",
    "authors": [
      "Shanshan Zhong",
      "Zhongzhan Huang",
      "Wushao Wen",
      "Jinghui Qin",
      "Liang Lin"
    ],
    "abstract": "  Diffusion models, which have emerged to become popular text-to-image\ngeneration models, can produce high-quality and content-rich images guided by\ntextual prompts. However, there are limitations to semantic understanding and\ncommonsense reasoning in existing models when the input prompts are concise\nnarrative, resulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-efficient\nfine-tuning approach called the Semantic Understanding and Reasoning adapter\n(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first\ncollect and annotate a new dataset SURD which consists of more than 57,000\nsemantically corrected multi-modal samples. Each sample contains a simple\nnarrative prompt, a complex keyword-based prompt, and a high-quality image.\nThen, we align the semantic representation of narrative prompts to the complex\nprompts and transfer knowledge of large language models (LLMs) to our\nSUR-adapter via knowledge distillation so that it can acquire the powerful\nsemantic understanding and reasoning capabilities to build a high-quality\ntextual semantic representation for text-to-image generation. We conduct\nexperiments by integrating multiple LLMs and popular pre-trained diffusion\nmodels to show the effectiveness of our approach in enabling diffusion models\nto understand and reason concise natural language without image quality\ndegradation. Our approach can make text-to-image diffusion models easier to use\nwith better user experience, which demonstrates our approach has the potential\nfor further advancing the development of user-friendly text-to-image generation\nmodels by bridging the semantic gap between simple narrative prompts and\ncomplex keyword-based prompts. The code is released at\nhttps://github.com/Qrange-group/SUR-adapter.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-09T05:48:38Z",
    "updated": "2023-11-29T08:18:14Z",
    "doi": null
  },
  "2209.13816": {
    "id": "http://arxiv.org/abs/2209.13816v3",
    "title": "Revisiting Few-Shot Learning from a Causal Perspective",
    "authors": [
      "Guoliang Lin",
      "Yongheng Xu",
      "Hanjiang Lai",
      "Jian Yin"
    ],
    "abstract": "  Few-shot learning with $N$-way $K$-shot scheme is an open challenge in\nmachine learning. Many metric-based approaches have been proposed to tackle\nthis problem, e.g., the Matching Networks and CLIP-Adapter. Despite that these\napproaches have shown significant progress, the mechanism of why these methods\nsucceed has not been well explored. In this paper, we try to interpret these\nmetric-based few-shot learning methods via causal mechanism. We show that the\nexisting approaches can be viewed as specific forms of front-door adjustment,\nwhich can alleviate the effect of spurious correlations and thus learn the\ncausality. This causal interpretation could provide us a new perspective to\nbetter understand these existing metric-based methods. Further, based on this\ncausal interpretation, we simply introduce two causal methods for metric-based\nfew-shot learning, which considers not only the relationship between examples\nbut also the diversity of representations. Experimental results demonstrate the\nsuperiority of our proposed methods in few-shot classification on various\nbenchmark datasets. Code is available in\nhttps://github.com/lingl1024/causalFewShot.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-28T03:46:02Z",
    "updated": "2024-05-07T02:27:42Z",
    "doi": null
  },
  "2111.11591": {
    "id": "http://arxiv.org/abs/2111.11591v2",
    "title": "Efficient Video Transformers with Spatial-Temporal Token Selection",
    "authors": [
      "Junke Wang",
      "Xitong Yang",
      "Hengduo Li",
      "Li Liu",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "  Video transformers have achieved impressive results on major video\nrecognition benchmarks, which however suffer from high computational cost. In\nthis paper, we present STTS, a token selection framework that dynamically\nselects a few informative tokens in both temporal and spatial dimensions\nconditioned on input video samples. Specifically, we formulate token selection\nas a ranking problem, which estimates the importance of each token through a\nlightweight scorer network and only those with top scores will be used for\ndownstream evaluation. In the temporal dimension, we keep the frames that are\nmost relevant to the action categories, while in the spatial dimension, we\nidentify the most discriminative region in feature maps without affecting the\nspatial context used in a hierarchical way in most video transformers. Since\nthe decision of token selection is non-differentiable, we employ a\nperturbed-maximum based differentiable Top-K operator for end-to-end training.\nWe mainly conduct extensive experiments on Kinetics-400 with a recently\nintroduced video transformer backbone, MViT. Our framework achieves similar\nresults while requiring 20% less computation. We also demonstrate our approach\nis generic for different transformer architectures and video datasets. Code is\navailable at https://github.com/wangjk666/STTS.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-23T00:35:58Z",
    "updated": "2022-07-16T09:15:15Z",
    "doi": null
  },
  "2401.03630": {
    "id": "http://arxiv.org/abs/2401.03630v2",
    "title": "Why Solving Multi-agent Path Finding with Large Language Model has not\n  Succeeded Yet",
    "authors": [
      "Weizhe Chen",
      "Sven Koenig",
      "Bistra Dilkina"
    ],
    "abstract": "  With the explosive influence caused by the success of large language models\n(LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work\nshowing that foundation models can be used to solve a large variety of tasks.\nHowever, there is very limited work that shares insights on multi-agent\nplanning. Multi-agent planning is different from other domains by combining the\ndifficulty of multi-agent coordination and planning, and making it hard to\nleverage external tools to facilitate the reasoning needed. In this paper, we\nfocus on the problem of multi-agent path finding (MAPF), which is also known as\nmulti-robot route planning, and study the performance of solving MAPF with\nLLMs. We first show the motivating success on an empty room map without\nobstacles, then the failure to plan on the harder room map and maze map of the\nstandard MAPF benchmark. We present our position on why directly solving MAPF\nwith LLMs has not been successful yet, and we use various experiments to\nsupport our hypothesis. Based on our results, we discussed how researchers with\ndifferent backgrounds could help with this problem from different perspectives.\n",
    "categories": [
      {
        "@term": "cs.MA",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-08T02:22:04Z",
    "updated": "2024-02-09T17:48:19Z",
    "doi": null
  },
  "2209.06040": {
    "id": "http://arxiv.org/abs/2209.06040v1",
    "title": "DMTNet: Dynamic Multi-scale Network for Dual-pixel Images Defocus\n  Deblurring with Transformer",
    "authors": [
      "Dafeng Zhang",
      "Xiaobing Wang"
    ],
    "abstract": "  Recent works achieve excellent results in defocus deblurring task based on\ndual-pixel data using convolutional neural network (CNN), while the scarcity of\ndata limits the exploration and attempt of vision transformer in this task. In\naddition, the existing works use fixed parameters and network architecture to\ndeblur images with different distribution and content information, which also\naffects the generalization ability of the model. In this paper, we propose a\ndynamic multi-scale network, named DMTNet, for dual-pixel images defocus\ndeblurring. DMTNet mainly contains two modules: feature extraction module and\nreconstruction module. The feature extraction module is composed of several\nvision transformer blocks, which uses its powerful feature extraction\ncapability to obtain richer features and improve the robustness of the model.\nThe reconstruction module is composed of several Dynamic Multi-scale\nSub-reconstruction Module (DMSSRM). DMSSRM can restore images by adaptively\nassigning weights to features from different scales according to the blur\ndistribution and content information of the input images. DMTNet combines the\nadvantages of transformer and CNN, in which the vision transformer improves the\nperformance ceiling of CNN, and the inductive bias of CNN enables transformer\nto extract more robust features without relying on a large amount of data.\nDMTNet might be the first attempt to use vision transformer to restore the\nblurring images to clarity. By combining with CNN, the vision transformer may\nachieve better performance on small datasets. Experimental results on the\npopular benchmarks demonstrate that our DMTNet significantly outperforms\nstate-of-the-art methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-13T14:47:09Z",
    "updated": "2022-09-13T14:47:09Z",
    "doi": "10.1109/ICME52920.2022.9859631"
  },
  "2111.12707": {
    "id": "http://arxiv.org/abs/2111.12707v4",
    "title": "MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation",
    "authors": [
      "Wenhao Li",
      "Hong Liu",
      "Hao Tang",
      "Pichao Wang",
      "Luc Van Gool"
    ],
    "abstract": "  Estimating 3D human poses from monocular videos is a challenging task due to\ndepth ambiguity and self-occlusion. Most existing works attempt to solve both\nissues by exploiting spatial and temporal relationships. However, those works\nignore the fact that it is an inverse problem where multiple feasible solutions\n(i.e., hypotheses) exist. To relieve this limitation, we propose a\nMulti-Hypothesis Transformer (MHFormer) that learns spatio-temporal\nrepresentations of multiple plausible pose hypotheses. In order to effectively\nmodel multi-hypothesis dependencies and build strong relationships across\nhypothesis features, the task is decomposed into three stages: (i) Generate\nmultiple initial hypothesis representations; (ii) Model self-hypothesis\ncommunication, merge multiple hypotheses into a single converged representation\nand then partition it into several diverged hypotheses; (iii) Learn\ncross-hypothesis communication and aggregate the multi-hypothesis features to\nsynthesize the final 3D pose. Through the above processes, the final\nrepresentation is enhanced and the synthesized pose is much more accurate.\nExtensive experiments show that MHFormer achieves state-of-the-art results on\ntwo challenging datasets: Human3.6M and MPI-INF-3DHP. Without bells and\nwhistles, its performance surpasses the previous best result by a large margin\nof 3% on Human3.6M. Code and models are available at\n\\url{https://github.com/Vegetebird/MHFormer}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-24T18:59:02Z",
    "updated": "2022-06-28T15:02:02Z",
    "doi": null
  },
  "2403.02460": {
    "id": "http://arxiv.org/abs/2403.02460v4",
    "title": "MagicClay: Sculpting Meshes With Generative Neural Fields",
    "authors": [
      "Amir Barda",
      "Vladimir G. Kim",
      "Noam Aigerman",
      "Amit H. Bermano",
      "Thibault Groueix"
    ],
    "abstract": "  The recent developments in neural fields have brought phenomenal capabilities\nto the field of shape generation, but they lack crucial properties, such as\nincremental control - a fundamental requirement for artistic work. Triangular\nmeshes, on the other hand, are the representation of choice for most geometry\nrelated tasks, offering efficiency and intuitive control, but do not lend\nthemselves to neural optimization. To support downstream tasks, previous art\ntypically proposes a two-step approach, where first a shape is generated using\nneural fields, and then a mesh is extracted for further processing. Instead, in\nthis paper we introduce a hybrid approach that maintains both a mesh and a\nSigned Distance Field (SDF) representations consistently. Using this\nrepresentation, we introduce MagicClay - an artist friendly tool for sculpting\nregions of a mesh according to textual prompts while keeping other regions\nuntouched. Our framework carefully and efficiently balances consistency between\nthe representations and regularizations in every step of the shape\noptimization; Relying on the mesh representation, we show how to render the SDF\nat higher resolutions and faster. In addition, we employ recent work in\ndifferentiable mesh reconstruction to adaptively allocate triangles in the mesh\nwhere required, as indicated by the SDF. Using an implemented prototype, we\ndemonstrate superior generated geometry compared to the state-of-the-art, and\nnovel consistent control, allowing sequential prompt-based edits to the same\nmesh for the first time.\n",
    "categories": [
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-04T20:20:14Z",
    "updated": "2024-10-09T17:52:09Z",
    "doi": null
  },
  "2302.03540": {
    "id": "http://arxiv.org/abs/2302.03540v1",
    "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal\n  Supervision",
    "authors": [
      "Eugene Kharitonov",
      "Damien Vincent",
      "Zal\u00e1n Borsos",
      "Rapha\u00ebl Marinier",
      "Sertan Girgin",
      "Olivier Pietquin",
      "Matt Sharifi",
      "Marco Tagliasacchi",
      "Neil Zeghidour"
    ],
    "abstract": "  We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can\nbe trained with minimal supervision. By combining two types of discrete speech\nrepresentations, we cast TTS as a composition of two sequence-to-sequence\ntasks: from text to high-level semantic tokens (akin to \"reading\") and from\nsemantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two\ntasks enables training of the \"speaking\" module using abundant audio-only data,\nand unlocks the highly efficient combination of pretraining and backtranslation\nto reduce the need for parallel data when training the \"reading\" component. To\ncontrol the speaker identity, we adopt example prompting, which allows\nSPEAR-TTS to generalize to unseen speakers using only a short sample of 3\nseconds, without any explicit speaker representation or speaker-id labels. Our\nexperiments demonstrate that SPEAR-TTS achieves a character error rate that is\ncompetitive with state-of-the-art methods using only 15 minutes of parallel\ndata, while matching ground-truth speech in terms of naturalness and acoustic\nquality, as measured in subjective tests.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-07T15:48:31Z",
    "updated": "2023-02-07T15:48:31Z",
    "doi": null
  },
  "2001.08361": {
    "id": "http://arxiv.org/abs/2001.08361v1",
    "title": "Scaling Laws for Neural Language Models",
    "authors": [
      "Jared Kaplan",
      "Sam McCandlish",
      "Tom Henighan",
      "Tom B. Brown",
      "Benjamin Chess",
      "Rewon Child",
      "Scott Gray",
      "Alec Radford",
      "Jeffrey Wu",
      "Dario Amodei"
    ],
    "abstract": "  We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-01-23T03:59:20Z",
    "updated": "2020-01-23T03:59:20Z",
    "doi": null
  },
  "2105.06070": {
    "id": "http://arxiv.org/abs/2105.06070v1",
    "title": "GAN Prior Embedded Network for Blind Face Restoration in the Wild",
    "authors": [
      "Tao Yang",
      "Peiran Ren",
      "Xuansong Xie",
      "Lei Zhang"
    ],
    "abstract": "  Blind face restoration (BFR) from severely degraded face images in the wild\nis a very challenging problem. Due to the high illness of the problem and the\ncomplex unknown degradation, directly training a deep neural network (DNN)\nusually cannot lead to acceptable results. Existing generative adversarial\nnetwork (GAN) based methods can produce better results but tend to generate\nover-smoothed restorations. In this work, we propose a new method by first\nlearning a GAN for high-quality face image generation and embedding it into a\nU-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN\nwith a set of synthesized low-quality face images. The GAN blocks are designed\nto ensure that the latent code and noise input to the GAN can be respectively\ngenerated from the deep and shallow features of the DNN, controlling the global\nface structure, local face details and background of the reconstructed image.\nThe proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can\ngenerate visually photo-realistic results. Our experiments demonstrated that\nthe proposed GPEN achieves significantly superior results to state-of-the-art\nBFR methods both quantitatively and qualitatively, especially for the\nrestoration of severely degraded face images in the wild. The source code and\nmodels can be found at https://github.com/yangxy/GPEN.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-05-13T04:14:00Z",
    "updated": "2021-05-13T04:14:00Z",
    "doi": null
  },
  "2312.13308": {
    "id": "http://arxiv.org/abs/2312.13308v2",
    "title": "SWinGS: Sliding Windows for Dynamic 3D Gaussian Splatting",
    "authors": [
      "Richard Shaw",
      "Michal Nazarczuk",
      "Jifei Song",
      "Arthur Moreau",
      "Sibi Catley-Chandar",
      "Helisa Dhamo",
      "Eduardo Perez-Pellitero"
    ],
    "abstract": "  Novel view synthesis has shown rapid progress recently, with methods capable\nof producing increasingly photorealistic results. 3D Gaussian Splatting has\nemerged as a promising method, producing high-quality renderings of scenes and\nenabling interactive viewing at real-time frame rates. However, it is limited\nto static scenes. In this work, we extend 3D Gaussian Splatting to reconstruct\ndynamic scenes. We model a scene's dynamics using dynamic MLPs, learning\ndeformations from temporally-local canonical representations to per-frame 3D\nGaussians. To disentangle static and dynamic regions, tuneable parameters weigh\neach Gaussian's respective MLP parameters, improving the dynamics modelling of\nimbalanced scenes. We introduce a sliding window training strategy that\npartitions the sequence into smaller manageable windows to handle arbitrary\nlength scenes while maintaining high rendering quality. We propose an adaptive\nsampling strategy to determine appropriate window size hyperparameters based on\nthe scene's motion, balancing training overhead with visual quality. Training a\nseparate dynamic 3D Gaussian model for each sliding window allows the canonical\nrepresentation to change, enabling the reconstruction of scenes with\nsignificant geometric changes. Temporal consistency is enforced using a\nfine-tuning step with self-supervising consistency loss on randomly sampled\nnovel views. As a result, our method produces high-quality renderings of\ngeneral dynamic scenes with competitive quantitative performance, which can be\nviewed in real-time in our dynamic interactive viewer.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-20T03:54:03Z",
    "updated": "2024-07-18T10:18:51Z",
    "doi": null
  },
  "2404.02101": {
    "id": "http://arxiv.org/abs/2404.02101v1",
    "title": "CameraCtrl: Enabling Camera Control for Text-to-Video Generation",
    "authors": [
      "Hao He",
      "Yinghao Xu",
      "Yuwei Guo",
      "Gordon Wetzstein",
      "Bo Dai",
      "Hongsheng Li",
      "Ceyuan Yang"
    ],
    "abstract": "  Controllability plays a crucial role in video generation since it allows\nusers to create desired content. However, existing models largely overlooked\nthe precise control of camera pose that serves as a cinematic language to\nexpress deeper narrative nuances. To alleviate this issue, we introduce\nCameraCtrl, enabling accurate camera pose control for text-to-video(T2V)\nmodels. After precisely parameterizing the camera trajectory, a plug-and-play\ncamera module is then trained on a T2V model, leaving others untouched.\nAdditionally, a comprehensive study on the effect of various datasets is also\nconducted, suggesting that videos with diverse camera distribution and similar\nappearances indeed enhance controllability and generalization. Experimental\nresults demonstrate the effectiveness of CameraCtrl in achieving precise and\ndomain-adaptive camera control, marking a step forward in the pursuit of\ndynamic and customized video storytelling from textual and camera pose inputs.\nOur project website is at: https://hehao13.github.io/projects-CameraCtrl/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-02T16:52:41Z",
    "updated": "2024-04-02T16:52:41Z",
    "doi": null
  },
  "2202.08455": {
    "id": "http://arxiv.org/abs/2202.08455v1",
    "title": "Transformer for Graphs: An Overview from Architecture Perspective",
    "authors": [
      "Erxue Min",
      "Runfa Chen",
      "Yatao Bian",
      "Tingyang Xu",
      "Kangfei Zhao",
      "Wenbing Huang",
      "Peilin Zhao",
      "Junzhou Huang",
      "Sophia Ananiadou",
      "Yu Rong"
    ],
    "abstract": "  Recently, Transformer model, which has achieved great success in many\nartificial intelligence fields, has demonstrated its great potential in\nmodeling graph-structured data. Till now, a great variety of Transformers has\nbeen proposed to adapt to the graph-structured data. However, a comprehensive\nliterature review and systematical evaluation of these Transformer variants for\ngraphs are still unavailable. It's imperative to sort out the existing\nTransformer models for graphs and systematically investigate their\neffectiveness on various graph tasks. In this survey, we provide a\ncomprehensive review of various Graph Transformer models from the architectural\ndesign perspective. We first disassemble the existing models and conclude three\ntypical ways to incorporate the graph information into the vanilla Transformer:\n1) GNNs as Auxiliary Modules, 2) Improved Positional Embedding from Graphs, and\n3) Improved Attention Matrix from Graphs. Furthermore, we implement the\nrepresentative components in three groups and conduct a comprehensive\ncomparison on various kinds of famous graph data benchmarks to investigate the\nreal performance gain of each component. Our experiments confirm the benefits\nof current graph-specific modules on Transformer and reveal their advantages on\ndifferent kinds of graph tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-02-17T06:02:06Z",
    "updated": "2022-02-17T06:02:06Z",
    "doi": null
  },
  "2212.09100": {
    "id": "http://arxiv.org/abs/2212.09100v3",
    "title": "SPARF: Large-Scale Learning of 3D Sparse Radiance Fields from Few Input\n  Images",
    "authors": [
      "Abdullah Hamdi",
      "Bernard Ghanem",
      "Matthias Nie\u00dfner"
    ],
    "abstract": "  Recent advances in Neural Radiance Fields (NeRFs) treat the problem of novel\nview synthesis as Sparse Radiance Field (SRF) optimization using sparse voxels\nfor efficient and fast rendering (plenoxels,InstantNGP). In order to leverage\nmachine learning and adoption of SRFs as a 3D representation, we present SPARF,\na large-scale ShapeNet-based synthetic dataset for novel view synthesis\nconsisting of $\\sim$ 17 million images rendered from nearly 40,000 shapes at\nhigh resolution (400 X 400 pixels). The dataset is orders of magnitude larger\nthan existing synthetic datasets for novel view synthesis and includes more\nthan one million 3D-optimized radiance fields with multiple voxel resolutions.\nFurthermore, we propose a novel pipeline (SuRFNet) that learns to generate\nsparse voxel radiance fields from only few views. This is done by using the\ndensely collected SPARF dataset and 3D sparse convolutions. SuRFNet employs\npartial SRFs from few/one images and a specialized SRF loss to learn to\ngenerate high-quality sparse voxel radiance fields that can be rendered from\nnovel views. Our approach achieves state-of-the-art results in the task of\nunconstrained novel view synthesis based on few views on ShapeNet as compared\nto recent baselines. The SPARF dataset is made public with the code and models\non the project website https://abdullahamdi.com/sparf/ .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68T45",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-18T14:56:22Z",
    "updated": "2023-08-21T12:53:09Z",
    "doi": "10.1109/ICCVW60793.2023.00315"
  },
  "2406.02549": {
    "id": "http://arxiv.org/abs/2406.02549v1",
    "title": "Dreamguider: Improved Training free Diffusion-based Conditional\n  Generation",
    "authors": [
      "Nithin Gopalakrishnan Nair",
      "Vishal M Patel"
    ],
    "abstract": "  Diffusion models have emerged as a formidable tool for training-free\nconditional generation.However, a key hurdle in inference-time guidance\ntechniques is the need for compute-heavy backpropagation through the diffusion\nnetwork for estimating the guidance direction. Moreover, these techniques often\nrequire handcrafted parameter tuning on a case-by-case basis. Although some\nrecent works have introduced minimal compute methods for linear inverse\nproblems, a generic lightweight guidance solution to both linear and non-linear\nguidance problems is still missing. To this end, we propose Dreamguider, a\nmethod that enables inference-time guidance without compute-heavy\nbackpropagation through the diffusion network. The key idea is to regulate the\ngradient flow through a time-varying factor. Moreover, we propose an empirical\nguidance scale that works for a wide variety of tasks, hence removing the need\nfor handcrafted parameter tuning. We further introduce an effective lightweight\naugmentation strategy that significantly boosts the performance during\ninference-time guidance. We present experiments using Dreamguider on multiple\ntasks across multiple datasets and models to show the effectiveness of the\nproposed modules. To facilitate further research, we will make the code public\nafter the review process.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-04T17:59:32Z",
    "updated": "2024-06-04T17:59:32Z",
    "doi": null
  },
  "1308.3052": {
    "id": "http://arxiv.org/abs/1308.3052v2",
    "title": "Gradient Magnitude Similarity Deviation: A Highly Efficient Perceptual\n  Image Quality Index",
    "authors": [
      "Wufeng Xue",
      "Lei Zhang",
      "Xuanqin Mou",
      "Alan C. Bovik"
    ],
    "abstract": "  It is an important task to faithfully evaluate the perceptual quality of\noutput images in many applications such as image compression, image restoration\nand multimedia streaming. A good image quality assessment (IQA) model should\nnot only deliver high quality prediction accuracy but also be computationally\nefficient. The efficiency of IQA metrics is becoming particularly important due\nto the increasing proliferation of high-volume visual data in high-speed\nnetworks. We present a new effective and efficient IQA model, called gradient\nmagnitude similarity deviation (GMSD). The image gradients are sensitive to\nimage distortions, while different local structures in a distorted image suffer\ndifferent degrees of degradations. This motivates us to explore the use of\nglobal variation of gradient based local quality map for overall image quality\nprediction. We find that the pixel-wise gradient magnitude similarity (GMS)\nbetween the reference and distorted images combined with a novel pooling\nstrategy the standard deviation of the GMS map can predict accurately\nperceptual image quality. The resulting GMSD algorithm is much faster than most\nstate-of-the-art IQA methods, and delivers highly competitive prediction\naccuracy.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2013-08-14T07:25:10Z",
    "updated": "2013-11-26T03:55:01Z",
    "doi": "10.1109/TIP.2013.2293423"
  },
  "2309.11715": {
    "id": "http://arxiv.org/abs/2309.11715v3",
    "title": "Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow\n  removal",
    "authors": [
      "Xiao Feng Zhang",
      "Tian Yi Song",
      "Jia Wei Yao"
    ],
    "abstract": "  Segment Anything (SAM), an advanced universal image segmentation model\ntrained on an expansive visual dataset, has set a new benchmark in image\nsegmentation and computer vision. However, it faced challenges when it came to\ndistinguishing between shadows and their backgrounds. To address this, we\ndeveloped Deshadow-Anything, considering the generalization of large-scale\ndatasets, and we performed Fine-tuning on large-scale datasets to achieve image\nshadow removal. The diffusion model can diffuse along the edges and textures of\nan image, helping to remove shadows while preserving the details of the image.\nFurthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input\nperturbation (DDPM-AIP) to accelerate the iterative training speed of\ndiffusion. Experiments on shadow removal tasks demonstrate that these methods\ncan effectively improve image restoration performance.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-21T01:35:13Z",
    "updated": "2024-01-03T02:01:09Z",
    "doi": null
  },
  "2408.00211": {
    "id": "http://arxiv.org/abs/2408.00211v1",
    "title": "Penzai + Treescope: A Toolkit for Interpreting, Visualizing, and Editing\n  Models As Data",
    "authors": [
      "Daniel D. Johnson"
    ],
    "abstract": "  Much of today's machine learning research involves interpreting, modifying or\nvisualizing models after they are trained. I present Penzai, a neural network\nlibrary designed to simplify model manipulation by representing models as\nsimple data structures, and Treescope, an interactive pretty-printer and array\nvisualizer that can visualize both model inputs/outputs and the models\nthemselves. Penzai models are built using declarative combinators that expose\nthe model forward pass in the structure of the model object itself, and use\nnamed axes to ensure each operation is semantically meaningful. With Penzai's\ntree-editing selector system, users can both insert and replace model\ncomponents, allowing them to intervene on intermediate values or make other\nedits to the model structure. Users can then get immediate feedback by\nvisualizing the modified model with Treescope. I describe the motivation and\nmain features of Penzai and Treescope, and discuss how treating the model as\ndata enables a variety of analyses and interventions to be implemented as\ndata-structure transformations, without requiring model designers to add\nexplicit hooks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-01T00:45:37Z",
    "updated": "2024-08-01T00:45:37Z",
    "doi": null
  },
  "2210.14862": {
    "id": "http://arxiv.org/abs/2210.14862v2",
    "title": "Visual Semantic Parsing: From Images to Abstract Meaning Representation",
    "authors": [
      "Mohamed Ashraf Abdelsalam",
      "Zhan Shi",
      "Federico Fancellu",
      "Kalliopi Basioti",
      "Dhaivat J. Bhatt",
      "Vladimir Pavlovic",
      "Afsaneh Fazly"
    ],
    "abstract": "  The success of scene graphs for visual scene understanding has brought\nattention to the benefits of abstracting a visual input (e.g., image) into a\nstructured representation, where entities (people and objects) are nodes\nconnected by edges specifying their relations. Building these representations,\nhowever, requires expensive manual annotation in the form of images paired with\ntheir scene graphs or frames. These formalisms remain limited in the nature of\nentities and relations they can capture. In this paper, we propose to leverage\na widely-used meaning representation in the field of natural language\nprocessing, the Abstract Meaning Representation (AMR), to address these\nshortcomings. Compared to scene graphs, which largely emphasize spatial\nrelationships, our visual AMR graphs are more linguistically informed, with a\nfocus on higher-level semantic concepts extrapolated from visual input.\nMoreover, they allow us to generate meta-AMR graphs to unify information\ncontained in multiple image descriptions under one representation. Through\nextensive experimentation and analysis, we demonstrate that we can re-purpose\nan existing text-to-AMR parser to parse images into AMRs. Our findings point to\nimportant future research directions for improved scene understanding.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-26T17:06:42Z",
    "updated": "2022-10-27T15:54:50Z",
    "doi": null
  },
  "2211.01095": {
    "id": "http://arxiv.org/abs/2211.01095v2",
    "title": "DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic\n  Models",
    "authors": [
      "Cheng Lu",
      "Yuhao Zhou",
      "Fan Bao",
      "Jianfei Chen",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "abstract": "  Diffusion probabilistic models (DPMs) have achieved impressive success in\nhigh-resolution image synthesis, especially in recent large-scale text-to-image\ngeneration applications. An essential technique for improving the sample\nquality of DPMs is guided sampling, which usually needs a large guidance scale\nto obtain the best sample quality. The commonly-used fast sampler for guided\nsampling is DDIM, a first-order diffusion ODE solver that generally needs 100\nto 250 steps for high-quality samples. Although recent works propose dedicated\nhigh-order solvers and achieve a further speedup for sampling without guidance,\ntheir effectiveness for guided sampling has not been well-tested before. In\nthis work, we demonstrate that previous high-order fast samplers suffer from\ninstability issues, and they even become slower than DDIM when the guidance\nscale grows large. To further speed up guided sampling, we propose\nDPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++\nsolves the diffusion ODE with the data prediction model and adopts thresholding\nmethods to keep the solution matches training data distribution. We further\npropose a multistep variant of DPM-Solver++ to address the instability issue by\nreducing the effective step size. Experiments show that DPM-Solver++ can\ngenerate high-quality samples within only 15 to 20 steps for guided sampling by\npixel-space and latent-space DPMs.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-02T13:14:30Z",
    "updated": "2023-05-06T17:15:37Z",
    "doi": null
  },
  "2403.09413": {
    "id": "http://arxiv.org/abs/2403.09413v2",
    "title": "Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting",
    "authors": [
      "Jaewoo Jung",
      "Jisang Han",
      "Honggyu An",
      "Jiwon Kang",
      "Seonghoon Park",
      "Seungryong Kim"
    ],
    "abstract": "  3D Gaussian splatting (3DGS) has recently demonstrated impressive\ncapabilities in real-time novel view synthesis and 3D reconstruction. However,\n3DGS heavily depends on the accurate initialization derived from\nStructure-from-Motion (SfM) methods. When the quality of the initial point\ncloud deteriorates, such as in the presence of noise or when using randomly\ninitialized point cloud, 3DGS often undergoes large performance drops. To\naddress this limitation, we propose a novel optimization strategy dubbed\nRAIN-GS (Relaing Accurate Initialization Constraint for 3D Gaussian Splatting).\nOur approach is based on an in-depth analysis of the original 3DGS optimization\nscheme and the analysis of the SfM initialization in the frequency domain.\nLeveraging simple modifications based on our analyses, RAIN-GS successfully\ntrains 3D Gaussians from sub-optimal point cloud (e.g., randomly initialized\npoint cloud), effectively relaxing the need for accurate initialization. We\ndemonstrate the efficacy of our strategy through quantitative and qualitative\ncomparisons on multiple datasets, where RAIN-GS trained with random point cloud\nachieves performance on-par with or even better than 3DGS trained with accurate\nSfM point cloud. Our project page and code can be found at\nhttps://ku-cvlab.github.io/RAIN-GS.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-14T14:04:21Z",
    "updated": "2024-05-28T14:14:16Z",
    "doi": null
  },
  "2406.05000": {
    "id": "http://arxiv.org/abs/2406.05000v1",
    "title": "AttnDreamBooth: Towards Text-Aligned Personalized Text-to-Image\n  Generation",
    "authors": [
      "Lianyu Pang",
      "Jian Yin",
      "Baoquan Zhao",
      "Feize Wu",
      "Fu Lee Wang",
      "Qing Li",
      "Xudong Mao"
    ],
    "abstract": "  Recent advances in text-to-image models have enabled high-quality\npersonalized image synthesis of user-provided concepts with flexible textual\ncontrol. In this work, we analyze the limitations of two primary techniques in\ntext-to-image personalization: Textual Inversion and DreamBooth. When\nintegrating the learned concept into new prompts, Textual Inversion tends to\noverfit the concept, while DreamBooth often overlooks it. We attribute these\nissues to the incorrect learning of the embedding alignment for the concept. We\nintroduce AttnDreamBooth, a novel approach that addresses these issues by\nseparately learning the embedding alignment, the attention map, and the subject\nidentity in different training stages. We also introduce a cross-attention map\nregularization term to enhance the learning of the attention map. Our method\ndemonstrates significant improvements in identity preservation and text\nalignment compared to the baseline methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-07T15:12:26Z",
    "updated": "2024-06-07T15:12:26Z",
    "doi": null
  },
  "1805.10180": {
    "id": "http://arxiv.org/abs/1805.10180v3",
    "title": "Pyramid Attention Network for Semantic Segmentation",
    "authors": [
      "Hanchao Li",
      "Pengfei Xiong",
      "Jie An",
      "Lingxue Wang"
    ],
    "abstract": "  A Pyramid Attention Network(PAN) is proposed to exploit the impact of global\ncontextual information in semantic segmentation. Different from most existing\nworks, we combine attention mechanism and spatial pyramid to extract precise\ndense features for pixel labeling instead of complicated dilated convolution\nand artificially designed decoder networks. Specifically, we introduce a\nFeature Pyramid Attention module to perform spatial pyramid attention structure\non high-level output and combining global pooling to learn a better feature\nrepresentation, and a Global Attention Upsample module on each decoder layer to\nprovide global context as a guidance of low-level features to select category\nlocalization details. The proposed approach achieves state-of-the-art\nperformance on PASCAL VOC 2012 and Cityscapes benchmarks with a new record of\nmIoU accuracy 84.0% on PASCAL VOC 2012, while training without COCO dataset.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-05-25T14:40:14Z",
    "updated": "2018-11-25T11:46:47Z",
    "doi": null
  },
  "2204.05562": {
    "id": "http://arxiv.org/abs/2204.05562v5",
    "title": "FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient\n  Package for Federated Graph Learning",
    "authors": [
      "Zhen Wang",
      "Weirui Kuang",
      "Yuexiang Xie",
      "Liuyi Yao",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "abstract": "  The incredible development of federated learning (FL) has benefited various\ntasks in the domains of computer vision and natural language processing, and\nthe existing frameworks such as TFF and FATE has made the deployment easy in\nreal-world applications. However, federated graph learning (FGL), even though\ngraph data are prevalent, has not been well supported due to its unique\ncharacteristics and requirements. The lack of FGL-related framework increases\nthe efforts for accomplishing reproducible research and deploying in real-world\napplications. Motivated by such strong demand, in this paper, we first discuss\nthe challenges in creating an easy-to-use FGL package and accordingly present\nour implemented package FederatedScope-GNN (FS-G), which provides (1) a unified\nview for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo\nand ModelZoo for out-of-the-box FGL capability; (3) an efficient model\nauto-tuning component; and (4) off-the-shelf privacy attack and defense\nabilities. We validate the effectiveness of FS-G by conducting extensive\nexperiments, which simultaneously gains many valuable insights about FGL for\nthe community. Moreover, we employ FS-G to serve the FGL application in\nreal-world E-commerce scenarios, where the attained improvements indicate great\npotential business benefits. We publicly release FS-G, as submodules of\nFederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL's\nresearch and enable broad applications that would otherwise be infeasible due\nto the lack of a dedicated package.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-12T06:48:06Z",
    "updated": "2022-08-01T15:50:24Z",
    "doi": null
  },
  "2312.12729": {
    "id": "http://arxiv.org/abs/2312.12729v1",
    "title": "Segment Anything Model Meets Image Harmonization",
    "authors": [
      "Haoxing Chen",
      "Yaohui Li",
      "Zhangxuan Gu",
      "Zhuoer Xu",
      "Jun Lan",
      "Huaxiong Li"
    ],
    "abstract": "  Image harmonization is a crucial technique in image composition that aims to\nseamlessly match the background by adjusting the foreground of composite\nimages. Current methods adopt either global-level or pixel-level feature\nmatching. Global-level feature matching ignores the proximity prior, treating\nforeground and background as separate entities. On the other hand, pixel-level\nfeature matching loses contextual information. Therefore, it is necessary to\nuse the information from semantic maps that describe different objects to guide\nharmonization. In this paper, we propose Semantic-guided Region-aware Instance\nNormalization (SRIN) that can utilize the semantic segmentation maps output by\na pre-trained Segment Anything Model (SAM) to guide the visual consistency\nlearning of foreground and background features. Abundant experiments\ndemonstrate the superiority of our method for image harmonization over\nstate-of-the-art methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-20T02:57:21Z",
    "updated": "2023-12-20T02:57:21Z",
    "doi": null
  },
  "2402.00631": {
    "id": "http://arxiv.org/abs/2402.00631v2",
    "title": "Beyond Inserting: Learning Identity Embedding for Semantic-Fidelity\n  Personalized Diffusion Generation",
    "authors": [
      "Yang Li",
      "Songlin Yang",
      "Wei Wang",
      "Jing Dong"
    ],
    "abstract": "  Advanced diffusion-based Text-to-Image (T2I) models, such as the Stable\nDiffusion Model, have made significant progress in generating diverse and\nhigh-quality images using text prompts alone. However, when non-famous users\nrequire personalized image generation for their identities (IDs), the T2I\nmodels fail to accurately generate their ID-related images. The main problem is\nthat pre-trained T2I models do not learn the mapping between the new ID prompts\nand their corresponding visual content. The previous methods either failed to\naccurately fit the face region or lost the interactive generative ability with\nother existing concepts in T2I models. In other words, they are unable to\ngenerate T2I-aligned and semantic-fidelity images for the given prompts with\nother concepts such as scenes (``Eiffel Tower''), actions (``holding a\nbasketball''), and facial attributes (``eyes closed''). In this paper, we focus\non inserting accurate and interactive ID embedding into the Stable Diffusion\nModel for semantic-fidelity personalized generation. We address this challenge\nfrom two perspectives: face-wise region fitting and semantic-fidelity token\noptimization. Specifically, we first visualize the attention overfit problem\nand propose a face-wise attention loss to fit the face region instead of\nentangling ID-unrelated information, such as face layout and background. This\nkey trick significantly enhances the ID accuracy and interactive generative\nability with other existing concepts. Then, we optimize one ID representation\nas multiple per-stage tokens where each token contains two disentangled\nfeatures. This expansion of the textual conditioning space improves\nsemantic-fidelity control. Extensive experiments validate that our results\nexhibit superior ID accuracy, text-based manipulation ability, and\ngeneralization compared to previous methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-31T11:52:33Z",
    "updated": "2024-03-22T14:00:37Z",
    "doi": null
  },
  "2205.02151": {
    "id": "http://arxiv.org/abs/2205.02151v1",
    "title": "Dual Cross-Attention Learning for Fine-Grained Visual Categorization and\n  Object Re-Identification",
    "authors": [
      "Haowei Zhu",
      "Wenjing Ke",
      "Dong Li",
      "Ji Liu",
      "Lu Tian",
      "Yi Shan"
    ],
    "abstract": "  Recently, self-attention mechanisms have shown impressive performance in\nvarious NLP and CV tasks, which can help capture sequential characteristics and\nderive global information. In this work, we explore how to extend\nself-attention modules to better learn subtle feature embeddings for\nrecognizing fine-grained objects, e.g., different bird species or person\nidentities. To this end, we propose a dual cross-attention learning (DCAL)\nalgorithm to coordinate with self-attention learning. First, we propose\nglobal-local cross-attention (GLCA) to enhance the interactions between global\nimages and local high-response regions, which can help reinforce the\nspatial-wise discriminative clues for recognition. Second, we propose pair-wise\ncross-attention (PWCA) to establish the interactions between image pairs. PWCA\ncan regularize the attention learning of an image by treating another image as\ndistractor and will be removed during inference. We observe that DCAL can\nreduce misleading attentions and diffuse the attention response to discover\nmore complementary parts for recognition. We conduct extensive evaluations on\nfine-grained visual categorization and object re-identification. Experiments\ndemonstrate that DCAL performs on par with state-of-the-art methods and\nconsistently improves multiple self-attention baselines, e.g., surpassing\nDeiT-Tiny and ViT-Base by 2.8% and 2.4% mAP on MSMT17, respectively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-04T16:14:26Z",
    "updated": "2022-05-04T16:14:26Z",
    "doi": null
  },
  "2407.04604": {
    "id": "http://arxiv.org/abs/2407.04604v2",
    "title": "PartCraft: Crafting Creative Objects by Parts",
    "authors": [
      "Kam Woh Ng",
      "Xiatian Zhu",
      "Yi-Zhe Song",
      "Tao Xiang"
    ],
    "abstract": "  This paper propels creative control in generative visual AI by allowing users\nto \"select\". Departing from traditional text or sketch-based methods, we for\nthe first time allow users to choose visual concepts by parts for their\ncreative endeavors. The outcome is fine-grained generation that precisely\ncaptures selected visual concepts, ensuring a holistically faithful and\nplausible result. To achieve this, we first parse objects into parts through\nunsupervised feature clustering. Then, we encode parts into text tokens and\nintroduce an entropy-based normalized attention loss that operates on them.\nThis loss design enables our model to learn generic prior topology knowledge\nabout object's part composition, and further generalize to novel part\ncompositions to ensure the generation looks holistically faithful. Lastly, we\nemploy a bottleneck encoder to project the part tokens. This not only enhances\nfidelity but also accelerates learning, by leveraging shared knowledge and\nfacilitating information exchange among instances. Visual results in the paper\nand supplementary material showcase the compelling power of PartCraft in\ncrafting highly customized, innovative creations, exemplified by the \"charming\"\nand creative birds. Code is released at https://github.com/kamwoh/partcraft.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-05T15:53:04Z",
    "updated": "2024-07-08T13:38:49Z",
    "doi": null
  },
  "2408.14506": {
    "id": "http://arxiv.org/abs/2408.14506v1",
    "title": "Distilling Long-tailed Datasets",
    "authors": [
      "Zhenghao Zhao",
      "Haoxuan Wang",
      "Yuzhang Shang",
      "Kai Wang",
      "Yan Yan"
    ],
    "abstract": "  Dataset distillation (DD) aims to distill a small, information-rich dataset\nfrom a larger one for efficient neural network training. However, existing DD\nmethods struggle with long-tailed datasets, which are prevalent in real-world\nscenarios. By investigating the reasons behind this unexpected result, we\nidentified two main causes: 1) Expert networks trained on imbalanced data\ndevelop biased gradients, leading to the synthesis of similarly imbalanced\ndistilled datasets. Parameter matching, a common technique in DD, involves\naligning the learning parameters of the distilled dataset with that of the\noriginal dataset. However, in the context of long-tailed datasets, matching\nbiased experts leads to inheriting the imbalance present in the original data,\ncausing the distilled dataset to inadequately represent tail classes. 2) The\nexperts trained on such datasets perform suboptimally on tail classes,\nresulting in misguided distillation supervision and poor-quality soft-label\ninitialization. To address these issues, we propose a novel long-tailed dataset\ndistillation method, Long-tailed Aware Dataset distillation (LAD).\nSpecifically, we propose Weight Mismatch Avoidance to avoid directly matching\nthe biased expert trajectories. It reduces the distance between the student and\nthe biased expert trajectories and prevents the tail class bias from being\ndistilled to the synthetic dataset. Moreover, we propose Adaptive Decoupled\nMatching, which jointly matches the decoupled backbone and classifier to\nimprove the tail class performance and initialize reliable soft labels. This\nwork pioneers the field of long-tailed dataset distillation (LTDD), marking the\nfirst effective effort to distill long-tailed datasets.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-24T15:36:36Z",
    "updated": "2024-08-24T15:36:36Z",
    "doi": null
  },
  "2305.06077": {
    "id": "http://arxiv.org/abs/2305.06077v2",
    "title": "Relightify: Relightable 3D Faces from a Single Image via Diffusion\n  Models",
    "authors": [
      "Foivos Paraperas Papantoniou",
      "Alexandros Lattas",
      "Stylianos Moschoglou",
      "Stefanos Zafeiriou"
    ],
    "abstract": "  Following the remarkable success of diffusion models on image generation,\nrecent works have also demonstrated their impressive ability to address a\nnumber of inverse problems in an unsupervised way, by properly constraining the\nsampling process based on a conditioning input. Motivated by this, in this\npaper, we present the first approach to use diffusion models as a prior for\nhighly accurate 3D facial BRDF reconstruction from a single image. We start by\nleveraging a high-quality UV dataset of facial reflectance (diffuse and\nspecular albedo and normals), which we render under varying illumination\nsettings to simulate natural RGB textures and, then, train an unconditional\ndiffusion model on concatenated pairs of rendered textures and reflectance\ncomponents. At test time, we fit a 3D morphable model to the given image and\nunwrap the face in a partial UV texture. By sampling from the diffusion model,\nwhile retaining the observed texture part intact, the model inpaints not only\nthe self-occluded areas but also the unknown reflectance components, in a\nsingle sequence of denoising steps. In contrast to existing methods, we\ndirectly acquire the observed texture from the input image, thus, resulting in\nmore faithful and consistent reflectance estimation. Through a series of\nqualitative and quantitative comparisons, we demonstrate superior performance\nin both texture completion as well as reflectance reconstruction tasks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-10T11:57:49Z",
    "updated": "2023-08-22T01:06:42Z",
    "doi": null
  },
  "1804.07007": {
    "id": "http://arxiv.org/abs/1804.07007v3",
    "title": "QuaSE: Accurate Text Style Transfer under Quantifiable Guidance",
    "authors": [
      "Yi Liao",
      "Lidong Bing",
      "Piji Li",
      "Shuming Shi",
      "Wai Lam",
      "Tong Zhang"
    ],
    "abstract": "  We propose the task of Quantifiable Sequence Editing (QuaSE): editing an\ninput sequence to generate an output sequence that satisfies a given numerical\noutcome value measuring a certain property of the sequence, with the\nrequirement of keeping the main content of the input sequence. For example, an\ninput sequence could be a word sequence, such as review sentence and\nadvertisement text. For a review sentence, the outcome could be the review\nrating; for an advertisement, the outcome could be the click-through rate. The\nmajor challenge in performing QuaSE is how to perceive the outcome-related\nwordings, and only edit them to change the outcome. In this paper, the proposed\nframework contains two latent factors, namely, outcome factor and content\nfactor, disentangled from the input sentence to allow convenient editing to\nchange the outcome and keep the content. Our framework explores the\npseudo-parallel sentences by modeling their content similarity and outcome\ndifferences to enable a better disentanglement of the latent factors, which\nallows generating an output to better satisfy the desired outcome and keep the\ncontent. The dual reconstruction structure further enhances the capability of\ngenerating expected output by exploiting the couplings of latent factors of\npseudo-parallel sentences. For evaluation, we prepared a dataset of Yelp review\nsentences with the ratings as outcome. Extensive experimental results are\nreported and discussed to elaborate the peculiarities of our framework.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-19T06:13:48Z",
    "updated": "2019-01-30T05:14:32Z",
    "doi": null
  },
  "2406.01014": {
    "id": "http://arxiv.org/abs/2406.01014v1",
    "title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective\n  Navigation via Multi-Agent Collaboration",
    "authors": [
      "Junyang Wang",
      "Haiyang Xu",
      "Haitao Jia",
      "Xi Zhang",
      "Ming Yan",
      "Weizhou Shen",
      "Ji Zhang",
      "Fei Huang",
      "Jitao Sang"
    ],
    "abstract": "  Mobile device operation tasks are increasingly becoming a popular multi-modal\nAI application scenario. Current Multi-modal Large Language Models (MLLMs),\nconstrained by their training data, lack the capability to function effectively\nas operation assistants. Instead, MLLM-based agents, which enhance capabilities\nthrough tool invocation, are gradually being applied to this scenario. However,\nthe two major navigation challenges in mobile device operation tasks, task\nprogress navigation and focus content navigation, are significantly complicated\nunder the single-agent architecture of existing work. This is due to the overly\nlong token sequences and the interleaved text-image data format, which limit\nperformance. To address these navigation challenges effectively, we propose\nMobile-Agent-v2, a multi-agent architecture for mobile device operation\nassistance. The architecture comprises three agents: planning agent, decision\nagent, and reflection agent. The planning agent generates task progress, making\nthe navigation of history operations more efficient. To retain focus content,\nwe design a memory unit that updates with task progress. Additionally, to\ncorrect erroneous operations, the reflection agent observes the outcomes of\neach operation and handles any mistakes accordingly. Experimental results\nindicate that Mobile-Agent-v2 achieves over a 30% improvement in task\ncompletion compared to the single-agent architecture of Mobile-Agent. The code\nis open-sourced at https://github.com/X-PLUG/MobileAgent.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-03T05:50:00Z",
    "updated": "2024-06-03T05:50:00Z",
    "doi": null
  },
  "1705.08741": {
    "id": "http://arxiv.org/abs/1705.08741v2",
    "title": "Train longer, generalize better: closing the generalization gap in large\n  batch training of neural networks",
    "authors": [
      "Elad Hoffer",
      "Itay Hubara",
      "Daniel Soudry"
    ],
    "abstract": "  Background: Deep learning models are typically trained using stochastic\ngradient descent or one of its variants. These methods update the weights using\ntheir gradient, estimated from a small fraction of the training data. It has\nbeen observed that when using large batch sizes there is a persistent\ndegradation in generalization performance - known as the \"generalization gap\"\nphenomena. Identifying the origin of this gap and closing it had remained an\nopen problem.\n  Contributions: We examine the initial high learning rate training phase. We\nfind that the weight distance from its initialization grows logarithmically\nwith the number of weight updates. We therefore propose a \"random walk on\nrandom landscape\" statistical model which is known to exhibit similar\n\"ultra-slow\" diffusion behavior. Following this hypothesis we conducted\nexperiments to show empirically that the \"generalization gap\" stems from the\nrelatively small number of updates rather than the batch size, and can be\ncompletely eliminated by adapting the training regime used. We further\ninvestigate different techniques to train models in the large-batch regime and\npresent a novel algorithm named \"Ghost Batch Normalization\" which enables\nsignificant decrease in the generalization gap without increasing the number of\nupdates. To validate our findings we conduct several additional experiments on\nMNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices\nand beliefs concerning training of deep models and suggest they may not be\noptimal to achieve good generalization.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-05-24T13:17:27Z",
    "updated": "2018-01-01T08:49:43Z",
    "doi": null
  },
  "1905.05460": {
    "id": "http://arxiv.org/abs/1905.05460v2",
    "title": "Cognitive Graph for Multi-Hop Reading Comprehension at Scale",
    "authors": [
      "Ming Ding",
      "Chang Zhou",
      "Qibin Chen",
      "Hongxia Yang",
      "Jie Tang"
    ],
    "abstract": "  We propose a new CogQA framework for multi-hop question answering in\nweb-scale documents. Inspired by the dual process theory in cognitive science,\nthe framework gradually builds a \\textit{cognitive graph} in an iterative\nprocess by coordinating an implicit extraction module (System 1) and an\nexplicit reasoning module (System 2). While giving accurate answers, our\nframework further provides explainable reasoning paths. Specifically, our\nimplementation based on BERT and graph neural network efficiently handles\nmillions of documents for multi-hop reasoning questions in the HotpotQA\nfullwiki dataset, achieving a winning joint $F_1$ score of 34.9 on the\nleaderboard, compared to 23.6 of the best competitor.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-05-14T08:55:04Z",
    "updated": "2019-06-04T09:44:29Z",
    "doi": null
  },
  "2404.14777": {
    "id": "http://arxiv.org/abs/2404.14777v2",
    "title": "ClinicalAgent: Clinical Trial Multi-Agent System with Large Language\n  Model-based Reasoning",
    "authors": [
      "Ling Yue",
      "Sixue Xing",
      "Jintai Chen",
      "Tianfan Fu"
    ],
    "abstract": "  Large Language Models (LLMs) and multi-agent systems have shown impressive\ncapabilities in natural language tasks but face challenges in clinical trial\napplications, primarily due to limited access to external knowledge.\nRecognizing the potential of advanced clinical trial tools that aggregate and\npredict based on the latest medical data, we propose an integrated solution to\nenhance their accessibility and utility. We introduce Clinical Agent System\n(ClinicalAgent), a clinical multi-agent system designed for clinical trial\ntasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct\nreasoning technology. This integration not only boosts LLM performance in\nclinical contexts but also introduces novel functionalities. The proposed\nmethod achieves competitive predictive performance in clinical trial outcome\nprediction (0.7908 PR-AUC), obtaining a 0.3326 improvement over the standard\nprompt Method. Publicly available code can be found at\nhttps://anonymous.4open.science/r/ClinicalAgent-6671.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-23T06:30:53Z",
    "updated": "2024-07-20T07:52:16Z",
    "doi": null
  },
  "2404.07990": {
    "id": "http://arxiv.org/abs/2404.07990v2",
    "title": "OpenBias: Open-set Bias Detection in Text-to-Image Generative Models",
    "authors": [
      "Moreno D'Inc\u00e0",
      "Elia Peruzzo",
      "Massimiliano Mancini",
      "Dejia Xu",
      "Vidit Goel",
      "Xingqian Xu",
      "Zhangyang Wang",
      "Humphrey Shi",
      "Nicu Sebe"
    ],
    "abstract": "  Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-11T17:59:56Z",
    "updated": "2024-08-05T12:55:47Z",
    "doi": null
  },
  "2301.07315": {
    "id": "http://arxiv.org/abs/2301.07315v1",
    "title": "Face Recognition in the age of CLIP & Billion image datasets",
    "authors": [
      "Aaditya Bhat",
      "Shrey Jain"
    ],
    "abstract": "  CLIP (Contrastive Language-Image Pre-training) models developed by OpenAI\nhave achieved outstanding results on various image recognition and retrieval\ntasks, displaying strong zero-shot performance. This means that they are able\nto perform effectively on tasks for which they have not been explicitly\ntrained. Inspired by the success of OpenAI CLIP, a new publicly available\ndataset called LAION-5B was collected which resulted in the development of open\nViT-H/14, ViT-G/14 models that outperform the OpenAI L/14 model. The LAION-5B\ndataset also released an approximate nearest neighbor index, with a web\ninterface for search & subset creation.\n  In this paper, we evaluate the performance of various CLIP models as\nzero-shot face recognizers. Our findings show that CLIP models perform well on\nface recognition tasks, but increasing the size of the CLIP model does not\nnecessarily lead to improved accuracy. Additionally, we investigate the\nrobustness of CLIP models against data poisoning attacks by testing their\nperformance on poisoned data. Through this analysis, we aim to understand the\npotential consequences and misuse of search engines built using CLIP models,\nwhich could potentially function as unintentional face recognition engines.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-18T05:34:57Z",
    "updated": "2023-01-18T05:34:57Z",
    "doi": null
  },
  "1804.04235": {
    "id": "http://arxiv.org/abs/1804.04235v1",
    "title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
    "authors": [
      "Noam Shazeer",
      "Mitchell Stern"
    ],
    "abstract": "  In several recently proposed stochastic optimization methods (e.g. RMSProp,\nAdam, Adadelta), parameter updates are scaled by the inverse square roots of\nexponential moving averages of squared past gradients. Maintaining these\nper-parameter second-moment estimators requires memory equal to the number of\nparameters. For the case of neural network weight matrices, we propose\nmaintaining only the per-row and per-column sums of these moving averages, and\nestimating the per-parameter second moments based on these sums. We demonstrate\nempirically that this method produces similar results to the baseline.\nSecondly, we show that adaptive methods can produce larger-than-desired updates\nwhen the decay rate of the second moment accumulator is too slow. We propose\nupdate clipping and a gradually increasing decay rate scheme as remedies.\nCombining these methods and dropping momentum, we achieve comparable results to\nthe published Adam regime in training the Transformer model on the WMT 2014\nEnglish-German machine translation task, while using very little auxiliary\nstorage in the optimizer. Finally, we propose scaling the parameter updates\nbased on the scale of the parameters themselves.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-11T21:42:32Z",
    "updated": "2018-04-11T21:42:32Z",
    "doi": null
  },
  "2212.07597": {
    "id": "http://arxiv.org/abs/2212.07597v1",
    "title": "Triangulating Python Performance Issues with Scalene",
    "authors": [
      "Emery D. Berger",
      "Sam Stern",
      "Juan Altmayer Pizzorno"
    ],
    "abstract": "  This paper proposes Scalene, a profiler specialized for Python. Scalene\ncombines a suite of innovations to precisely and simultaneously profile CPU,\nmemory, and GPU usage, all with low overhead. Scalene's CPU and memory\nprofilers help Python programmers direct their optimization efforts by\ndistinguishing between inefficient Python and efficient native execution time\nand memory usage. Scalene's memory profiler employs a novel sampling algorithm\nthat lets it operate with low overhead yet high precision. It also incorporates\na novel algorithm that automatically pinpoints memory leaks, whether within\nPython or across the Python-native boundary. Scalene tracks a new metric called\ncopy volume, which highlights costly copying operations that can occur when\nPython silently converts between C and Python data representations, or between\nCPU and GPU. Since its introduction, Scalene has been widely adopted, with over\n500,000 downloads to date. We present experience reports from developers who\nused Scalene to achieve significant performance improvements and memory\nsavings.\n",
    "categories": [
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PF",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-15T02:56:25Z",
    "updated": "2022-12-15T02:56:25Z",
    "doi": null
  },
  "2403.10242": {
    "id": "http://arxiv.org/abs/2403.10242v1",
    "title": "FDGaussian: Fast Gaussian Splatting from Single Image via\n  Geometric-aware Diffusion Model",
    "authors": [
      "Qijun Feng",
      "Zhen Xing",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "abstract": "  Reconstructing detailed 3D objects from single-view images remains a\nchallenging task due to the limited information available. In this paper, we\nintroduce FDGaussian, a novel two-stage framework for single-image 3D\nreconstruction. Recent methods typically utilize pre-trained 2D diffusion\nmodels to generate plausible novel views from the input image, yet they\nencounter issues with either multi-view inconsistency or lack of geometric\nfidelity. To overcome these challenges, we propose an orthogonal plane\ndecomposition mechanism to extract 3D geometric features from the 2D input,\nenabling the generation of consistent multi-view images. Moreover, we further\naccelerate the state-of-the-art Gaussian Splatting incorporating epipolar\nattention to fuse images from different viewpoints. We demonstrate that\nFDGaussian generates images with high consistency across different views and\nreconstructs high-quality 3D objects, both qualitatively and quantitatively.\nMore examples can be found at our website https://qjfeng.net/FDGaussian/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-15T12:24:36Z",
    "updated": "2024-03-15T12:24:36Z",
    "doi": null
  },
  "2002.06525": {
    "id": "http://arxiv.org/abs/2002.06525v1",
    "title": "Learning to Generate Multiple Style Transfer Outputs for an Input\n  Sentence",
    "authors": [
      "Kevin Lin",
      "Ming-Yu Liu",
      "Ming-Ting Sun",
      "Jan Kautz"
    ],
    "abstract": "  Text style transfer refers to the task of rephrasing a given text in a\ndifferent style. While various methods have been proposed to advance the state\nof the art, they often assume the transfer output follows a delta distribution,\nand thus their models cannot generate different style transfer results for a\ngiven input text. To address the limitation, we propose a one-to-many text\nstyle transfer framework. In contrast to prior works that learn a one-to-one\nmapping that converts an input sentence to one output sentence, our approach\nlearns a one-to-many mapping that can convert an input sentence to multiple\ndifferent output sentences, while preserving the input content. This is\nachieved by applying adversarial training with a latent decomposition scheme.\nSpecifically, we decompose the latent representation of the input sentence to a\nstyle code that captures the language style variation and a content code that\nencodes the language style-independent content. We then combine the content\ncode with the style code for generating a style transfer output. By combining\nthe same content code with a different style code, we generate a different\nstyle transfer output. Extensive experimental results with comparisons to\nseveral text style transfer approaches on multiple public datasets using a\ndiverse set of performance metrics validate effectiveness of the proposed\napproach.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-02-16T07:10:45Z",
    "updated": "2020-02-16T07:10:45Z",
    "doi": null
  },
  "2210.04287": {
    "id": "http://arxiv.org/abs/2210.04287v1",
    "title": "Learning to Decompose Visual Features with Latent Textual Prompts",
    "authors": [
      "Feng Wang",
      "Manling Li",
      "Xudong Lin",
      "Hairong Lv",
      "Alexander G. Schwing",
      "Heng Ji"
    ],
    "abstract": "  Recent advances in pre-training vision-language models like CLIP have shown\ngreat potential in learning transferable visual representations. Nonetheless,\nfor downstream inference, CLIP-like models suffer from either 1) degraded\naccuracy and robustness in the case of inaccurate text descriptions during\nretrieval-based inference (the challenge for zero-shot protocol); or 2)\nbreaking the well-established vision-language alignment (the challenge for\nlinear probing). To address them, we propose Decomposed Feature Prompting\n(DeFo). DeFo leverages a flexible number of learnable embeddings as textual\ninput while maintaining the vision-language dual-model architecture, which\nenables the model to learn decomposed visual features with the help of\nfeature-level textual prompts. We further use an additional linear layer to\nperform classification, allowing a scalable size of language inputs. Our\nempirical study shows DeFo's significance in improving the vision-language\nmodels. For example, DeFo obtains 73.2% test accuracy on ImageNet with a\nResNet-50 backbone without tuning any pretrained weights of both the vision and\nlanguage encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and\noutperforming state-of-the-art vision-language prompt tuning method by 7.6%.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-09T15:40:13Z",
    "updated": "2022-10-09T15:40:13Z",
    "doi": null
  },
  "2303.16491": {
    "id": "http://arxiv.org/abs/2303.16491v2",
    "title": "Implicit Diffusion Models for Continuous Super-Resolution",
    "authors": [
      "Sicheng Gao",
      "Xuhui Liu",
      "Bohan Zeng",
      "Sheng Xu",
      "Yanjing Li",
      "Xiaoyan Luo",
      "Jianzhuang Liu",
      "Xiantong Zhen",
      "Baochang Zhang"
    ],
    "abstract": "  Image super-resolution (SR) has attracted increasing attention due to its\nwide applications. However, current SR methods generally suffer from\nover-smoothing and artifacts, and most work only with fixed magnifications.\nThis paper introduces an Implicit Diffusion Model (IDM) for high-fidelity\ncontinuous image super-resolution. IDM integrates an implicit neural\nrepresentation and a denoising diffusion model in a unified end-to-end\nframework, where the implicit neural representation is adopted in the decoding\nprocess to learn continuous-resolution representation. Furthermore, we design a\nscale-controllable conditioning mechanism that consists of a low-resolution\n(LR) conditioning network and a scaling factor. The scaling factor regulates\nthe resolution and accordingly modulates the proportion of the LR information\nand generated features in the final output, which enables the model to\naccommodate the continuous-resolution requirement. Extensive experiments\nvalidate the effectiveness of our IDM and demonstrate its superior performance\nover prior arts.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-29T07:02:20Z",
    "updated": "2023-09-03T10:19:55Z",
    "doi": null
  },
  "1807.10267": {
    "id": "http://arxiv.org/abs/1807.10267v3",
    "title": "Generating 3D faces using Convolutional Mesh Autoencoders",
    "authors": [
      "Anurag Ranjan",
      "Timo Bolkart",
      "Soubhik Sanyal",
      "Michael J. Black"
    ],
    "abstract": "  Learned 3D representations of human faces are useful for computer vision\nproblems such as 3D face tracking and reconstruction from images, as well as\ngraphics applications such as character generation and animation. Traditional\nmodels learn a latent representation of a face using linear subspaces or\nhigher-order tensor generalizations. Due to this linearity, they can not\ncapture extreme deformations and non-linear expressions. To address this, we\nintroduce a versatile model that learns a non-linear representation of a face\nusing spectral convolutions on a mesh surface. We introduce mesh sampling\noperations that enable a hierarchical mesh representation that captures\nnon-linear variations in shape and expression at multiple scales within the\nmodel. In a variational setting, our model samples diverse realistic 3D faces\nfrom a multivariate Gaussian distribution. Our training data consists of 20,466\nmeshes of extreme expressions captured over 12 different subjects. Despite\nlimited training data, our trained model outperforms state-of-the-art face\nmodels with 50% lower reconstruction error, while using 75% fewer parameters.\nWe also show that, replacing the expression space of an existing\nstate-of-the-art face model with our autoencoder, achieves a lower\nreconstruction error. Our data, model and code are available at\nhttp://github.com/anuragranj/coma\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-07-26T17:53:50Z",
    "updated": "2018-07-31T20:13:00Z",
    "doi": null
  },
  "2402.10200": {
    "id": "http://arxiv.org/abs/2402.10200v2",
    "title": "Chain-of-Thought Reasoning Without Prompting",
    "authors": [
      "Xuezhi Wang",
      "Denny Zhou"
    ],
    "abstract": "  In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding effectively\nelicits reasoning capabilities from language models, which were previously\nobscured by standard greedy decoding.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-02-15T18:55:41Z",
    "updated": "2024-05-23T20:53:59Z",
    "doi": null
  },
  "2207.02196": {
    "id": "http://arxiv.org/abs/2207.02196v4",
    "title": "Accelerating Score-based Generative Models with Preconditioned Diffusion\n  Sampling",
    "authors": [
      "Hengyuan Ma",
      "Li Zhang",
      "Xiatian Zhu",
      "Jianfeng Feng"
    ],
    "abstract": "  Score-based generative models (SGMs) have recently emerged as a promising\nclass of generative models. However, a fundamental limitation is that their\ninference is very slow due to a need for many (e.g., 2000) iterations of\nsequential computations. An intuitive acceleration method is to reduce the\nsampling iterations which however causes severe performance degradation. We\ninvestigate this problem by viewing the diffusion sampling process as a\nMetropolis adjusted Langevin algorithm, which helps reveal the underlying cause\nto be ill-conditioned curvature. Under this insight, we propose a\nmodel-agnostic preconditioned diffusion sampling (PDS) method that leverages\nmatrix preconditioning to alleviate the aforementioned problem. Crucially, PDS\nis proven theoretically to converge to the original target distribution of a\nSGM, no need for retraining. Extensive experiments on three image datasets with\na variety of resolutions and diversity validate that PDS consistently\naccelerates off-the-shelf SGMs whilst maintaining the synthesis quality. In\nparticular, PDS can accelerate by up to 29x on more challenging high resolution\n(1024x1024) image generation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-05T17:55:42Z",
    "updated": "2022-12-05T13:22:07Z",
    "doi": null
  },
  "1810.08189": {
    "id": "http://arxiv.org/abs/1810.08189v2",
    "title": "Convolutional Collaborative Filter Network for Video Based\n  Recommendation Systems",
    "authors": [
      "Cheng-Kang Hsieh",
      "Miguel Campo",
      "Abhinav Taliyan",
      "Matt Nickens",
      "Mitkumar Pandya",
      "JJ Espinoza"
    ],
    "abstract": "  This analysis explores the temporal sequencing of objects in a movie trailer.\nTemporal sequencing of objects in a movie trailer (e.g., a long shot of an\nobject vs intermittent short shots) can convey information about the type of\nmovie, plot of the movie, role of the main characters, and the filmmakers\ncinematographic choices. When combined with historical customer data,\nsequencing analysis can be used to improve predictions of customer behavior.\nE.g., a customer buys tickets to a new movie and maybe the customer has seen\nmovies in the past that contained similar sequences. To explore object\nsequencing in movie trailers, we propose a video convolutional network to\ncapture actions and scenes that are predictive of customers' preferences. The\nmodel learns the specific nature of sequences for different types of objects\n(e.g., cars vs faces), and the role of sequences in predicting customer future\nbehavior. We show how such a temporal-aware model outperforms simple feature\npooling methods proposed in our previous works and, importantly, demonstrate\nthe additional model explain-ability allowed by such a model.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-10-18T17:57:58Z",
    "updated": "2018-10-22T20:43:16Z",
    "doi": null
  },
  "2407.05352": {
    "id": "http://arxiv.org/abs/2407.05352v1",
    "title": "Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model",
    "authors": [
      "Danni Yang",
      "Ruohan Dong",
      "Jiayi Ji",
      "Yiwei Ma",
      "Haowei Wang",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ],
    "abstract": "  Recently, diffusion models have increasingly demonstrated their capabilities\nin vision understanding. By leveraging prompt-based learning to construct\nsentences, these models have shown proficiency in classification and visual\ngrounding tasks. However, existing approaches primarily showcase their ability\nto perform sentence-level localization, leaving the potential for leveraging\ncontextual information for phrase-level understanding largely unexplored. In\nthis paper, we utilize Panoptic Narrative Grounding (PNG) as a proxy task to\ninvestigate this capability further. PNG aims to segment object instances\nmentioned by multiple noun phrases within a given narrative text. Specifically,\nwe introduce the DiffPNG framework, a straightforward yet effective approach\nthat fully capitalizes on the diffusion's architecture for segmentation by\ndecomposing the process into a sequence of localization, segmentation, and\nrefinement steps. The framework initially identifies anchor points using\ncross-attention mechanisms and subsequently performs segmentation with\nself-attention to achieve zero-shot PNG. Moreover, we introduce a refinement\nmodule based on SAM to enhance the quality of the segmentation masks. Our\nextensive experiments on the PNG dataset demonstrate that DiffPNG achieves\nstrong performance in the zero-shot PNG task setting, conclusively proving the\ndiffusion model's capability for context-aware, phrase-level understanding.\nSource code is available at \\url{https://github.com/nini0919/DiffPNG}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-07T13:06:34Z",
    "updated": "2024-07-07T13:06:34Z",
    "doi": null
  },
  "2211.12112": {
    "id": "http://arxiv.org/abs/2211.12112v1",
    "title": "Human Evaluation of Text-to-Image Models on a Multi-Task Benchmark",
    "authors": [
      "Vitali Petsiuk",
      "Alexander E. Siemenn",
      "Saisamrit Surbehera",
      "Zad Chin",
      "Keith Tyser",
      "Gregory Hunter",
      "Arvind Raghavan",
      "Yann Hicke",
      "Bryan A. Plummer",
      "Ori Kerret",
      "Tonio Buonassisi",
      "Kate Saenko",
      "Armando Solar-Lezama",
      "Iddo Drori"
    ],
    "abstract": "  We provide a new multi-task benchmark for evaluating text-to-image models. We\nperform a human evaluation comparing the most common open-source (Stable\nDiffusion) and commercial (DALL-E 2) models. Twenty computer science AI\ngraduate students evaluated the two models, on three tasks, at three difficulty\nlevels, across ten prompts each, providing 3,600 ratings. Text-to-image\ngeneration has seen rapid progress to the point that many recent models have\ndemonstrated their ability to create realistic high-resolution images for\nvarious prompts. However, current text-to-image methods and the broader body of\nresearch in vision-language understanding still struggle with intricate text\nprompts that contain many objects with multiple attributes and relationships.\nWe introduce a new text-to-image benchmark that contains a suite of thirty-two\ntasks over multiple applications that capture a model's ability to handle\ndifferent features of a text prompt. For example, asking a model to generate a\nvarying number of the same object to measure its ability to count or providing\na text prompt with several objects that each have a different attribute to\nidentify its ability to match objects and attributes correctly. Rather than\nsubjectively evaluating text-to-image results on a set of prompts, our new\nmulti-task benchmark consists of challenge tasks at three difficulty levels\n(easy, medium, and hard) and human ratings for each generated image.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-22T09:27:53Z",
    "updated": "2022-11-22T09:27:53Z",
    "doi": null
  },
  "1712.01026": {
    "id": "http://arxiv.org/abs/1712.01026v4",
    "title": "Wasserstein Divergence for GANs",
    "authors": [
      "Jiqing Wu",
      "Zhiwu Huang",
      "Janine Thoma",
      "Dinesh Acharya",
      "Luc Van Gool"
    ],
    "abstract": "  In many domains of computer vision, generative adversarial networks (GANs)\nhave achieved great success, among which the family of Wasserstein GANs (WGANs)\nis considered to be state-of-the-art due to the theoretical contributions and\ncompetitive qualitative performance. However, it is very challenging to\napproximate the $k$-Lipschitz constraint required by the Wasserstein-1\nmetric~(W-met). In this paper, we propose a novel Wasserstein\ndivergence~(W-div), which is a relaxed version of W-met and does not require\nthe $k$-Lipschitz constraint. As a concrete application, we introduce a\nWasserstein divergence objective for GANs~(WGAN-div), which can faithfully\napproximate W-div through optimization. Under various settings, including\nprogressive growing training, we demonstrate the stability of the proposed\nWGAN-div owing to its theoretical and practical advantages over WGANs. Also, we\nstudy the quantitative and visual performance of WGAN-div on standard image\nsynthesis benchmarks of computer vision, showing the superior performance of\nWGAN-div compared to the state-of-the-art methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-12-04T11:57:28Z",
    "updated": "2018-09-05T12:41:21Z",
    "doi": null
  },
  "2212.06593": {
    "id": "http://arxiv.org/abs/2212.06593v1",
    "title": "FastMIM: Expediting Masked Image Modeling Pre-training for Vision",
    "authors": [
      "Jianyuan Guo",
      "Kai Han",
      "Han Wu",
      "Yehui Tang",
      "Yunhe Wang",
      "Chang Xu"
    ],
    "abstract": "  The combination of transformers and masked image modeling (MIM) pre-training\nframework has shown great potential in various vision tasks. However, the\npre-training computational budget is too heavy and withholds the MIM from\nbecoming a practical training paradigm. This paper presents FastMIM, a simple\nand generic framework for expediting masked image modeling with the following\ntwo steps: (i) pre-training vision backbones with low-resolution input images;\nand (ii) reconstructing Histograms of Oriented Gradients (HOG) feature instead\nof original RGB values of the input images. In addition, we propose FastMIM-P\nto progressively enlarge the input resolution during pre-training stage to\nfurther enhance the transfer results of models with high capacity. We point out\nthat: (i) a wide range of input resolutions in pre-training phase can lead to\nsimilar performances in fine-tuning phase and downstream tasks such as\ndetection and segmentation; (ii) the shallow layers of encoder are more\nimportant during pre-training and discarding last several layers can speed up\nthe training stage with no harm to fine-tuning performance; (iii) the decoder\nshould match the size of selected network; and (iv) HOG is more stable than RGB\nvalues when resolution transfers;. Equipped with FastMIM, all kinds of vision\nbackbones can be pre-trained in an efficient way. For example, we can achieve\n83.8%/84.1% top-1 accuracy on ImageNet-1K with ViT-B/Swin-B as backbones.\nCompared to previous relevant approaches, we can achieve comparable or better\ntop-1 accuracy while accelerate the training procedure by $\\sim$5$\\times$. Code\ncan be found in https://github.com/ggjy/FastMIM.pytorch.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-13T14:09:32Z",
    "updated": "2022-12-13T14:09:32Z",
    "doi": null
  },
  "1806.02169": {
    "id": "http://arxiv.org/abs/1806.02169v2",
    "title": "StarGAN-VC: Non-parallel many-to-many voice conversion with star\n  generative adversarial networks",
    "authors": [
      "Hirokazu Kameoka",
      "Takuhiro Kaneko",
      "Kou Tanaka",
      "Nobukatsu Hojo"
    ],
    "abstract": "  This paper proposes a method that allows non-parallel many-to-many voice\nconversion (VC) by using a variant of a generative adversarial network (GAN)\ncalled StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it\n(1) requires no parallel utterances, transcriptions, or time alignment\nprocedures for speech generator training, (2) simultaneously learns\nmany-to-many mappings across different attribute domains using a single\ngenerator network, (3) is able to generate converted speech signals quickly\nenough to allow real-time implementations and (4) requires only several minutes\nof training examples to generate reasonably realistic-sounding speech.\nSubjective evaluation experiments on a non-parallel many-to-many speaker\nidentity conversion task revealed that the proposed method obtained higher\nsound quality and speaker similarity than a state-of-the-art method based on\nvariational autoencoding GANs.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-06-06T13:24:23Z",
    "updated": "2018-06-29T06:19:34Z",
    "doi": null
  },
  "2309.03044": {
    "id": "http://arxiv.org/abs/2309.03044v1",
    "title": "Method-Level Bug Severity Prediction using Source Code Metrics and LLMs",
    "authors": [
      "Ehsan Mashhadi",
      "Hossein Ahmadvand",
      "Hadi Hemmati"
    ],
    "abstract": "  In the past couple of decades, significant research efforts are devoted to\nthe prediction of software bugs. However, most existing work in this domain\ntreats all bugs the same, which is not the case in practice. It is important\nfor a defect prediction method to estimate the severity of the identified bugs\nso that the higher-severity ones get immediate attention. In this study, we\ninvestigate source code metrics, source code representation using large\nlanguage models (LLMs), and their combination in predicting bug severity labels\nof two prominent datasets. We leverage several source metrics at method-level\ngranularity to train eight different machine-learning models. Our results\nsuggest that Decision Tree and Random Forest models outperform other models\nregarding our several evaluation metrics. We then use the pre-trained CodeBERT\nLLM to study the source code representations' effectiveness in predicting bug\nseverity. CodeBERT finetuning improves the bug severity prediction results\nsignificantly in the range of 29%-140% for several evaluation metrics, compared\nto the best classic prediction model on source code metric. Finally, we\nintegrate source code metrics into CodeBERT as an additional input, using our\ntwo proposed architectures, which both enhance the CodeBERT model\neffectiveness.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-06T14:38:07Z",
    "updated": "2023-09-06T14:38:07Z",
    "doi": "10.5281/zenodo.8267597"
  },
  "2208.01753": {
    "id": "http://arxiv.org/abs/2208.01753v1",
    "title": "Two-Stream Transformer Architecture for Long Video Understanding",
    "authors": [
      "Edward Fish",
      "Jon Weinbren",
      "Andrew Gilbert"
    ],
    "abstract": "  Pure vision transformer architectures are highly effective for short video\nclassification and action recognition tasks. However, due to the quadratic\ncomplexity of self attention and lack of inductive bias, transformers are\nresource intensive and suffer from data inefficiencies. Long form video\nunderstanding tasks amplify data and memory efficiency problems in transformers\nmaking current approaches unfeasible to implement on data or memory restricted\ndomains. This paper introduces an efficient Spatio-Temporal Attention Network\n(STAN) which uses a two-stream transformer architecture to model dependencies\nbetween static image features and temporal contextual features. Our proposed\napproach can classify videos up to two minutes in length on a single GPU, is\ndata efficient, and achieves SOTA performance on several long video\nunderstanding tasks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-02T21:03:48Z",
    "updated": "2022-08-02T21:03:48Z",
    "doi": null
  },
  "1804.10253": {
    "id": "http://arxiv.org/abs/1804.10253v3",
    "title": "From Principal Subspaces to Principal Components with Linear\n  Autoencoders",
    "authors": [
      "Elad Plaut"
    ],
    "abstract": "  The autoencoder is an effective unsupervised learning model which is widely\nused in deep learning. It is well known that an autoencoder with a single\nfully-connected hidden layer, a linear activation function and a squared error\ncost function trains weights that span the same subspace as the one spanned by\nthe principal component loading vectors, but that they are not identical to the\nloading vectors. In this paper, we show how to recover the loading vectors from\nthe autoencoder weights.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-26T19:28:02Z",
    "updated": "2018-12-28T19:02:12Z",
    "doi": null
  },
  "2009.06520": {
    "id": "http://arxiv.org/abs/2009.06520v2",
    "title": "A Systematic Literature Review on the Use of Deep Learning in Software\n  Engineering Research",
    "authors": [
      "Cody Watson",
      "Nathan Cooper",
      "David Nader Palacio",
      "Kevin Moran",
      "Denys Poshyvanyk"
    ],
    "abstract": "  An increasingly popular set of techniques adopted by software engineering\n(SE) researchers to automate development tasks are those rooted in the concept\nof Deep Learning (DL). The popularity of such techniques largely stems from\ntheir automated feature engineering capabilities, which aid in modeling\nsoftware artifacts. However, due to the rapid pace at which DL techniques have\nbeen adopted, it is difficult to distill the current successes, failures, and\nopportunities of the current research landscape. In an effort to bring clarity\nto this crosscutting area of work, from its modern inception to the present,\nthis paper presents a systematic literature review of research at the\nintersection of SE & DL. The review canvases work appearing in the most\nprominent SE and DL conferences and journals and spans 128 papers across 23\nunique SE tasks. We center our analysis around the components of learning, a\nset of principles that govern the application of machine learning techniques\n(ML) to a given problem domain, discussing several aspects of the surveyed work\nat a granular level. The end result of our analysis is a research roadmap that\nboth delineates the foundations of DL techniques applied to SE research, and\nhighlights likely areas of fertile exploration for the future.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.NE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-09-14T15:28:28Z",
    "updated": "2021-09-23T18:11:47Z",
    "doi": null
  },
  "2305.16317": {
    "id": "http://arxiv.org/abs/2305.16317v3",
    "title": "Parallel Sampling of Diffusion Models",
    "authors": [
      "Andy Shih",
      "Suneel Belkhale",
      "Stefano Ermon",
      "Dorsa Sadigh",
      "Nima Anari"
    ],
    "abstract": "  Diffusion models are powerful generative models but suffer from slow\nsampling, often taking 1000 sequential denoising steps for one sample. As a\nresult, considerable efforts have been directed toward reducing the number of\ndenoising steps, but these methods hurt sample quality. Instead of reducing the\nnumber of denoising steps (trading quality for speed), in this paper we explore\nan orthogonal approach: can we run the denoising steps in parallel (trading\ncompute for speed)? In spite of the sequential nature of the denoising steps,\nwe show that surprisingly it is possible to parallelize sampling via Picard\niterations, by guessing the solution of future denoising steps and iteratively\nrefining until convergence. With this insight, we present ParaDiGMS, a novel\nmethod to accelerate the sampling of pretrained diffusion models by denoising\nmultiple steps in parallel. ParaDiGMS is the first diffusion sampling method\nthat enables trading compute for speed and is even compatible with existing\nfast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we\nimprove sampling speed by 2-4x across a range of robotics and image generation\nmodels, giving state-of-the-art sampling speeds of 0.2s on 100-step\nDiffusionPolicy and 14.6s on 1000-step StableDiffusion-v2 with no measurable\ndegradation of task reward, FID score, or CLIP score.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-25T17:59:42Z",
    "updated": "2023-10-16T01:51:04Z",
    "doi": null
  },
  "2109.08306": {
    "id": "http://arxiv.org/abs/2109.08306v1",
    "title": "SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based\n  Sentiment Analysis",
    "authors": [
      "Chengxi Li",
      "Feiyu Gao",
      "Jiajun Bu",
      "Lu Xu",
      "Xiang Chen",
      "Yu Gu",
      "Zirui Shao",
      "Qi Zheng",
      "Ningyu Zhang",
      "Yongpan Wang",
      "Zhi Yu"
    ],
    "abstract": "  Aspect-based sentiment analysis (ABSA) is an emerging fine-grained sentiment\nanalysis task that aims to extract aspects, classify corresponding sentiment\npolarities and find opinions as the causes of sentiment. The latest research\ntends to solve the ABSA task in a unified way with end-to-end frameworks. Yet,\nthese frameworks get fine-tuned from downstream tasks without any task-adaptive\nmodification. Specifically, they do not use task-related knowledge well or\nexplicitly model relations between aspect and opinion terms, hindering them\nfrom better performance. In this paper, we propose SentiPrompt to use sentiment\nknowledge enhanced prompts to tune the language model in the unified framework.\nWe inject sentiment knowledge regarding aspects, opinions, and polarities into\nprompt and explicitly model term relations via constructing consistency and\npolarity judgment templates from the ground truth triplets. Experimental\nresults demonstrate that our approach can outperform strong baselines on\nTriplet Extraction, Pair Extraction, and Aspect Term Extraction with Sentiment\nClassification by a notable margin.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-17T01:56:06Z",
    "updated": "2021-09-17T01:56:06Z",
    "doi": null
  },
  "2204.02849": {
    "id": "http://arxiv.org/abs/2204.02849v2",
    "title": "KNN-Diffusion: Image Generation via Large-Scale Retrieval",
    "authors": [
      "Shelly Sheynin",
      "Oron Ashual",
      "Adam Polyak",
      "Uriel Singer",
      "Oran Gafni",
      "Eliya Nachmani",
      "Yaniv Taigman"
    ],
    "abstract": "  Recent text-to-image models have achieved impressive results. However, since\nthey require large-scale datasets of text-image pairs, it is impractical to\ntrain them on new domains where data is scarce or not labeled. In this work, we\npropose using large-scale retrieval methods, in particular, efficient\nk-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a\nsubstantially small and efficient text-to-image diffusion model without any\ntext, (2) generating out-of-distribution images by simply swapping the\nretrieval database at inference time, and (3) performing text-driven local\nsemantic manipulations while preserving object identity. To demonstrate the\nrobustness of our method, we apply our kNN approach on two state-of-the-art\ndiffusion backbones, and show results on several different datasets. As\nevaluated by human studies and automatic metrics, our method achieves\nstate-of-the-art results compared to existing approaches that train\ntext-to-image generation models using images only (without paired text data)\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-04-06T14:13:35Z",
    "updated": "2022-10-02T11:55:59Z",
    "doi": null
  },
  "2309.04828": {
    "id": "http://arxiv.org/abs/2309.04828v1",
    "title": "FAIR: Flow Type-Aware Pre-Training of Compiler Intermediate\n  Representations",
    "authors": [
      "Changan Niu",
      "Chuanyi Li",
      "Vincent Ng",
      "David Lo",
      "Bin Luo"
    ],
    "abstract": "  While the majority of existing pre-trained models from code learn source code\nfeatures such as code tokens and abstract syntax trees, there are some other\nworks that focus on learning from compiler intermediate representations (IRs).\nExisting IR-based models typically utilize IR features such as instructions,\ncontrol and data flow graphs (CDFGs), call graphs, etc. However, these methods\nconfuse variable nodes and instruction nodes in a CDFG and fail to distinguish\ndifferent types of flows, and the neural networks they use fail to capture\nlong-distance dependencies and have over-smoothing and over-squashing problems.\nTo address these weaknesses, we propose FAIR, a Flow type-Aware pre-trained\nmodel for IR that involves employing (1) a novel input representation of IR\nprograms; (2) Graph Transformer to address over-smoothing, over-squashing and\nlong-dependencies problems; and (3) five pre-training tasks that we\nspecifically propose to enable FAIR to learn the semantics of IR tokens, flow\ntype information, and the overall representation of IR. Experimental results\nshow that FAIR can achieve state-of-the-art results on four code-related\ndownstream tasks.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-09T15:51:49Z",
    "updated": "2023-09-09T15:51:49Z",
    "doi": null
  },
  "2105.06597": {
    "id": "http://arxiv.org/abs/2105.06597v4",
    "title": "RetGen: A Joint framework for Retrieval and Grounded Text Generation\n  Modeling",
    "authors": [
      "Yizhe Zhang",
      "Siqi Sun",
      "Xiang Gao",
      "Yuwei Fang",
      "Chris Brockett",
      "Michel Galley",
      "Jianfeng Gao",
      "Bill Dolan"
    ],
    "abstract": "  Recent advances in large-scale pre-training such as GPT-3 allow seemingly\nhigh quality text to be generated from a given prompt. However, such generation\nsystems often suffer from problems of hallucinated facts, and are not\ninherently designed to incorporate useful external information. Grounded\ngeneration models appear to offer remedies, but their training typically relies\non rarely-available parallel data where information-relevant documents are\nprovided for context. We propose a framework that alleviates this data\nconstraint by jointly training a grounded generator and document retriever on\nthe language model signal. The model learns to reward retrieval of the\ndocuments with the highest utility in generation, and attentively combines them\nusing a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We\ndemonstrate that both generator and retriever can take advantage of this joint\ntraining and work synergistically to produce more informative and relevant text\nin both prose and dialogue generation.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-05-14T00:11:38Z",
    "updated": "2022-02-24T18:29:55Z",
    "doi": null
  },
  "2011.12026": {
    "id": "http://arxiv.org/abs/2011.12026v2",
    "title": "Adversarial Generation of Continuous Images",
    "authors": [
      "Ivan Skorokhodov",
      "Savva Ignatyev",
      "Mohamed Elhoseiny"
    ],
    "abstract": "  In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-11-24T11:06:40Z",
    "updated": "2021-06-28T09:00:05Z",
    "doi": null
  },
  "2309.00841": {
    "id": "http://arxiv.org/abs/2309.00841v1",
    "title": "LeanContext: Cost-Efficient Domain-Specific Question Answering Using\n  LLMs",
    "authors": [
      "Md Adnan Arefeen",
      "Biplob Debnath",
      "Srimat Chakradhar"
    ],
    "abstract": "  Question-answering (QA) is a significant application of Large Language Models\n(LLMs), shaping chatbot capabilities across healthcare, education, and customer\nservice. However, widespread LLM integration presents a challenge for small\nbusinesses due to the high expenses of LLM API usage. Costs rise rapidly when\ndomain-specific data (context) is used alongside queries for accurate\ndomain-specific LLM responses. One option is to summarize the context by using\nLLMs and reduce the context. However, this can also filter out useful\ninformation that is necessary to answer some domain-specific queries. In this\npaper, we shift from human-oriented summarizers to AI model-friendly summaries.\nOur approach, LeanContext, efficiently extracts $k$ key sentences from the\ncontext that are closely aligned with the query. The choice of $k$ is neither\nstatic nor random; we introduce a reinforcement learning technique that\ndynamically determines $k$ based on the query and context. The rest of the less\nimportant sentences are reduced using a free open source text reduction method.\nWe evaluate LeanContext against several recent query-aware and query-unaware\ncontext reduction approaches on prominent datasets (arxiv papers and BBC news\narticles). Despite cost reductions of $37.29\\%$ to $67.81\\%$, LeanContext's\nROUGE-1 score decreases only by $1.41\\%$ to $2.65\\%$ compared to a baseline\nthat retains the entire context (no summarization). Additionally, if free\npretrained LLM-based summarizers are used to reduce context (into human\nconsumable summaries), LeanContext can further modify the reduced context to\nenhance the accuracy (ROUGE-1 score) by $13.22\\%$ to $24.61\\%$.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-02T06:33:18Z",
    "updated": "2023-09-02T06:33:18Z",
    "doi": null
  },
  "2308.07732": {
    "id": "http://arxiv.org/abs/2308.07732v1",
    "title": "UniTR: A Unified and Efficient Multi-Modal Transformer for\n  Bird's-Eye-View Representation",
    "authors": [
      "Haiyang Wang",
      "Hao Tang",
      "Shaoshuai Shi",
      "Aoxue Li",
      "Zhenguo Li",
      "Bernt Schiele",
      "Liwei Wang"
    ],
    "abstract": "  Jointly processing information from multiple sensors is crucial to achieving\naccurate and robust perception for reliable autonomous driving systems.\nHowever, current 3D perception research follows a modality-specific paradigm,\nleading to additional computation overheads and inefficient collaboration\nbetween different sensor data. In this paper, we present an efficient\nmulti-modal backbone for outdoor 3D perception named UniTR, which processes a\nvariety of modalities with unified modeling and shared parameters. Unlike\nprevious works, UniTR introduces a modality-agnostic transformer encoder to\nhandle these view-discrepant sensor data for parallel modal-wise representation\nlearning and automatic cross-modal interaction without additional fusion steps.\nMore importantly, to make full use of these complementary sensor types, we\npresent a novel multi-modal integration strategy by both considering\nsemantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood\nrelations. UniTR is also a fundamentally task-agnostic backbone that naturally\nsupports different 3D perception tasks. It sets a new state-of-the-art\nperformance on the nuScenes benchmark, achieving +1.1 NDS higher for 3D object\ndetection and +12.0 higher mIoU for BEV map segmentation with lower inference\nlatency. Code will be available at https://github.com/Haiyang-W/UniTR .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-15T12:13:44Z",
    "updated": "2023-08-15T12:13:44Z",
    "doi": null
  },
  "2011.11961": {
    "id": "http://arxiv.org/abs/2011.11961v4",
    "title": "MODNet: Real-Time Trimap-Free Portrait Matting via Objective\n  Decomposition",
    "authors": [
      "Zhanghan Ke",
      "Jiayu Sun",
      "Kaican Li",
      "Qiong Yan",
      "Rynson W. H. Lau"
    ],
    "abstract": "  Existing portrait matting methods either require auxiliary inputs that are\ncostly to obtain or involve multiple stages that are computationally expensive,\nmaking them less suitable for real-time applications. In this work, we present\na light-weight matting objective decomposition network (MODNet) for portrait\nmatting in real-time with a single input image. The key idea behind our\nefficient design is by optimizing a series of sub-objectives simultaneously via\nexplicit constraints. In addition, MODNet includes two novel techniques for\nimproving model efficiency and robustness. First, an Efficient Atrous Spatial\nPyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for\nsemantic estimation. Second, a self-supervised sub-objectives consistency (SOC)\nstrategy is proposed to adapt MODNet to real-world data to address the domain\nshift problem common to trimap-free methods. MODNet is easy to be trained in an\nend-to-end manner. It is much faster than contemporaneous methods and runs at\n67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms\nprior trimap-free methods by a large margin on both Adobe Matting Dataset and a\ncarefully designed photographic portrait matting (PPM-100) benchmark proposed\nby us. Further, MODNet achieves remarkable results on daily photos and videos.\nOur code and models are available at https://github.com/ZHKKKe/MODNet, and the\nPPM-100 benchmark is released at https://github.com/ZHKKKe/PPM.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-11-24T08:38:36Z",
    "updated": "2022-03-18T04:49:53Z",
    "doi": null
  },
  "2005.00341": {
    "id": "http://arxiv.org/abs/2005.00341v1",
    "title": "Jukebox: A Generative Model for Music",
    "authors": [
      "Prafulla Dhariwal",
      "Heewoo Jun",
      "Christine Payne",
      "Jong Wook Kim",
      "Alec Radford",
      "Ilya Sutskever"
    ],
    "abstract": "  We introduce Jukebox, a model that generates music with singing in the raw\naudio domain. We tackle the long context of raw audio using a multi-scale\nVQ-VAE to compress it to discrete codes, and modeling those using\nautoregressive Transformers. We show that the combined model at scale can\ngenerate high-fidelity and diverse songs with coherence up to multiple minutes.\nWe can condition on artist and genre to steer the musical and vocal style, and\non unaligned lyrics to make the singing more controllable. We are releasing\nthousands of non cherry-picked samples at https://jukebox.openai.com, along\nwith model weights and code at https://github.com/openai/jukebox\n",
    "categories": [
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-30T09:02:45Z",
    "updated": "2020-04-30T09:02:45Z",
    "doi": null
  },
  "2409.08091": {
    "id": "http://arxiv.org/abs/2409.08091v2",
    "title": "EZIGen: Enhancing zero-shot subject-driven image generation with precise\n  subject encoding and decoupled guidance",
    "authors": [
      "Zicheng Duan",
      "Yuxuan Ding",
      "Chenhui Gou",
      "Ziqin Zhou",
      "Ethan Smith",
      "Lingqiao Liu"
    ],
    "abstract": "  Zero-shot subject-driven image generation aims to produce images that\nincorporate a subject from a given example image. The challenge lies in\npreserving the subject's identity while aligning with the text prompt which\noften requires modifying certain aspects of the subject's appearance. Despite\nadvancements in diffusion model based methods, existing approaches still\nstruggle to balance identity preservation with text prompt alignment. In this\nstudy, we conducted an in-depth investigation into this issue and uncovered key\ninsights for achieving effective identity preservation while maintaining a\nstrong balance. Our key findings include: (1) the design of the subject image\nencoder significantly impacts identity preservation quality, and (2) separating\ntext and subject guidance is crucial for both text alignment and identity\npreservation. Building on these insights, we introduce a new approach called\nEZIGen, which employs two main strategies: a carefully crafted subject image\nEncoder based on the pretrained UNet of the Stable Diffusion model to ensure\nhigh-quality identity transfer, following a process that decouples the guidance\nstages and iteratively refines the initial image layout. Through these\nstrategies, EZIGen achieves state-of-the-art results on multiple subject-driven\nbenchmarks with a unified model and 100 times less training data. The demo page\nis available at: https://zichengduan.github.io/pages/EZIGen/index.html.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-09-12T14:44:45Z",
    "updated": "2024-10-01T17:52:18Z",
    "doi": null
  },
  "1711.11586": {
    "id": "http://arxiv.org/abs/1711.11586v4",
    "title": "Toward Multimodal Image-to-Image Translation",
    "authors": [
      "Jun-Yan Zhu",
      "Richard Zhang",
      "Deepak Pathak",
      "Trevor Darrell",
      "Alexei A. Efros",
      "Oliver Wang",
      "Eli Shechtman"
    ],
    "abstract": "  Many image-to-image translation problems are ambiguous, as a single input\nimage may correspond to multiple possible outputs. In this work, we aim to\nmodel a \\emph{distribution} of possible outputs in a conditional generative\nmodeling setting. The ambiguity of the mapping is distilled in a\nlow-dimensional latent vector, which can be randomly sampled at test time. A\ngenerator learns to map the given input, combined with this latent code, to the\noutput. We explicitly encourage the connection between output and the latent\ncode to be invertible. This helps prevent a many-to-one mapping from the latent\ncode to the output during training, also known as the problem of mode collapse,\nand produces more diverse results. We explore several variants of this approach\nby employing different training objectives, network architectures, and methods\nof injecting the latent code. Our proposed method encourages bijective\nconsistency between the latent encoding and output modes. We present a\nsystematic comparison of our method and other variants on both perceptual\nrealism and diversity.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-11-30T18:59:01Z",
    "updated": "2018-10-24T00:29:43Z",
    "doi": null
  },
  "2311.15744": {
    "id": "http://arxiv.org/abs/2311.15744v1",
    "title": "One More Step: A Versatile Plug-and-Play Module for Rectifying Diffusion\n  Schedule Flaws and Enhancing Low-Frequency Controls",
    "authors": [
      "Minghui Hu",
      "Jianbin Zheng",
      "Chuanxia Zheng",
      "Chaoyue Wang",
      "Dacheng Tao",
      "Tat-Jen Cham"
    ],
    "abstract": "  It is well known that many open-released foundational diffusion models have\ndifficulty in generating images that substantially depart from average\nbrightness, despite such images being present in the training data. This is due\nto an inconsistency: while denoising starts from pure Gaussian noise during\ninference, the training noise schedule retains residual data even in the final\ntimestep distribution, due to difficulties in numerical conditioning in\nmainstream formulation, leading to unintended bias during inference. To\nmitigate this issue, certain $\\epsilon$-prediction models are combined with an\nad-hoc offset-noise methodology. In parallel, some contemporary models have\nadopted zero-terminal SNR noise schedules together with\n$\\mathbf{v}$-prediction, which necessitate major alterations to pre-trained\nmodels. However, such changes risk destabilizing a large multitude of\ncommunity-driven applications anchored on these pre-trained models. In light of\nthis, our investigation revisits the fundamental causes, leading to our\nproposal of an innovative and principled remedy, called One More Step (OMS). By\nintegrating a compact network and incorporating an additional simple yet\neffective step during inference, OMS elevates image fidelity and harmonizes the\ndichotomy between training and inference, while preserving original model\nparameters. Once trained, various pre-trained diffusion models with the same\nlatent domain can share the same OMS module.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-27T12:02:42Z",
    "updated": "2023-11-27T12:02:42Z",
    "doi": null
  },
  "2209.07046": {
    "id": "http://arxiv.org/abs/2209.07046v2",
    "title": "Exploring Visual Interpretability for Contrastive Language-Image\n  Pre-training",
    "authors": [
      "Yi Li",
      "Hualiang Wang",
      "Yiqun Duan",
      "Hang Xu",
      "Xiaomeng Li"
    ],
    "abstract": "  Contrastive Language-Image Pre-training (CLIP) learns rich representations\nvia readily available supervision of natural language. It improves the\nperformance of downstream vision tasks, including but not limited to the\nzero-shot, long tail, segmentation, retrieval, caption, and video. However, the\nvisual explainability of CLIP is rarely studied, especially for the raw feature\nmap. To provide visual explanations of its predictions, we propose the\nImage-Text Similarity Map (ITSM). Based on it, we surprisingly find that CLIP\nprefers the background regions than the foregrounds, and shows erroneous\nvisualization results against human understanding. This phenomenon is universal\nfor both vision transformers and convolutional networks, which suggests this\nproblem is unique and not owing to certain network. Experimentally, we find the\ndevil is in the pooling part, where inappropriate pooling methods lead to a\nphenomenon called semantic shift. For this problem, we propose the Explainable\nContrastive Language-Image Pre-training (ECLIP), which corrects the\nexplainability via the Masked Max Pooling. Specifically, to avoid the semantic\nshift, we replace the original attention pooling by max pooling to focus on the\nconfident foreground, with guidance from free attention during training.\nExperiments on three datasets suggest that ECLIP greatly improves the\nexplainability of CLIP, and beyond previous explainability methods at large\nmargins. The code will be released later.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "65D19 (Primary), 54H30 (Secondary)",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.4.0; I.4.10",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-15T05:01:03Z",
    "updated": "2022-11-27T01:07:52Z",
    "doi": null
  },
  "2306.08964": {
    "id": "http://arxiv.org/abs/2306.08964v2",
    "title": "Exploring Multi-Timestep Multi-Stage Diffusion Features for\n  Hyperspectral Image Classification",
    "authors": [
      "Jingyi Zhou",
      "Jiamu Sheng",
      "Jiayuan Fan",
      "Peng Ye",
      "Tong He",
      "Bin Wang",
      "Tao Chen"
    ],
    "abstract": "  The effectiveness of spectral-spatial feature learning is crucial for the\nhyperspectral image (HSI) classification task. Diffusion models, as a new class\nof groundbreaking generative models, have the ability to learn both contextual\nsemantics and textual details from the distinct timestep dimension, enabling\nthe modeling of complex spectral-spatial relations in HSIs. However, existing\ndiffusion-based HSI classification methods only utilize manually selected\nsingle-timestep single-stage features, limiting the full exploration and\nexploitation of rich contextual semantics and textual information hidden in the\ndiffusion model. To address this issue, we propose a novel diffusion-based\nfeature learning framework that explores Multi-Timestep Multi-Stage Diffusion\nfeatures for HSI classification for the first time, called MTMSD. Specifically,\nthe diffusion model is first pretrained with unlabeled HSI patches to mine the\nconnotation of unlabeled data, and then is used to extract the multi-timestep\nmulti-stage diffusion features. To effectively and efficiently leverage\nmulti-timestep multi-stage features,two strategies are further developed. One\nstrategy is class & timestep-oriented multi-stage feature purification module\nwith the inter-class and inter-timestep prior for reducing the redundancy of\nmulti-stage features and alleviating memory constraints. The other one is\nselective timestep feature fusion module with the guidance of global features\nto adaptively select different timestep features for integrating texture and\nsemantics. Both strategies facilitate the generality and adaptability of the\nMTMSD framework for diverse patterns of different HSI data. Extensive\nexperiments are conducted on four public HSI datasets, and the results\ndemonstrate that our method outperforms state-of-the-art methods for HSI\nclassification, especially on the challenging Houston 2018 dataset.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-15T08:56:58Z",
    "updated": "2024-06-03T04:50:18Z",
    "doi": null
  },
  "2208.14743": {
    "id": "http://arxiv.org/abs/2208.14743v1",
    "title": "SimpleRecon: 3D Reconstruction Without 3D Convolutions",
    "authors": [
      "Mohamed Sayed",
      "John Gibson",
      "Jamie Watson",
      "Victor Prisacariu",
      "Michael Firman",
      "Cl\u00e9ment Godard"
    ],
    "abstract": "  Traditionally, 3D indoor scene reconstruction from posed images happens in\ntwo phases: per-image depth estimation, followed by depth merging and surface\nreconstruction. Recently, a family of methods have emerged that perform\nreconstruction directly in final 3D volumetric feature space. While these\nmethods have shown impressive reconstruction results, they rely on expensive 3D\nconvolutional layers, limiting their application in resource-constrained\nenvironments. In this work, we instead go back to the traditional route, and\nshow how focusing on high quality multi-view depth prediction leads to highly\naccurate 3D reconstructions using simple off-the-shelf depth fusion. We propose\na simple state-of-the-art multi-view depth estimator with two main\ncontributions: 1) a carefully-designed 2D CNN which utilizes strong image\npriors alongside a plane-sweep feature volume and geometric losses, combined\nwith 2) the integration of keyframe and geometric metadata into the cost volume\nwhich allows informed depth plane scoring. Our method achieves a significant\nlead over the current state-of-the-art for depth estimation and close or better\nfor 3D reconstruction on ScanNet and 7-Scenes, yet still allows for online\nreal-time low-memory reconstruction. Code, models and results are available at\nhttps://nianticlabs.github.io/simplerecon\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-31T09:46:34Z",
    "updated": "2022-08-31T09:46:34Z",
    "doi": null
  },
  "2211.17084": {
    "id": "http://arxiv.org/abs/2211.17084v1",
    "title": "High-Fidelity Guided Image Synthesis with Latent Diffusion Models",
    "authors": [
      "Jaskirat Singh",
      "Stephen Gould",
      "Liang Zheng"
    ],
    "abstract": "  Controllable image synthesis with user scribbles has gained huge public\ninterest with the recent advent of text-conditioned latent diffusion models.\nThe user scribbles control the color composition while the text prompt provides\ncontrol over the overall image semantics. However, we note that prior works in\nthis direction suffer from an intrinsic domain shift problem, wherein the\ngenerated outputs often lack details and resemble simplistic representations of\nthe target domain. In this paper, we propose a novel guided image synthesis\nframework, which addresses this problem by modeling the output image as the\nsolution of a constrained optimization problem. We show that while computing an\nexact solution to the optimization is infeasible, an approximation of the same\ncan be achieved while just requiring a single pass of the reverse diffusion\nprocess. Additionally, we show that by simply defining a cross-attention based\ncorrespondence between the input text tokens and the user stroke-painting, the\nuser is also able to control the semantics of different painted regions without\nrequiring any conditional training or finetuning. Human user study results show\nthat the proposed approach outperforms the previous state-of-the-art by over\n85.32% on the overall user satisfaction scores. Project page for our paper is\navailable at https://1jsingh.github.io/gradop.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-30T15:43:20Z",
    "updated": "2022-11-30T15:43:20Z",
    "doi": null
  },
  "2206.01821": {
    "id": "http://arxiv.org/abs/2206.01821v1",
    "title": "EAANet: Efficient Attention Augmented Convolutional Networks",
    "authors": [
      "Runqing Zhang",
      "Tianshu Zhu"
    ],
    "abstract": "  Humans can effectively find salient regions in complex scenes. Self-attention\nmechanisms were introduced into Computer Vision (CV) to achieve this. Attention\nAugmented Convolutional Network (AANet) is a mixture of convolution and\nself-attention, which increases the accuracy of a typical ResNet. However, The\ncomplexity of self-attention is O(n2) in terms of computation and memory usage\nwith respect to the number of input tokens. In this project, we propose EAANet:\nEfficient Attention Augmented Convolutional Networks, which incorporates\nefficient self-attention mechanisms in a convolution and self-attention hybrid\narchitecture to reduce the model's memory footprint. Our best model show\nperformance improvement over AA-Net and ResNet18. We also explore different\nmethods to augment Convolutional Network with self-attention mechanisms and\nshow the difficulty of training those methods compared to ResNet. Finally, we\nshow that augmenting efficient self-attention mechanisms with ResNet scales\nbetter with input size than normal self-attention mechanisms. Therefore, our\nEAANet is more capable of working with high-resolution images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-03T21:22:12Z",
    "updated": "2022-06-03T21:22:12Z",
    "doi": null
  },
  "2110.04994": {
    "id": "http://arxiv.org/abs/2110.04994v1",
    "title": "Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision\n  Datasets from 3D Scans",
    "authors": [
      "Ainaz Eftekhar",
      "Alexander Sax",
      "Roman Bachmann",
      "Jitendra Malik",
      "Amir Zamir"
    ],
    "abstract": "  This paper introduces a pipeline to parametrically sample and render\nmulti-task vision datasets from comprehensive 3D scans from the real world.\nChanging the sampling parameters allows one to \"steer\" the generated datasets\nto emphasize specific information. In addition to enabling interesting lines of\nresearch, we show the tooling and generated data suffice to train robust vision\nmodels.\n  Common architectures trained on a generated starter dataset reached\nstate-of-the-art performance on multiple common vision tasks and benchmarks,\ndespite having seen no benchmark or non-pipeline data. The depth estimation\nnetwork outperforms MiDaS and the surface normal estimation network is the\nfirst to achieve human-level performance for in-the-wild surface normal\nestimation -- at least according to one metric on the OASIS benchmark.\n  The Dockerized pipeline with CLI, the (mostly python) code, PyTorch\ndataloaders for the generated data, the generated starter dataset, download\nscripts and other utilities are available through our project website,\nhttps://omnidata.vision.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-11T04:21:46Z",
    "updated": "2021-10-11T04:21:46Z",
    "doi": null
  },
  "2203.13954": {
    "id": "http://arxiv.org/abs/2203.13954v2",
    "title": "GEN-VLKT: Simplify Association and Enhance Interaction Understanding for\n  HOI Detection",
    "authors": [
      "Yue Liao",
      "Aixi Zhang",
      "Miao Lu",
      "Yongliang Wang",
      "Xiaobo Li",
      "Si Liu"
    ],
    "abstract": "  The task of Human-Object Interaction~(HOI) detection could be divided into\ntwo core problems, i.e., human-object association and interaction\nunderstanding. In this paper, we reveal and address the disadvantages of the\nconventional query-driven HOI detectors from the two aspects. For the\nassociation, previous two-branch methods suffer from complex and costly\npost-matching, while single-branch methods ignore the features distinction in\ndifferent tasks. We propose Guided-Embedding Network~(GEN) to attain a\ntwo-branch pipeline without post-matching. In GEN, we design an instance\ndecoder to detect humans and objects with two independent query sets and a\nposition Guided Embedding~(p-GE) to mark the human and object in the same\nposition as a pair. Besides, we design an interaction decoder to classify\ninteractions, where the interaction queries are made of instance Guided\nEmbeddings (i-GE) generated from the outputs of each instance decoder layer.\nFor the interaction understanding, previous methods suffer from long-tailed\ndistribution and zero-shot discovery. This paper proposes a Visual-Linguistic\nKnowledge Transfer (VLKT) training strategy to enhance interaction\nunderstanding by transferring knowledge from a visual-linguistic pre-trained\nmodel CLIP. In specific, we extract text embeddings for all labels with CLIP to\ninitialize the classifier and adopt a mimic loss to minimize the visual feature\ndistance between GEN and CLIP. As a result, GEN-VLKT outperforms the state of\nthe art by large margins on multiple datasets, e.g., +5.05 mAP on HICO-Det. The\nsource codes are available at https://github.com/YueLiao/gen-vlkt.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-26T01:04:13Z",
    "updated": "2022-04-14T13:07:54Z",
    "doi": null
  },
  "2111.00273": {
    "id": "http://arxiv.org/abs/2111.00273v4",
    "title": "Cross-Modality Fusion Transformer for Multispectral Object Detection",
    "authors": [
      "Fang Qingyun",
      "Han Dapeng",
      "Wang Zhaokui"
    ],
    "abstract": "  Multispectral image pairs can provide the combined information, making object\ndetection applications more reliable and robust in the open world. To fully\nexploit the different modalities, we present a simple yet effective\ncross-modality feature fusion approach, named Cross-Modality Fusion Transformer\n(CFT) in this paper. Unlike prior CNNs-based works, guided by the transformer\nscheme, our network learns long-range dependencies and integrates global\ncontextual information in the feature extraction stage. More importantly, by\nleveraging the self attention of the transformer, the network can naturally\ncarry out simultaneous intra-modality and inter-modality fusion, and robustly\ncapture the latent interactions between RGB and Thermal domains, thereby\nsignificantly improving the performance of multispectral object detection.\nExtensive experiments and ablation studies on multiple datasets demonstrate\nthat our approach is effective and achieves state-of-the-art detection\nperformance. Our code and models are available at\nhttps://github.com/DocF/multispectral-object-detection.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-30T15:34:12Z",
    "updated": "2022-10-04T09:52:39Z",
    "doi": null
  },
  "2007.14062": {
    "id": "http://arxiv.org/abs/2007.14062v2",
    "title": "Big Bird: Transformers for Longer Sequences",
    "authors": [
      "Manzil Zaheer",
      "Guru Guruganesh",
      "Avinava Dubey",
      "Joshua Ainslie",
      "Chris Alberti",
      "Santiago Ontanon",
      "Philip Pham",
      "Anirudh Ravula",
      "Qifan Wang",
      "Li Yang",
      "Amr Ahmed"
    ],
    "abstract": "  Transformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is\nthe quadratic dependency (mainly in terms of memory) on the sequence length due\nto their full attention mechanism. To remedy this, we propose, BigBird, a\nsparse attention mechanism that reduces this quadratic dependency to linear. We\nshow that BigBird is a universal approximator of sequence functions and is\nTuring complete, thereby preserving these properties of the quadratic, full\nattention model. Along the way, our theoretical analysis reveals some of the\nbenefits of having $O(1)$ global tokens (such as CLS), that attend to the\nentire sequence as part of the sparse attention mechanism. The proposed sparse\nattention can handle sequences of length up to 8x of what was previously\npossible using similar hardware. As a consequence of the capability to handle\nlonger context, BigBird drastically improves performance on various NLP tasks\nsuch as question answering and summarization. We also propose novel\napplications to genomics data.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-28T08:34:04Z",
    "updated": "2021-01-08T07:41:50Z",
    "doi": null
  },
  "2306.14898": {
    "id": "http://arxiv.org/abs/2306.14898v3",
    "title": "InterCode: Standardizing and Benchmarking Interactive Coding with\n  Execution Feedback",
    "authors": [
      "John Yang",
      "Akshara Prabhakar",
      "Karthik Narasimhan",
      "Shunyu Yao"
    ],
    "abstract": "  Humans write code in a fundamentally interactive manner and rely on constant\nexecution feedback to correct errors, resolve ambiguities, and decompose tasks.\nWhile LLMs have recently exhibited promising coding capabilities, current\ncoding benchmarks mostly consider a static instruction-to-code sequence\ntransduction process, which has the potential for error propagation and a\ndisconnect between the generated code and its final execution environment. To\naddress this gap, we introduce InterCode, a lightweight, flexible, and\neasy-to-use framework of interactive coding as a standard reinforcement\nlearning (RL) environment, with code as actions and execution feedback as\nobservations. Our framework is language and platform agnostic, uses\nself-contained Docker environments to provide safe and reproducible execution,\nand is compatible out-of-the-box with traditional seq2seq coding methods, while\nenabling the development of new methods for interactive code generation. We use\nInterCode to create three interactive code environments with Bash, SQL, and\nPython as action spaces, leveraging data from the static NL2Bash, Spider, and\nMBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating\nmultiple state-of-the-art LLMs configured with different prompting strategies\nsuch as ReAct and Plan & Solve. Our results showcase the benefits of\ninteractive code generation and demonstrate that InterCode can serve as a\nchallenging benchmark for advancing code understanding and generation\ncapabilities. InterCode is designed to be easily extensible and can even be\nused to create new tasks such as Capture the Flag, a popular coding puzzle that\nis inherently multi-step and involves multiple programming languages. Project\nsite with code and data: https://intercode-benchmark.github.io\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-26T17:59:50Z",
    "updated": "2023-10-30T17:52:18Z",
    "doi": null
  },
  "2211.16940": {
    "id": "http://arxiv.org/abs/2211.16940v3",
    "title": "DiffPose: Toward More Reliable 3D Pose Estimation",
    "authors": [
      "Jia Gong",
      "Lin Geng Foo",
      "Zhipeng Fan",
      "Qiuhong Ke",
      "Hossein Rahmani",
      "Jun Liu"
    ],
    "abstract": "  Monocular 3D human pose estimation is quite challenging due to the inherent\nambiguity and occlusion, which often lead to high uncertainty and\nindeterminacy. On the other hand, diffusion models have recently emerged as an\neffective tool for generating high-quality images from noise. Inspired by their\ncapability, we explore a novel pose estimation framework (DiffPose) that\nformulates 3D pose estimation as a reverse diffusion process. We incorporate\nnovel designs into our DiffPose to facilitate the diffusion process for 3D pose\nestimation: a pose-specific initialization of pose uncertainty distributions, a\nGaussian Mixture Model-based forward diffusion process, and a\ncontext-conditioned reverse diffusion process. Our proposed DiffPose\nsignificantly outperforms existing methods on the widely used pose estimation\nbenchmarks Human3.6M and MPI-INF-3DHP. Project page:\nhttps://gongjia0208.github.io/Diffpose/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-30T12:22:22Z",
    "updated": "2023-04-09T06:46:06Z",
    "doi": null
  },
  "2208.07339": {
    "id": "http://arxiv.org/abs/2208.07339v2",
    "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
    "authors": [
      "Tim Dettmers",
      "Mike Lewis",
      "Younes Belkada",
      "Luke Zettlemoyer"
    ],
    "abstract": "  Large language models have been widely adopted but require significant GPU\nmemory for inference. We develop a procedure for Int8 matrix multiplication for\nfeed-forward and attention projection layers in transformers, which cut the\nmemory needed for inference by half while retaining full precision performance.\nWith our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted\nto Int8, and used immediately without performance degradation. This is made\npossible by understanding and working around properties of highly systematic\nemergent features in transformer language models that dominate attention and\ntransformer predictive performance. To cope with these features, we develop a\ntwo-part quantization procedure, LLM.int8(). We first use vector-wise\nquantization with separate normalization constants for each inner product in\nthe matrix multiplication, to quantize most of the features. However, for the\nemergent outliers, we also include a new mixed-precision decomposition scheme,\nwhich isolates the outlier feature dimensions into a 16-bit matrix\nmultiplication while still more than 99.9% of values are multiplied in 8-bit.\nUsing LLM.int8(), we show empirically it is possible to perform inference in\nLLMs with up to 175B parameters without any performance degradation. This\nresult makes such models much more accessible, for example making it possible\nto use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our\nsoftware.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-15T17:08:50Z",
    "updated": "2022-11-10T18:14:31Z",
    "doi": null
  },
  "2203.10314": {
    "id": "http://arxiv.org/abs/2203.10314v1",
    "title": "Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from\n  Point Clouds",
    "authors": [
      "Chenhang He",
      "Ruihuang Li",
      "Shuai Li",
      "Lei Zhang"
    ],
    "abstract": "  Transformer has demonstrated promising performance in many 2D vision tasks.\nHowever, it is cumbersome to compute the self-attention on large-scale point\ncloud data because point cloud is a long sequence and unevenly distributed in\n3D space. To solve this issue, existing methods usually compute self-attention\nlocally by grouping the points into clusters of the same size, or perform\nconvolutional self-attention on a discretized representation. However, the\nformer results in stochastic point dropout, while the latter typically has\nnarrow attention fields. In this paper, we propose a novel voxel-based\narchitecture, namely Voxel Set Transformer (VoxSeT), to detect 3D objects from\npoint clouds by means of set-to-set translation. VoxSeT is built upon a\nvoxel-based set attention (VSA) module, which reduces the self-attention in\neach voxel by two cross-attentions and models features in a hidden space\ninduced by a group of latent codes. With the VSA module, VoxSeT can manage\nvoxelized point clusters with arbitrary size in a wide range, and process them\nin parallel with linear complexity. The proposed VoxSeT integrates the high\nperformance of transformer with the efficiency of voxel-based model, which can\nbe used as a good alternative to the convolutional and point-based backbones.\nVoxSeT reports competitive results on the KITTI and Waymo detection benchmarks.\nThe source codes can be found at \\url{https://github.com/skyhehe123/VoxSeT}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-19T12:31:46Z",
    "updated": "2022-03-19T12:31:46Z",
    "doi": null
  },
  "2407.11288": {
    "id": "http://arxiv.org/abs/2407.11288v1",
    "title": "Zero-Shot Adaptation for Approximate Posterior Sampling of Diffusion\n  Models in Inverse Problems",
    "authors": [
      "Ya\u015far Utku Al\u00e7alar",
      "Mehmet Ak\u00e7akaya"
    ],
    "abstract": "  Diffusion models have emerged as powerful generative techniques for solving\ninverse problems. Despite their success in a variety of inverse problems in\nimaging, these models require many steps to converge, leading to slow inference\ntime. Recently, there has been a trend in diffusion models for employing\nsophisticated noise schedules that involve more frequent iterations of\ntimesteps at lower noise levels, thereby improving image generation and\nconvergence speed. However, application of these ideas for solving inverse\nproblems with diffusion models remain challenging, as these noise schedules do\nnot perform well when using empirical tuning for the forward model\nlog-likelihood term weights. To tackle these challenges, we propose zero-shot\napproximate posterior sampling (ZAPS) that leverages connections to zero-shot\nphysics-driven deep learning. ZAPS fixes the number of sampling steps, and uses\nzero-shot training with a physics-guided loss function to learn log-likelihood\nweights at each irregular timestep. We apply ZAPS to the recently proposed\ndiffusion posterior sampling method as baseline, though ZAPS can also be used\nwith other posterior sampling diffusion models. We further approximate the\nHessian of the logarithm of the prior using a diagonalization approach with\nlearnable diagonal entries for computational efficiency. These parameters are\noptimized over a fixed number of epochs with a given computational budget. Our\nresults for various noisy inverse problems, including Gaussian and motion\ndeblurring, inpainting, and super-resolution show that ZAPS reduces inference\ntime, provides robustness to irregular noise schedules and improves\nreconstruction quality. Code is available at https://github.com/ualcalar17/ZAPS\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-16T00:09:37Z",
    "updated": "2024-07-16T00:09:37Z",
    "doi": null
  },
  "2312.12575": {
    "id": "http://arxiv.org/abs/2312.12575v3",
    "title": "LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities\n  (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks",
    "authors": [
      "Saad Ullah",
      "Mingji Han",
      "Saurabh Pujar",
      "Hammond Pearce",
      "Ayse Coskun",
      "Gianluca Stringhini"
    ],
    "abstract": "  Large Language Models (LLMs) have been suggested for use in automated\nvulnerability repair, but benchmarks showing they can consistently identify\nsecurity-related bugs are lacking. We thus develop SecLLMHolmes, a fully\nautomated evaluation framework that performs the most detailed investigation to\ndate on whether LLMs can reliably identify and reason about security-related\nbugs. We construct a set of 228 code scenarios and analyze eight of the most\ncapable LLMs across eight different investigative dimensions using our\nframework. Our evaluation shows LLMs provide non-deterministic responses,\nincorrect and unfaithful reasoning, and perform poorly in real-world scenarios.\nMost importantly, our findings reveal significant non-robustness in even the\nmost advanced models like `PaLM2' and `GPT-4': by merely changing function or\nvariable names, or by the addition of library functions in the source code,\nthese models can yield incorrect answers in 26% and 17% of cases, respectively.\nThese findings demonstrate that further LLM advances are needed before LLMs can\nbe used as general purpose security assistants.\n",
    "categories": [
      {
        "@term": "cs.CR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-19T20:19:43Z",
    "updated": "2024-07-24T07:49:14Z",
    "doi": null
  },
  "2310.01602": {
    "id": "http://arxiv.org/abs/2310.01602v1",
    "title": "CAT-LM: Training Language Models on Aligned Code And Tests",
    "authors": [
      "Nikitha Rao",
      "Kush Jain",
      "Uri Alon",
      "Claire Le Goues",
      "Vincent J. Hellendoorn"
    ],
    "abstract": "  Testing is an integral part of the software development process. Yet, writing\ntests is time-consuming and therefore often neglected. Classical test\ngeneration tools such as EvoSuite generate behavioral test suites by optimizing\nfor coverage, but tend to produce tests that are hard to understand. Language\nmodels trained on code can generate code that is highly similar to that written\nby humans, but current models are trained to generate each file separately, as\nis standard practice in natural language processing, and thus fail to consider\nthe code-under-test context when producing a test file. In this work, we\npropose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style\nlanguage model with 2.7 Billion parameters, trained on a corpus of Python and\nJava projects. We utilize a novel pretraining signal that explicitly considers\nthe mapping between code and test files when available. We also drastically\nincrease the maximum sequence length of inputs to 8,192 tokens, 4x more than\ntypical code generation models, to ensure that the code context is available to\nthe model when generating test code. We analyze its usefulness for realistic\napplications, showing that sampling with filtering (e.g., by compilability,\ncoverage) allows it to efficiently produce tests that achieve coverage similar\nto ones written by developers while resembling their writing style. By\nutilizing the code context, CAT-LM generates more valid tests than even much\nlarger language models trained with more data (CodeGen 16B and StarCoder) and\nsubstantially outperforms a recent test-specific model (TeCo) at test\ncompletion. Overall, our work highlights the importance of incorporating\nsoftware-specific insights when training language models for code and paves the\nway to more powerful automated test generation.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-02T19:52:22Z",
    "updated": "2023-10-02T19:52:22Z",
    "doi": null
  },
  "2408.10161": {
    "id": "http://arxiv.org/abs/2408.10161v2",
    "title": "NeuFlow v2: High-Efficiency Optical Flow Estimation on Edge Devices",
    "authors": [
      "Zhiyong Zhang",
      "Aniket Gupta",
      "Huaizu Jiang",
      "Hanumant Singh"
    ],
    "abstract": "  Real-time high-accuracy optical flow estimation is crucial for various\nreal-world applications. While recent learning-based optical flow methods have\nachieved high accuracy, they often come with significant computational costs.\nIn this paper, we propose a highly efficient optical flow method that balances\nhigh accuracy with reduced computational demands. Building upon NeuFlow v1, we\nintroduce new components including a much more light-weight backbone and a fast\nrefinement module. Both these modules help in keeping the computational demands\nlight while providing close to state of the art accuracy. Compares to other\nstate of the art methods, our model achieves a 10x-70x speedup while\nmaintaining comparable performance on both synthetic and real-world data. It is\ncapable of running at over 20 FPS on 512x384 resolution images on a Jetson Orin\nNano. The full training and evaluation code is available at\nhttps://github.com/neufieldrobotics/NeuFlow_v2.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-19T17:13:34Z",
    "updated": "2024-08-21T23:23:10Z",
    "doi": null
  },
  "2211.06146": {
    "id": "http://arxiv.org/abs/2211.06146v2",
    "title": "An unobtrusive quality supervision approach for medical image annotation",
    "authors": [
      "Sonja Kunzmann",
      "Mathias \u00d6ttl",
      "Prathmesh Madhu",
      "Felix Denzinger",
      "Andreas Maier"
    ],
    "abstract": "  Image annotation is one essential prior step to enable data-driven\nalgorithms. In medical imaging, having large and reliably annotated data sets\nis crucial to recognize various diseases robustly. However, annotator\nperformance varies immensely, thus impacts model training. Therefore, often\nmultiple annotators should be employed, which is however expensive and\nresource-intensive. Hence, it is desirable that users should annotate unseen\ndata and have an automated system to unobtrusively rate their performance\nduring this process. We examine such a system based on whole slide images\n(WSIs) showing lung fluid cells. We evaluate two methods the generation of\nsynthetic individual cell images: conditional Generative Adversarial Networks\nand Diffusion Models (DM). For qualitative and quantitative evaluation, we\nconduct a user study to highlight the suitability of generated cells. Users\ncould not detect 52.12% of generated images by DM proofing the feasibility to\nreplace the original cells with synthetic cells without being noticed.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-11T11:57:26Z",
    "updated": "2022-11-22T15:15:36Z",
    "doi": null
  },
  "1307.5551": {
    "id": "http://arxiv.org/abs/1307.5551v1",
    "title": "Regularized Discrete Optimal Transport",
    "authors": [
      "Sira Ferradans",
      "Nicolas Papadakis",
      "Gabriel Peyr\u00e9",
      "Jean-Fran\u00e7ois Aujol"
    ],
    "abstract": "  This article introduces a generalization of the discrete optimal transport,\nwith applications to color image manipulations. This new formulation includes a\nrelaxation of the mass conservation constraint and a regularization term. These\ntwo features are crucial for image processing tasks, which necessitate to take\ninto account families of multimodal histograms, with large mass variation\nacross modes.\n  The corresponding relaxed and regularized transportation problem is the\nsolution of a convex optimization problem. Depending on the regularization\nused, this minimization can be solved using standard linear programming methods\nor first order proximal splitting schemes.\n  The resulting transportation plan can be used as a color transfer map, which\nis robust to mass variation across images color palettes. Furthermore, the\nregularization of the transport plan helps to remove colorization artifacts due\nto noise amplification.\n  We also extend this framework to the computation of barycenters of\ndistributions. The barycenter is the solution of an optimization problem, which\nis separately convex with respect to the barycenter and the transportation\nplans, but not jointly convex. A block coordinate descent scheme converges to a\nstationary point of the energy. We show that the resulting algorithm can be\nused for color normalization across several images. The relaxed and regularized\nbarycenter defines a common color palette for those images. Applying color\ntransfer toward this average palette performs a color normalization of the\ninput images.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DM",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.OC",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2013-07-21T17:55:10Z",
    "updated": "2013-07-21T17:55:10Z",
    "doi": null
  },
  "2311.15510": {
    "id": "http://arxiv.org/abs/2311.15510v2",
    "title": "CaesarNeRF: Calibrated Semantic Representation for Few-shot\n  Generalizable Neural Rendering",
    "authors": [
      "Haidong Zhu",
      "Tianyu Ding",
      "Tianyi Chen",
      "Ilya Zharkov",
      "Ram Nevatia",
      "Luming Liang"
    ],
    "abstract": "  Generalizability and few-shot learning are key challenges in Neural Radiance\nFields (NeRF), often due to the lack of a holistic understanding in pixel-level\nrendering. We introduce CaesarNeRF, an end-to-end approach that leverages\nscene-level CAlibratEd SemAntic Representation along with pixel-level\nrepresentations to advance few-shot, generalizable neural rendering,\nfacilitating a holistic understanding without compromising high-quality\ndetails. CaesarNeRF explicitly models pose differences of reference views to\ncombine scene-level semantic representations, providing a calibrated holistic\nunderstanding. This calibration process aligns various viewpoints with precise\nlocation and is further enhanced by sequential refinement to capture varying\ndetails. Extensive experiments on public datasets, including LLFF, Shiny,\nmip-NeRF 360, and MVImgNet, show that CaesarNeRF delivers state-of-the-art\nperformance across varying numbers of reference views, proving effective even\nwith a single reference image.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-27T03:09:58Z",
    "updated": "2024-07-10T03:41:39Z",
    "doi": null
  },
  "2211.03264": {
    "id": "http://arxiv.org/abs/2211.03264v3",
    "title": "Few-shot Image Generation with Diffusion Models",
    "authors": [
      "Jingyuan Zhu",
      "Huimin Ma",
      "Jiansheng Chen",
      "Jian Yuan"
    ],
    "abstract": "  Denoising diffusion probabilistic models (DDPMs) have been proven capable of\nsynthesizing high-quality images with remarkable diversity when trained on\nlarge amounts of data. However, to our knowledge, few-shot image generation\ntasks have yet to be studied with DDPM-based approaches. Modern approaches are\nmainly built on Generative Adversarial Networks (GANs) and adapt models\npre-trained on large source domains to target domains using a few available\nsamples. In this paper, we make the first attempt to study when do DDPMs\noverfit and suffer severe diversity degradation as training data become scarce.\nThen we fine-tune DDPMs pre-trained on large source domains to solve the\noverfitting problem when training data is limited. Although the directly\nfine-tuned models accelerate convergence and improve generation quality and\ndiversity compared with training from scratch, they still fail to retain some\ndiverse features and can only produce coarse images. Therefore, we design a\nDDPM pairwise adaptation (DDPM-PA) approach to optimize few-shot DDPM domain\nadaptation. DDPM-PA efficiently preserves information learned from source\ndomains by keeping the relative pairwise distances between generated samples\nduring adaptation. Besides, DDPM-PA enhances the learning of high-frequency\ndetails from source models and limited training data. DDPM-PA further improves\ngeneration quality and diversity and achieves results better than current\nstate-of-the-art GAN-based approaches. We demonstrate the effectiveness of our\napproach on a series of few-shot image generation tasks qualitatively and\nquantitatively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-07T02:18:27Z",
    "updated": "2023-03-07T05:43:56Z",
    "doi": null
  },
  "2209.11133": {
    "id": "http://arxiv.org/abs/2209.11133v2",
    "title": "PACT: Perception-Action Causal Transformer for Autoregressive Robotics\n  Pre-Training",
    "authors": [
      "Rogerio Bonatti",
      "Sai Vemprala",
      "Shuang Ma",
      "Felipe Frujeri",
      "Shuhang Chen",
      "Ashish Kapoor"
    ],
    "abstract": "  Robotics has long been a field riddled with complex systems architectures\nwhose modules and connections, whether traditional or learning-based, require\nsignificant human expertise and prior knowledge. Inspired by large pre-trained\nlanguage models, this work introduces a paradigm for pre-training a general\npurpose representation that can serve as a starting point for multiple tasks on\na given robot. We present the Perception-Action Causal Transformer (PACT), a\ngenerative transformer-based architecture that aims to build representations\ndirectly from robot data in a self-supervised fashion. Through autoregressive\nprediction of states and actions over time, our model implicitly encodes\ndynamics and behaviors for a particular robot. Our experimental evaluation\nfocuses on the domain of mobile agents, where we show that this robot-specific\nrepresentation can function as a single starting point to achieve distinct\ntasks such as safe navigation, localization and mapping. We evaluate two form\nfactors: a wheeled robot that uses a LiDAR sensor as perception input (MuSHR),\nand a simulated agent that uses first-person RGB images (Habitat). We show that\nfinetuning small task-specific networks on top of the larger pretrained model\nresults in significantly better performance compared to training a single model\nfrom scratch for all tasks simultaneously, and comparable performance to\ntraining a separate large model for each task independently. By sharing a\ncommon good-quality representation across tasks we can lower overall model\ncapacity and speed up the real-time deployment of such systems.\n",
    "categories": [
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-22T16:20:17Z",
    "updated": "2022-09-23T18:14:39Z",
    "doi": null
  },
  "1603.01670": {
    "id": "http://arxiv.org/abs/1603.01670v2",
    "title": "Network Morphism",
    "authors": [
      "Tao Wei",
      "Changhu Wang",
      "Yong Rui",
      "Chang Wen Chen"
    ],
    "abstract": "  We present in this paper a systematic study on how to morph a well-trained\nneural network to a new one so that its network function can be completely\npreserved. We define this as \\emph{network morphism} in this research. After\nmorphing a parent network, the child network is expected to inherit the\nknowledge from its parent network and also has the potential to continue\ngrowing into a more powerful one with much shortened training time. The first\nrequirement for this network morphism is its ability to handle diverse morphing\ntypes of networks, including changes of depth, width, kernel size, and even\nsubnet. To meet this requirement, we first introduce the network morphism\nequations, and then develop novel morphing algorithms for all these morphing\ntypes for both classic and convolutional neural networks. The second\nrequirement for this network morphism is its ability to deal with non-linearity\nin a network. We propose a family of parametric-activation functions to\nfacilitate the morphing of any continuous non-linear activation neurons.\nExperimental results on benchmark datasets and typical neural networks\ndemonstrate the effectiveness of the proposed network morphism scheme.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.NE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-03-05T02:06:43Z",
    "updated": "2016-03-08T16:36:00Z",
    "doi": null
  },
  "2109.04398": {
    "id": "http://arxiv.org/abs/2109.04398v4",
    "title": "Neural-IMLS: Self-supervised Implicit Moving Least-Squares Network for\n  Surface Reconstruction",
    "authors": [
      "Zixiong Wang",
      "Pengfei Wang",
      "Pengshuai Wang",
      "Qiujie Dong",
      "Junjie Gao",
      "Shuangmin Chen",
      "Shiqing Xin",
      "Changhe Tu",
      "Wenping Wang"
    ],
    "abstract": "  Surface reconstruction is very challenging when the input point clouds,\nparticularly real scans, are noisy and lack normals. Observing that the\nMultilayer Perceptron (MLP) and the implicit moving least-square function\n(IMLS) provide a dual representation of the underlying surface, we introduce\nNeural-IMLS, a novel approach that directly learns the noise-resistant signed\ndistance function (SDF) from unoriented raw point clouds in a self-supervised\nfashion. We use the IMLS to regularize the distance values reported by the MLP\nwhile using the MLP to regularize the normals of the data points for running\nthe IMLS. We also prove that at the convergence, our neural network, benefiting\nfrom the mutual learning mechanism between the MLP and the IMLS, produces a\nfaithful SDF whose zero-level set approximates the underlying surface. We\nconducted extensive experiments on various benchmarks, including synthetic\nscans and real scans. The experimental results show that {\\em Neural-IMLS} can\nreconstruct faithful shapes on various benchmarks with noise and missing parts.\nThe source code can be found at~\\url{https://github.com/bearprin/Neural-IMLS}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-09T16:37:01Z",
    "updated": "2023-09-06T06:47:49Z",
    "doi": "10.1109/TVCG.2023.3284233"
  },
  "2202.00660": {
    "id": "http://arxiv.org/abs/2202.00660v3",
    "title": "Interactron: Embodied Adaptive Object Detection",
    "authors": [
      "Klemen Kotar",
      "Roozbeh Mottaghi"
    ],
    "abstract": "  Over the years various methods have been proposed for the problem of object\ndetection. Recently, we have witnessed great strides in this domain owing to\nthe emergence of powerful deep neural networks. However, there are typically\ntwo main assumptions common among these approaches. First, the model is trained\non a fixed training set and is evaluated on a pre-recorded test set. Second,\nthe model is kept frozen after the training phase, so no further updates are\nperformed after the training is finished. These two assumptions limit the\napplicability of these methods to real-world settings. In this paper, we\npropose Interactron, a method for adaptive object detection in an interactive\nsetting, where the goal is to perform object detection in images observed by an\nembodied agent navigating in different environments. Our idea is to continue\ntraining during inference and adapt the model at test time without any explicit\nsupervision via interacting with the environment. Our adaptive object detection\nmodel provides a 7.2 point improvement in AP (and 12.7 points in AP50) over\nDETR, a recent, high-performance object detector. Moreover, we show that our\nobject detection model adapts to environments with completely different\nappearance characteristics, and performs well in them. The code is available\nat: https://github.com/allenai/interactron .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-02-01T18:56:14Z",
    "updated": "2022-07-22T19:37:10Z",
    "doi": null
  },
  "1909.01387": {
    "id": "http://arxiv.org/abs/1909.01387v1",
    "title": "Making Efficient Use of Demonstrations to Solve Hard Exploration\n  Problems",
    "authors": [
      "Tom Le Paine",
      "Caglar Gulcehre",
      "Bobak Shahriari",
      "Misha Denil",
      "Matt Hoffman",
      "Hubert Soyer",
      "Richard Tanburn",
      "Steven Kapturowski",
      "Neil Rabinowitz",
      "Duncan Williams",
      "Gabriel Barth-Maron",
      "Ziyu Wang",
      "Nando de Freitas",
      "Worlds Team"
    ],
    "abstract": "  This paper introduces R2D3, an agent that makes efficient use of\ndemonstrations to solve hard exploration problems in partially observable\nenvironments with highly variable initial conditions. We also introduce a suite\nof eight tasks that combine these three properties, and show that R2D3 can\nsolve several of the tasks where other state of the art methods (both with and\nwithout demonstrations) fail to see even a single successful trajectory after\ntens of billions of steps of exploration.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-09-03T18:20:48Z",
    "updated": "2019-09-03T18:20:48Z",
    "doi": null
  },
  "2309.07920": {
    "id": "http://arxiv.org/abs/2309.07920v2",
    "title": "Large-Vocabulary 3D Diffusion Model with Transformer",
    "authors": [
      "Ziang Cao",
      "Fangzhou Hong",
      "Tong Wu",
      "Liang Pan",
      "Ziwei Liu"
    ],
    "abstract": "  Creating diverse and high-quality 3D assets with an automatic generative\nmodel is highly desirable. Despite extensive efforts on 3D generation, most\nexisting works focus on the generation of a single category or a few\ncategories. In this paper, we introduce a diffusion-based feed-forward\nframework for synthesizing massive categories of real-world 3D objects with a\nsingle generative model. Notably, there are three major challenges for this\nlarge-vocabulary 3D generation: a) the need for expressive yet efficient 3D\nrepresentation; b) large diversity in geometry and texture across categories;\nc) complexity in the appearances of real-world objects. To this end, we propose\na novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, for\nhandling challenges via three aspects. 1) Considering efficiency and\nrobustness, we adopt a revised triplane representation and improve the fitting\nspeed and accuracy. 2) To handle the drastic variations in geometry and\ntexture, we regard the features of all 3D objects as a combination of\ngeneralized 3D knowledge and specialized 3D features. To extract generalized 3D\nknowledge from diverse categories, we propose a novel 3D-aware transformer with\nshared cross-plane attention. It learns the cross-plane relations across\ndifferent planes and aggregates the generalized 3D knowledge with specialized\n3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhance\nthe generalized 3D knowledge in the encoded triplanes for handling categories\nwith complex appearances. Extensive experiments on ShapeNet and OmniObject3D\n(over 200 diverse real-world categories) convincingly demonstrate that a single\nDiffTF model achieves state-of-the-art large-vocabulary 3D object generation\nperformance with large diversity, rich semantics, and high quality.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-14T17:59:53Z",
    "updated": "2023-09-15T07:56:34Z",
    "doi": null
  },
  "2210.05063": {
    "id": "http://arxiv.org/abs/2210.05063v2",
    "title": "Improving Dense Contrastive Learning with Dense Negative Pairs",
    "authors": [
      "Berk Iskender",
      "Zhenlin Xu",
      "Simon Kornblith",
      "En-Hung Chu",
      "Maryam Khademi"
    ],
    "abstract": "  Many contrastive representation learning methods learn a single global\nrepresentation of an entire image. However, dense contrastive representation\nlearning methods such as DenseCL (Wang et al., 2021) can learn better\nrepresentations for tasks requiring stronger spatial localization of features,\nsuch as multi-label classification, detection, and segmentation. In this work,\nwe study how to improve the quality of the representations learned by DenseCL\nby modifying the training scheme and objective function, and propose DenseCL++.\nWe also conduct several ablation studies to better understand the effects of:\n(i) various techniques to form dense negative pairs among augmentations of\ndifferent images, (ii) cross-view dense negative and positive pairs, and (iii)\nan auxiliary reconstruction task. Our results show 3.5% and 4% mAP improvement\nover SimCLR (Chen et al., 2020a) andDenseCL in COCO multi-label classification.\nIn COCO and VOC segmentation tasks, we achieve 1.8% and 0.7% mIoU improvements\nover SimCLR, respectively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-11T00:26:59Z",
    "updated": "2023-01-10T23:47:45Z",
    "doi": null
  },
  "1705.10413": {
    "id": "http://arxiv.org/abs/1705.10413v1",
    "title": "Learning to Generate Chairs with Generative Adversarial Nets",
    "authors": [
      "Evgeny Zamyatin",
      "Andrey Filchenkov"
    ],
    "abstract": "  Generative adversarial networks (GANs) has gained tremendous popularity\nlately due to an ability to reinforce quality of its predictive model with\ngenerated objects and the quality of the generative model with and supervised\nfeedback. GANs allow to synthesize images with a high degree of realism.\nHowever, the learning process of such models is a very complicated optimization\nproblem and certain limitation for such models were found. It affects the\nchoice of certain layers and nonlinearities when designing architectures. In\nparticular, it does not allow to train convolutional GAN models with\nfully-connected hidden layers. In our work, we propose a modification of the\npreviously described set of rules, as well as new approaches to designing\narchitectures that will allow us to train more powerful GAN models. We show the\neffectiveness of our methods on the problem of synthesizing projections of 3D\nobjects with the possibility of interpolation by class and view point.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-05-29T23:15:50Z",
    "updated": "2017-05-29T23:15:50Z",
    "doi": null
  },
  "2205.10793": {
    "id": "http://arxiv.org/abs/2205.10793v2",
    "title": "Knowledge Distillation via the Target-aware Transformer",
    "authors": [
      "Sihao Lin",
      "Hongwei Xie",
      "Bing Wang",
      "Kaicheng Yu",
      "Xiaojun Chang",
      "Xiaodan Liang",
      "Gang Wang"
    ],
    "abstract": "  Knowledge distillation becomes a de facto standard to improve the performance\nof small neural networks. Most of the previous works propose to regress the\nrepresentational features from the teacher to the student in a one-to-one\nspatial matching fashion. However, people tend to overlook the fact that, due\nto the architecture differences, the semantic information on the same spatial\nlocation usually vary. This greatly undermines the underlying assumption of the\none-to-one distillation approach. To this end, we propose a novel one-to-all\nspatial matching knowledge distillation approach. Specifically, we allow each\npixel of the teacher feature to be distilled to all spatial locations of the\nstudent features given its similarity, which is generated from a target-aware\ntransformer. Our approach surpasses the state-of-the-art methods by a\nsignificant margin on various computer vision benchmarks, such as ImageNet,\nPascal VOC and COCOStuff10k. Code is available at\nhttps://github.com/sihaoevery/TaT.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-22T10:26:54Z",
    "updated": "2024-04-08T16:59:24Z",
    "doi": null
  },
  "2304.11029": {
    "id": "http://arxiv.org/abs/2304.11029v4",
    "title": "CLaMP: Contrastive Language-Music Pre-training for Cross-Modal Symbolic\n  Music Information Retrieval",
    "authors": [
      "Shangda Wu",
      "Dingyao Yu",
      "Xu Tan",
      "Maosong Sun"
    ],
    "abstract": "  We introduce CLaMP: Contrastive Language-Music Pre-training, which learns\ncross-modal representations between natural language and symbolic music using a\nmusic encoder and a text encoder trained jointly with a contrastive loss. To\npre-train CLaMP, we collected a large dataset of 1.4 million music-text pairs.\nIt employed text dropout as a data augmentation technique and bar patching to\nefficiently represent music data which reduces sequence length to less than\n10\\%. In addition, we developed a masked music model pre-training objective to\nenhance the music encoder's comprehension of musical context and structure.\nCLaMP integrates textual information to enable semantic search and zero-shot\nclassification for symbolic music, surpassing the capabilities of previous\nmodels. To support the evaluation of semantic search and music classification,\nwe publicly release WikiMusicText (WikiMT), a dataset of 1010 lead sheets in\nABC notation, each accompanied by a title, artist, genre, and description. In\ncomparison to state-of-the-art models that require fine-tuning, zero-shot CLaMP\ndemonstrated comparable or superior performance on score-oriented datasets. Our\nmodels and code are available at\nhttps://github.com/microsoft/muzic/tree/main/clamp.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-21T15:23:00Z",
    "updated": "2023-10-18T17:16:28Z",
    "doi": null
  },
  "1612.06778": {
    "id": "http://arxiv.org/abs/1612.06778v3",
    "title": "SCDV : Sparse Composite Document Vectors using soft clustering over\n  distributional representations",
    "authors": [
      "Dheeraj Mekala",
      "Vivek Gupta",
      "Bhargavi Paranjape",
      "Harish Karnick"
    ],
    "abstract": "  We present a feature vector formation technique for documents - Sparse\nComposite Document Vector (SCDV) - which overcomes several shortcomings of the\ncurrent distributional paragraph vector representations that are widely used\nfor text representation. In SCDV, word embedding's are clustered to capture\nmultiple semantic contexts in which words occur. They are then chained together\nto form document topic-vectors that can express complex, multi-topic documents.\nThrough extensive experiments on multi-class and multi-label classification\ntasks, we outperform the previous state-of-the-art method, NTSG (Liu et al.,\n2015a). We also show that SCDV embedding's perform well on heterogeneous tasks\nlike Topic Coherence, context-sensitive Learning and Information Retrieval.\nMoreover, we achieve significant reduction in training and prediction times\ncompared to other representation methods. SCDV achieves best of both worlds -\nbetter performance with lower time and space complexity.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-12-20T17:38:57Z",
    "updated": "2017-05-12T09:48:04Z",
    "doi": null
  },
  "2007.01758": {
    "id": "http://arxiv.org/abs/2007.01758v1",
    "title": "Collaborative Learning for Faster StyleGAN Embedding",
    "authors": [
      "Shanyan Guan",
      "Ying Tai",
      "Bingbing Ni",
      "Feida Zhu",
      "Feiyue Huang",
      "Xiaokang Yang"
    ],
    "abstract": "  The latent code of the recent popular model StyleGAN has learned disentangled\nrepresentations thanks to the multi-layer style-based generator. Embedding a\ngiven image back to the latent space of StyleGAN enables wide interesting\nsemantic image editing applications. Although previous works are able to yield\nimpressive inversion results based on an optimization framework, which however\nsuffers from the efficiency issue. In this work, we propose a novel\ncollaborative learning framework that consists of an efficient embedding\nnetwork and an optimization-based iterator. On one hand, with the progress of\ntraining, the embedding network gives a reasonable latent code initialization\nfor the iterator. On the other hand, the updated latent code from the iterator\nin turn supervises the embedding network. In the end, high-quality latent code\ncan be obtained efficiently with a single forward pass through our embedding\nnetwork. Extensive experiments demonstrate the effectiveness and efficiency of\nour work.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-03T15:27:37Z",
    "updated": "2020-07-03T15:27:37Z",
    "doi": null
  },
  "1604.06174": {
    "id": "http://arxiv.org/abs/1604.06174v2",
    "title": "Training Deep Nets with Sublinear Memory Cost",
    "authors": [
      "Tianqi Chen",
      "Bing Xu",
      "Chiyuan Zhang",
      "Carlos Guestrin"
    ],
    "abstract": "  We propose a systematic approach to reduce the memory consumption of deep\nneural network training. Specifically, we design an algorithm that costs\nO(sqrt(n)) memory to train a n layer network, with only the computational cost\nof an extra forward pass per mini-batch. As many of the state-of-the-art models\nhit the upper bound of the GPU memory, our algorithm allows deeper and more\ncomplex models to be explored, and helps advance the innovations in deep\nlearning research. We focus on reducing the memory cost to store the\nintermediate feature maps and gradients during training. Computation graph\nanalysis is used for automatic in-place operation and memory sharing\noptimizations. We show that it is possible to trade computation for memory -\ngiving a more memory efficient training algorithm with a little extra\ncomputation cost. In the extreme case, our analysis also shows that the memory\nconsumption can be reduced to O(log n) with as little as O(n log n) extra cost\nfor forward computation. Our experiments show that we can reduce the memory\ncost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent\nadditional running time cost on ImageNet problems. Similarly, significant\nmemory cost reduction is observed in training complex recurrent neural networks\non very long sequences.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-04-21T04:15:27Z",
    "updated": "2016-04-22T19:21:36Z",
    "doi": null
  },
  "1711.10337": {
    "id": "http://arxiv.org/abs/1711.10337v4",
    "title": "Are GANs Created Equal? A Large-Scale Study",
    "authors": [
      "Mario Lucic",
      "Karol Kurach",
      "Marcin Michalski",
      "Sylvain Gelly",
      "Olivier Bousquet"
    ],
    "abstract": "  Generative adversarial networks (GAN) are a powerful subclass of generative\nmodels. Despite a very rich research activity leading to numerous interesting\nGAN algorithms, it is still very hard to assess which algorithm(s) perform\nbetter than others. We conduct a neutral, multi-faceted large-scale empirical\nstudy on state-of-the art models and evaluation measures. We find that most\nmodels can reach similar scores with enough hyperparameter optimization and\nrandom restarts. This suggests that improvements can arise from a higher\ncomputational budget and tuning more than fundamental algorithmic changes. To\novercome some limitations of the current metrics, we also propose several data\nsets on which precision and recall can be computed. Our experimental results\nsuggest that future GAN research should be based on more systematic and\nobjective evaluation procedures. Finally, we did not find evidence that any of\nthe tested algorithms consistently outperforms the non-saturating GAN\nintroduced in \\cite{goodfellow2014generative}.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-11-28T15:19:53Z",
    "updated": "2018-10-29T15:34:15Z",
    "doi": null
  },
  "2207.02696": {
    "id": "http://arxiv.org/abs/2207.02696v1",
    "title": "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for\n  real-time object detectors",
    "authors": [
      "Chien-Yao Wang",
      "Alexey Bochkovskiy",
      "Hong-Yuan Mark Liao"
    ],
    "abstract": "  YOLOv7 surpasses all known object detectors in both speed and accuracy in the\nrange from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all\nknown real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6\nobject detector (56 FPS V100, 55.9% AP) outperforms both transformer-based\ndetector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed\nand 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask\nR-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as\nwell as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR,\nDeformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors\nin speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from\nscratch without using any other datasets or pre-trained weights. Source code is\nreleased in https://github.com/WongKinYiu/yolov7.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-06T14:01:58Z",
    "updated": "2022-07-06T14:01:58Z",
    "doi": null
  },
  "2312.00206": {
    "id": "http://arxiv.org/abs/2312.00206v2",
    "title": "SparseGS: Real-Time 360\u00b0 Sparse View Synthesis using Gaussian\n  Splatting",
    "authors": [
      "Haolin Xiong",
      "Sairisheek Muttukuru",
      "Rishi Upadhyay",
      "Pradyumna Chari",
      "Achuta Kadambi"
    ],
    "abstract": "  The problem of novel view synthesis has grown significantly in popularity\nrecently with the introduction of Neural Radiance Fields (NeRFs) and other\nimplicit scene representation methods. A recent advance, 3D Gaussian Splatting\n(3DGS), leverages an explicit representation to achieve real-time rendering\nwith high-quality results. However, 3DGS still requires an abundance of\ntraining views to generate a coherent scene representation. In few shot\nsettings, similar to NeRF, 3DGS tends to overfit to training views, causing\nbackground collapse and excessive floaters, especially as the number of\ntraining views are reduced. We propose a method to enable training coherent\n3DGS-based radiance fields of 360-degree scenes from sparse training views. We\nintegrate depth priors with generative and explicit constraints to reduce\nbackground collapse, remove floaters, and enhance consistency from unseen\nviewpoints. Experiments show that our method outperforms base 3DGS by 6.4% in\nLPIPS and by 12.2% in PSNR, and NeRF-based methods by at least 17.6% in LPIPS\non the MipNeRF-360 dataset with substantially less training and inference cost.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-30T21:38:22Z",
    "updated": "2024-05-13T05:11:37Z",
    "doi": null
  },
  "2011.06391": {
    "id": "http://arxiv.org/abs/2011.06391v2",
    "title": "FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph\n  Neural Networks",
    "authors": [
      "Md. Khaledur Rahman",
      "Majedul Haque Sujon",
      "Ariful Azad"
    ],
    "abstract": "  We develop a fused matrix multiplication kernel that unifies sampled\ndense-dense matrix multiplication and sparse-dense matrix multiplication under\na single operation called FusedMM. By using user-defined functions, FusedMM can\ncapture almost all computational patterns needed by popular graph embedding and\nGNN approaches. FusedMM is an order of magnitude faster than its equivalent\nkernels in Deep Graph Library. The superior performance of FusedMM comes from\nthe low-level vectorized kernels, a suitable load balancing scheme and an\nefficient utilization of the memory bandwidth. FusedMM can tune its performance\nusing a code generator and perform equally well on Intel, AMD and ARM\nprocessors. FusedMM speeds up an end-to-end graph embedding algorithm by up to\n28x on different processors.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-11-07T18:06:57Z",
    "updated": "2021-10-27T01:35:27Z",
    "doi": null
  },
  "2212.04098": {
    "id": "http://arxiv.org/abs/2212.04098v3",
    "title": "EPCL: Frozen CLIP Transformer is An Efficient Point Cloud Encoder",
    "authors": [
      "Xiaoshui Huang",
      "Zhou Huang",
      "Sheng Li",
      "Wentao Qu",
      "Tong He",
      "Yuenan Hou",
      "Yifan Zuo",
      "Wanli Ouyang"
    ],
    "abstract": "  The pretrain-finetune paradigm has achieved great success in NLP and 2D image\nfields because of the high-quality representation ability and transferability\nof their pretrained models. However, pretraining such a strong model is\ndifficult in the 3D point cloud field due to the limited amount of point cloud\nsequences. This paper introduces \\textbf{E}fficient \\textbf{P}oint\n\\textbf{C}loud \\textbf{L}earning (EPCL), an effective and efficient point cloud\nlearner for directly training high-quality point cloud models with a frozen\nCLIP transformer. Our EPCL connects the 2D and 3D modalities by semantically\naligning the image features and point cloud features without paired 2D-3D data.\nSpecifically, the input point cloud is divided into a series of local patches,\nwhich are converted to token embeddings by the designed point cloud tokenizer.\nThese token embeddings are concatenated with a task token and fed into the\nfrozen CLIP transformer to learn point cloud representation. The intuition is\nthat the proposed point cloud tokenizer projects the input point cloud into a\nunified token space that is similar to the 2D images. Comprehensive experiments\non 3D detection, semantic segmentation, classification and few-shot learning\ndemonstrate that the CLIP transformer can serve as an efficient point cloud\nencoder and our method achieves promising performance on both indoor and\noutdoor benchmarks. In particular, performance gains brought by our EPCL are\n$\\textbf{19.7}$ AP$_{50}$ on ScanNet V2 detection, $\\textbf{4.4}$ mIoU on S3DIS\nsegmentation and $\\textbf{1.2}$ mIoU on SemanticKITTI segmentation compared to\ncontemporary pretrained models. Code is available at\n\\url{https://github.com/XiaoshuiHuang/EPCL}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-08T06:27:11Z",
    "updated": "2023-12-10T16:47:58Z",
    "doi": null
  },
  "2003.11080": {
    "id": "http://arxiv.org/abs/2003.11080v5",
    "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating\n  Cross-lingual Generalization",
    "authors": [
      "Junjie Hu",
      "Sebastian Ruder",
      "Aditya Siddhant",
      "Graham Neubig",
      "Orhan Firat",
      "Melvin Johnson"
    ],
    "abstract": "  Much recent progress in applications of machine learning models to NLP has\nbeen driven by benchmarks that evaluate models across a wide variety of tasks.\nHowever, these broad-coverage benchmarks have been mostly limited to English,\nand despite an increasing interest in multilingual models, a benchmark that\nenables the comprehensive evaluation of such methods on a diverse range of\nlanguages and tasks is still missing. To this end, we introduce the\nCross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a\nmulti-task benchmark for evaluating the cross-lingual generalization\ncapabilities of multilingual representations across 40 languages and 9 tasks.\nWe demonstrate that while models tested on English reach human performance on\nmany tasks, there is still a sizable gap in the performance of cross-lingually\ntransferred models, particularly on syntactic and sentence retrieval tasks.\nThere is also a wide spread of results across languages. We release the\nbenchmark to encourage research on cross-lingual learning methods that transfer\nlinguistic knowledge across a diverse and representative set of languages and\ntasks.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-03-24T19:09:37Z",
    "updated": "2020-09-04T17:56:31Z",
    "doi": null
  },
  "1906.02940": {
    "id": "http://arxiv.org/abs/1906.02940v3",
    "title": "Selfie: Self-supervised Pretraining for Image Embedding",
    "authors": [
      "Trieu H. Trinh",
      "Minh-Thang Luong",
      "Quoc V. Le"
    ],
    "abstract": "  We introduce a pretraining technique called Selfie, which stands for SELFie\nsupervised Image Embedding. Selfie generalizes the concept of masked language\nmodeling of BERT (Devlin et al., 2019) to continuous data, such as images, by\nmaking use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given\nmasked-out patches in an input image, our method learns to select the correct\npatch, among other \"distractor\" patches sampled from the same image, to fill in\nthe masked location. This classification objective sidesteps the need for\npredicting exact pixel values of the target patches. The pretraining\narchitecture of Selfie includes a network of convolutional blocks to process\npatches followed by an attention pooling network to summarize the content of\nunmasked patches before predicting masked ones. During finetuning, we reuse the\nconvolutional weights found by pretraining. We evaluate Selfie on three\nbenchmarks (CIFAR-10, ImageNet 32 x 32, and ImageNet 224 x 224) with varying\namounts of labeled data, from 5% to 100% of the training sets. Our pretraining\nmethod provides consistent improvements to ResNet-50 across all settings\ncompared to the standard supervised training of the same network. Notably, on\nImageNet 224 x 224 with 60 examples per class (5%), our method improves the\nmean accuracy of ResNet-50 from 35.6% to 46.7%, an improvement of 11.1 points\nin absolute accuracy. Our pretraining method also improves ResNet-50 training\nstability, especially on low data regime, by significantly lowering the\nstandard deviation of test accuracies across different runs.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-06-07T07:47:24Z",
    "updated": "2019-07-27T08:03:46Z",
    "doi": null
  },
  "2108.07058": {
    "id": "http://arxiv.org/abs/2108.07058v2",
    "title": "FaPN: Feature-aligned Pyramid Network for Dense Image Prediction",
    "authors": [
      "Shihua Huang",
      "Zhichao Lu",
      "Ran Cheng",
      "Cheng He"
    ],
    "abstract": "  Recent advancements in deep neural networks have made remarkable\nleap-forwards in dense image prediction. However, the issue of feature\nalignment remains as neglected by most existing approaches for simplicity.\nDirect pixel addition between upsampled and local features leads to feature\nmaps with misaligned contexts that, in turn, translate to mis-classifications\nin prediction, especially on object boundaries. In this paper, we propose a\nfeature alignment module that learns transformation offsets of pixels to\ncontextually align upsampled higher-level features; and another feature\nselection module to emphasize the lower-level features with rich spatial\ndetails. We then integrate these two modules in a top-down pyramidal\narchitecture and present the Feature-aligned Pyramid Network (FaPN). Extensive\nexperimental evaluations on four dense prediction tasks and four datasets have\ndemonstrated the efficacy of FaPN, yielding an overall improvement of 1.2 - 2.6\npoints in AP / mIoU over FPN when paired with Faster / Mask R-CNN. In\nparticular, our FaPN achieves the state-of-the-art of 56.7% mIoU on ADE20K when\nintegrated within Mask-Former. The code is available from\nhttps://github.com/EMI-Group/FaPN.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-08-16T12:52:42Z",
    "updated": "2021-08-17T13:11:34Z",
    "doi": null
  },
  "2107.06263": {
    "id": "http://arxiv.org/abs/2107.06263v3",
    "title": "CMT: Convolutional Neural Networks Meet Vision Transformers",
    "authors": [
      "Jianyuan Guo",
      "Kai Han",
      "Han Wu",
      "Yehui Tang",
      "Xinghao Chen",
      "Yunhe Wang",
      "Chang Xu"
    ],
    "abstract": "  Vision transformers have been successfully applied to image recognition tasks\ndue to their ability to capture long-range dependencies within an image.\nHowever, there are still gaps in both performance and computational cost\nbetween transformers and existing convolutional neural networks (CNNs). In this\npaper, we aim to address this issue and develop a network that can outperform\nnot only the canonical transformers, but also the high-performance\nconvolutional models. We propose a new transformer based hybrid network by\ntaking advantage of transformers to capture long-range dependencies, and of\nCNNs to model local features. Furthermore, we scale it to obtain a family of\nmodels, called CMTs, obtaining much better accuracy and efficiency than\nprevious convolution and transformer based models. In particular, our CMT-S\nachieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on\nFLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S\nalso generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%),\nand other challenging vision datasets such as COCO (44.3% mAP), with\nconsiderably less computational cost.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-07-13T17:47:19Z",
    "updated": "2022-06-14T14:05:23Z",
    "doi": null
  },
  "2209.04881": {
    "id": "http://arxiv.org/abs/2209.04881v1",
    "title": "On The Computational Complexity of Self-Attention",
    "authors": [
      "Feyza Duman Keles",
      "Pruthuvi Mahesakya Wijewardena",
      "Chinmay Hegde"
    ],
    "abstract": "  Transformer architectures have led to remarkable progress in many\nstate-of-art applications. However, despite their successes, modern\ntransformers rely on the self-attention mechanism, whose time- and\nspace-complexity is quadratic in the length of the input. Several approaches\nhave been proposed to speed up self-attention mechanisms to achieve\nsub-quadratic running time; however, the large majority of these works are not\naccompanied by rigorous error guarantees. In this work, we establish lower\nbounds on the computational complexity of self-attention in a number of\nscenarios. We prove that the time complexity of self-attention is necessarily\nquadratic in the input length, unless the Strong Exponential Time Hypothesis\n(SETH) is false. This argument holds even if the attention computation is\nperformed only approximately, and for a variety of attention mechanisms. As a\ncomplement to our lower bounds, we show that it is indeed possible to\napproximate dot-product self-attention using finite Taylor series in\nlinear-time, at the cost of having an exponential dependence on the polynomial\norder.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CC",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-11T14:38:10Z",
    "updated": "2022-09-11T14:38:10Z",
    "doi": null
  },
  "2404.12642": {
    "id": "http://arxiv.org/abs/2404.12642v1",
    "title": "Cooperative Sentiment Agents for Multimodal Sentiment Analysis",
    "authors": [
      "Shanmin Wang",
      "Hui Shuai",
      "Qingshan Liu",
      "Fei Wang"
    ],
    "abstract": "  In this paper, we propose a new Multimodal Representation Learning (MRL)\nmethod for Multimodal Sentiment Analysis (MSA), which facilitates the adaptive\ninteraction between modalities through Cooperative Sentiment Agents, named\nCo-SA. Co-SA comprises two critical components: the Sentiment Agents\nEstablishment (SAE) phase and the Sentiment Agents Cooperation (SAC) phase.\nDuring the SAE phase, each sentiment agent deals with an unimodal signal and\nhighlights explicit dynamic sentiment variations within the modality via the\nModality-Sentiment Disentanglement (MSD) and Deep Phase Space Reconstruction\n(DPSR) modules. Subsequently, in the SAC phase, Co-SA meticulously designs\ntask-specific interaction mechanisms for sentiment agents so that coordinating\nmultimodal signals to learn the joint representation. Specifically, Co-SA\nequips an independent policy model for each sentiment agent that captures\nsignificant properties within the modality. These policies are optimized\nmutually through the unified reward adaptive to downstream tasks. Benefitting\nfrom the rewarding mechanism, Co-SA transcends the limitation of pre-defined\nfusion modes and adaptively captures unimodal properties for MRL in the\nmultimodal interaction setting. To demonstrate the effectiveness of Co-SA, we\napply it to address Multimodal Sentiment Analysis (MSA) and Multimodal Emotion\nRecognition (MER) tasks. Our comprehensive experimental results demonstrate\nthat Co-SA excels at discovering diverse cross-modal features, encompassing\nboth common and complementary aspects. The code can be available at\nhttps://github.com/smwanghhh/Co-SA.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-19T05:48:09Z",
    "updated": "2024-04-19T05:48:09Z",
    "doi": null
  },
  "2308.00692": {
    "id": "http://arxiv.org/abs/2308.00692v3",
    "title": "LISA: Reasoning Segmentation via Large Language Model",
    "authors": [
      "Xin Lai",
      "Zhuotao Tian",
      "Yukang Chen",
      "Yanwei Li",
      "Yuhui Yuan",
      "Shu Liu",
      "Jiaya Jia"
    ],
    "abstract": "  Although perception systems have made remarkable advancements in recent\nyears, they still rely on explicit human instruction or pre-defined categories\nto identify the target objects before executing visual recognition tasks. Such\nsystems cannot actively reason and comprehend implicit user intention. In this\nwork, we propose a new segmentation task -- reasoning segmentation. The task is\ndesigned to output a segmentation mask given a complex and implicit query text.\nFurthermore, we establish a benchmark comprising over one thousand\nimage-instruction-mask data samples, incorporating intricate reasoning and\nworld knowledge for evaluation purposes. Finally, we present LISA: large\nLanguage Instructed Segmentation Assistant, which inherits the language\ngeneration capabilities of multimodal Large Language Models (LLMs) while also\npossessing the ability to produce segmentation masks. We expand the original\nvocabulary with a <SEG> token and propose the embedding-as-mask paradigm to\nunlock the segmentation capability. Remarkably, LISA can handle cases involving\ncomplex reasoning and world knowledge. Also, it demonstrates robust zero-shot\ncapability when trained exclusively on reasoning-free datasets. In addition,\nfine-tuning the model with merely 239 reasoning segmentation data samples\nresults in further performance enhancement. Both quantitative and qualitative\nexperiments show our method effectively unlocks new reasoning segmentation\ncapabilities for multimodal LLMs. Code, models, and data are available at\nhttps://github.com/dvlab-research/LISA.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-01T17:50:17Z",
    "updated": "2024-05-01T05:10:13Z",
    "doi": null
  },
  "2404.03566": {
    "id": "http://arxiv.org/abs/2404.03566v1",
    "title": "PointInfinity: Resolution-Invariant Point Diffusion Models",
    "authors": [
      "Zixuan Huang",
      "Justin Johnson",
      "Shoubhik Debnath",
      "James M. Rehg",
      "Chao-Yuan Wu"
    ],
    "abstract": "  We present PointInfinity, an efficient family of point cloud diffusion\nmodels. Our core idea is to use a transformer-based architecture with a\nfixed-size, resolution-invariant latent representation. This enables efficient\ntraining with low-resolution point clouds, while allowing high-resolution point\nclouds to be generated during inference. More importantly, we show that scaling\nthe test-time resolution beyond the training resolution improves the fidelity\nof generated point clouds and surfaces. We analyze this phenomenon and draw a\nlink to classifier-free guidance commonly used in diffusion models,\ndemonstrating that both allow trading off fidelity and variability during\ninference. Experiments on CO3D show that PointInfinity can efficiently generate\nhigh-resolution point clouds (up to 131k points, 31 times more than Point-E)\nwith state-of-the-art quality.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-04T16:24:32Z",
    "updated": "2024-04-04T16:24:32Z",
    "doi": null
  },
  "2205.09712": {
    "id": "http://arxiv.org/abs/2205.09712v1",
    "title": "Selection-Inference: Exploiting Large Language Models for Interpretable\n  Logical Reasoning",
    "authors": [
      "Antonia Creswell",
      "Murray Shanahan",
      "Irina Higgins"
    ],
    "abstract": "  Large language models (LLMs) have been shown to be capable of impressive\nfew-shot generalisation to new tasks. However, they still tend to perform\npoorly on multi-step logical reasoning problems. Here we carry out a\ncomprehensive evaluation of LLMs on 50 tasks that probe different aspects of\nlogical reasoning. We show that language models tend to perform fairly well at\nsingle step inference or entailment tasks, but struggle to chain together\nmultiple reasoning steps to solve more complex problems. In light of this, we\npropose a Selection-Inference (SI) framework that exploits pre-trained LLMs as\ngeneral processing modules, and alternates between selection and inference to\ngenerate a series of interpretable, casual reasoning steps leading to the final\nanswer. We show that a 7B parameter LLM used within the SI framework in a\n5-shot generalisation setting, with no fine-tuning, yields a performance\nimprovement of over 100% compared to an equivalent vanilla baseline on a suite\nof 10 logical reasoning tasks. The same model in the same setting even\noutperforms a significantly larger 280B parameter baseline on the same suite of\ntasks. Moreover, answers produced by the SI framework are accompanied by a\ncausal natural-language-based reasoning trace, which has important implications\nfor the safety and trustworthiness of the system.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-19T17:25:28Z",
    "updated": "2022-05-19T17:25:28Z",
    "doi": null
  },
  "1908.04471": {
    "id": "http://arxiv.org/abs/1908.04471v2",
    "title": "Einconv: Exploring Unexplored Tensor Network Decompositions for\n  Convolutional Neural Networks",
    "authors": [
      "Kohei Hayashi",
      "Taiki Yamaguchi",
      "Yohei Sugawara",
      "Shin-ichi Maeda"
    ],
    "abstract": "  Tensor decomposition methods are widely used for model compression and fast\ninference in convolutional neural networks (CNNs). Although many decompositions\nare conceivable, only CP decomposition and a few others have been applied in\npractice, and no extensive comparisons have been made between available\nmethods. Previous studies have not determined how many decompositions are\navailable, nor which of them is optimal. In this study, we first characterize a\ndecomposition class specific to CNNs by adopting a flexible graphical notation.\nThe class includes such well-known CNN modules as depthwise separable\nconvolution layers and bottleneck layers, but also previously unknown modules\nwith nonlinear activations. We also experimentally compare the tradeoff between\nprediction accuracy and time/space complexity for modules found by enumerating\nall possible decompositions, or by using a neural architecture search. We find\nsome nonlinear decompositions outperform existing ones.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-08-13T03:11:46Z",
    "updated": "2019-11-27T09:08:40Z",
    "doi": null
  },
  "2005.00052": {
    "id": "http://arxiv.org/abs/2005.00052v3",
    "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
    "authors": [
      "Jonas Pfeiffer",
      "Ivan Vuli\u0107",
      "Iryna Gurevych",
      "Sebastian Ruder"
    ],
    "abstract": "  The main goal behind state-of-the-art pre-trained multilingual models such as\nmultilingual BERT and XLM-R is enabling and bootstrapping NLP applications in\nlow-resource languages through zero-shot or few-shot cross-lingual transfer.\nHowever, due to limited model capacity, their transfer performance is the\nweakest exactly on such low-resource languages and languages unseen during\npre-training. We propose MAD-X, an adapter-based framework that enables high\nportability and parameter-efficient transfer to arbitrary tasks and languages\nby learning modular language and task representations. In addition, we\nintroduce a novel invertible adapter architecture and a strong baseline method\nfor adapting a pre-trained multilingual model to a new language. MAD-X\noutperforms the state of the art in cross-lingual transfer across a\nrepresentative set of typologically diverse languages on named entity\nrecognition and causal commonsense reasoning, and achieves competitive results\non question answering. Our code and adapters are available at AdapterHub.ml\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-30T18:54:43Z",
    "updated": "2020-10-06T10:17:45Z",
    "doi": null
  },
  "2305.17858": {
    "id": "http://arxiv.org/abs/2305.17858v1",
    "title": "FastMESH: Fast Surface Reconstruction by Hexagonal Mesh-based Neural\n  Rendering",
    "authors": [
      "Yisu Zhang",
      "Jianke Zhu",
      "Lixiang Lin"
    ],
    "abstract": "  Despite the promising results of multi-view reconstruction, the recent neural\nrendering-based methods, such as implicit surface rendering (IDR) and volume\nrendering (NeuS), not only incur a heavy computational burden on training but\nalso have the difficulties in disentangling the geometric and appearance.\nAlthough having achieved faster training speed than implicit representation and\nhash coding, the explicit voxel-based method obtains the inferior results on\nrecovering surface. To address these challenges, we propose an effective\nmesh-based neural rendering approach, named FastMESH, which only samples at the\nintersection of ray and mesh. A coarse-to-fine scheme is introduced to\nefficiently extract the initial mesh by space carving. More importantly, we\nsuggest a hexagonal mesh model to preserve surface regularity by constraining\nthe second-order derivatives of vertices, where only low level of positional\nencoding is engaged for neural rendering. The experiments demonstrate that our\napproach achieves the state-of-the-art results on both reconstruction and novel\nview synthesis. Besides, we obtain 10-fold acceleration on training comparing\nto the implicit representation-based methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-29T02:43:14Z",
    "updated": "2023-05-29T02:43:14Z",
    "doi": null
  },
  "2203.16248": {
    "id": "http://arxiv.org/abs/2203.16248v1",
    "title": "InstaFormer: Instance-Aware Image-to-Image Translation with Transformer",
    "authors": [
      "Soohyun Kim",
      "Jongbeom Baek",
      "Jihye Park",
      "Gyeongnyeon Kim",
      "Seungryong Kim"
    ],
    "abstract": "  We present a novel Transformer-based network architecture for instance-aware\nimage-to-image translation, dubbed InstaFormer, to effectively integrate\nglobal- and instance-level information. By considering extracted content\nfeatures from an image as tokens, our networks discover global consensus of\ncontent features by considering context information through a self-attention\nmodule in Transformers. By augmenting such tokens with an instance-level\nfeature extracted from the content feature with respect to bounding box\ninformation, our framework is capable of learning an interaction between object\ninstances and the global image, thus boosting the instance-awareness. We\nreplace layer normalization (LayerNorm) in standard Transformers with adaptive\ninstance normalization (AdaIN) to enable a multi-modal translation with style\ncodes. In addition, to improve the instance-awareness and translation quality\nat object regions, we present an instance-level content contrastive loss\ndefined between input and translated image. We conduct experiments to\ndemonstrate the effectiveness of our InstaFormer over the latest methods and\nprovide extensive ablation studies.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-30T12:30:22Z",
    "updated": "2022-03-30T12:30:22Z",
    "doi": null
  },
  "2212.08158": {
    "id": "http://arxiv.org/abs/2212.08158v3",
    "title": "MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal\n  Contributions in Vision and Language Models & Tasks",
    "authors": [
      "Letitia Parcalabescu",
      "Anette Frank"
    ],
    "abstract": "  Vision and language models (VL) are known to exploit unrobust indicators in\nindividual modalities (e.g., introduced by distributional biases) instead of\nfocusing on relevant information in each modality. That a unimodal model\nachieves similar accuracy on a VL task to a multimodal one, indicates that\nso-called unimodal collapse occurred. However, accuracy-based tests fail to\ndetect e.g., when the model prediction is wrong, while the model used relevant\ninformation from a modality. Instead, we propose MM-SHAP, a\nperformance-agnostic multimodality score based on Shapley values that reliably\nquantifies in which proportions a multimodal model uses individual modalities.\nWe apply MM-SHAP in two ways: (1) to compare models for their average degree of\nmultimodality, and (2) to measure for individual models the contribution of\nindividual modalities for different tasks and datasets. Experiments with six VL\nmodels -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight\nthat unimodal collapse can occur to different degrees and in different\ndirections, contradicting the wide-spread assumption that unimodal collapse is\none-sided. Based on our results, we recommend MM-SHAP for analysing multimodal\ntasks, to diagnose and guide progress towards multimodal integration. Code\navailable at \\url{https://github.com/Heidelberg-NLP/MM-SHAP}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68Txx",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.2.7; I.2.10",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-15T21:41:06Z",
    "updated": "2024-09-18T19:00:22Z",
    "doi": "10.18653/v1/2023.acl-long.223"
  },
  "2112.06624": {
    "id": "http://arxiv.org/abs/2112.06624v1",
    "title": "Pedestrian Trajectory Prediction via Spatial Interaction Transformer\n  Network",
    "authors": [
      "Tong Su",
      "Yu Meng",
      "Yan Xu"
    ],
    "abstract": "  As a core technology of the autonomous driving system, pedestrian trajectory\nprediction can significantly enhance the function of active vehicle safety and\nreduce road traffic injuries. In traffic scenes, when encountering with\noncoming people, pedestrians may make sudden turns or stop immediately, which\noften leads to complicated trajectories. To predict such unpredictable\ntrajectories, we can gain insights into the interaction between pedestrians. In\nthis paper, we present a novel generative method named Spatial Interaction\nTransformer (SIT), which learns the spatio-temporal correlation of pedestrian\ntrajectories through attention mechanisms. Furthermore, we introduce the\nconditional variational autoencoder (CVAE) framework to model the future latent\nmotion states of pedestrians. In particular, the experiments based on\nlarge-scale trafc dataset nuScenes [2] show that SIT has an outstanding\nperformance than state-of-the-art (SOTA) methods. Experimental evaluation on\nthe challenging ETH and UCY datasets conrms the robustness of our proposed\nmodel\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-12-13T13:08:04Z",
    "updated": "2021-12-13T13:08:04Z",
    "doi": null
  },
  "2312.11459": {
    "id": "http://arxiv.org/abs/2312.11459v3",
    "title": "VolumeDiffusion: Flexible Text-to-3D Generation with Efficient\n  Volumetric Encoder",
    "authors": [
      "Zhicong Tang",
      "Shuyang Gu",
      "Chunyu Wang",
      "Ting Zhang",
      "Jianmin Bao",
      "Dong Chen",
      "Baining Guo"
    ],
    "abstract": "  This paper introduces a pioneering 3D volumetric encoder designed for\ntext-to-3D generation. To scale up the training data for the diffusion model, a\nlightweight network is developed to efficiently acquire feature volumes from\nmulti-view images. The 3D volumes are then trained on a diffusion model for\ntext-to-3D generation using a 3D U-Net. This research further addresses the\nchallenges of inaccurate object captions and high-dimensional feature volumes.\nThe proposed model, trained on the public Objaverse dataset, demonstrates\npromising outcomes in producing diverse and recognizable samples from text\nprompts. Notably, it empowers finer control over object part characteristics\nthrough textual cues, fostering model creativity by seamlessly combining\nmultiple concepts within a single object. This research significantly\ncontributes to the progress of 3D generation by introducing an efficient,\nflexible, and scalable representation methodology.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-18T18:59:05Z",
    "updated": "2024-08-13T04:35:02Z",
    "doi": null
  },
  "2404.16510": {
    "id": "http://arxiv.org/abs/2404.16510v1",
    "title": "Interactive3D: Create What You Want by Interactive 3D Generation",
    "authors": [
      "Shaocong Dong",
      "Lihe Ding",
      "Zhanpeng Huang",
      "Zibin Wang",
      "Tianfan Xue",
      "Dan Xu"
    ],
    "abstract": "  3D object generation has undergone significant advancements, yielding\nhigh-quality results. However, fall short of achieving precise user control,\noften yielding results that do not align with user expectations, thus limiting\ntheir applicability. User-envisioning 3D object generation faces significant\nchallenges in realizing its concepts using current generative models due to\nlimited interaction capabilities. Existing methods mainly offer two approaches:\n(i) interpreting textual instructions with constrained controllability, or (ii)\nreconstructing 3D objects from 2D images. Both of them limit customization to\nthe confines of the 2D reference and potentially introduce undesirable\nartifacts during the 3D lifting process, restricting the scope for direct and\nversatile 3D modifications. In this work, we introduce Interactive3D, an\ninnovative framework for interactive 3D generation that grants users precise\ncontrol over the generative process through extensive 3D interaction\ncapabilities. Interactive3D is constructed in two cascading stages, utilizing\ndistinct 3D representations. The first stage employs Gaussian Splatting for\ndirect user interaction, allowing modifications and guidance of the generative\ndirection at any intermediate step through (i) Adding and Removing components,\n(ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv)\nSemantic Editing. Subsequently, the Gaussian splats are transformed into\nInstantNGP. We introduce a novel (v) Interactive Hash Refinement module to\nfurther add details and extract the geometry in the second stage. Our\nexperiments demonstrate that Interactive3D markedly improves the\ncontrollability and quality of 3D generation. Our project webpage is available\nat \\url{https://interactive-3d.github.io/}.\n",
    "categories": [
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-25T11:06:57Z",
    "updated": "2024-04-25T11:06:57Z",
    "doi": null
  },
  "2208.02812": {
    "id": "http://arxiv.org/abs/2208.02812v2",
    "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with\n  Point-to-Pixel Prompting",
    "authors": [
      "Ziyi Wang",
      "Xumin Yu",
      "Yongming Rao",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "  Nowadays, pre-training big models on large-scale datasets has become a\ncrucial topic in deep learning. The pre-trained models with high representation\nability and transferability achieve a great success and dominate many\ndownstream tasks in natural language processing and 2D vision. However, it is\nnon-trivial to promote such a pretraining-tuning paradigm to the 3D vision,\ngiven the limited training data that are relatively inconvenient to collect. In\nthis paper, we provide a new perspective of leveraging pre-trained 2D knowledge\nin 3D domain to tackle this problem, tuning pre-trained image models with the\nnovel Point-to-Pixel prompting for point cloud analysis at a minor parameter\ncost. Following the principle of prompting engineering, we transform point\nclouds into colorful images with geometry-preserved projection and\ngeometry-aware coloring to adapt to pre-trained image models, whose weights are\nkept frozen during the end-to-end optimization of point cloud analysis tasks.\nWe conduct extensive experiments to demonstrate that cooperating with our\nproposed Point-to-Pixel Prompting, better pre-trained image model will lead to\nconsistently better performance in 3D vision. Enjoying prosperous development\nfrom image pre-training field, our method attains 89.3% accuracy on the hardest\nsetting of ScanObjectNN, surpassing conventional point cloud models with much\nfewer trainable parameters. Our framework also exhibits very competitive\nperformance on ModelNet classification and ShapeNet Part Segmentation. Code is\navailable at https://github.com/wangzy22/P2P.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-08-04T17:59:03Z",
    "updated": "2022-10-12T09:00:15Z",
    "doi": null
  },
  "2107.07589": {
    "id": "http://arxiv.org/abs/2107.07589v2",
    "title": "A Comparison of Modern General-Purpose Visual SLAM Approaches",
    "authors": [
      "Alexey Merzlyakov",
      "Steve Macenski"
    ],
    "abstract": "  Advancing maturity in mobile and legged robotics technologies is changing the\nlandscapes where robots are being deployed and found. This innovation calls for\na transformation in simultaneous localization and mapping (SLAM) systems to\nsupport this new generation of service and consumer robots. No longer can\ntraditionally robust 2D lidar systems dominate while robots are being deployed\nin multi-story indoor, outdoor unstructured, and urban domains with\nincreasingly inexpensive stereo and RGB-D cameras. Visual SLAM (VSLAM) systems\nhave been a topic of study for decades and a small number of openly available\nimplementations have stood out: ORB-SLAM3, OpenVSLAM and RTABMap.\n  This paper presents a comparison of these 3 modern, feature rich, and\nuniquely robust VSLAM techniques that have yet to be benchmarked against each\nother, using several different datasets spanning multiple domains negotiated by\nservice robots. ORB-SLAM3 and OpenVSLAM each were not compared against at least\none of these datasets previously in literature and we provide insight through\nthis lens. This analysis is motivated to find general purpose, feature\ncomplete, and multi-domain VSLAM options to support a broad class of robot\napplications for integration into the new and improved ROS 2 Nav2 System as\nsuitable alternatives to traditional 2D lidar solutions.\n",
    "categories": [
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-07-15T20:00:03Z",
    "updated": "2021-08-05T21:34:11Z",
    "doi": null
  },
  "1605.04624": {
    "id": "http://arxiv.org/abs/1605.04624v1",
    "title": "Learning to Rank Personalized Search Results in Professional Networks",
    "authors": [
      "Viet Ha-Thuc",
      "Shakti Sinha"
    ],
    "abstract": "  LinkedIn search is deeply personalized - for the same queries, different\nsearchers expect completely different results. This paper presents our approach\nto achieving this by mining various data sources available in LinkedIn to infer\nsearchers' intents (such as hiring, job seeking, etc.), as well as extending\nthe concept of homophily to capture the searcher-result similarities on many\naspects. Then, learning-to-rank (LTR) is applied to combine these signals with\nstandard search features.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-05-16T00:59:07Z",
    "updated": "2016-05-16T00:59:07Z",
    "doi": null
  },
  "2002.00569": {
    "id": "http://arxiv.org/abs/2002.00569v3",
    "title": "DiverseDepth: Affine-invariant Depth Prediction Using Diverse Data",
    "authors": [
      "Wei Yin",
      "Xinlong Wang",
      "Chunhua Shen",
      "Yifan Liu",
      "Zhi Tian",
      "Songcen Xu",
      "Changming Sun",
      "Dou Renyin"
    ],
    "abstract": "  We present a method for depth estimation with monocular images, which can\npredict high-quality depth on diverse scenes up to an affine transformation,\nthus preserving accurate shapes of a scene. Previous methods that predict\nmetric depth often work well only for a specific scene. In contrast, learning\nrelative depth (information of being closer or further) can enjoy better\ngeneralization, with the price of failing to recover the accurate geometric\nshape of the scene. In this work, we propose a dataset and methods to tackle\nthis dilemma, aiming to predict accurate depth up to an affine transformation\nwith good generalization to diverse scenes. First we construct a large-scale\nand diverse dataset, termed Diverse Scene Depth dataset (DiverseDepth), which\nhas a broad range of scenes and foreground contents. Compared with previous\nlearning objectives, i.e., learning metric depth or relative depth, we propose\nto learn the affine-invariant depth using our diverse dataset to ensure both\ngeneralization and high-quality geometric shapes of scenes. Furthermore, in\norder to train the model on the complex dataset effectively, we propose a\nmulti-curriculum learning method. Experiments show that our method outperforms\nprevious methods on 8 datasets by a large margin with the zero-shot test\nsetting, demonstrating the excellent generalization capacity of the learned\nmodel to diverse scenes. The reconstructed point clouds with the predicted\ndepth show that our method can recover high-quality 3D shapes. Code and dataset\nare available at: https://tinyurl.com/DiverseDepth\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-02-03T05:38:33Z",
    "updated": "2020-03-28T08:26:57Z",
    "doi": null
  },
  "2307.02421": {
    "id": "http://arxiv.org/abs/2307.02421v2",
    "title": "DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models",
    "authors": [
      "Chong Mou",
      "Xintao Wang",
      "Jiechong Song",
      "Ying Shan",
      "Jian Zhang"
    ],
    "abstract": "  Despite the ability of existing large-scale text-to-image (T2I) models to\ngenerate high-quality images from detailed textual descriptions, they often\nlack the ability to precisely edit the generated or real images. In this paper,\nwe propose a novel image editing method, DragonDiffusion, enabling Drag-style\nmanipulation on Diffusion models. Specifically, we construct classifier\nguidance based on the strong correspondence of intermediate features in the\ndiffusion model. It can transform the editing signals into gradients via\nfeature correspondence loss to modify the intermediate representation of the\ndiffusion model. Based on this guidance strategy, we also build a multi-scale\nguidance to consider both semantic and geometric alignment. Moreover, a\ncross-branch self-attention is added to maintain the consistency between the\noriginal image and the editing result. Our method, through an efficient design,\nachieves various editing modes for the generated or real images, such as object\nmoving, object resizing, object appearance replacement, and content dragging.\nIt is worth noting that all editing and content preservation signals come from\nthe image itself, and the model does not require fine-tuning or additional\nmodules. Our source code will be available at\nhttps://github.com/MC-E/DragonDiffusion.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-05T16:43:56Z",
    "updated": "2023-11-20T09:08:42Z",
    "doi": null
  },
  "2102.04664": {
    "id": "http://arxiv.org/abs/2102.04664v2",
    "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\n  and Generation",
    "authors": [
      "Shuai Lu",
      "Daya Guo",
      "Shuo Ren",
      "Junjie Huang",
      "Alexey Svyatkovskiy",
      "Ambrosio Blanco",
      "Colin Clement",
      "Dawn Drain",
      "Daxin Jiang",
      "Duyu Tang",
      "Ge Li",
      "Lidong Zhou",
      "Linjun Shou",
      "Long Zhou",
      "Michele Tufano",
      "Ming Gong",
      "Ming Zhou",
      "Nan Duan",
      "Neel Sundaresan",
      "Shao Kun Deng",
      "Shengyu Fu",
      "Shujie Liu"
    ],
    "abstract": "  Benchmark datasets have a significant impact on accelerating research in\nprogramming language tasks. In this paper, we introduce CodeXGLUE, a benchmark\ndataset to foster machine learning research for program understanding and\ngeneration. CodeXGLUE includes a collection of 10 tasks across 14 datasets and\na platform for model evaluation and comparison. CodeXGLUE also features three\nbaseline systems, including the BERT-style, GPT-style, and Encoder-Decoder\nmodels, to make it easy for researchers to use the platform. The availability\nof such data and baselines can help the development and validation of new\nmethods that can be applied to various program understanding and generation\nproblems.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-02-09T06:16:25Z",
    "updated": "2021-03-16T08:28:37Z",
    "doi": null
  },
  "2303.15413": {
    "id": "http://arxiv.org/abs/2303.15413v5",
    "title": "Debiasing Scores and Prompts of 2D Diffusion for View-consistent\n  Text-to-3D Generation",
    "authors": [
      "Susung Hong",
      "Donghoon Ahn",
      "Seungryong Kim"
    ],
    "abstract": "  Existing score-distilling text-to-3D generation techniques, despite their\nconsiderable promise, often encounter the view inconsistency problem. One of\nthe most notable issues is the Janus problem, where the most canonical view of\nan object (\\textit{e.g}., face or head) appears in other views. In this work,\nwe explore existing frameworks for score-distilling text-to-3D generation and\nidentify the main causes of the view inconsistency problem -- the embedded bias\nof 2D diffusion models. Based on these findings, we propose two approaches to\ndebias the score-distillation frameworks for view-consistent text-to-3D\ngeneration. Our first approach, called score debiasing, involves cutting off\nthe score estimated by 2D diffusion models and gradually increasing the\ntruncation value throughout the optimization process. Our second approach,\ncalled prompt debiasing, identifies conflicting words between user prompts and\nview prompts using a language model, and adjusts the discrepancy between view\nprompts and the viewing direction of an object. Our experimental results show\nthat our methods improve the realism of the generated 3D objects by\nsignificantly reducing artifacts and achieve a good trade-off between\nfaithfulness to the 2D diffusion models and 3D consistency with little\noverhead. Our project page is available\nat~\\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-27T17:31:13Z",
    "updated": "2023-12-19T22:03:12Z",
    "doi": null
  },
  "2104.07204": {
    "id": "http://arxiv.org/abs/2104.07204v2",
    "title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese\n  Pre-trained Language Models",
    "authors": [
      "Yuxuan Lai",
      "Yijia Liu",
      "Yansong Feng",
      "Songfang Huang",
      "Dongyan Zhao"
    ],
    "abstract": "  Chinese pre-trained language models usually process text as a sequence of\ncharacters, while ignoring more coarse granularity, e.g., words. In this work,\nwe propose a novel pre-training paradigm for Chinese -- Lattice-BERT, which\nexplicitly incorporates word representations along with characters, thus can\nmodel a sentence in a multi-granularity manner. Specifically, we construct a\nlattice graph from the characters and words in a sentence and feed all these\ntext units into transformers. We design a lattice position attention mechanism\nto exploit the lattice structures in self-attention layers. We further propose\na masked segment prediction task to push the model to learn from rich but\nredundant information inherent in lattices, while avoiding learning unexpected\ntricks. Experiments on 11 Chinese natural language understanding tasks show\nthat our model can bring an average increase of 1.5% under the 12-layer\nsetting, which achieves new state-of-the-art among base-size models on the CLUE\nbenchmarks. Further analysis shows that Lattice-BERT can harness the lattice\nstructures, and the improvement comes from the exploration of redundant\ninformation and multi-granularity representations. Our code will be available\nat https://github.com/alibaba/pretrained-language-models/LatticeBERT.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-04-15T02:36:49Z",
    "updated": "2021-05-28T11:41:35Z",
    "doi": null
  },
  "1911.02744": {
    "id": "http://arxiv.org/abs/1911.02744v1",
    "title": "PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud\n  Representation",
    "authors": [
      "Can Qin",
      "Haoxuan You",
      "Lichen Wang",
      "C. -C. Jay Kuo",
      "Yun Fu"
    ],
    "abstract": "  Domain Adaptation (DA) approaches achieved significant improvements in a wide\nrange of machine learning and computer vision tasks (i.e., classification,\ndetection, and segmentation). However, as far as we are aware, there are few\nmethods yet to achieve domain adaptation directly on 3D point cloud data. The\nunique challenge of point cloud data lies in its abundant spatial geometric\ninformation, and the semantics of the whole object is contributed by including\nregional geometric structures. Specifically, most general-purpose DA methods\nthat struggle for global feature alignment and ignore local geometric\ninformation are not suitable for 3D domain alignment. In this paper, we propose\na novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN\njointly aligns the global and local features in multi-level. For local\nalignment, we propose Self-Adaptive (SA) node module with an adjusted receptive\nfield to model the discriminative local structures for aligning domains. To\nrepresent hierarchically scaled features, node-attention module is further\nintroduced to weight the relationship of SA nodes across objects and domains.\nFor global alignment, an adversarial-training strategy is employed to learn and\nalign global features across domains. Since there is no common evaluation\nbenchmark for 3D point cloud DA scenario, we build a general benchmark (i.e.,\nPointDA-10) extracted from three popular 3D object/scene datasets (i.e.,\nModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification\nfashion. Extensive experiments on PointDA-10 illustrate the superiority of our\nmodel over the state-of-the-art general-purpose DA methods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-11-07T04:03:07Z",
    "updated": "2019-11-07T04:03:07Z",
    "doi": null
  },
  "1709.07857": {
    "id": "http://arxiv.org/abs/1709.07857v2",
    "title": "Using Simulation and Domain Adaptation to Improve Efficiency of Deep\n  Robotic Grasping",
    "authors": [
      "Konstantinos Bousmalis",
      "Alex Irpan",
      "Paul Wohlhart",
      "Yunfei Bai",
      "Matthew Kelcey",
      "Mrinal Kalakrishnan",
      "Laura Downs",
      "Julian Ibarz",
      "Peter Pastor",
      "Kurt Konolige",
      "Sergey Levine",
      "Vincent Vanhoucke"
    ],
    "abstract": "  Instrumenting and collecting annotated visual grasping datasets to train\nmodern machine learning algorithms can be extremely time-consuming and\nexpensive. An appealing alternative is to use off-the-shelf simulators to\nrender synthetic data for which ground-truth annotations are generated\nautomatically. Unfortunately, models trained purely on simulated data often\nfail to generalize to the real world. We study how randomized simulated\nenvironments and domain adaptation methods can be extended to train a grasping\nsystem to grasp novel objects from raw monocular RGB images. We extensively\nevaluate our approaches with a total of more than 25,000 physical test grasps,\nstudying a range of simulation conditions and domain adaptation methods,\nincluding a novel extension of pixel-level domain adaptation that we term the\nGraspGAN. We show that, by using synthetic data and domain adaptation, we are\nable to reduce the number of real-world samples needed to achieve a given level\nof performance by up to 50 times, using only randomly generated simulated\nobjects. We also show that by using only unlabeled real-world data and our\nGraspGAN methodology, we obtain real-world grasping performance without any\nreal-world labels that is similar to that achieved with 939,777 labeled\nreal-world samples.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-09-22T17:23:12Z",
    "updated": "2017-09-25T21:35:45Z",
    "doi": null
  },
  "2109.08478": {
    "id": "http://arxiv.org/abs/2109.08478v1",
    "title": "Multimodal Incremental Transformer with Visual Grounding for Visual\n  Dialogue Generation",
    "authors": [
      "Feilong Chen",
      "Fandong Meng",
      "Xiuyi Chen",
      "Peng Li",
      "Jie Zhou"
    ],
    "abstract": "  Visual dialogue is a challenging task since it needs to answer a series of\ncoherent questions on the basis of understanding the visual environment.\nPrevious studies focus on the implicit exploration of multimodal co-reference\nby implicitly attending to spatial image features or object-level image\nfeatures but neglect the importance of locating the objects explicitly in the\nvisual content, which is associated with entities in the textual content.\nTherefore, in this paper we propose a {\\bf M}ultimodal {\\bf I}ncremental {\\bf\nT}ransformer with {\\bf V}isual {\\bf G}rounding, named MITVG, which consists of\ntwo key parts: visual grounding and multimodal incremental transformer. Visual\ngrounding aims to explicitly locate related objects in the image guided by\ntextual entities, which helps the model exclude the visual content that does\nnot need attention. On the basis of visual grounding, the multimodal\nincremental transformer encodes the multi-turn dialogue history combined with\nvisual scene step by step according to the order of the dialogue and then\ngenerates a contextually and visually coherent response. Experimental results\non the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the\nproposed model, which achieves comparable performance.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-17T11:39:29Z",
    "updated": "2021-09-17T11:39:29Z",
    "doi": null
  },
  "1707.01613": {
    "id": "http://arxiv.org/abs/1707.01613v4",
    "title": "SSGAN: Secure Steganography Based on Generative Adversarial Networks",
    "authors": [
      "Haichao Shi",
      "Jing Dong",
      "Wei Wang",
      "Yinlong Qian",
      "Xiaoyu Zhang"
    ],
    "abstract": "  In this paper, a novel strategy of Secure Steganograpy based on Generative\nAdversarial Networks is proposed to generate suitable and secure covers for\nsteganography. The proposed architecture has one generative network, and two\ndiscriminative networks. The generative network mainly evaluates the visual\nquality of the generated images for steganography, and the discriminative\nnetworks are utilized to assess their suitableness for information hiding.\nDifferent from the existing work which adopts Deep Convolutional Generative\nAdversarial Networks, we utilize another form of generative adversarial\nnetworks. By using this new form of generative adversarial networks,\nsignificant improvements are made on the convergence speed, the training\nstability and the image quality. Furthermore, a sophisticated steganalysis\nnetwork is reconstructed for the discriminative network, and the network can\nbetter evaluate the performance of the generated images. Numerous experiments\nare conducted on the publicly available datasets to demonstrate the\neffectiveness and robustness of the proposed method.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-07-06T02:05:51Z",
    "updated": "2018-11-24T02:32:03Z",
    "doi": null
  },
  "1812.06834": {
    "id": "http://arxiv.org/abs/1812.06834v3",
    "title": "A Tutorial on Deep Latent Variable Models of Natural Language",
    "authors": [
      "Yoon Kim",
      "Sam Wiseman",
      "Alexander M. Rush"
    ],
    "abstract": "  There has been much recent, exciting work on combining the complementary\nstrengths of latent variable models and deep learning. Latent variable modeling\nmakes it easy to explicitly specify model constraints through conditional\nindependence properties, while deep learning makes it possible to parameterize\nthese conditional likelihoods with powerful function approximators. While these\n\"deep latent variable\" models provide a rich, flexible framework for modeling\nmany real-world phenomena, difficulties exist: deep parameterizations of\nconditional likelihoods usually make posterior inference intractable, and\nlatent variable objectives often complicate backpropagation by introducing\npoints of non-differentiability. This tutorial explores these issues in depth\nthrough the lens of variational inference.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-12-17T15:26:29Z",
    "updated": "2019-08-05T01:14:32Z",
    "doi": null
  },
  "2009.14068": {
    "id": "http://arxiv.org/abs/2009.14068v1",
    "title": "Graph convolutional regression of cardiac depolarization from sparse\n  endocardial maps",
    "authors": [
      "Felix Meister",
      "Tiziano Passerini",
      "Chlo\u00e9 Audigier",
      "\u00c8ric Lluch",
      "Viorel Mihalef",
      "Hiroshi Ashikaga",
      "Andreas Maier",
      "Henry Halperin",
      "Tommaso Mansi"
    ],
    "abstract": "  Electroanatomic mapping as routinely acquired in ablation therapy of\nventricular tachycardia is the gold standard method to identify the\narrhythmogenic substrate. To reduce the acquisition time and still provide maps\nwith high spatial resolution, we propose a novel deep learning method based on\ngraph convolutional neural networks to estimate the depolarization time in the\nmyocardium, given sparse catheter data on the left ventricular endocardium,\nECG, and magnetic resonance images. The training set consists of data produced\nby a computational model of cardiac electrophysiology on a large cohort of\nsynthetically generated geometries of ischemic hearts. The predicted\ndepolarization pattern has good agreement with activation times computed by the\ncardiac electrophysiology model in a validation set of five swine heart\ngeometries with complex scar and border zone morphologies. The mean absolute\nerror hereby measures 8 ms on the entire myocardium when providing 50\\% of the\nendocardial ground truth in over 500 computed depolarization patterns.\nFurthermore, when considering a complete animal data set with high density\nelectroanatomic mapping data as reference, the neural network can accurately\nreproduce the endocardial depolarization pattern, even when a small percentage\nof measurements are provided as input features (mean absolute error of 7 ms\nwith 50\\% of input samples). The results show that the proposed method, trained\non synthetically generated data, may generalize to real data.\n",
    "categories": [
      {
        "@term": "physics.med-ph",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "q-bio.QM",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "q-bio.TO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-09-28T09:21:14Z",
    "updated": "2020-09-28T09:21:14Z",
    "doi": null
  },
  "2303.14124": {
    "id": "http://arxiv.org/abs/2303.14124v1",
    "title": "Towards Scalable Neural Representation for Diverse Videos",
    "authors": [
      "Bo He",
      "Xitong Yang",
      "Hanyu Wang",
      "Zuxuan Wu",
      "Hao Chen",
      "Shuaiyi Huang",
      "Yixuan Ren",
      "Ser-Nam Lim",
      "Abhinav Shrivastava"
    ],
    "abstract": "  Implicit neural representations (INR) have gained increasing attention in\nrepresenting 3D scenes and images, and have been recently applied to encode\nvideos (e.g., NeRV, E-NeRV). While achieving promising results, existing\nINR-based methods are limited to encoding a handful of short videos (e.g.,\nseven 5-second videos in the UVG dataset) with redundant visual content,\nleading to a model design that fits individual video frames independently and\nis not efficiently scalable to a large number of diverse videos. This paper\nfocuses on developing neural representations for a more practical setup --\nencoding long and/or a large number of videos with diverse visual content. We\nfirst show that instead of dividing videos into small subsets and encoding them\nwith separate models, encoding long and diverse videos jointly with a unified\nmodel achieves better compression results. Based on this observation, we\npropose D-NeRV, a novel neural representation framework designed to encode\ndiverse videos by (i) decoupling clip-specific visual content from motion\ninformation, (ii) introducing temporal reasoning into the implicit neural\nnetwork, and (iii) employing the task-oriented flow as intermediate output to\nreduce spatial redundancies. Our new model largely surpasses NeRV and\ntraditional video compression techniques on UCF101 and UVG datasets on the\nvideo compression task. Moreover, when used as an efficient data-loader, D-NeRV\nachieves 3%-10% higher accuracy than NeRV on action recognition tasks on the\nUCF101 dataset under the same compression ratios.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-24T16:32:19Z",
    "updated": "2023-03-24T16:32:19Z",
    "doi": null
  },
  "1708.04538": {
    "id": "http://arxiv.org/abs/1708.04538v3",
    "title": "Artistic style transfer for videos and spherical images",
    "authors": [
      "Manuel Ruder",
      "Alexey Dosovitskiy",
      "Thomas Brox"
    ],
    "abstract": "  Manually re-drawing an image in a certain artistic style takes a professional\nartist a long time. Doing this for a video sequence single-handedly is beyond\nimagination. We present two computational approaches that transfer the style\nfrom one image (for example, a painting) to a whole video sequence. In our\nfirst approach, we adapt to videos the original image style transfer technique\nby Gatys et al. based on energy minimization. We introduce new ways of\ninitialization and new loss functions to generate consistent and stable\nstylized video sequences even in cases with large motion and strong occlusion.\nOur second approach formulates video stylization as a learning problem. We\npropose a deep network architecture and training procedures that allow us to\nstylize arbitrary-length videos in a consistent and stable way, and nearly in\nreal time. We show that the proposed methods clearly outperform simpler\nbaselines both qualitatively and quantitatively. Finally, we propose a way to\nadapt these approaches also to 360 degree images and videos as they emerge with\nrecent virtual reality hardware.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-08-13T21:17:59Z",
    "updated": "2018-08-05T09:07:11Z",
    "doi": "10.1007/s11263-018-1089-z"
  },
  "2203.02986": {
    "id": "http://arxiv.org/abs/2203.02986v1",
    "title": "Modeling Coreference Relations in Visual Dialog",
    "authors": [
      "Mingxiao Li",
      "Marie-Francine Moens"
    ],
    "abstract": "  Visual dialog is a vision-language task where an agent needs to answer a\nseries of questions grounded in an image based on the understanding of the\ndialog history and the image. The occurrences of coreference relations in the\ndialog makes it a more challenging task than visual question-answering. Most\nprevious works have focused on learning better multi-modal representations or\non exploring different ways of fusing visual and language features, while the\ncoreferences in the dialog are mainly ignored. In this paper, based on\nlinguistic knowledge and discourse features of human dialog we propose two soft\nconstraints that can improve the model's ability of resolving coreferences in\ndialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset\nshows that our model, which integrates two novel and linguistically inspired\nsoft constraints in a deep transformer neural architecture, obtains new\nstate-of-the-art performance in terms of recall at 1 and other evaluation\nmetrics compared to current existing models and this without pretraining on\nother vision-language datasets. Our qualitative results also demonstrate the\neffectiveness of the method that we propose.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-06T15:22:24Z",
    "updated": "2022-03-06T15:22:24Z",
    "doi": null
  },
  "1802.10026": {
    "id": "http://arxiv.org/abs/1802.10026v4",
    "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs",
    "authors": [
      "Timur Garipov",
      "Pavel Izmailov",
      "Dmitrii Podoprikhin",
      "Dmitry Vetrov",
      "Andrew Gordon Wilson"
    ],
    "abstract": "  The loss functions of deep neural networks are complex and their geometric\nproperties are not well understood. We show that the optima of these complex\nloss functions are in fact connected by simple curves over which training and\ntest accuracy are nearly constant. We introduce a training procedure to\ndiscover these high-accuracy pathways between modes. Inspired by this new\ngeometric insight, we also propose a new ensembling method entitled Fast\nGeometric Ensembling (FGE). Using FGE we can train high-performing ensembles in\nthe time required to train a single model. We achieve improved performance\ncompared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10,\nCIFAR-100, and ImageNet.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-02-27T17:13:28Z",
    "updated": "2018-10-30T11:39:49Z",
    "doi": null
  },
  "2407.15017": {
    "id": "http://arxiv.org/abs/2407.15017v3",
    "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
    "authors": [
      "Mengru Wang",
      "Yunzhi Yao",
      "Ziwen Xu",
      "Shuofei Qiao",
      "Shumin Deng",
      "Peng Wang",
      "Xiang Chen",
      "Jia-Chen Gu",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "  Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial\nfor advancing towards trustworthy AGI. This paper reviews knowledge mechanism\nanalysis from a novel taxonomy including knowledge utilization and evolution.\nKnowledge utilization delves into the mechanism of memorization, comprehension\nand application, and creation. Knowledge evolution focuses on the dynamic\nprogression of knowledge within individual and group LLMs. Moreover, we discuss\nwhat knowledge LLMs have learned, the reasons for the fragility of parametric\nknowledge, and the potential dark knowledge (hypothesis) that will be\nchallenging to address. We hope this work can help understand knowledge in LLMs\nand provide insights for future research.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.HC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-07-22T06:15:59Z",
    "updated": "2024-10-06T15:42:55Z",
    "doi": null
  },
  "1412.6550": {
    "id": "http://arxiv.org/abs/1412.6550v4",
    "title": "FitNets: Hints for Thin Deep Nets",
    "authors": [
      "Adriana Romero",
      "Nicolas Ballas",
      "Samira Ebrahimi Kahou",
      "Antoine Chassang",
      "Carlo Gatta",
      "Yoshua Bengio"
    ],
    "abstract": "  While depth tends to improve network performances, it also makes\ngradient-based training more difficult since deeper networks tend to be more\nnon-linear. The recently proposed knowledge distillation approach is aimed at\nobtaining small and fast-to-execute models, and it has shown that a student\nnetwork could imitate the soft output of a larger teacher network or ensemble\nof networks. In this paper, we extend this idea to allow the training of a\nstudent that is deeper and thinner than the teacher, using not only the outputs\nbut also the intermediate representations learned by the teacher as hints to\nimprove the training process and final performance of the student. Because the\nstudent intermediate hidden layer will generally be smaller than the teacher's\nintermediate hidden layer, additional parameters are introduced to map the\nstudent hidden layer to the prediction of the teacher hidden layer. This allows\none to train deeper students that can generalize better or run faster, a\ntrade-off that is controlled by the chosen student capacity. For example, on\nCIFAR-10, a deep student network with almost 10.4 times less parameters\noutperforms a larger, state-of-the-art teacher network.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.NE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2014-12-19T22:40:51Z",
    "updated": "2015-03-27T11:52:28Z",
    "doi": null
  },
  "1804.01947": {
    "id": "http://arxiv.org/abs/1804.01947v3",
    "title": "Sliced-Wasserstein Autoencoder: An Embarrassingly Simple Generative\n  Model",
    "authors": [
      "Soheil Kolouri",
      "Phillip E. Pope",
      "Charles E. Martin",
      "Gustavo K. Rohde"
    ],
    "abstract": "  In this paper we study generative modeling via autoencoders while using the\nelegant geometric properties of the optimal transport (OT) problem and the\nWasserstein distances. We introduce Sliced-Wasserstein Autoencoders (SWAE),\nwhich are generative models that enable one to shape the distribution of the\nlatent space into any samplable probability distribution without the need for\ntraining an adversarial network or defining a closed-form for the distribution.\nIn short, we regularize the autoencoder loss with the sliced-Wasserstein\ndistance between the distribution of the encoded training samples and a\npredefined samplable distribution. We show that the proposed formulation has an\nefficient numerical solution that provides similar capabilities to Wasserstein\nAutoencoders (WAE) and Variational Autoencoders (VAE), while benefiting from an\nembarrassingly simple implementation.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-05T16:45:06Z",
    "updated": "2018-06-27T00:05:29Z",
    "doi": null
  },
  "1808.01454": {
    "id": "http://arxiv.org/abs/1808.01454v1",
    "title": "T2Net: Synthetic-to-Realistic Translation for Solving Single-Image Depth\n  Estimation Tasks",
    "authors": [
      "Chuanxia Zheng",
      "Tat-Jen Cham",
      "Jianfei Cai"
    ],
    "abstract": "  Current methods for single-image depth estimation use training datasets with\nreal image-depth pairs or stereo pairs, which are not easy to acquire. We\npropose a framework, trained on synthetic image-depth pairs and unpaired real\nimages, that comprises an image translation network for enhancing realism of\ninput images, followed by a depth prediction network. A key idea is having the\nfirst network act as a wide-spectrum input translator, taking in either\nsynthetic or real images, and ideally producing minimally modified realistic\nimages. This is done via a reconstruction loss when the training input is real,\nand GAN loss when synthetic, removing the need for heuristic\nself-regularization. The second network is trained on a task loss for synthetic\nimage-depth pairs, with extra GAN loss to unify real and synthetic feature\ndistributions. Importantly, the framework can be trained end-to-end, leading to\ngood results, even surpassing early deep-learning methods that use real paired\ndata.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-08-04T09:10:14Z",
    "updated": "2018-08-04T09:10:14Z",
    "doi": null
  },
  "1804.05313": {
    "id": "http://arxiv.org/abs/1804.05313v2",
    "title": "FSCNMF: Fusing Structure and Content via Non-negative Matrix\n  Factorization for Embedding Information Networks",
    "authors": [
      "Sambaran Bandyopadhyay",
      "Harsh Kara",
      "Aswin Kannan",
      "M N Murty"
    ],
    "abstract": "  Analysis and visualization of an information network can be facilitated\nbetter using an appropriate embedding of the network. Network embedding learns\na compact low-dimensional vector representation for each node of the network,\nand uses this lower dimensional representation for different network analysis\ntasks. Only the structure of the network is considered by a majority of the\ncurrent embedding algorithms. However, some content is associated with each\nnode, in most of the practical applications, which can help to understand the\nunderlying semantics of the network. It is not straightforward to integrate the\ncontent of each node in the current state-of-the-art network embedding methods.\n  In this paper, we propose a nonnegative matrix factorization based\noptimization framework, namely FSCNMF which considers both the network\nstructure and the content of the nodes while learning a lower dimensional\nrepresentation of each node in the network. Our approach systematically\nregularizes structure based on content and vice versa to exploit the\nconsistency between the structure and content to the best possible extent. We\nfurther extend the basic FSCNMF to an advanced method, namely FSCNMF++ to\ncapture the higher order proximities in the network. We conduct experiments on\nreal world information networks for different types of machine learning\napplications such as node clustering, visualization, and multi-class\nclassification. The results show that our method can represent the network\nsignificantly better than the state-of-the-art algorithms and improve the\nperformance across all the applications that we consider.\n",
    "categories": [
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-04-15T07:09:34Z",
    "updated": "2018-07-04T14:11:35Z",
    "doi": null
  },
  "2403.10255": {
    "id": "http://arxiv.org/abs/2403.10255v1",
    "title": "Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion\n  Model and Implicit Neural Decoder",
    "authors": [
      "Jinseok Kim",
      "Tae-Kyun Kim"
    ],
    "abstract": "  Super-resolution (SR) and image generation are important tasks in computer\nvision and are widely adopted in real-world applications. Most existing\nmethods, however, generate images only at fixed-scale magnification and suffer\nfrom over-smoothing and artifacts. Additionally, they do not offer enough\ndiversity of output images nor image consistency at different scales. Most\nrelevant work applied Implicit Neural Representation (INR) to the denoising\ndiffusion model to obtain continuous-resolution yet diverse and high-quality SR\nresults. Since this model operates in the image space, the larger the\nresolution of image is produced, the more memory and inference time is\nrequired, and it also does not maintain scale-specific consistency. We propose\na novel pipeline that can super-resolve an input image or generate from a\nrandom noise a novel image at arbitrary scales. The method consists of a\npretrained auto-encoder, a latent diffusion model, and an implicit neural\ndecoder, and their learning strategies. The proposed method adopts diffusion\nprocesses in a latent space, thus efficient, yet aligned with output image\nspace decoded by MLPs at arbitrary scales. More specifically, our\narbitrary-scale decoder is designed by the symmetric decoder w/o up-scaling\nfrom the pretrained auto-encoder, and Local Implicit Image Function (LIIF) in\nseries. The latent diffusion process is learnt by the denoising and the\nalignment losses jointly. Errors in output images are backpropagated via the\nfixed decoder, improving the quality of output images. In the extensive\nexperiments using multiple public benchmarks on the two tasks i.e. image\nsuper-resolution and novel image generation at arbitrary scales, the proposed\nmethod outperforms relevant methods in metrics of image quality, diversity and\nscale consistency. It is significantly better than the relevant prior-art in\nthe inference speed and memory usage.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-15T12:45:40Z",
    "updated": "2024-03-15T12:45:40Z",
    "doi": null
  },
  "1811.01287": {
    "id": "http://arxiv.org/abs/1811.01287v1",
    "title": "Towards Sparse Hierarchical Graph Classifiers",
    "authors": [
      "C\u0103t\u0103lina Cangea",
      "Petar Veli\u010dkovi\u0107",
      "Nikola Jovanovi\u0107",
      "Thomas Kipf",
      "Pietro Li\u00f2"
    ],
    "abstract": "  Recent advances in representation learning on graphs, mainly leveraging graph\nconvolutional networks, have brought a substantial improvement on many\ngraph-based benchmark tasks. While novel approaches to learning node embeddings\nare highly suitable for node classification and link prediction, their\napplication to graph classification (predicting a single label for the entire\ngraph) remains mostly rudimentary, typically using a single global pooling step\nto aggregate node features or a hand-designed, fixed heuristic for hierarchical\ncoarsening of the graph structure. An important step towards ameliorating this\nis differentiable graph coarsening---the ability to reduce the size of the\ngraph in an adaptive, data-dependent manner within a graph neural network\npipeline, analogous to image downsampling within CNNs. However, the previous\nprominent approach to pooling has quadratic memory requirements during training\nand is therefore not scalable to large graphs. Here we combine several recent\nadvances in graph neural network design to demonstrate that competitive\nhierarchical graph classification results are possible without sacrificing\nsparsity. Our results are verified on several established graph classification\nbenchmarks, and highlight an important direction for future research in\ngraph-based neural networks.\n",
    "categories": [
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-11-03T21:39:43Z",
    "updated": "2018-11-03T21:39:43Z",
    "doi": null
  },
  "1806.03968": {
    "id": "http://arxiv.org/abs/1806.03968v1",
    "title": "CapsGAN: Using Dynamic Routing for Generative Adversarial Networks",
    "authors": [
      "Raeid Saqur",
      "Sal Vivona"
    ],
    "abstract": "  In this paper, we propose a novel technique for generating images in the 3D\ndomain from images with high degree of geometrical transformations. By\ncoalescing two popular concurrent methods that have seen rapid ascension to the\nmachine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and\nCapsule networks (Sabour, Hinton et. al.) - we present: \\textbf{CapsGAN}. We\nshow that CapsGAN performs better than or equal to traditional CNN based GANs\nin generating images with high geometric transformations using rotated MNIST.\nIn the process, we also show the efficacy of using capsules architecture in the\nGANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the\nperformance control and training stability by experimenting with using\nWasserstein distance (gradient clipping, penalty) and Spectral Normalization.\nThe experimental findings of this paper should propel the application of\ncapsules and GANs in the still exciting and nascent domain of 3D image\ngeneration, and plausibly video (frame) generation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-06-07T21:33:46Z",
    "updated": "2018-06-07T21:33:46Z",
    "doi": null
  },
  "2205.08787": {
    "id": "http://arxiv.org/abs/2205.08787v1",
    "title": "Cross-subject Action Unit Detection with Meta Learning and\n  Transformer-based Relation Modeling",
    "authors": [
      "Jiyuan Cao",
      "Zhilei Liu",
      "Yong Zhang"
    ],
    "abstract": "  Facial Action Unit (AU) detection is a crucial task for emotion analysis from\nfacial movements. The apparent differences of different subjects sometimes\nmislead changes brought by AUs, resulting in inaccurate results. However, most\nof the existing AU detection methods based on deep learning didn't consider the\nidentity information of different subjects. The paper proposes a\nmeta-learning-based cross-subject AU detection model to eliminate the\nidentity-caused differences. Besides, a transformer-based relation learning\nmodule is introduced to learn the latent relations of multiple AUs. To be\nspecific, our proposed work is composed of two sub-tasks. The first sub-task is\nmeta-learning-based AU local region representation learning, called MARL, which\nlearns discriminative representation of local AU regions that incorporates the\nshared information of multiple subjects and eliminates identity-caused\ndifferences. The second sub-task uses the local region representation of AU of\nthe first sub-task as input, then adds relationship learning based on the\ntransformer encoder architecture to capture AU relationships. The entire\ntraining process is cascaded. Ablation study and visualization show that our\nMARL can eliminate identity-caused differences, thus obtaining a robust and\ngeneralized AU discriminative embedding representation. Our results prove that\non the two public datasets BP4D and DISFA, our method is superior to the\nstate-of-the-art technology, and the F1 score is improved by 1.3% and 1.4%,\nrespectively.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-18T08:17:59Z",
    "updated": "2022-05-18T08:17:59Z",
    "doi": "10.1109/IJCNN55064.2022.9891984"
  },
  "1901.10124": {
    "id": "http://arxiv.org/abs/1901.10124v1",
    "title": "Adversarial Adaptation of Scene Graph Models for Understanding Civic\n  Issues",
    "authors": [
      "Shanu Kumar",
      "Shubham Atreja",
      "Anjali Singh",
      "Mohit Jain"
    ],
    "abstract": "  Citizen engagement and technology usage are two emerging trends driven by\nsmart city initiatives. Governments around the world are adopting technology\nfor faster resolution of civic issues. Typically, citizens report issues, such\nas broken roads, garbage dumps, etc. through web portals and mobile apps, in\norder for the government authorities to take appropriate actions. Several\nmediums -- text, image, audio, video -- are used to report these issues.\nThrough a user study with 13 citizens and 3 authorities, we found that image is\nthe most preferred medium to report civic issues. However, analyzing civic\nissue related images is challenging for the authorities as it requires manual\neffort. Moreover, previous works have been limited to identifying a specific\nset of issues from images. In this work, given an image, we propose to generate\na Civic Issue Graph consisting of a set of objects and the semantic relations\nbetween them, which are representative of the underlying civic issue. We also\nrelease two multi-modal (text and images) datasets, that can help in further\nanalysis of civic issues from images. We present a novel approach for\nadversarial training of existing scene graph models that enables the use of\nscene graphs for new applications in the absence of any labelled training data.\nWe conduct several experiments to analyze the efficacy of our approach, and\nusing human evaluation, we establish the appropriateness of our model at\nrepresenting different civic issues.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CY",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-01-29T06:02:45Z",
    "updated": "2019-01-29T06:02:45Z",
    "doi": null
  },
  "2401.01060": {
    "id": "http://arxiv.org/abs/2401.01060v1",
    "title": "Learning in the Wild: Towards Leveraging Unlabeled Data for Effectively\n  Tuning Pre-trained Code Models",
    "authors": [
      "Shuzheng Gao",
      "Wenxin Mao",
      "Cuiyun Gao",
      "Li Li",
      "Xing Hu",
      "Xin Xia",
      "Michael R. Lyu"
    ],
    "abstract": "  Pre-trained code models have recently achieved substantial improvements in\nmany code intelligence tasks. These models are first pre-trained on large-scale\nunlabeled datasets in a task-agnostic manner using self-supervised learning,\nand then fine-tuned on labeled datasets in downstream tasks. However, the\nlabeled datasets are usually limited in size (i.e., human intensive efforts),\nwhich may hinder the performance of pre-trained code models in specific tasks.\nTo mitigate this, one possible solution is to leverage the large-scale\nunlabeled data in the tuning stage by pseudo-labeling. However, directly\nemploying the pseudo-labeled data can bring a large amount of noise, i.e.,\nincorrect labels, leading to suboptimal performance. How to effectively\nleverage the noisy pseudo-labeled data is a challenging yet under-explored\nproblem.In this paper, we propose a novel approach named HINT to improve\npre-trained code models with large-scale unlabeled datasets by better utilizing\nthe pseudo-labeled data. HINT includes two main modules: HybrId pseudo-labeled\ndata selection and Noise-tolerant Training. In the hybrid pseudo-data selection\nmodule, considering the robustness issue, apart from directly measuring the\nquality of pseudo labels through training loss, we further propose to employ a\nretrieval-based method to filter low-quality pseudo-labeled data. The\nnoise-tolerant training module aims to further mitigate the influence of errors\nin pseudo labels by training the model with a noise-tolerant loss function and\nby regularizing the consistency of model predictions.The experimental results\nshow that HINT can better leverage those unlabeled data in a task-specific way\nand provide complementary benefits for pre-trained models, e.g., improving the\nbest baseline model by 15.33%, 16.50%, and 8.98% on code summarization, defect\ndetection, and assertion generation, respectively.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-01-02T06:39:00Z",
    "updated": "2024-01-02T06:39:00Z",
    "doi": "10.1145/3597503.3639216"
  },
  "2302.06646": {
    "id": "http://arxiv.org/abs/2302.06646v1",
    "title": "Simple Hardware-Efficient Long Convolutions for Sequence Modeling",
    "authors": [
      "Daniel Y. Fu",
      "Elliot L. Epstein",
      "Eric Nguyen",
      "Armin W. Thomas",
      "Michael Zhang",
      "Tri Dao",
      "Atri Rudra",
      "Christopher R\u00e9"
    ],
    "abstract": "  State space models (SSMs) have high performance on long sequence modeling but\nrequire sophisticated initialization techniques and specialized implementations\nfor high quality and runtime performance. We study whether a simple alternative\ncan match SSMs in performance and efficiency: directly learning long\nconvolutions over the sequence. We find that a key requirement to achieving\nhigh performance is keeping the convolution kernels smooth. We find that simple\ninterventions--such as squashing the kernel weights--result in smooth kernels\nand recover SSM performance on a range of tasks including the long range arena,\nimage classification, language modeling, and brain data modeling. Next, we\ndevelop FlashButterfly, an IO-aware algorithm to improve the runtime\nperformance of long convolutions. FlashButterfly appeals to classic Butterfly\ndecompositions of the convolution to reduce GPU memory IO and increase FLOP\nutilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows\nus to train on Path256, a challenging task with sequence length 64K, where we\nset state-of-the-art by 29.1 points while training 7.2$\\times$ faster than\nprior work. Lastly, we introduce an extension to FlashButterfly that learns the\ncoefficients of the Butterfly decomposition, increasing expressivity without\nincreasing runtime. Using this extension, we outperform a Transformer on\nWikiText103 by 0.2 PPL with 30% fewer parameters.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-13T19:19:23Z",
    "updated": "2023-02-13T19:19:23Z",
    "doi": null
  },
  "2312.06713": {
    "id": "http://arxiv.org/abs/2312.06713v1",
    "title": "TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint\n  Video",
    "authors": [
      "Minye Wu",
      "Zehao Wang",
      "Georgios Kouros",
      "Tinne Tuytelaars"
    ],
    "abstract": "  Neural Radiance Fields (NeRF) revolutionize the realm of visual media by\nproviding photorealistic Free-Viewpoint Video (FVV) experiences, offering\nviewers unparalleled immersion and interactivity. However, the technology's\nsignificant storage requirements and the computational complexity involved in\ngeneration and rendering currently limit its broader application. To close this\ngap, this paper presents Temporal Tri-Plane Radiance Fields (TeTriRF), a novel\ntechnology that significantly reduces the storage size for Free-Viewpoint Video\n(FVV) while maintaining low-cost generation and rendering. TeTriRF introduces a\nhybrid representation with tri-planes and voxel grids to support scaling up to\nlong-duration sequences and scenes with complex motions or rapid changes. We\npropose a group training scheme tailored to achieving high training efficiency\nand yielding temporally consistent, low-entropy scene representations.\nLeveraging these properties of the representations, we introduce a compression\npipeline with off-the-shelf video codecs, achieving an order of magnitude less\nstorage size compared to the state-of-the-art. Our experiments demonstrate that\nTeTriRF can achieve competitive quality with a higher compression rate.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-10T23:00:24Z",
    "updated": "2023-12-10T23:00:24Z",
    "doi": null
  },
  "2307.08597": {
    "id": "http://arxiv.org/abs/2307.08597v1",
    "title": "Multimodal Diffusion Segmentation Model for Object Segmentation from\n  Manipulation Instructions",
    "authors": [
      "Yui Iioka",
      "Yu Yoshida",
      "Yuiga Wada",
      "Shumpei Hatanaka",
      "Komei Sugiura"
    ],
    "abstract": "  In this study, we aim to develop a model that comprehends a natural language\ninstruction (e.g., \"Go to the living room and get the nearest pillow to the\nradio art on the wall\") and generates a segmentation mask for the target\neveryday object. The task is challenging because it requires (1) the\nunderstanding of the referring expressions for multiple objects in the\ninstruction, (2) the prediction of the target phrase of the sentence among the\nmultiple phrases, and (3) the generation of pixel-wise segmentation masks\nrather than bounding boxes. Studies have been conducted on languagebased\nsegmentation methods; however, they sometimes mask irrelevant regions for\ncomplex sentences. In this paper, we propose the Multimodal Diffusion\nSegmentation Model (MDSM), which generates a mask in the first stage and\nrefines it in the second stage. We introduce a crossmodal parallel feature\nextraction mechanism and extend diffusion probabilistic models to handle\ncrossmodal features. To validate our model, we built a new dataset based on the\nwell-known Matterport3D and REVERIE datasets. This dataset consists of\ninstructions with complex referring expressions accompanied by real indoor\nenvironmental images that feature various target objects, in addition to\npixel-wise segmentation masks. The performance of MDSM surpassed that of the\nbaseline method by a large margin of +10.13 mean IoU.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.RO",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-07-17T16:07:07Z",
    "updated": "2023-07-17T16:07:07Z",
    "doi": null
  },
  "2211.08428": {
    "id": "http://arxiv.org/abs/2211.08428v2",
    "title": "CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming",
    "authors": [
      "Qihua Zhou",
      "Ruibin Li",
      "Song Guo",
      "Peiran Dong",
      "Yi Liu",
      "Jingcai Guo",
      "Zhenda Xu"
    ],
    "abstract": "  Recent years have witnessed the dramatic growth of Internet video traffic,\nwhere the video bitstreams are often compressed and delivered in low quality to\nfit the streamer's uplink bandwidth. To alleviate the quality degradation, it\ncomes the rise of Neural-enhanced Video Streaming (NVS), which shows great\nprospects for recovering low-quality videos by mostly deploying neural\nsuper-resolution (SR) on the media server. Despite its benefit, we reveal that\ncurrent mainstream works with SR enhancement have not achieved the desired\nrate-distortion trade-off between bitrate saving and quality restoration, due\nto: (1) overemphasizing the enhancement on the decoder side while omitting the\nco-design of encoder, (2) limited generative capacity to recover high-fidelity\nperceptual details, and (3) optimizing the compression-and-restoration pipeline\nfrom the resolution perspective solely, without considering color bit-depth.\nAiming at overcoming these limitations, we are the first to conduct an\nencoder-decoder (i.e., codec) synergy by leveraging the inherent\nvisual-generative property of diffusion models. Specifically, we present the\nCodec-aware Diffusion Modeling (CaDM), a novel NVS paradigm to significantly\nreduce streaming delivery bitrates while holding pretty higher restoration\ncapacity over existing methods. First, CaDM improves the encoder's compression\nefficiency by simultaneously reducing resolution and color bit-depth of video\nframes. Second, CaDM empowers the decoder with high-quality enhancement by\nmaking the denoising diffusion restoration aware of encoder's resolution-color\nconditions. Evaluation on public cloud services with OpenMMLab benchmarks shows\nthat CaDM effectively saves up to 5.12 - 21.44 times bitrates based on common\nvideo standards and achieves much better recovery quality (e.g., FID of 0.61)\nover state-of-the-art neural-enhancing methods.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-11-15T05:14:48Z",
    "updated": "2023-03-08T14:25:10Z",
    "doi": null
  },
  "2309.16585": {
    "id": "http://arxiv.org/abs/2309.16585v4",
    "title": "Text-to-3D using Gaussian Splatting",
    "authors": [
      "Zilong Chen",
      "Feng Wang",
      "Yikai Wang",
      "Huaping Liu"
    ],
    "abstract": "  Automatic text-to-3D generation that combines Score Distillation Sampling\n(SDS) with the optimization of volume rendering has achieved remarkable\nprogress in synthesizing realistic 3D objects. Yet most existing text-to-3D\nmethods by SDS and volume rendering suffer from inaccurate geometry, e.g., the\nJanus issue, since it is hard to explicitly integrate 3D priors into implicit\n3D representations. Besides, it is usually time-consuming for them to generate\nelaborate 3D models with rich colors. In response, this paper proposes GSGEN, a\nnovel method that adopts Gaussian Splatting, a recent state-of-the-art\nrepresentation, to text-to-3D generation. GSGEN aims at generating high-quality\n3D objects and addressing existing shortcomings by exploiting the explicit\nnature of Gaussian Splatting that enables the incorporation of 3D prior.\nSpecifically, our method adopts a progressive optimization strategy, which\nincludes a geometry optimization stage and an appearance refinement stage. In\ngeometry optimization, a coarse representation is established under 3D point\ncloud diffusion prior along with the ordinary 2D SDS optimization, ensuring a\nsensible and 3D-consistent rough shape. Subsequently, the obtained Gaussians\nundergo an iterative appearance refinement to enrich texture details. In this\nstage, we increase the number of Gaussians by compactness-based densification\nto enhance continuity and improve fidelity. With these designs, our approach\ncan generate 3D assets with delicate details and accurate geometry. Extensive\nevaluations demonstrate the effectiveness of our method, especially for\ncapturing high-frequency components. Our code is available at\nhttps://github.com/gsgen3d/gsgen\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-09-28T16:44:31Z",
    "updated": "2024-04-02T05:10:02Z",
    "doi": null
  },
  "2310.08992": {
    "id": "http://arxiv.org/abs/2310.08992v3",
    "title": "CodeChain: Towards Modular Code Generation Through Chain of\n  Self-revisions with Representative Sub-modules",
    "authors": [
      "Hung Le",
      "Hailin Chen",
      "Amrita Saha",
      "Akash Gokul",
      "Doyen Sahoo",
      "Shafiq Joty"
    ],
    "abstract": "  Large Language Models (LLMs) have already become quite proficient at solving\nsimpler programming tasks like those in HumanEval or MBPP benchmarks. However,\nsolving more complex and competitive programming tasks is still quite\nchallenging for these models - possibly due to their tendency to generate\nsolutions as monolithic code blocks instead of decomposing them into logical\nsub-tasks and sub-modules. On the other hand, experienced programmers\ninstinctively write modularized code with abstraction for solving complex\ntasks, often reusing previously developed modules. To address this gap, we\npropose CodeChain, a novel framework for inference that elicits modularized\ncode generation through a chain of self-revisions, each being guided by some\nrepresentative sub-modules generated in previous iterations. Concretely,\nCodeChain first instructs the LLM to generate modularized codes through\nchain-of-thought prompting. Then it applies a chain of self-revisions by\niterating the two steps: 1) extracting and clustering the generated sub-modules\nand selecting the cluster representatives as the more generic and re-usable\nimplementations, and 2) augmenting the original chain-of-thought prompt with\nthese selected module-implementations and instructing the LLM to re-generate\nnew modularized solutions. We find that by naturally encouraging the LLM to\nreuse the previously developed and verified sub-modules, CodeChain can\nsignificantly boost both modularity as well as correctness of the generated\nsolutions, achieving relative pass@1 improvements of 35% on APPS and 76% on\nCodeContests. It is shown to be effective on both OpenAI LLMs as well as\nopen-sourced LLMs like WizardCoder. We also conduct comprehensive ablation\nstudies with different methods of prompting, number of clusters, model sizes,\nprogram qualities, etc., to provide useful insights that underpin CodeChain's\nsuccess.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.PL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-13T10:17:48Z",
    "updated": "2024-03-14T03:29:09Z",
    "doi": null
  },
  "1911.04252": {
    "id": "http://arxiv.org/abs/1911.04252v4",
    "title": "Self-training with Noisy Student improves ImageNet classification",
    "authors": [
      "Qizhe Xie",
      "Minh-Thang Luong",
      "Eduard Hovy",
      "Quoc V. Le"
    ],
    "abstract": "  We present Noisy Student Training, a semi-supervised learning approach that\nworks well even when labeled data is abundant. Noisy Student Training achieves\n88.4% top-1 accuracy on ImageNet, which is 2.0% better than the\nstate-of-the-art model that requires 3.5B weakly labeled Instagram images. On\nrobustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to\n83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces\nImageNet-P mean flip rate from 27.8 to 12.2.\n  Noisy Student Training extends the idea of self-training and distillation\nwith the use of equal-or-larger student models and noise added to the student\nduring learning. On ImageNet, we first train an EfficientNet model on labeled\nimages and use it as a teacher to generate pseudo labels for 300M unlabeled\nimages. We then train a larger EfficientNet as a student model on the\ncombination of labeled and pseudo labeled images. We iterate this process by\nputting back the student as the teacher. During the learning of the student, we\ninject noise such as dropout, stochastic depth, and data augmentation via\nRandAugment to the student so that the student generalizes better than the\nteacher. Models are available at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.\nCode is available at https://github.com/google-research/noisystudent.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-11-11T18:59:27Z",
    "updated": "2020-06-19T17:36:57Z",
    "doi": null
  },
  "2404.03577": {
    "id": "http://arxiv.org/abs/2404.03577v1",
    "title": "Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning\n  Skills in Large Language Models",
    "authors": [
      "Yantao Liu",
      "Zijun Yao",
      "Xin Lv",
      "Yuchen Fan",
      "Shulin Cao",
      "Jifan Yu",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "  Providing knowledge documents for large language models (LLMs) has emerged as\na promising solution to update the static knowledge inherent in their\nparameters. However, knowledge in the document may conflict with the memory of\nLLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leads\nto the necessity of examining the capability of LLMs to assimilate supplemental\nexternal knowledge that conflicts with their memory. While previous studies\nhave explained to what extent LLMs extract conflicting knowledge from the\nprovided text, they neglect the necessity to reason with conflicting knowledge.\nFurthermore, there lack a detailed analysis on strategies to enable LLMs to\nresolve conflicting knowledge via prompting, decoding strategy, and supervised\nfine-tuning. To address these limitations, we construct a new dataset, dubbed\nKNOT, for knowledge conflict resolution examination in the form of question\nanswering. KNOT facilitates in-depth analysis by dividing reasoning with\nconflicting knowledge into three levels: (1) Direct Extraction, which directly\nextracts conflicting knowledge to answer questions. (2) Explicit Reasoning,\nwhich reasons with conflicting knowledge when the reasoning path is explicitly\nprovided in the question. (3) Implicit Reasoning, where reasoning with\nconflicting knowledge requires LLMs to infer the reasoning path independently\nto answer questions. We also conduct extensive experiments on KNOT to establish\nempirical guidelines for LLMs to utilize conflicting knowledge in complex\ncircumstances. Dataset and associated codes can be accessed at\nhttps://github.com/THU-KEG/KNOT .\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-04-04T16:40:11Z",
    "updated": "2024-04-04T16:40:11Z",
    "doi": null
  },
  "2301.13195": {
    "id": "http://arxiv.org/abs/2301.13195v2",
    "title": "Adaptive Computation with Elastic Input Sequence",
    "authors": [
      "Fuzhao Xue",
      "Valerii Likhosherstov",
      "Anurag Arnab",
      "Neil Houlsby",
      "Mostafa Dehghani",
      "Yang You"
    ],
    "abstract": "  Humans have the ability to adapt the type of information they use, the\nprocedure they employ, and the amount of time they spend when solving problems.\nHowever, most standard neural networks have a fixed function type and\ncomputation budget regardless of the sample's nature or difficulty. Adaptivity\nis a powerful paradigm as it not only imbues practitioners with flexibility\npertaining to the downstream usage of these models but can also serve as a\npowerful inductive bias for solving certain challenging classes of problems. In\nthis work, we introduce a new approach called AdaTape, which allows for dynamic\ncomputation in neural networks through adaptive tape tokens. AdaTape utilizes\nan elastic input sequence by equipping an architecture with a dynamic\nread-and-write tape. Specifically, we adaptively generate input sequences using\ntape tokens obtained from a tape bank which can be either trainable or derived\nfrom input data. We examine the challenges and requirements to obtain dynamic\nsequence content and length, and propose the Adaptive Tape Reading (ATR)\nalgorithm to achieve both goals. Through extensive experiments on image\nrecognition tasks, we show that AdaTape can achieve better performance while\nmaintaining the computational cost. To facilitate further research, we have\nreleased code at https://github.com/google-research/scenic.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-01-30T18:57:24Z",
    "updated": "2023-06-03T06:19:23Z",
    "doi": null
  },
  "1912.00953": {
    "id": "http://arxiv.org/abs/1912.00953v2",
    "title": "LOGAN: Latent Optimisation for Generative Adversarial Networks",
    "authors": [
      "Yan Wu",
      "Jeff Donahue",
      "David Balduzzi",
      "Karen Simonyan",
      "Timothy Lillicrap"
    ],
    "abstract": "  Training generative adversarial networks requires balancing of delicate\nadversarial dynamics. Even with careful tuning, training may diverge or end up\nin a bad equilibrium with dropped modes. In this work, we improve CS-GAN with\nnatural gradient-based latent optimisation and show that it improves\nadversarial dynamics by enhancing interactions between the discriminator and\nthe generator. Our experiments demonstrate that latent optimisation can\nsignificantly improve GAN training, obtaining state-of-the-art performance for\nthe ImageNet ($128 \\times 128$) dataset. Our model achieves an Inception Score\n(IS) of $148$ and an Fr\\'echet Inception Distance (FID) of $3.4$, an\nimprovement of $17\\%$ and $32\\%$ in IS and FID respectively, compared with the\nbaseline BigGAN-deep model with the same architecture and number of parameters.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-12-02T17:30:05Z",
    "updated": "2020-07-01T16:53:32Z",
    "doi": null
  },
  "2010.05646": {
    "id": "http://arxiv.org/abs/2010.05646v2",
    "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High\n  Fidelity Speech Synthesis",
    "authors": [
      "Jungil Kong",
      "Jaehyeon Kim",
      "Jaekyoung Bae"
    ],
    "abstract": "  Several recent work on speech synthesis have employed generative adversarial\nnetworks (GANs) to produce raw waveforms. Although such methods improve the\nsampling efficiency and memory usage, their sample quality has not yet reached\nthat of autoregressive and flow-based generative models. In this work, we\npropose HiFi-GAN, which achieves both efficient and high-fidelity speech\nsynthesis. As speech audio consists of sinusoidal signals with various periods,\nwe demonstrate that modeling periodic patterns of an audio is crucial for\nenhancing sample quality. A subjective human evaluation (mean opinion score,\nMOS) of a single speaker dataset indicates that our proposed method\ndemonstrates similarity to human quality while generating 22.05 kHz\nhigh-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We\nfurther show the generality of HiFi-GAN to the mel-spectrogram inversion of\nunseen speakers and end-to-end speech synthesis. Finally, a small footprint\nversion of HiFi-GAN generates samples 13.4 times faster than real-time on CPU\nwith comparable quality to an autoregressive counterpart.\n",
    "categories": [
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.AS",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-10-12T12:33:43Z",
    "updated": "2020-10-23T09:12:04Z",
    "doi": null
  },
  "2403.07807": {
    "id": "http://arxiv.org/abs/2403.07807v1",
    "title": "StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting",
    "authors": [
      "Kunhao Liu",
      "Fangneng Zhan",
      "Muyu Xu",
      "Christian Theobalt",
      "Ling Shao",
      "Shijian Lu"
    ],
    "abstract": "  We introduce StyleGaussian, a novel 3D style transfer technique that allows\ninstant transfer of any image's style to a 3D scene at 10 frames per second\n(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style\ntransfer without compromising its real-time rendering ability and multi-view\nconsistency. It achieves instant style transfer with three steps: embedding,\ntransfer, and decoding. Initially, 2D VGG scene features are embedded into\nreconstructed 3D Gaussians. Next, the embedded features are transformed\naccording to a reference style image. Finally, the transformed features are\ndecoded into the stylized RGB. StyleGaussian has two novel designs. The first\nis an efficient feature rendering strategy that first renders low-dimensional\nfeatures and then maps them into high-dimensional features while embedding VGG\nfeatures. It cuts the memory consumption significantly and enables 3DGS to\nrender the high-dimensional memory-intensive features. The second is a\nK-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized\nfeatures, it eliminates the 2D CNN operations that compromise strict multi-view\nconsistency. Extensive experiments show that StyleGaussian achieves instant 3D\nstylization with superior stylization quality while preserving real-time\nrendering and strict multi-view consistency. Project page:\nhttps://kunhao-liu.github.io/StyleGaussian/\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-03-12T16:44:52Z",
    "updated": "2024-03-12T16:44:52Z",
    "doi": null
  },
  "2304.13157": {
    "id": "http://arxiv.org/abs/2304.13157v1",
    "title": "Generative Relevance Feedback with Large Language Models",
    "authors": [
      "Iain Mackie",
      "Shubham Chatterjee",
      "Jeffrey Dalton"
    ],
    "abstract": "  Current query expansion models use pseudo-relevance feedback to improve\nfirst-pass retrieval effectiveness; however, this fails when the initial\nresults are not relevant. Instead of building a language model from retrieved\nresults, we propose Generative Relevance Feedback (GRF) that builds\nprobabilistic feedback models from long-form text generated from Large Language\nModels. We study the effective methods for generating text by varying the\nzero-shot generation subtasks: queries, entities, facts, news articles,\ndocuments, and essays. We evaluate GRF on document retrieval benchmarks\ncovering a diverse set of queries and document collections, and the results\nshow that GRF methods significantly outperform previous PRF methods.\nSpecifically, we improve MAP between 5-19% and NDCG@10 17-24% compared to RM3\nexpansion, and achieve the best R@1k effectiveness on all datasets compared to\nstate-of-the-art sparse, dense, and expansion models.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "H.3.3",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-04-25T21:35:29Z",
    "updated": "2023-04-25T21:35:29Z",
    "doi": "10.1145/3539618.3591992"
  },
  "1703.04782": {
    "id": "http://arxiv.org/abs/1703.04782v3",
    "title": "Online Learning Rate Adaptation with Hypergradient Descent",
    "authors": [
      "Atilim Gunes Baydin",
      "Robert Cornish",
      "David Martinez Rubio",
      "Mark Schmidt",
      "Frank Wood"
    ],
    "abstract": "  We introduce a general method for improving the convergence rate of\ngradient-based optimizers that is easy to implement and works well in practice.\nWe demonstrate the effectiveness of the method in a range of optimization\nproblems by applying it to stochastic gradient descent, stochastic gradient\ndescent with Nesterov momentum, and Adam, showing that it significantly reduces\nthe need for the manual tuning of the initial learning rate for these commonly\nused algorithms. Our method works by dynamically updating the learning rate\nduring optimization using the gradient with respect to the learning rate of the\nupdate rule itself. Computing this \"hypergradient\" needs little additional\ncomputation, requires only one extra copy of the original gradient to be stored\nin memory, and relies upon nothing more than what is provided by reverse-mode\nautomatic differentiation.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "68T05",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "G.1.6; I.2.6",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2017-03-14T22:28:27Z",
    "updated": "2018-02-26T01:36:49Z",
    "doi": null
  },
  "2409.04847": {
    "id": "http://arxiv.org/abs/2409.04847v1",
    "title": "Rethinking The Training And Evaluation of Rich-Context Layout-to-Image\n  Generation",
    "authors": [
      "Jiaxin Cheng",
      "Zixu Zhao",
      "Tong He",
      "Tianjun Xiao",
      "Yicong Zhou",
      "Zheng Zhang"
    ],
    "abstract": "  Recent advancements in generative models have significantly enhanced their\ncapacity for image generation, enabling a wide range of applications such as\nimage editing, completion and video editing. A specialized area within\ngenerative modeling is layout-to-image (L2I) generation, where predefined\nlayouts of objects guide the generative process. In this study, we introduce a\nnovel regional cross-attention module tailored to enrich layout-to-image\ngeneration. This module notably improves the representation of layout regions,\nparticularly in scenarios where existing methods struggle with highly complex\nand detailed textual descriptions. Moreover, while current open-vocabulary L2I\nmethods are trained in an open-set setting, their evaluations often occur in\nclosed-set environments. To bridge this gap, we propose two metrics to assess\nL2I performance in open-vocabulary scenarios. Additionally, we conduct a\ncomprehensive user study to validate the consistency of these metrics with\nhuman preferences.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-09-07T14:57:03Z",
    "updated": "2024-09-07T14:57:03Z",
    "doi": null
  },
  "2212.05922": {
    "id": "http://arxiv.org/abs/2212.05922v3",
    "title": "Audiovisual Masked Autoencoders",
    "authors": [
      "Mariana-Iuliana Georgescu",
      "Eduardo Fonseca",
      "Radu Tudor Ionescu",
      "Mario Lucic",
      "Cordelia Schmid",
      "Anurag Arnab"
    ],
    "abstract": "  Can we leverage the audiovisual information already present in video to\nimprove self-supervised representation learning? To answer this question, we\nstudy various pretraining architectures and objectives within the masked\nautoencoding framework, motivated by the success of similar methods in natural\nlanguage and image understanding. We show that we can achieve significant\nimprovements on audiovisual downstream classification tasks, surpassing the\nstate-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our\naudiovisual pretraining scheme for multiple unimodal downstream tasks using a\nsingle audiovisual pretrained model. We additionally demonstrate the\ntransferability of our representations, achieving state-of-the-art audiovisual\nresults on Epic Kitchens without pretraining specifically for this dataset.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SD",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-12-09T17:34:53Z",
    "updated": "2024-01-04T16:52:34Z",
    "doi": null
  },
  "2004.04398": {
    "id": "http://arxiv.org/abs/2004.04398v2",
    "title": "Online Meta-Learning for Multi-Source and Semi-Supervised Domain\n  Adaptation",
    "authors": [
      "Da Li",
      "Timothy Hospedales"
    ],
    "abstract": "  Domain adaptation (DA) is the topical problem of adapting models from\nlabelled source datasets so that they perform well on target datasets where\nonly unlabelled or partially labelled data is available. Many methods have been\nproposed to address this problem through different ways to minimise the domain\nshift between source and target datasets. In this paper we take an orthogonal\nperspective and propose a framework to further enhance performance by\nmeta-learning the initial conditions of existing DA algorithms. This is\nchallenging compared to the more widely considered setting of few-shot\nmeta-learning, due to the length of the computation graph involved. Therefore\nwe propose an online shortest-path meta-learning framework that is both\ncomputationally tractable and practically effective for improving DA\nperformance. We present variants for both multi-source unsupervised domain\nadaptation (MSDA), and semi-supervised domain adaptation (SSDA). Importantly,\nour approach is agnostic to the base adaptation algorithm, and can be applied\nto improve many techniques. Experimentally, we demonstrate improvements on\nclassic (DANN) and recent (MCD and MME) techniques for MSDA and SSDA, and\nultimately achieve state of the art results on several DA benchmarks including\nthe largest scale DomainNet.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-09T07:48:22Z",
    "updated": "2020-07-27T12:55:37Z",
    "doi": null
  },
  "1911.02685": {
    "id": "http://arxiv.org/abs/1911.02685v3",
    "title": "A Comprehensive Survey on Transfer Learning",
    "authors": [
      "Fuzhen Zhuang",
      "Zhiyuan Qi",
      "Keyu Duan",
      "Dongbo Xi",
      "Yongchun Zhu",
      "Hengshu Zhu",
      "Hui Xiong",
      "Qing He"
    ],
    "abstract": "  Transfer learning aims at improving the performance of target learners on\ntarget domains by transferring the knowledge contained in different but related\nsource domains. In this way, the dependence on a large number of target domain\ndata can be reduced for constructing target learners. Due to the wide\napplication prospects, transfer learning has become a popular and promising\narea in machine learning. Although there are already some valuable and\nimpressive surveys on transfer learning, these surveys introduce approaches in\na relatively isolated way and lack the recent advances in transfer learning.\nDue to the rapid expansion of the transfer learning area, it is both necessary\nand challenging to comprehensively review the relevant studies. This survey\nattempts to connect and systematize the existing transfer learning researches,\nas well as to summarize and interpret the mechanisms and the strategies of\ntransfer learning in a comprehensive way, which may help readers have a better\nunderstanding of the current research status and ideas. Unlike previous\nsurveys, this survey paper reviews more than forty representative transfer\nlearning approaches, especially homogeneous transfer learning approaches, from\nthe perspectives of data and model. The applications of transfer learning are\nalso briefly introduced. In order to show the performance of different transfer\nlearning models, over twenty representative transfer learning models are used\nfor experiments. The models are performed on three different datasets, i.e.,\nAmazon Reviews, Reuters-21578, and Office-31. And the experimental results\ndemonstrate the importance of selecting appropriate transfer learning models\nfor different applications in practice.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-11-07T00:15:02Z",
    "updated": "2020-06-23T15:52:46Z",
    "doi": null
  },
  "1802.05365": {
    "id": "http://arxiv.org/abs/1802.05365v2",
    "title": "Deep contextualized word representations",
    "authors": [
      "Matthew E. Peters",
      "Mark Neumann",
      "Mohit Iyyer",
      "Matt Gardner",
      "Christopher Clark",
      "Kenton Lee",
      "Luke Zettlemoyer"
    ],
    "abstract": "  We introduce a new type of deep contextualized word representation that\nmodels both (1) complex characteristics of word use (e.g., syntax and\nsemantics), and (2) how these uses vary across linguistic contexts (i.e., to\nmodel polysemy). Our word vectors are learned functions of the internal states\nof a deep bidirectional language model (biLM), which is pre-trained on a large\ntext corpus. We show that these representations can be easily added to existing\nmodels and significantly improve the state of the art across six challenging\nNLP problems, including question answering, textual entailment and sentiment\nanalysis. We also present an analysis showing that exposing the deep internals\nof the pre-trained network is crucial, allowing downstream models to mix\ndifferent types of semi-supervision signals.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-02-15T00:05:11Z",
    "updated": "2018-03-22T21:59:40Z",
    "doi": null
  },
  "2006.00719": {
    "id": "http://arxiv.org/abs/2006.00719v3",
    "title": "ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning",
    "authors": [
      "Zhewei Yao",
      "Amir Gholami",
      "Sheng Shen",
      "Mustafa Mustafa",
      "Kurt Keutzer",
      "Michael W. Mahoney"
    ],
    "abstract": "  We introduce ADAHESSIAN, a second order stochastic optimization algorithm\nwhich dynamically incorporates the curvature of the loss function via ADAptive\nestimates of the HESSIAN. Second order algorithms are among the most powerful\noptimization algorithms with superior convergence properties as compared to\nfirst order methods such as SGD and Adam. The main disadvantage of traditional\nsecond order methods is their heavier per iteration computation and poor\naccuracy as compared to first order methods. To address these, we incorporate\nseveral novel approaches in ADAHESSIAN, including: (i) a fast Hutchinson based\nmethod to approximate the curvature matrix with low computational overhead;\n(ii) a root-mean-square exponential moving average to smooth out variations of\nthe Hessian diagonal across different iterations; and (iii) a block diagonal\naveraging to reduce the variance of Hessian diagonal elements. We show that\nADAHESSIAN achieves new state-of-the-art results by a large margin as compared\nto other adaptive optimization methods, including variants of Adam. In\nparticular, we perform extensive tests on CV, NLP, and recommendation system\ntasks and find that ADAHESSIAN: (i) achieves 1.80%/1.45% higher accuracy on\nResNets20/32 on Cifar10, and 5.55% higher accuracy on ImageNet as compared to\nAdam; (ii) outperforms AdamW for transformers by 0.13/0.33 BLEU score on\nIWSLT14/WMT14 and 2.7/1.0 PPL on PTB/Wikitext-103; (iii) outperforms AdamW for\nSqueezeBert by 0.41 points on GLUE; and (iv) achieves 0.032% better score than\nAdagrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the\ncost per iteration of ADAHESSIAN is comparable to first order methods, and that\nit exhibits robustness towards its hyperparameters.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.NA",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "math.NA",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-06-01T05:00:51Z",
    "updated": "2021-04-29T00:52:09Z",
    "doi": null
  },
  "2109.03867": {
    "id": "http://arxiv.org/abs/2109.03867v4",
    "title": "LSB: Local Self-Balancing MCMC in Discrete Spaces",
    "authors": [
      "Emanuele Sansone"
    ],
    "abstract": "  We present the Local Self-Balancing sampler (LSB), a local Markov Chain Monte\nCarlo (MCMC) method for sampling in purely discrete domains, which is able to\nautonomously adapt to the target distribution and to reduce the number of\ntarget evaluations required to converge. LSB is based on (i) a parametrization\nof locally balanced proposals, (ii) a newly proposed objective function based\non mutual information and (iii) a self-balancing learning procedure, which\nminimises the proposed objective to update the proposal parameters. Experiments\non energy-based models and Markov networks show that LSB converges using a\nsmaller number of queries to the oracle distribution compared to recent local\nMCMC samplers.\n",
    "categories": [
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-09-08T18:31:26Z",
    "updated": "2022-07-04T19:17:34Z",
    "doi": null
  },
  "2312.11666": {
    "id": "http://arxiv.org/abs/2312.11666v1",
    "title": "HAAR: Text-Conditioned Generative Model of 3D Strand-based Human\n  Hairstyles",
    "authors": [
      "Vanessa Sklyarova",
      "Egor Zakharov",
      "Otmar Hilliges",
      "Michael J. Black",
      "Justus Thies"
    ],
    "abstract": "  We present HAAR, a new strand-based generative model for 3D human hairstyles.\nSpecifically, based on textual inputs, HAAR produces 3D hairstyles that could\nbe used as production-level assets in modern computer graphics engines. Current\nAI-based generative models take advantage of powerful 2D priors to reconstruct\n3D content in the form of point clouds, meshes, or volumetric functions.\nHowever, by using the 2D priors, they are intrinsically limited to only\nrecovering the visual parts. Highly occluded hair structures can not be\nreconstructed with those methods, and they only model the ''outer shell'',\nwhich is not ready to be used in physics-based rendering or simulation\npipelines. In contrast, we propose a first text-guided generative method that\nuses 3D hair strands as an underlying representation. Leveraging 2D visual\nquestion-answering (VQA) systems, we automatically annotate synthetic hair\nmodels that are generated from a small set of artist-created hairstyles. This\nallows us to train a latent diffusion model that operates in a common hairstyle\nUV space. In qualitative and quantitative studies, we demonstrate the\ncapabilities of the proposed model and compare it to existing hairstyle\ngeneration approaches.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-18T19:19:32Z",
    "updated": "2023-12-18T19:19:32Z",
    "doi": null
  },
  "1801.03244": {
    "id": "http://arxiv.org/abs/1801.03244v1",
    "title": "eCommerceGAN : A Generative Adversarial Network for E-commerce",
    "authors": [
      "Ashutosh Kumar",
      "Arijit Biswas",
      "Subhajit Sanyal"
    ],
    "abstract": "  E-commerce companies such as Amazon, Alibaba and Flipkart process billions of\norders every year. However, these orders represent only a small fraction of all\nplausible orders. Exploring the space of all plausible orders could help us\nbetter understand the relationships between the various entities in an\ne-commerce ecosystem, namely the customers and the products they purchase. In\nthis paper, we propose a Generative Adversarial Network (GAN) for orders made\nin e-commerce websites. Once trained, the generator in the GAN could generate\nany number of plausible orders. Our contributions include: (a) creating a dense\nand low-dimensional representation of e-commerce orders, (b) train an\necommerceGAN (ecGAN) with real orders to show the feasibility of the proposed\nparadigm, and (c) train an ecommerce-conditional-GAN (ec^2GAN) to generate the\nplausible orders involving a particular product. We propose several qualitative\nmethods to evaluate ecGAN and demonstrate its effectiveness. The ec^2GAN is\nused for various kinds of characterization of possible orders involving a\nproduct that has just been introduced into the e-commerce system. The proposed\napproach ec^2GAN performs significantly better than the baseline in most of the\nscenarios.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-01-10T05:58:09Z",
    "updated": "2018-01-10T05:58:09Z",
    "doi": null
  },
  "2406.11280": {
    "id": "http://arxiv.org/abs/2406.11280v1",
    "title": "i-SRT: Aligning Large Multimodal Models for Videos by Iterative\n  Self-Retrospective Judgment",
    "authors": [
      "Daechul Ahn",
      "Yura Choi",
      "San Kim",
      "Youngjae Yu",
      "Dongyeop Kang",
      "Jonghyun Choi"
    ],
    "abstract": "  Aligning Video Large Multimodal Models (VLMMs) face challenges such as\nmodality misalignment and verbose responses. Although iterative approaches such\nas self-rewarding or iterative direct preference optimization (DPO) recently\nshowed a significant improvement in language model alignment, particularly on\nreasoning tasks, self-aligned models applied to large video-language models\noften result in lengthy and irrelevant responses. To address these challenges,\nwe propose a novel method that employs self-retrospection to enhance both\nresponse generation and preference modeling, and call iterative\nself-retrospective judgment (i-SRT). By revisiting and evaluating already\ngenerated content and preference in loop, i-SRT improves the alignment between\ntextual and visual modalities, reduce verbosity, and enhances content\nrelevance. Our empirical evaluations across diverse video question answering\nbenchmarks demonstrate that i-SRT significantly outperforms prior arts. We are\ncommitted to opensourcing our code, models, and datasets to encourage further\ninvestigation.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-06-17T07:33:30Z",
    "updated": "2024-06-17T07:33:30Z",
    "doi": null
  },
  "2306.00783": {
    "id": "http://arxiv.org/abs/2306.00783v2",
    "title": "FaceDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and\n  Relighting with Diffusion Models",
    "authors": [
      "Hao Zhang",
      "Yanbo Xu",
      "Tianyuan Dai",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ],
    "abstract": "  The ability to create high-quality 3D faces from a single image has become\nincreasingly important with wide applications in video conferencing, AR/VR, and\nadvanced video editing in movie industries. In this paper, we propose Face\nDiffusion NeRF (FaceDNeRF), a new generative method to reconstruct high-quality\nFace NeRFs from single images, complete with semantic editing and relighting\ncapabilities. FaceDNeRF utilizes high-resolution 3D GAN inversion and expertly\ntrained 2D latent-diffusion model, allowing users to manipulate and construct\nFace NeRFs in zero-shot learning without the need for explicit 3D data. With\ncarefully designed illumination and identity preserving loss, as well as\nmulti-modal pre-training, FaceDNeRF offers users unparalleled control over the\nediting process enabling them to create and edit face NeRFs using just\nsingle-view images, text prompts, and explicit target lighting. The advanced\nfeatures of FaceDNeRF have been designed to produce more impressive results\nthan existing 2D editing approaches that rely on 2D segmentation maps for\neditable attributes. Experiments show that our FaceDNeRF achieves exceptionally\nrealistic results and unprecedented flexibility in editing compared with\nstate-of-the-art 3D face reconstruction and editing methods. Our code will be\navailable at https://github.com/BillyXYB/FaceDNeRF.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-01T15:14:39Z",
    "updated": "2023-12-04T16:25:49Z",
    "doi": null
  },
  "2306.12991": {
    "id": "http://arxiv.org/abs/2306.12991v2",
    "title": "Speech Emotion Diarization: Which Emotion Appears When?",
    "authors": [
      "Yingzhi Wang",
      "Mirco Ravanelli",
      "Alya Yacoubi"
    ],
    "abstract": "  Speech Emotion Recognition (SER) typically relies on utterance-level\nsolutions. However, emotions conveyed through speech should be considered as\ndiscrete speech events with definite temporal boundaries, rather than\nattributes of the entire utterance. To reflect the fine-grained nature of\nspeech emotions, we propose a new task: Speech Emotion Diarization (SED). Just\nas Speaker Diarization answers the question of \"Who speaks when?\", Speech\nEmotion Diarization answers the question of \"Which emotion appears when?\". To\nfacilitate the evaluation of the performance and establish a common benchmark\nfor researchers, we introduce the Zaion Emotion Dataset (ZED), an openly\naccessible speech emotion dataset that includes non-acted emotions recorded in\nreal-life conditions, along with manually-annotated boundaries of emotion\nsegments within the utterance. We provide competitive baselines and open-source\nthe code and the pre-trained models.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-22T15:47:36Z",
    "updated": "2023-10-20T11:31:26Z",
    "doi": null
  },
  "2203.13817": {
    "id": "http://arxiv.org/abs/2203.13817v1",
    "title": "AutoAvatar: Autoregressive Neural Fields for Dynamic Avatar Modeling",
    "authors": [
      "Ziqian Bai",
      "Timur Bagautdinov",
      "Javier Romero",
      "Michael Zollh\u00f6fer",
      "Ping Tan",
      "Shunsuke Saito"
    ],
    "abstract": "  Neural fields such as implicit surfaces have recently enabled avatar modeling\nfrom raw scans without explicit temporal correspondences. In this work, we\nexploit autoregressive modeling to further extend this notion to capture\ndynamic effects, such as soft-tissue deformations. Although autoregressive\nmodels are naturally capable of handling dynamics, it is non-trivial to apply\nthem to implicit representations, as explicit state decoding is infeasible due\nto prohibitive memory requirements. In this work, for the first time, we enable\nautoregressive modeling of implicit avatars. To reduce the memory bottleneck\nand efficiently model dynamic implicit surfaces, we introduce the notion of\narticulated observer points, which relate implicit states to the explicit\nsurface of a parametric human body model. We demonstrate that encoding implicit\nsurfaces as a set of height fields defined on articulated observer points leads\nto significantly better generalization compared to a latent representation. The\nexperiments show that our approach outperforms the state of the art, achieving\nplausible dynamic deformations even for unseen motions.\nhttps://zqbai-jeremy.github.io/autoavatar\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-25T17:59:59Z",
    "updated": "2022-03-25T17:59:59Z",
    "doi": null
  },
  "1901.11333": {
    "id": "http://arxiv.org/abs/1901.11333v4",
    "title": "IMaT: Unsupervised Text Attribute Transfer via Iterative Matching and\n  Translation",
    "authors": [
      "Zhijing Jin",
      "Di Jin",
      "Jonas Mueller",
      "Nicholas Matthews",
      "Enrico Santus"
    ],
    "abstract": "  Text attribute transfer aims to automatically rewrite sentences such that\nthey possess certain linguistic attributes, while simultaneously preserving\ntheir semantic content. This task remains challenging due to a lack of\nsupervised parallel data. Existing approaches try to explicitly disentangle\ncontent and attribute information, but this is difficult and often results in\npoor content-preservation and ungrammaticality. In contrast, we propose a\nsimpler approach, Iterative Matching and Translation (IMaT), which: (1)\nconstructs a pseudo-parallel corpus by aligning a subset of semantically\nsimilar sentences from the source and the target corpora; (2) applies a\nstandard sequence-to-sequence model to learn the attribute transfer; (3)\niteratively improves the learned transfer function by refining imperfections in\nthe alignment. In sentiment modification and formality transfer tasks, our\nmethod outperforms complex state-of-the-art systems by a large margin. As an\nauxiliary contribution, we produce a publicly-available test set with\nhuman-generated transfer references.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-01-31T12:41:57Z",
    "updated": "2020-01-24T05:12:45Z",
    "doi": null
  },
  "2306.01732": {
    "id": "http://arxiv.org/abs/2306.01732v1",
    "title": "Video Colorization with Pre-trained Text-to-Image Diffusion Models",
    "authors": [
      "Hanyuan Liu",
      "Minshan Xie",
      "Jinbo Xing",
      "Chengze Li",
      "Tien-Tsin Wong"
    ],
    "abstract": "  Video colorization is a challenging task that involves inferring plausible\nand temporally consistent colors for grayscale frames. In this paper, we\npresent ColorDiffuser, an adaptation of a pre-trained text-to-image latent\ndiffusion model for video colorization. With the proposed adapter-based\napproach, we repropose the pre-trained text-to-image model to accept input\ngrayscale video frames, with the optional text description, for video\ncolorization. To enhance the temporal coherence and maintain the vividness of\ncolorization across frames, we propose two novel techniques: the Color\nPropagation Attention and Alternated Sampling Strategy. Color Propagation\nAttention enables the model to refine its colorization decision based on a\nreference latent frame, while Alternated Sampling Strategy captures\nspatiotemporal dependencies by using the next and previous adjacent latent\nframes alternatively as reference during the generative diffusion sampling\nsteps. This encourages bidirectional color information propagation between\nadjacent video frames, leading to improved color consistency across frames. We\nconduct extensive experiments on benchmark datasets, and the results\ndemonstrate the effectiveness of our proposed framework. The evaluations show\nthat ColorDiffuser achieves state-of-the-art performance in video colorization,\nsurpassing existing methods in terms of color fidelity, temporal consistency,\nand visual quality.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-06-02T17:58:00Z",
    "updated": "2023-06-02T17:58:00Z",
    "doi": null
  },
  "2311.10089": {
    "id": "http://arxiv.org/abs/2311.10089v1",
    "title": "Emu Edit: Precise Image Editing via Recognition and Generation Tasks",
    "authors": [
      "Shelly Sheynin",
      "Adam Polyak",
      "Uriel Singer",
      "Yuval Kirstain",
      "Amit Zohar",
      "Oron Ashual",
      "Devi Parikh",
      "Yaniv Taigman"
    ],
    "abstract": "  Instruction-based image editing holds immense potential for a variety of\napplications, as it enables users to perform any editing operation using a\nnatural language instruction. However, current models in this domain often\nstruggle with accurately executing user instructions. We present Emu Edit, a\nmulti-task image editing model which sets state-of-the-art results in\ninstruction-based image editing. To develop Emu Edit we train it to multi-task\nacross an unprecedented range of tasks, such as region-based editing, free-form\nediting, and Computer Vision tasks, all of which are formulated as generative\ntasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we\nprovide it with learned task embeddings which guide the generation process\ntowards the correct edit type. Both these elements are essential for Emu Edit's\noutstanding performance. Furthermore, we show that Emu Edit can generalize to\nnew tasks, such as image inpainting, super-resolution, and compositions of\nediting tasks, with just a few labeled examples. This capability offers a\nsignificant advantage in scenarios where high-quality samples are scarce.\nLastly, to facilitate a more rigorous and informed assessment of instructable\nimage editing models, we release a new challenging and versatile benchmark that\nincludes seven different image editing tasks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-11-16T18:55:58Z",
    "updated": "2023-11-16T18:55:58Z",
    "doi": null
  },
  "2312.16457": {
    "id": "http://arxiv.org/abs/2312.16457v2",
    "title": "City-on-Web: Real-time Neural Rendering of Large-scale Scenes on the Web",
    "authors": [
      "Kaiwen Song",
      "Xiaoyi Zeng",
      "Chenqu Ren",
      "Juyong Zhang"
    ],
    "abstract": "  Existing neural radiance field-based methods can achieve real-time rendering\nof small scenes on the web platform. However, extending these methods to\nlarge-scale scenes still poses significant challenges due to limited resources\nin computation, memory, and bandwidth. In this paper, we propose City-on-Web,\nthe first method for real-time rendering of large-scale scenes on the web. We\npropose a block-based volume rendering method to guarantee 3D consistency and\ncorrect occlusion between blocks, and introduce a Level-of-Detail strategy\ncombined with dynamic loading/unloading of resources to significantly reduce\nmemory demands. Our system achieves real-time rendering of large-scale scenes\nat approximately 32FPS with RTX 3060 GPU on the web and maintains rendering\nquality comparable to the current state-of-the-art novel view synthesis\nmethods.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-27T08:00:47Z",
    "updated": "2024-04-01T03:10:53Z",
    "doi": null
  },
  "2310.02242": {
    "id": "http://arxiv.org/abs/2310.02242v1",
    "title": "Hierarchical Generation of Human-Object Interactions with Diffusion\n  Probabilistic Models",
    "authors": [
      "Huaijin Pi",
      "Sida Peng",
      "Minghui Yang",
      "Xiaowei Zhou",
      "Hujun Bao"
    ],
    "abstract": "  This paper presents a novel approach to generating the 3D motion of a human\ninteracting with a target object, with a focus on solving the challenge of\nsynthesizing long-range and diverse motions, which could not be fulfilled by\nexisting auto-regressive models or path planning-based methods. We propose a\nhierarchical generation framework to solve this challenge. Specifically, our\nframework first generates a set of milestones and then synthesizes the motion\nalong them. Therefore, the long-range motion generation could be reduced to\nsynthesizing several short motion sequences guided by milestones. The\nexperiments on the NSM, COUCH, and SAMP datasets show that our approach\noutperforms previous methods by a large margin in both quality and diversity.\nThe source code is available on our project page\nhttps://zju3dv.github.io/hghoi.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-03T17:50:23Z",
    "updated": "2023-10-03T17:50:23Z",
    "doi": null
  },
  "2302.04869": {
    "id": "http://arxiv.org/abs/2302.04869v1",
    "title": "Reversible Vision Transformers",
    "authors": [
      "Karttikeya Mangalam",
      "Haoqi Fan",
      "Yanghao Li",
      "Chao-Yuan Wu",
      "Bo Xiong",
      "Christoph Feichtenhofer",
      "Jitendra Malik"
    ],
    "abstract": "  We present Reversible Vision Transformers, a memory efficient architecture\ndesign for visual recognition. By decoupling the GPU memory requirement from\nthe depth of the model, Reversible Vision Transformers enable scaling up\narchitectures with efficient memory usage. We adapt two popular models, namely\nVision Transformer and Multiscale Vision Transformers, to reversible variants\nand benchmark extensively across both model sizes and tasks of image\nclassification, object detection and video classification. Reversible Vision\nTransformers achieve a reduced memory footprint of up to 15.5x at roughly\nidentical model complexity, parameters and accuracy, demonstrating the promise\nof reversible vision transformers as an efficient backbone for hardware\nresource limited training regimes. Finally, we find that the additional\ncomputational burden of recomputing activations is more than overcome for\ndeeper models, where throughput can increase up to 2.3x over their\nnon-reversible counterparts. Full code and trained models are available at\nhttps://github.com/facebookresearch/slowfast. A simpler, easy to understand and\nmodify version is also available at https://github.com/karttikeya/minREV\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-02-09T18:59:54Z",
    "updated": "2023-02-09T18:59:54Z",
    "doi": null
  },
  "2207.11718": {
    "id": "http://arxiv.org/abs/2207.11718v1",
    "title": "TIPS: Text-Induced Pose Synthesis",
    "authors": [
      "Prasun Roy",
      "Subhankar Ghosh",
      "Saumik Bhattacharya",
      "Umapada Pal",
      "Michael Blumenstein"
    ],
    "abstract": "  In computer vision, human pose synthesis and transfer deal with probabilistic\nimage generation of a person in a previously unseen pose from an already\navailable observation of that person. Though researchers have recently proposed\nseveral methods to achieve this task, most of these techniques derive the\ntarget pose directly from the desired target image on a specific dataset,\nmaking the underlying process challenging to apply in real-world scenarios as\nthe generation of the target image is the actual aim. In this paper, we first\npresent the shortcomings of current pose transfer algorithms and then propose a\nnovel text-based pose transfer technique to address those issues. We divide the\nproblem into three independent stages: (a) text to pose representation, (b)\npose refinement, and (c) pose rendering. To the best of our knowledge, this is\none of the first attempts to develop a text-based pose transfer framework where\nwe also introduce a new dataset DF-PASS, by adding descriptive pose annotations\nfor the images of the DeepFashion dataset. The proposed method generates\npromising results with significant qualitative and quantitative scores in our\nexperiments.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-24T11:14:46Z",
    "updated": "2022-07-24T11:14:46Z",
    "doi": null
  },
  "2206.00927": {
    "id": "http://arxiv.org/abs/2206.00927v3",
    "title": "DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling\n  in Around 10 Steps",
    "authors": [
      "Cheng Lu",
      "Yuhao Zhou",
      "Fan Bao",
      "Jianfei Chen",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "abstract": "  Diffusion probabilistic models (DPMs) are emerging powerful generative\nmodels. Despite their high-quality generation performance, DPMs still suffer\nfrom their slow sampling as they generally need hundreds or thousands of\nsequential function evaluations (steps) of large neural networks to draw a\nsample. Sampling from DPMs can be viewed alternatively as solving the\ncorresponding diffusion ordinary differential equations (ODEs). In this work,\nwe propose an exact formulation of the solution of diffusion ODEs. The\nformulation analytically computes the linear part of the solution, rather than\nleaving all terms to black-box ODE solvers as adopted in previous works. By\napplying change-of-variable, the solution can be equivalently simplified to an\nexponentially weighted integral of the neural network. Based on our\nformulation, we propose DPM-Solver, a fast dedicated high-order solver for\ndiffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for\nboth discrete-time and continuous-time DPMs without any further training.\nExperimental results show that DPM-Solver can generate high-quality samples in\nonly 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in\n10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10\ndataset, and a $4\\sim 16\\times$ speedup compared with previous state-of-the-art\ntraining-free samplers on various datasets.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-02T08:43:16Z",
    "updated": "2022-10-13T12:04:36Z",
    "doi": null
  },
  "2111.13677": {
    "id": "http://arxiv.org/abs/2111.13677v3",
    "title": "SWAT: Spatial Structure Within and Among Tokens",
    "authors": [
      "Kumara Kahatapitiya",
      "Michael S. Ryoo"
    ],
    "abstract": "  Modeling visual data as tokens (i.e., image patches) using attention\nmechanisms, feed-forward networks or convolutions has been highly effective in\nrecent years. Such methods usually have a common pipeline: a tokenization\nmethod, followed by a set of layers/blocks for information mixing, both within\nand among tokens. When image patches are converted into tokens, they are often\nflattened, discarding the spatial structure within each patch. As a result, any\nprocessing that follows (eg: multi-head self-attention) may fail to recover\nand/or benefit from such information. In this paper, we argue that models can\nhave significant gains when spatial structure is preserved during tokenization,\nand is explicitly used during the mixing stage. We propose two key\ncontributions: (1) Structure-aware Tokenization and, (2) Structure-aware\nMixing, both of which can be combined with existing models with minimal effort.\nWe introduce a family of models (SWAT), showing improvements over the likes of\nDeiT, MLP-Mixer and Swin Transformer, across multiple benchmarks including\nImageNet classification and ADE20K segmentation. Our code is available at\nhttps://github.com/kkahatapitiya/SWAT.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-11-26T18:59:38Z",
    "updated": "2023-11-20T16:37:05Z",
    "doi": null
  },
  "2110.02488": {
    "id": "http://arxiv.org/abs/2110.02488v2",
    "title": "ABC: Attention with Bounded-memory Control",
    "authors": [
      "Hao Peng",
      "Jungo Kasai",
      "Nikolaos Pappas",
      "Dani Yogatama",
      "Zhaofeng Wu",
      "Lingpeng Kong",
      "Roy Schwartz",
      "Noah A. Smith"
    ],
    "abstract": "  Transformer architectures have achieved state-of-the-art results on a variety\nof sequence modeling tasks. However, their attention mechanism comes with a\nquadratic complexity in sequence lengths, making the computational overhead\nprohibitive, especially for long sequences. Attention context can be seen as a\nrandom-access memory with each token taking a slot. Under this perspective, the\nmemory size grows linearly with the sequence length, and so does the overhead\nof reading from it. One way to improve the efficiency is to bound the memory\nsize. We show that disparate approaches can be subsumed into one abstraction,\nattention with bounded-memory control (ABC), and they vary in their\norganization of the memory. ABC reveals new, unexplored possibilities. First,\nit connects several efficient attention variants that would otherwise seem\napart. Second, this abstraction gives new insights--an established approach\n(Wang et al., 2020b) previously thought to be not applicable in causal\nattention, actually is. Last, we present a new instance of ABC, which draws\ninspiration from existing ABC approaches, but replaces their heuristic\nmemory-organizing functions with a learned, contextualized one. Our experiments\non language modeling, machine translation, and masked language model finetuning\nshow that our approach outperforms previous efficient attention models;\ncompared to the strong transformer baselines, it significantly improves the\ninference time and space efficiency with no or negligible accuracy loss.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-10-06T03:53:25Z",
    "updated": "2022-06-01T23:58:25Z",
    "doi": null
  },
  "1812.06162": {
    "id": "http://arxiv.org/abs/1812.06162v1",
    "title": "An Empirical Model of Large-Batch Training",
    "authors": [
      "Sam McCandlish",
      "Jared Kaplan",
      "Dario Amodei",
      "OpenAI Dota Team"
    ],
    "abstract": "  In an increasing number of domains it has been demonstrated that deep\nlearning models can be trained using relatively large batch sizes without\nsacrificing data efficiency. However the limits of this massive data\nparallelism seem to differ from domain to domain, ranging from batches of tens\nof thousands in ImageNet to batches of millions in RL agents that play the game\nDota 2. To our knowledge there is limited conceptual understanding of why these\nlimits to batch size differ or how we might choose the correct batch size in a\nnew domain. In this paper, we demonstrate that a simple and easy-to-measure\nstatistic called the gradient noise scale predicts the largest useful batch\nsize across many domains and applications, including a number of supervised\nlearning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word),\nreinforcement learning domains (Atari and Dota), and even generative model\ntraining (autoencoders on SVHN). We find that the noise scale increases as the\nloss decreases over a training run and depends on the model size primarily\nthrough improved model performance. Our empirically-motivated theory also\ndescribes the tradeoff between compute-efficiency and time-efficiency, and\nprovides a rough model of the benefits of adaptive batch-size training.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2018-12-14T20:49:09Z",
    "updated": "2018-12-14T20:49:09Z",
    "doi": null
  },
  "2207.11192": {
    "id": "http://arxiv.org/abs/2207.11192v2",
    "title": "Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image\n  Synthesis",
    "authors": [
      "Sangyun Lee",
      "Hyungjin Chung",
      "Jaehyeon Kim",
      "Jong Chul Ye"
    ],
    "abstract": "  Recently, diffusion models have shown remarkable results in image synthesis\nby gradually removing noise and amplifying signals. Although the simple\ngenerative process surprisingly works well, is this the best way to generate\nimage data? For instance, despite the fact that human perception is more\nsensitive to the low frequencies of an image, diffusion models themselves do\nnot consider any relative importance of each frequency component. Therefore, to\nincorporate the inductive bias for image data, we propose a novel generative\nprocess that synthesizes images in a coarse-to-fine manner. First, we\ngeneralize the standard diffusion models by enabling diffusion in a rotated\ncoordinate system with different velocities for each component of the vector.\nWe further propose a blur diffusion as a special case, where each frequency\ncomponent of an image is diffused at different speeds. Specifically, the\nproposed blur diffusion consists of a forward process that blurs an image and\nadds noise gradually, after which a corresponding reverse process deblurs an\nimage and removes noise progressively. Experiments show that the proposed model\noutperforms the previous method in FID on LSUN bedroom and church datasets.\nCode is available at https://github.com/sangyun884/blur-diffusion.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-16T15:00:21Z",
    "updated": "2022-11-21T12:44:45Z",
    "doi": null
  },
  "2207.01056": {
    "id": "http://arxiv.org/abs/2207.01056v2",
    "title": "Counterfactually Measuring and Eliminating Social Bias in\n  Vision-Language Pre-training Models",
    "authors": [
      "Yi Zhang",
      "Junyang Wang",
      "Jitao Sang"
    ],
    "abstract": "  Vision-Language Pre-training (VLP) models have achieved state-of-the-art\nperformance in numerous cross-modal tasks. Since they are optimized to capture\nthe statistical properties of intra- and inter-modality, there remains risk to\nlearn social biases presented in the data as well. In this work, we (1)\nintroduce a counterfactual-based bias measurement \\emph{CounterBias} to\nquantify the social bias in VLP models by comparing the [MASK]ed prediction\nprobabilities of factual and counterfactual samples; (2) construct a novel\nVL-Bias dataset including 24K image-text pairs for measuring gender bias in VLP\nmodels, from which we observed that significant gender bias is prevalent in VLP\nmodels; and (3) propose a VLP debiasing method \\emph{FairVLP} to minimize the\ndifference in the [MASK]ed prediction probabilities between factual and\ncounterfactual image-text pairs for VLP debiasing. Although CounterBias and\nFairVLP focus on social bias, they are generalizable to serve as tools and\nprovide new insights to probe and regularize more knowledge in VLP models.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-03T14:39:32Z",
    "updated": "2022-07-12T02:31:50Z",
    "doi": null
  },
  "2305.07440": {
    "id": "http://arxiv.org/abs/2305.07440v2",
    "title": "Optimizing Memory Mapping Using Deep Reinforcement Learning",
    "authors": [
      "Pengming Wang",
      "Mikita Sazanovich",
      "Berkin Ilbeyi",
      "Phitchaya Mangpo Phothilimthana",
      "Manish Purohit",
      "Han Yang Tay",
      "Ng\u00e2n V\u0169",
      "Miaosen Wang",
      "Cosmin Paduraru",
      "Edouard Leurent",
      "Anton Zhernov",
      "Po-Sen Huang",
      "Julian Schrittwieser",
      "Thomas Hubert",
      "Robert Tung",
      "Paula Kurylowicz",
      "Kieran Milan",
      "Oriol Vinyals",
      "Daniel J. Mankowitz"
    ],
    "abstract": "  Resource scheduling and allocation is a critical component of many high\nimpact systems ranging from congestion control to cloud computing. Finding more\noptimal solutions to these problems often has significant impact on resource\nand time savings, reducing device wear-and-tear, and even potentially improving\ncarbon emissions. In this paper, we focus on a specific instance of a\nscheduling problem, namely the memory mapping problem that occurs during\ncompilation of machine learning programs: That is, mapping tensors to different\nmemory layers to optimize execution time.\n  We introduce an approach for solving the memory mapping problem using\nReinforcement Learning. RL is a solution paradigm well-suited for sequential\ndecision making problems that are amenable to planning, and combinatorial\nsearch spaces with high-dimensional data inputs. We formulate the problem as a\nsingle-player game, which we call the mallocGame, such that high-reward\ntrajectories of the game correspond to efficient memory mappings on the target\nhardware. We also introduce a Reinforcement Learning agent, mallocMuZero, and\nshow that it is capable of playing this game to discover new and improved\nmemory mapping solutions that lead to faster execution times on real ML\nworkloads on ML accelerators. We compare the performance of mallocMuZero to the\ndefault solver used by the Accelerated Linear Algebra (XLA) compiler on a\nbenchmark of realistic ML workloads. In addition, we show that mallocMuZero is\ncapable of improving the execution time of the recently published AlphaTensor\nmatrix multiplication model.\n",
    "categories": [
      {
        "@term": "cs.PF",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-11T11:55:16Z",
    "updated": "2023-10-17T09:53:45Z",
    "doi": null
  },
  "2010.04259": {
    "id": "http://arxiv.org/abs/2010.04259v1",
    "title": "Unsupervised Joint $k$-node Graph Representations with Compositional\n  Energy-Based Models",
    "authors": [
      "Leonardo Cotta",
      "Carlos H. C. Teixeira",
      "Ananthram Swami",
      "Bruno Ribeiro"
    ],
    "abstract": "  Existing Graph Neural Network (GNN) methods that learn inductive unsupervised\ngraph representations focus on learning node and edge representations by\npredicting observed edges in the graph. Although such approaches have shown\nadvances in downstream node classification tasks, they are ineffective in\njointly representing larger $k$-node sets, $k{>}2$. We propose MHM-GNN, an\ninductive unsupervised graph representation approach that combines joint\n$k$-node representations with energy-based models (hypergraph Markov networks)\nand GNNs. To address the intractability of the loss that arises from this\ncombination, we endow our optimization with a loss upper bound using a\nfinite-sample unbiased Markov Chain Monte Carlo estimator. Our experiments show\nthat the unsupervised MHM-GNN representations of MHM-GNN produce better\nunsupervised representations than existing approaches from the literature.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-10-08T21:13:37Z",
    "updated": "2020-10-08T21:13:37Z",
    "doi": null
  },
  "2310.14108": {
    "id": "http://arxiv.org/abs/2310.14108v1",
    "title": "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
    "authors": [
      "Mohammadreza Salehi",
      "Mehrdad Farajtabar",
      "Maxwell Horton",
      "Fartash Faghri",
      "Hadi Pouransari",
      "Raviteja Vemulapalli",
      "Oncel Tuzel",
      "Ali Farhadi",
      "Mohammad Rastegari",
      "Sachin Mehta"
    ],
    "abstract": "  Contrastive language image pretraining (CLIP) is a standard method for\ntraining vision-language models. While CLIP is scalable, promptable, and robust\nto distribution shifts on image classification tasks, it lacks object\nlocalization capabilities. This paper studies the following question: Can we\naugment CLIP training with task-specific vision models from model zoos to\nimprove its visual representations? Towards this end, we leverage open-source\ntask-specific vision models to generate pseudo-labels for an uncurated and\nnoisy image-text dataset. Subsequently, we train CLIP models on these\npseudo-labels in addition to the contrastive training on image and text pairs.\nThis simple setup shows substantial improvements of up to 16.3% across\ndifferent vision tasks, including segmentation, detection, depth estimation,\nand surface normal estimation. Importantly, these enhancements are achieved\nwithout compromising CLIP's existing capabilities, including its proficiency in\npromptable zero-shot classification.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-10-21T20:20:13Z",
    "updated": "2023-10-21T20:20:13Z",
    "doi": null
  },
  "1904.09092": {
    "id": "http://arxiv.org/abs/1904.09092v1",
    "title": "Weakly Supervised Adversarial Domain Adaptation for Semantic\n  Segmentation in Urban Scenes",
    "authors": [
      "Qi Wang",
      "Junyu Gao",
      "Xuelong Li"
    ],
    "abstract": "  Semantic segmentation, a pixel-level vision task, is developed rapidly by\nusing convolutional neural networks (CNNs). Training CNNs requires a large\namount of labeled data, but manually annotating data is difficult. For\nemancipating manpower, in recent years, some synthetic datasets are released.\nHowever, they are still different from real scenes, which causes that training\na model on the synthetic data (source domain) cannot achieve a good performance\non real urban scenes (target domain). In this paper, we propose a weakly\nsupervised adversarial domain adaptation to improve the segmentation\nperformance from synthetic data to real scenes, which consists of three deep\nneural networks. To be specific, a detection and segmentation (\"DS\" for short)\nmodel focuses on detecting objects and predicting segmentation map; a\npixel-level domain classifier (\"PDC\" for short) tries to distinguish the image\nfeatures from which domains; an object-level domain classifier (\"ODC\" for\nshort) discriminates the objects from which domains and predicts the objects\nclasses. PDC and ODC are treated as the discriminators, and DS is considered as\nthe generator. By adversarial learning, DS is supposed to learn\ndomain-invariant features. In experiments, our proposed method yields the new\nrecord of mIoU metric in the same problem.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-04-19T06:30:36Z",
    "updated": "2019-04-19T06:30:36Z",
    "doi": "10.1109/TIP.2019.2910667"
  },
  "2202.07603": {
    "id": "http://arxiv.org/abs/2202.07603v1",
    "title": "Fairness Indicators for Systematic Assessments of Visual Feature\n  Extractors",
    "authors": [
      "Priya Goyal",
      "Adriana Romero Soriano",
      "Caner Hazirbas",
      "Levent Sagun",
      "Nicolas Usunier"
    ],
    "abstract": "  Does everyone equally benefit from computer vision systems? Answers to this\nquestion become more and more important as computer vision systems are deployed\nat large scale, and can spark major concerns when they exhibit vast performance\ndiscrepancies between people from various demographic and social backgrounds.\nSystematic diagnosis of fairness, harms, and biases of computer vision systems\nis an important step towards building socially responsible systems. To initiate\nan effort towards standardized fairness audits, we propose three fairness\nindicators, which aim at quantifying harms and biases of visual systems. Our\nindicators use existing publicly available datasets collected for fairness\nevaluations, and focus on three main types of harms and bias identified in the\nliterature, namely harmful label associations, disparity in learned\nrepresentations of social and demographic traits, and biased performance on\ngeographically diverse images from across the world.We define precise\nexperimental protocols applicable to a wide range of computer vision models.\nThese indicators are part of an ever-evolving suite of fairness probes and are\nnot intended to be a substitute for a thorough analysis of the broader impact\nof the new computer vision technologies. Yet, we believe it is a necessary\nfirst step towards (1) facilitating the widespread adoption and mandate of the\nfairness assessments in computer vision research, and (2) tracking progress\ntowards building socially responsible models. To study the practical\neffectiveness and broad applicability of our proposed indicators to any visual\nsystem, we apply them to off-the-shelf models built using widely adopted model\ntraining paradigms which vary in their ability to whether they can predict\nlabels on a given image or only produce the embeddings. We also systematically\nstudy the effect of data domain and model size.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CY",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-02-15T17:45:33Z",
    "updated": "2022-02-15T17:45:33Z",
    "doi": null
  },
  "2207.04132": {
    "id": "http://arxiv.org/abs/2207.04132v2",
    "title": "Cross-Attention Transformer for Video Interpolation",
    "authors": [
      "Hannah Halin Kim",
      "Shuzhi Yu",
      "Shuai Yuan",
      "Carlo Tomasi"
    ],
    "abstract": "  We propose TAIN (Transformers and Attention for video INterpolation), a\nresidual neural network for video interpolation, which aims to interpolate an\nintermediate frame given two consecutive image frames around it. We first\npresent a novel vision transformer module, named Cross Similarity (CS), to\nglobally aggregate input image features with similar appearance as those of the\npredicted interpolated frame. These CS features are then used to refine the\ninterpolated prediction. To account for occlusions in the CS features, we\npropose an Image Attention (IA) module to allow the network to focus on CS\nfeatures from one frame over those of the other. TAIN outperforms existing\nmethods that do not require flow estimation and performs comparably to\nflow-based methods while being computationally efficient in terms of inference\ntime on Vimeo90k, UCF101, and SNU-FILM benchmarks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-08T21:38:54Z",
    "updated": "2022-12-02T02:48:37Z",
    "doi": null
  },
  "2106.00592": {
    "id": "http://arxiv.org/abs/2106.00592v2",
    "title": "Semi-Supervised Domain Generalization with Stochastic StyleMatch",
    "authors": [
      "Kaiyang Zhou",
      "Chen Change Loy",
      "Ziwei Liu"
    ],
    "abstract": "  Ideally, visual learning algorithms should be generalizable, for dealing with\nany unseen domain shift when deployed in a new target environment; and\ndata-efficient, for reducing development costs by using as little labels as\npossible. To this end, we study semi-supervised domain generalization (SSDG),\nwhich aims to learn a domain-generalizable model using multi-source,\npartially-labeled training data. We design two benchmarks that cover\nstate-of-the-art methods developed in two related fields, i.e., domain\ngeneralization (DG) and semi-supervised learning (SSL). We find that the DG\nmethods, which by design are unable to handle unlabeled data, perform poorly\nwith limited labels in SSDG; the SSL methods, especially FixMatch, obtain much\nbetter results but are still far away from the basic vanilla model trained\nusing full labels. We propose StyleMatch, a simple approach that extends\nFixMatch with a couple of new ingredients tailored for SSDG: 1) stochastic\nmodeling for reducing overfitting in scarce labels, and 2) multi-view\nconsistency learning for enhancing domain generalization. Despite the concise\ndesigns, StyleMatch achieves significant improvements in SSDG. We hope our\napproach and the comprehensive benchmarks can pave the way for future research\non generalizable and data-efficient learning systems. The source code is\nreleased at \\url{https://github.com/KaiyangZhou/ssdg-benchmark}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-01T16:00:08Z",
    "updated": "2021-12-16T01:05:48Z",
    "doi": null
  },
  "2006.15437": {
    "id": "http://arxiv.org/abs/2006.15437v1",
    "title": "GPT-GNN: Generative Pre-Training of Graph Neural Networks",
    "authors": [
      "Ziniu Hu",
      "Yuxiao Dong",
      "Kuansan Wang",
      "Kai-Wei Chang",
      "Yizhou Sun"
    ],
    "abstract": "  Graph neural networks (GNNs) have been demonstrated to be powerful in\nmodeling graph-structured data. However, training GNNs usually requires\nabundant task-specific labeled data, which is often arduously expensive to\nobtain. One effective way to reduce the labeling effort is to pre-train an\nexpressive GNN model on unlabeled data with self-supervision and then transfer\nthe learned model to downstream tasks with only a few labels. In this paper, we\npresent the GPT-GNN framework to initialize GNNs by generative pre-training.\nGPT-GNN introduces a self-supervised attributed graph generation task to\npre-train a GNN so that it can capture the structural and semantic properties\nof the graph. We factorize the likelihood of the graph generation into two\ncomponents: 1) Attribute Generation and 2) Edge Generation. By modeling both\ncomponents, GPT-GNN captures the inherent dependency between node attributes\nand graph structure during the generative process. Comprehensive experiments on\nthe billion-scale Open Academic Graph and Amazon recommendation data\ndemonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models\nwithout pre-training by up to 9.1% across various downstream tasks.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-06-27T20:12:33Z",
    "updated": "2020-06-27T20:12:33Z",
    "doi": null
  },
  "2305.11337": {
    "id": "http://arxiv.org/abs/2305.11337v1",
    "title": "RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent\n  Geometry and Texture",
    "authors": [
      "Liangchen Song",
      "Liangliang Cao",
      "Hongyu Xu",
      "Kai Kang",
      "Feng Tang",
      "Junsong Yuan",
      "Yang Zhao"
    ],
    "abstract": "  The techniques for 3D indoor scene capturing are widely used, but the meshes\nproduced leave much to be desired. In this paper, we propose \"RoomDreamer\",\nwhich leverages powerful natural language to synthesize a new room with a\ndifferent style. Unlike existing image synthesis methods, our work addresses\nthe challenge of synthesizing both geometry and texture aligned to the input\nscene structure and prompt simultaneously. The key insight is that a scene\nshould be treated as a whole, taking into account both scene texture and\ngeometry. The proposed framework consists of two significant components:\nGeometry Guided Diffusion and Mesh Optimization. Geometry Guided Diffusion for\n3D Scene guarantees the consistency of the scene style by applying the 2D prior\nto the entire scene simultaneously. Mesh Optimization improves the geometry and\ntexture jointly and eliminates the artifacts in the scanned scene. To validate\nthe proposed method, real indoor scenes scanned with smartphones are used for\nextensive experiments, through which the effectiveness of our method is\ndemonstrated.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-18T22:57:57Z",
    "updated": "2023-05-18T22:57:57Z",
    "doi": null
  },
  "2209.05773": {
    "id": "http://arxiv.org/abs/2209.05773v1",
    "title": "CAIBC: Capturing All-round Information Beyond Color for Text-based\n  Person Retrieval",
    "authors": [
      "Zijie Wang",
      "Aichun Zhu",
      "Jingyi Xue",
      "Xili Wan",
      "Chao Liu",
      "Tian Wang",
      "Yifeng Li"
    ],
    "abstract": "  Given a natural language description, text-based person retrieval aims to\nidentify images of a target person from a large-scale person image database.\nExisting methods generally face a \\textbf{color over-reliance problem}, which\nmeans that the models rely heavily on color information when matching\ncross-modal data. Indeed, color information is an important decision-making\naccordance for retrieval, but the over-reliance on color would distract the\nmodel from other key clues (e.g. texture information, structural information,\netc.), and thereby lead to a sub-optimal retrieval performance. To solve this\nproblem, in this paper, we propose to \\textbf{C}apture \\textbf{A}ll-round\n\\textbf{I}nformation \\textbf{B}eyond \\textbf{C}olor (\\textbf{CAIBC}) via a\njointly optimized multi-branch architecture for text-based person retrieval.\nCAIBC contains three branches including an RGB branch, a grayscale (GRS) branch\nand a color (CLR) branch. Besides, with the aim of making full use of all-round\ninformation in a balanced and effective way, a mutual learning mechanism is\nemployed to enable the three branches which attend to varied aspects of\ninformation to communicate with and learn from each other. Extensive\nexperimental analysis is carried out to evaluate our proposed CAIBC method on\nthe CUHK-PEDES and RSTPReid datasets in both \\textbf{supervised} and\n\\textbf{weakly supervised} text-based person retrieval settings, which\ndemonstrates that CAIBC significantly outperforms existing methods and achieves\nthe state-of-the-art performance on all the three tasks.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.MM",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-13T07:10:58Z",
    "updated": "2022-09-13T07:10:58Z",
    "doi": "10.1145/3503161.3548057"
  },
  "2305.18670": {
    "id": "http://arxiv.org/abs/2305.18670v2",
    "title": "SAVE: Spectral-Shift-Aware Adaptation of Image Diffusion Models for\n  Text-driven Video Editing",
    "authors": [
      "Nazmul Karim",
      "Umar Khalid",
      "Mohsen Joneidi",
      "Chen Chen",
      "Nazanin Rahnavard"
    ],
    "abstract": "  Text-to-Image (T2I) diffusion models have achieved remarkable success in\nsynthesizing high-quality images conditioned on text prompts. Recent methods\nhave tried to replicate the success by either training text-to-video (T2V)\nmodels on a very large number of text-video pairs or adapting T2I models on\ntext-video pairs independently. Although the latter is computationally less\nexpensive, it still takes a significant amount of time for per-video adaption.\nTo address this issue, we propose SAVE, a novel spectral-shift-aware adaptation\nframework, in which we fine-tune the spectral shift of the parameter space\ninstead of the parameters themselves. Specifically, we take the spectral\ndecomposition of the pre-trained T2I weights and only update the singular\nvalues while freezing the corresponding singular vectors. In addition, we\nintroduce a spectral shift regularizer aimed at placing tighter constraints on\nlarger singular values compared to smaller ones. This form of regularization\nenables the model to grasp finer details within the video that align with the\nprovided textual descriptions. We also offer theoretical justification for our\nproposed regularization technique. Since we are only dealing with spectral\nshifts, the proposed method reduces the adaptation time significantly (approx.\n10 times) and has fewer resource constraints for training. Such attributes\nposit SAVE to be more suitable for real-world applications, e.g. editing\nundesirable content during video streaming. We validate the effectiveness of\nSAVE with an extensive experimental evaluation under different settings, e.g.\nstyle transfer, object replacement, privacy preservation, etc.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-05-30T01:00:31Z",
    "updated": "2023-12-01T05:37:12Z",
    "doi": null
  },
  "2210.09729": {
    "id": "http://arxiv.org/abs/2210.09729v1",
    "title": "HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes",
    "authors": [
      "Zan Wang",
      "Yixin Chen",
      "Tengyu Liu",
      "Yixin Zhu",
      "Wei Liang",
      "Siyuan Huang"
    ],
    "abstract": "  Learning to generate diverse scene-aware and goal-oriented human motions in\n3D scenes remains challenging due to the mediocre characteristics of the\nexisting datasets on Human-Scene Interaction (HSI); they only have limited\nscale/quality and lack semantics. To fill in the gap, we propose a large-scale\nand semantic-rich synthetic HSI dataset, denoted as HUMANISE, by aligning the\ncaptured human motion sequences with various 3D indoor scenes. We automatically\nannotate the aligned motions with language descriptions that depict the action\nand the unique interacting objects in the scene; e.g., sit on the armchair near\nthe desk. HUMANISE thus enables a new generation task, language-conditioned\nhuman motion generation in 3D scenes. The proposed task is challenging as it\nrequires joint modeling of the 3D scene, human motion, and natural language. To\ntackle this task, we present a novel scene-and-language conditioned generative\nmodel that can produce 3D human motions of the desirable action interacting\nwith the specified objects. Our experiments demonstrate that our model\ngenerates diverse and semantically consistent human motions in 3D scenes.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-18T10:14:11Z",
    "updated": "2022-10-18T10:14:11Z",
    "doi": null
  },
  "2205.05982": {
    "id": "http://arxiv.org/abs/2205.05982v1",
    "title": "Vectorized and performance-portable Quicksort",
    "authors": [
      "Mark Blacher",
      "Joachim Giesen",
      "Peter Sanders",
      "Jan Wassenberg"
    ],
    "abstract": "  Recent works showed that implementations of Quicksort using vector CPU\ninstructions can outperform the non-vectorized algorithms in widespread use.\nHowever, these implementations are typically single-threaded, implemented for a\nparticular instruction set, and restricted to a small set of key types. We lift\nthese three restrictions: our proposed 'vqsort' algorithm integrates into the\nstate-of-the-art parallel sorter 'ips4o', with a geometric mean speedup of\n1.59. The same implementation works on seven instruction sets (including SVE\nand RISC-V V) across four platforms. It also supports floating-point and 16-128\nbit integer keys. To the best of our knowledge, this is the fastest sort for\nnon-tuple keys on CPUs, up to 20 times as fast as the sorting algorithms\nimplemented in standard libraries. This paper focuses on the practical\nengineering aspects enabling the speed and portability, which we have not yet\nseen demonstrated for a Quicksort implementation. Furthermore, we introduce\ncompact and transpose-free sorting networks for in-register sorting of small\narrays, and a vector-friendly pivot sampling strategy that is robust against\nadversarial input.\n",
    "categories": [
      {
        "@term": "cs.IR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.DC",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "H.3; C.4; D.1.3",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-12T09:41:31Z",
    "updated": "2022-05-12T09:41:31Z",
    "doi": null
  },
  "2102.06171": {
    "id": "http://arxiv.org/abs/2102.06171v1",
    "title": "High-Performance Large-Scale Image Recognition Without Normalization",
    "authors": [
      "Andrew Brock",
      "Soham De",
      "Samuel L. Smith",
      "Karen Simonyan"
    ],
    "abstract": "  Batch normalization is a key component of most image classification models,\nbut it has many undesirable properties stemming from its dependence on the\nbatch size and interactions between examples. Although recent work has\nsucceeded in training deep ResNets without normalization layers, these models\ndo not match the test accuracies of the best batch-normalized networks, and are\noften unstable for large learning rates or strong data augmentations. In this\nwork, we develop an adaptive gradient clipping technique which overcomes these\ninstabilities, and design a significantly improved class of Normalizer-Free\nResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on\nImageNet while being up to 8.7x faster to train, and our largest models attain\na new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free\nmodels attain significantly better performance than their batch-normalized\ncounterparts when finetuning on ImageNet after large-scale pre-training on a\ndataset of 300 million labeled images, with our best models obtaining an\naccuracy of 89.2%. Our code is available at https://github.com/deepmind/\ndeepmind-research/tree/master/nfnets\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-02-11T18:23:20Z",
    "updated": "2021-02-11T18:23:20Z",
    "doi": null
  },
  "2007.08668": {
    "id": "http://arxiv.org/abs/2007.08668v4",
    "title": "BRP-NAS: Prediction-based NAS using GCNs",
    "authors": [
      "\u0141ukasz Dudziak",
      "Thomas Chau",
      "Mohamed S. Abdelfattah",
      "Royson Lee",
      "Hyeji Kim",
      "Nicholas D. Lane"
    ],
    "abstract": "  Neural architecture search (NAS) enables researchers to automatically explore\nbroad design spaces in order to improve efficiency of neural networks. This\nefficiency is especially important in the case of on-device deployment, where\nimprovements in accuracy should be balanced out with computational demands of a\nmodel. In practice, performance metrics of model are computationally expensive\nto obtain. Previous work uses a proxy (e.g., number of operations) or a\nlayer-wise measurement of neural network layers to estimate end-to-end hardware\nperformance but the imprecise prediction diminishes the quality of NAS. To\naddress this problem, we propose BRP-NAS, an efficient hardware-aware NAS\nenabled by an accurate performance predictor-based on graph convolutional\nnetwork (GCN). What is more, we investigate prediction quality on different\nmetrics and show that sample efficiency of the predictor-based NAS can be\nimproved by considering binary relations of models and an iterative data\nselection strategy. We show that our proposed method outperforms all prior\nmethods on NAS-Bench-101 and NAS-Bench-201, and that our predictor can\nconsistently learn to extract useful features from the DARTS search space,\nimproving upon the second-order baseline. Finally, to raise awareness of the\nfact that accurate latency estimation is not a trivial task, we release\nLatBench -- a latency dataset of NAS-Bench-201 models running on a broad range\nof devices.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "eess.SP",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-07-16T21:58:43Z",
    "updated": "2021-01-19T17:29:16Z",
    "doi": null
  },
  "1903.07291": {
    "id": "http://arxiv.org/abs/1903.07291v2",
    "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization",
    "authors": [
      "Taesung Park",
      "Ming-Yu Liu",
      "Ting-Chun Wang",
      "Jun-Yan Zhu"
    ],
    "abstract": "  We propose spatially-adaptive normalization, a simple but effective layer for\nsynthesizing photorealistic images given an input semantic layout. Previous\nmethods directly feed the semantic layout as input to the deep network, which\nis then processed through stacks of convolution, normalization, and\nnonlinearity layers. We show that this is suboptimal as the normalization\nlayers tend to ``wash away'' semantic information. To address the issue, we\npropose using the input layout for modulating the activations in normalization\nlayers through a spatially-adaptive, learned transformation. Experiments on\nseveral challenging datasets demonstrate the advantage of the proposed method\nover existing approaches, regarding both visual fidelity and alignment with\ninput layouts. Finally, our model allows user control over both semantic and\nstyle. Code is available at https://github.com/NVlabs/SPADE .\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "I.5; I.5.4; I.3.3",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-03-18T08:12:23Z",
    "updated": "2019-11-05T15:41:27Z",
    "doi": null
  },
  "2206.02780": {
    "id": "http://arxiv.org/abs/2206.02780v2",
    "title": "GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions",
    "authors": [
      "Gene Chou",
      "Ilya Chugunov",
      "Felix Heide"
    ],
    "abstract": "  We investigate the generalization capabilities of neural signed distance\nfunctions (SDFs) for learning 3D object representations for unseen and\nunlabeled point clouds. Existing methods can fit SDFs to a handful of object\nclasses and boast fine detail or fast inference speeds, but do not generalize\nwell to unseen shapes. We introduce a two-stage semi-supervised meta-learning\napproach that transfers shape priors from labeled to unlabeled data to\nreconstruct unseen object categories. The first stage uses an episodic training\nscheme to simulate training on unlabeled data and meta-learns initial shape\npriors. The second stage then introduces unlabeled data with disjoint classes\nin a semi-supervised scheme to diversify these priors and achieve\ngeneralization. We assess our method on both synthetic data and real collected\npoint clouds. Experimental results and analysis validate that our approach\noutperforms existing neural SDF methods and is capable of robust zero-shot\ninference on 100+ unseen classes. Code can be found at\nhttps://github.com/princeton-computational-imaging/gensdf.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.GR",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-06-06T17:58:29Z",
    "updated": "2022-10-05T02:09:09Z",
    "doi": null
  },
  "2210.02390": {
    "id": "http://arxiv.org/abs/2210.02390v3",
    "title": "Bayesian Prompt Learning for Image-Language Model Generalization",
    "authors": [
      "Mohammad Mahdi Derakhshani",
      "Enrique Sanchez",
      "Adrian Bulat",
      "Victor Guilherme Turrisi da Costa",
      "Cees G. M. Snoek",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ],
    "abstract": "  Foundational image-language models have generated considerable interest due\nto their efficient adaptation to downstream tasks by prompt learning. Prompt\nlearning treats part of the language model input as trainable while freezing\nthe rest, and optimizes an Empirical Risk Minimization objective. However,\nEmpirical Risk Minimization is known to suffer from distributional shifts which\nhurt generalizability to prompts unseen during training. By leveraging the\nregularization ability of Bayesian methods, we frame prompt learning from the\nBayesian perspective and formulate it as a variational inference problem. Our\napproach regularizes the prompt space, reduces overfitting to the seen prompts\nand improves the prompt generalization on unseen prompts. Our framework is\nimplemented by modeling the input prompt space in a probabilistic manner, as an\na priori distribution which makes our proposal compatible with prompt learning\napproaches that are unconditional or conditional on the image. We demonstrate\nempirically on 15 benchmarks that Bayesian prompt learning provides an\nappropriate coverage of the prompt space, prevents learning spurious features,\nand exploits transferable invariant features. This results in better\ngeneralization of unseen prompts, even across different datasets and domains.\nCode available at: https://github.com/saic-fi/Bayesian-Prompt-Learning\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-05T17:05:56Z",
    "updated": "2023-08-20T13:08:34Z",
    "doi": null
  },
  "1912.04216": {
    "id": "http://arxiv.org/abs/1912.04216v2",
    "title": "cGANs with Multi-Hinge Loss",
    "authors": [
      "Ilya Kavalerov",
      "Wojciech Czaja",
      "Rama Chellappa"
    ],
    "abstract": "  We propose a new algorithm to incorporate class conditional information into\nthe critic of GANs via a multi-class generalization of the commonly used Hinge\nloss that is compatible with both supervised and semi-supervised settings. We\nstudy the compromise between training a state of the art generator and an\naccurate classifier simultaneously, and propose a way to use our algorithm to\nmeasure the degree to which a generator and critic are class conditional. We\nshow the trade-off between a generator-critic pair respecting class\nconditioning inputs and generating the highest quality images. With our\nmulti-hinge loss modification we are able to improve Inception Scores and\nFrechet Inception Distance on the Imagenet dataset. We make our tensorflow code\navailable at https://github.com/ilyakava/gan.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "stat.ML",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2019-12-09T17:51:50Z",
    "updated": "2020-11-21T21:01:27Z",
    "doi": null
  },
  "2207.09763": {
    "id": "http://arxiv.org/abs/2207.09763v1",
    "title": "GIPSO: Geometrically Informed Propagation for Online Adaptation in 3D\n  LiDAR Segmentation",
    "authors": [
      "Cristiano Saltori",
      "Evgeny Krivosheev",
      "St\u00e9phane Lathuili\u00e8re",
      "Nicu Sebe",
      "Fabio Galasso",
      "Giuseppe Fiameni",
      "Elisa Ricci",
      "Fabio Poiesi"
    ],
    "abstract": "  3D point cloud semantic segmentation is fundamental for autonomous driving.\nMost approaches in the literature neglect an important aspect, i.e., how to\ndeal with domain shift when handling dynamic scenes. This can significantly\nhinder the navigation capabilities of self-driving vehicles. This paper\nadvances the state of the art in this research field. Our first contribution\nconsists in analysing a new unexplored scenario in point cloud segmentation,\nnamely Source-Free Online Unsupervised Domain Adaptation (SF-OUDA). We\nexperimentally show that state-of-the-art methods have a rather limited ability\nto adapt pre-trained deep network models to unseen domains in an online manner.\nOur second contribution is an approach that relies on adaptive self-training\nand geometric-feature propagation to adapt a pre-trained source model online\nwithout requiring either source data or target labels. Our third contribution\nis to study SF-OUDA in a challenging setup where source data is synthetic and\ntarget data is point clouds captured in the real world. We use the recent\nSynLiDAR dataset as a synthetic source and introduce two new synthetic (source)\ndatasets, which can stimulate future synthetic-to-real autonomous driving\nresearch. Our experiments show the effectiveness of our segmentation approach\non thousands of real-world point clouds. Code and synthetic datasets are\navailable at https://github.com/saltoricristiano/gipso-sfouda.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-07-20T09:06:07Z",
    "updated": "2022-07-20T09:06:07Z",
    "doi": null
  },
  "2312.02120": {
    "id": "http://arxiv.org/abs/2312.02120v2",
    "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
    "authors": [
      "Yuxiang Wei",
      "Zhe Wang",
      "Jiawei Liu",
      "Yifeng Ding",
      "Lingming Zhang"
    ],
    "abstract": "  We introduce Magicoder, a series of fully open-source (code, weights, and\ndata) Large Language Models (LLMs) for code that significantly closes the gap\nwith top code models while having no more than 7B parameters. Magicoder models\nare trained on 75K synthetic instruction data using OSS-Instruct, a novel\napproach to enlightening LLMs with open-source code snippets to generate\ndiverse instruction data for code. Our main motivation is to mitigate the\ninherent bias of the synthetic data generated by LLMs through the wealth of\nopen-source references for the production of more realistic and controllable\ndata. The orthogonality of OSS-Instruct and other data generation methods like\nEvol-Instruct further enables us to build an enhanced MagicoderS. Both\nMagicoder and MagicoderS substantially outperform state-of-the-art code models\nwith similar or even larger sizes on a wide range of coding benchmarks.\nNotably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent\nChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a\nnew direction for crafting diverse synthetic instruction data for code using\nabundant open-source references.\n",
    "categories": [
      {
        "@term": "cs.CL",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-12-04T18:50:35Z",
    "updated": "2024-06-07T02:50:56Z",
    "doi": null
  },
  "2408.00998": {
    "id": "http://arxiv.org/abs/2408.00998v2",
    "title": "FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation",
    "authors": [
      "Xiang Gao",
      "Jiaying Liu"
    ],
    "abstract": "  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2024-08-02T04:13:38Z",
    "updated": "2024-08-06T12:01:17Z",
    "doi": null
  },
  "2303.17968": {
    "id": "http://arxiv.org/abs/2303.17968v1",
    "title": "VDN-NeRF: Resolving Shape-Radiance Ambiguity via View-Dependence\n  Normalization",
    "authors": [
      "Bingfan Zhu",
      "Yanchao Yang",
      "Xulong Wang",
      "Youyi Zheng",
      "Leonidas Guibas"
    ],
    "abstract": "  We propose VDN-NeRF, a method to train neural radiance fields (NeRFs) for\nbetter geometry under non-Lambertian surface and dynamic lighting conditions\nthat cause significant variation in the radiance of a point when viewed from\ndifferent angles. Instead of explicitly modeling the underlying factors that\nresult in the view-dependent phenomenon, which could be complex yet not\ninclusive, we develop a simple and effective technique that normalizes the\nview-dependence by distilling invariant information already encoded in the\nlearned NeRFs. We then jointly train NeRFs for view synthesis with\nview-dependence normalization to attain quality geometry. Our experiments show\nthat even though shape-radiance ambiguity is inevitable, the proposed\nnormalization can minimize its effect on geometry, which essentially aligns the\noptimal capacity needed for explaining view-dependent variations. Our method\napplies to various baselines and significantly improves geometry without\nchanging the volume rendering pipeline, even if the data is captured under a\nmoving light source. Code is available at: https://github.com/BoifZ/VDN-NeRF.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-31T11:13:17Z",
    "updated": "2023-03-31T11:13:17Z",
    "doi": null
  },
  "2210.01069": {
    "id": "http://arxiv.org/abs/2210.01069v1",
    "title": "Dual-former: Hybrid Self-attention Transformer for Efficient Image\n  Restoration",
    "authors": [
      "Sixiang Chen",
      "Tian Ye",
      "Yun Liu",
      "Erkang Chen"
    ],
    "abstract": "  Recently, image restoration transformers have achieved comparable performance\nwith previous state-of-the-art CNNs. However, how to efficiently leverage such\narchitectures remains an open problem. In this work, we present Dual-former\nwhose critical insight is to combine the powerful global modeling ability of\nself-attention modules and the local modeling ability of convolutions in an\noverall architecture. With convolution-based Local Feature Extraction modules\nequipped in the encoder and the decoder, we only adopt a novel Hybrid\nTransformer Block in the latent layer to model the long-distance dependence in\nspatial dimensions and handle the uneven distribution between channels. Such a\ndesign eliminates the substantial computational complexity in previous image\nrestoration transformers and achieves superior performance on multiple image\nrestoration tasks. Experiments demonstrate that Dual-former achieves a 1.91dB\ngain over the state-of-the-art MAXIM method on the Indoor dataset for single\nimage dehazing while consuming only 4.2% GFLOPs as MAXIM. For single image\nderaining, it exceeds the SOTA method by 0.1dB PSNR on the average results of\nfive datasets with only 21.5% GFLOPs. Dual-former also substantially surpasses\nthe latest desnowing method on various datasets, with fewer parameters.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-10-03T16:39:21Z",
    "updated": "2022-10-03T16:39:21Z",
    "doi": null
  },
  "2203.02358": {
    "id": "http://arxiv.org/abs/2203.02358v1",
    "title": "ViT-P: Rethinking Data-efficient Vision Transformers from Locality",
    "authors": [
      "Bin Chen",
      "Ran Wang",
      "Di Ming",
      "Xin Feng"
    ],
    "abstract": "  Recent advances of Transformers have brought new trust to computer vision\ntasks. However, on small dataset, Transformers is hard to train and has lower\nperformance than convolutional neural networks. We make vision transformers as\ndata-efficient as convolutional neural networks by introducing multi-focal\nattention bias. Inspired by the attention distance in a well-trained ViT, we\nconstrain the self-attention of ViT to have multi-scale localized receptive\nfield. The size of receptive field is adaptable during training so that optimal\nconfiguration can be learned. We provide empirical evidence that proper\nconstrain of receptive field can reduce the amount of training data for vision\ntransformers. On Cifar100, our ViT-P Base model achieves the state-of-the-art\naccuracy (83.16%) trained from scratch. We also perform analysis on ImageNet to\nshow our method does not lose accuracy on large data sets.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-03-04T14:49:48Z",
    "updated": "2022-03-04T14:49:48Z",
    "doi": null
  },
  "2205.03891": {
    "id": "http://arxiv.org/abs/2205.03891v1",
    "title": "Cross-lingual Adaptation for Recipe Retrieval with Mixup",
    "authors": [
      "Bin Zhu",
      "Chong-Wah Ngo",
      "Jingjing Chen",
      "Wing-Kwong Chan"
    ],
    "abstract": "  Cross-modal recipe retrieval has attracted research attention in recent\nyears, thanks to the availability of large-scale paired data for training.\nNevertheless, obtaining adequate recipe-image pairs covering the majority of\ncuisines for supervised learning is difficult if not impossible. By\ntransferring knowledge learnt from a data-rich cuisine to a data-scarce\ncuisine, domain adaptation sheds light on this practical problem. Nevertheless,\nexisting works assume recipes in source and target domains are mostly\noriginated from the same cuisine and written in the same language. This paper\nstudies unsupervised domain adaptation for image-to-recipe retrieval, where\nrecipes in source and target domains are in different languages. Moreover, only\nrecipes are available for training in the target domain. A novel recipe mixup\nmethod is proposed to learn transferable embedding features between the two\ndomains. Specifically, recipe mixup produces mixed recipes to form an\nintermediate domain by discretely exchanging the section(s) between source and\ntarget recipes. To bridge the domain gap, recipe mixup loss is proposed to\nenforce the intermediate domain to locate in the shortest geodesic path between\nsource and target domains in the recipe embedding space. By using Recipe 1M\ndataset as source domain (English) and Vireo-FoodTransfer dataset as target\ndomain (Chinese), empirical experiments verify the effectiveness of recipe\nmixup for cross-lingual adaptation in the context of image-to-recipe retrieval.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-05-08T15:04:39Z",
    "updated": "2022-05-08T15:04:39Z",
    "doi": null
  },
  "2004.05498": {
    "id": "http://arxiv.org/abs/2004.05498v1",
    "title": "FDA: Fourier Domain Adaptation for Semantic Segmentation",
    "authors": [
      "Yanchao Yang",
      "Stefano Soatto"
    ],
    "abstract": "  We describe a simple method for unsupervised domain adaptation, whereby the\ndiscrepancy between the source and target distributions is reduced by swapping\nthe low-frequency spectrum of one with the other. We illustrate the method in\nsemantic segmentation, where densely annotated images are aplenty in one domain\n(synthetic data), but difficult to obtain in another (real images). Current\nstate-of-the-art methods are complex, some requiring adversarial optimization\nto render the backbone of a neural network invariant to the discrete domain\nselection variable. Our method does not require any training to perform the\ndomain alignment, just a simple Fourier Transform and its inverse. Despite its\nsimplicity, it achieves state-of-the-art performance in the current benchmarks,\nwhen integrated into a relatively standard semantic segmentation model. Our\nresults indicate that even simple procedures can discount nuisance variability\nin the data that more sophisticated methods struggle to learn away.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2020-04-11T22:20:48Z",
    "updated": "2020-04-11T22:20:48Z",
    "doi": null
  },
  "1606.01540": {
    "id": "http://arxiv.org/abs/1606.01540v1",
    "title": "OpenAI Gym",
    "authors": [
      "Greg Brockman",
      "Vicki Cheung",
      "Ludwig Pettersson",
      "Jonas Schneider",
      "John Schulman",
      "Jie Tang",
      "Wojciech Zaremba"
    ],
    "abstract": "  OpenAI Gym is a toolkit for reinforcement learning research. It includes a\ngrowing collection of benchmark problems that expose a common interface, and a\nwebsite where people can share their results and compare the performance of\nalgorithms. This whitepaper discusses the components of OpenAI Gym and the\ndesign decisions that went into the software.\n",
    "categories": [
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2016-06-05T17:54:48Z",
    "updated": "2016-06-05T17:54:48Z",
    "doi": null
  },
  "2308.09932": {
    "id": "http://arxiv.org/abs/2308.09932v2",
    "title": "Unveiling Memorization in Code Models",
    "authors": [
      "Zhou Yang",
      "Zhipeng Zhao",
      "Chenyu Wang",
      "Jieke Shi",
      "Dongsun Kim",
      "DongGyun Han",
      "David Lo"
    ],
    "abstract": "  The availability of large-scale datasets, advanced architectures, and\npowerful computational resources have led to effective code models that\nautomate diverse software engineering activities. The datasets usually consist\nof billions of lines of code from both open-source and private repositories. A\ncode model memorizes and produces source code verbatim, which potentially\ncontains vulnerabilities, sensitive information, or code with strict licenses,\nleading to potential security and privacy issues. This paper investigates an\nimportant problem: to what extent do code models memorize their training data?\nWe conduct an empirical study to explore memorization in large pre-trained code\nmodels. Our study highlights that simply extracting 20,000 outputs (each having\n512 tokens) from a code model can produce over 40,125 code snippets that are\nmemorized from the training data. To provide a better understanding, we build a\ntaxonomy of memorized contents with 3 categories and 14 subcategories. The\nresults show that the prompts sent to the code models affect the distribution\nof memorized contents. We identify several key factors of memorization.\nSpecifically, given the same architecture, larger models suffer more from\nmemorization problems. A code model produces more memorization when it is\nallowed to generate longer outputs. We also find a strong positive correlation\nbetween the number of an output's occurrences in the training data and that in\nthe generated outputs, which indicates that a potential way to reduce\nmemorization is to remove duplicates in the training data. We then identify\neffective metrics that infer whether an output contains memorization\naccurately. We also make suggestions to deal with memorization.\n",
    "categories": [
      {
        "@term": "cs.SE",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-08-19T07:25:39Z",
    "updated": "2024-01-12T02:39:25Z",
    "doi": "10.1145/3597503.3639074"
  },
  "2303.11224": {
    "id": "http://arxiv.org/abs/2303.11224v1",
    "title": "Cascaded Latent Diffusion Models for High-Resolution Chest X-ray\n  Synthesis",
    "authors": [
      "Tobias Weber",
      "Michael Ingrisch",
      "Bernd Bischl",
      "David R\u00fcgamer"
    ],
    "abstract": "  While recent advances in large-scale foundational models show promising\nresults, their application to the medical domain has not yet been explored in\ndetail. In this paper, we progress into the realms of large-scale modeling in\nmedical synthesis by proposing Cheff - a foundational cascaded latent diffusion\nmodel, which generates highly-realistic chest radiographs providing\nstate-of-the-art quality on a 1-megapixel scale. We further propose MaCheX,\nwhich is a unified interface for public chest datasets and forms the largest\nopen collection of chest X-rays up to date. With Cheff conditioned on\nradiological reports, we further guide the synthesis process over text prompts\nand unveil the research area of report-to-chest-X-ray generation.\n",
    "categories": [
      {
        "@term": "eess.IV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.LG",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2023-03-20T16:00:20Z",
    "updated": "2023-03-20T16:00:20Z",
    "doi": null
  },
  "2209.10074": {
    "id": "http://arxiv.org/abs/2209.10074v1",
    "title": "PicT: A Slim Weakly Supervised Vision Transformer for Pavement Distress\n  Classification",
    "authors": [
      "Wenhao Tang",
      "Sheng Huang",
      "Xiaoxian Zhang",
      "Luwen Huangfu"
    ],
    "abstract": "  Automatic pavement distress classification facilitates improving the\nefficiency of pavement maintenance and reducing the cost of labor and\nresources. A recently influential branch of this task divides the pavement\nimage into patches and addresses these issues from the perspective of\nmulti-instance learning. However, these methods neglect the correlation between\npatches and suffer from a low efficiency in the model optimization and\ninference. Meanwhile, Swin Transformer is able to address both of these issues\nwith its unique strengths. Built upon Swin Transformer, we present a vision\nTransformer named \\textbf{P}avement \\textbf{I}mage \\textbf{C}lassification\n\\textbf{T}ransformer (\\textbf{PicT}) for pavement distress classification. In\norder to better exploit the discriminative information of pavement images at\nthe patch level, the \\textit{Patch Labeling Teacher} is proposed to leverage a\nteacher model to dynamically generate pseudo labels of patches from image\nlabels during each iteration, and guides the model to learn the discriminative\nfeatures of patches. The broad classification head of Swin Transformer may\ndilute the discriminative features of distressed patches in the feature\naggregation step due to the small distressed area ratio of the pavement image.\nTo overcome this drawback, we present a \\textit{Patch Refiner} to cluster\npatches into different groups and only select the highest distress-risk group\nto yield a slim head for the final image classification. We evaluate our method\non CQU-BPDD. Extensive results show that \\textbf{PicT} outperforms the\nsecond-best performed model by a large margin of $+2.4\\%$ in P@R on detection\ntask, $+3.9\\%$ in $F1$ on recognition task, and 1.8x throughput, while enjoying\n7x faster training speed using the same computing resources. Our codes and\nmodels have been released on\n\\href{https://github.com/DearCaat/PicT}{https://github.com/DearCaat/PicT}.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2022-09-21T02:33:49Z",
    "updated": "2022-09-21T02:33:49Z",
    "doi": null
  },
  "2106.03798": {
    "id": "http://arxiv.org/abs/2106.03798v4",
    "title": "DoubleField: Bridging the Neural Surface and Radiance Fields for\n  High-fidelity Human Reconstruction and Rendering",
    "authors": [
      "Ruizhi Shao",
      "Hongwen Zhang",
      "He Zhang",
      "Mingjia Chen",
      "Yanpei Cao",
      "Tao Yu",
      "Yebin Liu"
    ],
    "abstract": "  We introduce DoubleField, a novel framework combining the merits of both\nsurface field and radiance field for high-fidelity human reconstruction and\nrendering. Within DoubleField, the surface field and radiance field are\nassociated together by a shared feature embedding and a surface-guided sampling\nstrategy. Moreover, a view-to-view transformer is introduced to fuse multi-view\nfeatures and learn view-dependent features directly from high-resolution\ninputs. With the modeling power of DoubleField and the view-to-view\ntransformer, our method significantly improves the reconstruction quality of\nboth geometry and appearance, while supporting direct inference, scene-specific\nhigh-resolution finetuning, and fast rendering. The efficacy of DoubleField is\nvalidated by the quantitative evaluations on several datasets and the\nqualitative results in a real-world sparse multi-view system, showing its\nsuperior capability for high-quality human model reconstruction and\nphoto-realistic free-viewpoint human rendering. Data and source code will be\nmade public for the research purpose. Please refer to our project page:\nhttp://www.liuyebin.com/dbfield/dbfield.html.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-06-07T17:08:17Z",
    "updated": "2022-03-27T18:14:48Z",
    "doi": null
  },
  "2108.10840": {
    "id": "http://arxiv.org/abs/2108.10840v1",
    "title": "Meta Self-Learning for Multi-Source Domain Adaptation: A Benchmark",
    "authors": [
      "Shuhao Qiu",
      "Chuang Zhu",
      "Wenli Zhou"
    ],
    "abstract": "  In recent years, deep learning-based methods have shown promising results in\ncomputer vision area. However, a common deep learning model requires a large\namount of labeled data, which is labor-intensive to collect and label. What's\nmore, the model can be ruined due to the domain shift between training data and\ntesting data. Text recognition is a broadly studied field in computer vision\nand suffers from the same problems noted above due to the diversity of fonts\nand complicated backgrounds. In this paper, we focus on the text recognition\nproblem and mainly make three contributions toward these problems. First, we\ncollect a multi-source domain adaptation dataset for text recognition,\nincluding five different domains with over five million images, which is the\nfirst multi-domain text recognition dataset to our best knowledge. Secondly, we\npropose a new method called Meta Self-Learning, which combines the\nself-learning method with the meta-learning paradigm and achieves a better\nrecognition result under the scene of multi-domain adaptation. Thirdly,\nextensive experiments are conducted on the dataset to provide a benchmark and\nalso show the effectiveness of our method. The code of our work and dataset are\navailable soon at https://bupt-ai-cz.github.io/Meta-SelfLearning/.\n",
    "categories": [
      {
        "@term": "cs.CV",
        "@scheme": "http://arxiv.org/schemas/atom"
      },
      {
        "@term": "cs.AI",
        "@scheme": "http://arxiv.org/schemas/atom"
      }
    ],
    "published": "2021-08-24T17:07:34Z",
    "updated": "2021-08-24T17:07:34Z",
    "doi": null
  }
}